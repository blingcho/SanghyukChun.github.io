<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big-Data | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2015-03-25T02:12:38+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[BIG 2014 세션들]]></title>
    <link href="http://SanghyukChun.github.io/45/"/>
    <updated>2014-04-09T10:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/45</id>
    <content type="html"><![CDATA[<h5>Introduction</h5>


<p>BIG 2014에 대한 설명은 <a href="http://SanghyukChun.github.io/44">이전 글</a>을 참고하길 바란다. 이 글은 BIG 2014 일정 4월 7일 월요일과 4월 8일 화요일 세션들에 대해 내가 기록한 것들을 간략하게 정리한 글이다. 첫 날 세션 중에서 흥미로운 세션은 Chris Volinsky의 mobile data analysis에 대한 talk과 Visualization talk 중에서 Matrix factorization을 사용해 진행한 paper도 나름 흥미로웠다. 그 밖에 나머지 세션들은 그냥 저냥 별로 interest하거나 inspiring하는 세션은 많이 없었다. 2일차 세션 역시 나에게 흥미가 가는 세션은 코넬 대학교의 Machine learning을 전공하는 교수님이 했던 talk과 kaggle engineer talk 정도였다.</p>


<hr>


<h4>1일차</h4>


<h5>Keynote talk 1: Shaping Cities of the Future using Mobile Data - Chris Volinsky</h5>


<p>이 talk을 진행한 <a href="http://www2.research.att.com/~volinsky/">Chris Volinsky</a>는 자그마치 Netflix prize의 winner 중 한 명이라고 한다. Machine Learning의 성공으로 꼽히는 대표적인 example 중 하나인 Netflix의 Algorithm을 만들어낸 사람의 talk을 듣게 될 줄이야. 내용도 흥미로운 내용이 많았다. 기본적인 아이디어는 우리가 살고 있는 장소, 더 구체적으로 말하자면 도시를 데이터를 기반으로 더 살기 좋은 장소로 만들자는 것이다. 다시 말해서 데이터 사이언스로 private한 data를 분석해 public good으로 만드려는 것이다. 이런 것의 예로 Netflix의 recommend system을 들었다. 즉, 그 자체로는 큰 의미가 없거나 private한 데이터들을 모아 big picture를 그리는 것이다. 이런 아이디어로 데이터를 공개하는 도시들이 많아지고 있는데, 뉴욕 시카고 텍사스 오스틴 등이 있다고 한다. 그리고 이런 데이터는 대부분 <a href="http://www.data.gov/">http://www.data.gov/</a>에서 확인가능하다고 한다. 나도 아직 제대로 본 적은 없는데 꽤 좋은 자료가 많은 듯. 실제로 이런 데이터를 사용해서 시카고의 범죄 데이터와 다른 데이터 간의 연관성 (뉴욕도 이런 실험을 했었다), 데이터 visuallization 등을 구축하고, 위에서 언급한 도시들은 API도 제공하고 portal도 제공한다고 한다. 이런 아이디어를 통해 더 나은 도시를 만들고자 하는 것이다. 이런 예는 정말정말 많다. 위에서 언급한 범죄 정보도 있고, street bump (문제가 있는 street), traffic, 날씨 등등.. 이런 접근을 통해 도시는 더 효율적이 되고 더 나은 movement를 장려하고 더 적은 전기, 물, traffic을 사용한다고 한다. </p>


<p>사실 최근 이런 움직임이 가속화되는 가장 큰 이유는 모바일 데이터인데, 실제 이 talk의 연사는 AT&T lab 소속으로, 모바일 데이터를 다룰 일이 많다고 한다. 모바일 데이터 중 하나는 위치정보인데, 안타깝게도 완전한 위치정보를 얻는 것은 불가능하다. GPS가 항상 켜져있는 것도 아니고 GPS가 항상 정보를 송신하는 것은 아니기 때문. 대신 서로 다른 전파탑과의 통신 기록이 남는데, 이 기록을 사용해 대략적인 위치를 추적하는 것이 가능하다고 한다.</p>


<p>Data access problem에는 다음과 같은 특성이 있는데 (1) No Content Ever, (2) Anonymize (always), (3) Aggregate (when possible), (4) Reduce granularity, (5) Principle of Least Privilege 가 그것이라고 한다. 아무튼 이 연사는 모바일 데이터를 사용해 사용자들의 움직임의 패턴을 분석해냈는데, 예를 들어 사람들이 아침 8시와 오후 6시에 각각 다른 장소 예를 들어 잁터와 집에 있을 것이다라는 가정을 하고 자도에 scatting을 해보면 실제 사람들의 traffic을 알 수 있다고 한다. 이때 쓰는 데이터는 사람들이 얼마나 많이 전화하고 문자를 하느냐 등의 정보로, 이를 사용하면 얼마나 많은 시간을 차 안에서 보내는지, 얼마나 많은 사람들이 대중교통을 쓰는지 등등을 알 수 있다고 한다. 실제로 이런 분석을 해보면 뉴욕보다 캘리포니아가 더 green하고 communication이 적다한다.</p>


<p>이 뿐 아니라 모바일 데이터를 응용하면 사람들의 움직임의 dynamics도 관측이 가능하다. 그렇다면 이런 질문이 가능한데, 만약 우리가 (통신사가) 데이터를 제공한다면 이를 얼마나 더 좋은 곳에 사용할 수 있을 것인가라는 궁금증이 생긴다. 실제 사람들의 문자와 전화 패턴만을 분석하여 (특정 사람의 정보가 아니라 특정 송신탑에 걸리는 network traffic을 분석한다) 사람들의 생활 양식을 알 수도 있고, 다음 버스가 언제 올 것이며 택시는 어디에 있을 것인가 등등을 inform하는 방식으로 우리 삶을 개선시킬 수 있다. 이런 예로 traffic의 흐름을 어떤 조건에 따라 예측할 수 있다면 우리의 삶은 크게 개선될 수 있다. 그래서 한 번 송신탑들이 받는 시간에 따른 데이터 시퀀스 정보를 사용해서 사람들이 움직이는 traffic을 그려봤는데, 대부분의 사람들이 중심에 살기는 하지만 엄청 멀리 사는 사람도 있고.. 하여간 엄청 복잡하단다. 그래서 이걸 supervised learning으로 learning하는데, label은 어떤 상황 (비가 오거나 주말, 주중, 낮, 밤 등등등) 에 데이터 시퀀스가 어떻게 변화할 것이냐를 learning하는 것이다. 이런 데이터 시퀀스로 traffic을 예상할 수 있기 때문이다. 즉, 우리가 알고싶은 정보를 기존의 데이터로 표현하고 기존의 데이터를 learning하는 것이다. 알고리듬은 simple nearest neighbor를 사용했는데, metric을 <a href="http://en.wikipedia.org/wiki/Earth_mover's_distance">earth mover's distance</a>로 썼다고 한다. 이를 통해 learning해본 결과, 다양한 상황에 따라 어떻게 traffic data가 변화하는지를 learning할 수 있었고 실제 오차률도 작았다고 한다.</p>


<p>이 외에도 모바일 데이터를 (전화와 문자 사용 빈도) clustering한 결과 약 4개와 7개 cluster가 가장 optimal한 cluster인 것으로 나왔는데, 이런 clustering을 통해 요금제를 세분화하거나 마케팅을 세분화하는 등의 접근이 가능할 수 있다.</p>


<p>전반적으로 우리가 사용하기 힘들어보이는 데이터를 어떻게 의미있는 데이터로 만들어내느냐에 대한 얘기가 많았다. 매우 인상깊었다.</p>


<h5>Paper talk 1: Telling Commerce Stories Through Pictures, eBay Data Lab - eBay</h5>


<p>간단하게, eBay라는 엄청나게 거대한 big commerce data를 가진 업체가 자신들의 가정에 따라 데이터를 분석하고 이를 시각화하고 그에 대해 스토리를 풀어내는 talk이었다. 데이터 수집 및 분석 환경은 구글 페이스북 등 다른 인터넷 기업들과 별로 다를 것 없이 유사하고, 데이터 분석을 통해 5W1H (Who, What, When, Where, Why, How) storytelling을 이끌어내더라. 이때 문제라면 스케일이 너무 크기 때문에 이런 정보를 리얼 타임에 처리하고 이를 통해 실제 활용가능한 액션을 도출하는 것이 어렵다는 것. Visuallization을 하면 좋은 점이 이런 과정을 크게 줄일 수 있고, 비기술자도 쉽게 이해할 수 있다는 것이다. 그래서 간단한 분석으로 tax에 따른 seller와 buyer의 분포를 봤더니 tax가 싼 방향으로 시장이 형성된다. 예를 들어 CA는 밖으로 나가는 세금이 비싸서 대부분의 리테일러와 구매자가 CA 사람들이다. 이걸 Cross border로 확장할 수도 있고 global trading에도 쓸 수 있다고 한다.</p>


<p>Talk자체는 그냥 그랬고, 그냥 eBay에서 데이터 분석을 어떻게 쓰고 있는가 살펴볼 수 있는 talk이었다.</p>


<h5>Paper talk 2: Visual Analytics for interactive exploration of large-scale documents via nonnegative matrix factorization - 조지아텍</h5>


<p>이날 세션 중 두 번째로 흥미로웠다. <a href="http://www.cc.gatech.edu/~joyfull/" taget="new">Jaegul Choo</a>라는 분이 쓴 논문인데, Visuallization이라는 범주를 벗어나서, Matrix Factorization을 사용해 뭔가 Classification스럽게 사용했었다는 것, 그림이 굉장히 Deep Neural Network랑 유사하다는 점 두 가지가 흥미로웠다. 내용은 그닥 볼 것 없다. 그냥 예를 들어 갤탭과 아이패드 중 뭐가 나은가 보고 싶은데 리뷰가 각각 1300개 2000개가 있을 때 이걸 다 읽을 수는 없으니깐 데이터 마이닝을 적당히 하고 이를 시작화하면 의사결정에 도움이 된다, 그리고 이런 Visuallization을 nonnegative matrix factorization으로 풀어보겠다 라는 내용이다. NMF은 그냥 Topic Modeling으로만 사용한다는데, 내가 보기에는 단순한 Clustering으로 보였다. 즉, 이를 사용해 classification이나 clustering같은 general한 ML문제를 풀 수 있을 것 같다는 것이 나의 아이디어. 그리고 LDA보다 NMF가 엄청나게 빠르더라. Convergence도 빠르고 iteration도 적게 걸리고, 안정도 빨리 된다. (다 같은 얘기같지만..) 아무튼 이런 방법으로 visualization이 가능하다고 한다. <a href="http://www.cc.gatech.edu/~joyfull/resources/2014_big_vanmf.pdf">포스터</a>는 링크를 보면 되는데, 별거 없고 차라리 Visualization tool을 만든 <a href="http://www.cc.gatech.edu/~joyfull/resources/2013_tvcg_utopian.pdf" taget="new">논문</a>을 보는게 나은 것 같다. 제대로 읽어보지는 않았는데 NMF은 여기 나온다.</p>


<p>이 talk을 듣고 궁금해서 찾아봤는데 <a href="http://jmlr.org/proceedings/papers/v5/lee09a/lee09a.pdf" taget="new">EEG를 NMF로 Classification하는 논문</a>도 있더라. 여러모로 흥미로운 주제인 것 같다.</p>


<h5>Paper talk 3: mAnalytics: A Big Data Analytic Platform for Precision Marketing - China Mobile</h5>


<p>중국의 통신 기업 China Mobile이 어떻게 데이터를 분석하는가에 대한 내용인데.. 그냥 시스템이 어떻게 돌아가는지에 대한 내용이었다. Recommendation에 쓴다는 것 같은데 (mAnalytics의 m이 Marketing의 M) 내가 흥미를 가질만한 내용은 없었다.</p>


<h5>Invitation talk 1: Computational Education: A Big Data Opportunity? Electronic textbook, internet-based classes, new models of funding educations - MicroSoft</h5>


<p>MS의 엔지니어가 와서 했던 talk인데, 쉽게 생각해서 전자 textbook을 만들고 인터넷 베이스 클래스를 만들 때 기존에 존재하는 좋은 교육 시스템을 모아서 더 좋은 새로운 시스템을 만들자라는 내용이다. 그리고 그걸 데이터 기반으로 하는거지. '좋은' 시스템은 Algorithmically ML based로 분석하고 이를 모아서 일종의 앙상블처럼 취합하는 듯. 전반적으로 NLP의 내용이 많았다. 예를 들어 section의 난이도가 어떠냐를 분석하는건 syntatic complexity를 통해 결정하는데, 이건 완전 통짜 NLP.. 아무튼 이런식으로 good/bad를 labeling하고 구체적으로 probabilistic decision model을 만들어낸다고 한다 (이 경우는 good/bad binary class model). 이건 좀 졸아서 적은게 많이 없는데, 아무튼 Syntatic Complexity는 단어의 길이랑, 단어당 syllable의 개수, 문장 길이 등등으로 판별한다고 한다. 아무튼 결국 이렇게 새로운 textbook과 curriculum을 개발하는게 최종 목적인듯</p>


<p>Talk은 졸려서 많이 못들었는데, 일단 교육을 데이터로 접근한다는게 굉장히 신선했다.</p>


<h5>Paper talk 5: Scholarly Big Data-based Prescriptive Analytics System Enhancing Research Capability - KISTI</h5>


<p>text data (document) 분석하는 시스템 빌딩하는 것 같은데 발표 자료도 문제가 있고해서 뭔지 잘 모르겠더라. 시스템은 완성된 모양인데, 웹과 앱으로 deploy가 되어있다. 주소를 첨부한다. <a href="http://inscite-advisory.kisti.re.kr/search" taget="new">http://inscite-advisory.kisti.re.kr/search</a>, <a href="https://play.google.com/store/apps/details?id=net.xenix.inscite&hl=ko" taget="new">https://play.google.com/store/apps/details?id=net.xenix.inscite&hl=ko</a> 시스템은 어쨌거나 꽤 잘 만든 것 같다. UI도 그렇고 돌아가는 것도 그렇고..</p>


<p>추가: 웹에서 설명을 찾았다. 인사이트 어댑티브는 KISTI 소프트웨어연구센터. 컴퓨터 지능연구실에서 개발한 테크놀러지 인텔리전스 서비스입니다. 인사이트 어댑티브 서비스는 총 4개의 기술 심층 분석 서비스와 총3개의 기관(국가)심층 분석 서비스로 구성되며 최종적으로 기술 분석 보고서를 자동으로 생성하여 pdf 형태로 제공합니다. 인사이트 어댑티브 서비스는 논문, 특허, 웹의 다양한 정보를 기반으로 기술에 대한 심층적인 분석과 예측 결과를 제공할 뿐 아니라 사용자 의도를 지능적으로 인식하여 사용자에게 적응형, 맞춤형 서비스 또한 제공합니다.</p>


<h5>Paper talk 6: Building an Analytic Platform for The Web - Internet Memory</h5>


<p>데이터 분석용 시스템 논문이다. 기본 아이디어는 웹 데이터가 영구하지 않기 때문에 계속 보관해야하고, 또 엄청 크기때문에 분산 시스템으로 구축해야한다는 것이다. 그 이상은 잘 모르겠다. 내가 이해하기로는 이 talk은 web data가 시간이 지나면서 변하거나 없어지는 정보가 존재하는데 그 정보를 어떻게 잘 처리해서 그걸 잘 처리하는 시스템, 혹은 플랫폼을 만들었다는 것인거 같은데 talk은 영 별로더라. 아 그리고 preprocessing 얘기가 자꾸 나오는데 데이터를 처리하는 방법에 대해서도 다루는건가 잘 모르겠더라.</p>


<h5>Paper talk 7: Integration, Cross-Verification, Participation and Open Data: Opportunities and Challenges for Public Health</h5>


<p>Healthcare에 대한 talk이었는데, 정확히는 기억이 안나지만 노트해놓은 것을 보니 그냥 여러개의 데이터 소스를 섞어서 prediction을 하는 모양이다.</p>


<p>Challenges로는 new data sources integration & cross-correlation / citizens participation and data donors / open data가 있는데, 이것들을 cross-valdation, non-medical data sources for event-based surveillance 으로 해결한다고 한다.</p>


<hr>


<h4>2일차</h4>


<h5>Keynote talk: In-Memory Real-Time Big Data Processing: What It Takes to Innovate and Change Industry</h5>


<p>그냥 in-memory DB에 대한 talk이었다. 솔직히 이게 왜 여기에서 keynote talk으로 들어갔는지 이해가 안된다.</p>


<h5>Paper talk 1: A Cloud-based Framework for Evaluation on Big Data</h5>


<p>talk의 목표는 “Bring the algorithms to data, not data to algorithms" 인데, 그래서 정작 어떻게 하겠다는건지는 잘 모르겠더라. 그냥 데이터를 cloud로 저장하는 시스템을 만든 듯</p>


<h5>Paper talk 2: Metronome, Building Blocks for Data Products</h5>


<p>Dataset management system 논문이었다. 역시 딱히 흥미가 가지는 않았다.</p>


<h5>Invited talk: Big Data of the People, for the People: Understanding the Collective Wisdom of Users - Conell</h5>


<p>이 talk은 이날 talk 중에서 가장 흥미를 끄는 talk이었는데, 일단 발표자가 machine learning을 하는 사람이었어서 나랑 view point가 좀 맞는 편이었다.</p>


<p>이 talk의 motivation은 Human interaction data를 처리하는 것인데, 이게 무엇이냐 하면 그냥 사람이 interaction하면서 발생하는 data를 의미한다. 예를 들어 사람들의 클릭률 정보라거나 어느 페이지에 오래 있는지 등의 interation에서 발생하는 정보이다. 그런데 이런 정보의 문제가 무엇이냐 하면 내가 관측한 data가 실제 machine learning system에서 사용하는 training data와는 다르다는 것이다. 무슨 얘기냐하면, 사람들의 행동이 어떤 distribution을 따르는 것이 아니라 내가 준 상황 내에서 본인이 고를 수 있는 최선을 고르기 때문에 실제 general model의 training data로 사용할 수 없다는 것이다. 즉, 유저들의 decision process를 먼저 이해해야하는데, 이런 관점으로 바라보게 된다면 다음과 같은 새로운 접근 방법이 가능하다. Decision -> feedback -> learning algorithm. 무슨 얘기이냐 하면 사용자가 내린 결정에 대해 우리가 feedback을 주는 방식으로 learning algorithm을 만들 수 있다는 것이다.</p>


<p>간단한 예를 들어보자. 만약 우리가 두 개의 랭킹 function 중 하나를 선택해야하는 decision making problem이 있다고 하자. 대부분의 경우 real industry에서 하는 가장 합리적인 선택은 A/B test를 하는 것이다. Abandonment rate, reformulation rate, queries per session, click per query, click @1, max reciprocal rank, mean reciprocal rank, time to first click, time to last click 등의 정보들을 비교해 A와 B 중 어느 결정이 더 합리적인지를 밝혀내는 것이다. 그런데 <a href="ArXiv.org">ArXiv.org</a> 를 통해 case study를 해본 결과, 이런 여러가지 metric 중에서 그 어떤 metric도 expected order에 영향을 미치는 absolute metric이 없다는 결론이 나왔다고 한다. (이에 대해서 내 생각을 말해보자면, A/B test라는 것이 일종의 Maximum likelihood estimation 이기 때문에 발생하는 문제라고 생각한다. 우리가 봐야하는 정보는 엄청나게 많은데 매우 제한적인 정보만을 가지고 예측을 하기 때문에 정확하지 않은 결론으로 귀결되는 것이다.)</p>


<p>다시 말하지만 observed data와 training data는 다르다. Observed data는 user의 decision이고, 결국에 우리가 explicit feedback을 주면 해당 decision에 영향을 주게 된다. 즉, 이 decision 혹은 observed data는 training data와는 다르게 된다. 따라서 우리는 decision process를 개선할 수 있는 feedback function을 design해야하고, 우리가 machine learning으로 기여할 수 있는 부분은 이런 feedback function을 위한 learning algorithm을 만들고 feedback function을 개선시키는 것이다.</p>


<p>그래서 이 얘기를 하면서 Balanced interleaving라는 얘기가 나오는데 무슨 얘기인지 까먹었다. 아무튼 이런 문제를 dueling bandit problem으로 생각해 regret을 minimization시켜서 feedback function을 개선한다고 한다. 이때 retrieval function이 유한한 상황에서 dueling bandit로 인해 발생하는 reget은 theorically bounded된다고 한다. (그냥 쉽게 생각하면 이 알고리듬을 사용했을 때 기대되는 성능이 좋다는 의미이다)</p>


<p>그리고 또 하나는 coactive feedback model인데, unknown utility function algorithm/user interaction, relationship to other online learning models observe context x, learning algorithms presents y, user return y with utility function for different algorithms 라고 하며 이 과정이 일어날 때 마다 regret이 update 된다고 한다. 이런 feedback model에서는 interaction이 given x, feedback이 개선된 prediction y이며, 이 x와 y를 supervised learning으로 learning시킨다. language translate 등이 이런 방법으로 알고리듬을 개선시킨다고 한다.</p>


<p>이런 예로 발표자가 예전에 개발한 preference perceptron이라는 알고리듬을 소개하는데, 내용이 너무 빨리 지나가서 정확히 적지는 못하고 논문만 찾아봤는데 나중에 천천히 읽어봐야겠다.</p>


<p>결론적으로 이 talk에서 하고자하는 얘기를 정리해보면, 실제 service provider 입장에서 어떤 특정 decision을 내려야하는 경우가 많다. 예를 들어 search 알고리듬을 바꾸거나 하는 경우가 있는데, 어떤 algorithm을 선택해야할 것이냐, 혹은 바꾸는 것이 좋냐 나쁘냐를 결정해야하는 경우가 많이 있다. 그런데 이 decision making을 하는 과정에서 feedback function을 주고 이를 통해 decision을 개선해 decision의 질을 높인다. 약간 game theory 비슷한 느낌이었는데, 가장 적절한 feedback function을 고르겠다는 얘기도 조금 나온 것으로 보아 일종의 reinforcement learning이 아닐까 생각된다.</p>


<h5>Keynote talk: Evolution from Apache Hadoop to the Enterprise Data Hub: a new foundation for the Modern Information Architecture - Cloudera</h5>


<p><a href="http://www.cloudera.com/">cloudera</a>라는 기업의 product에 대한 설명이었다. 결국 이런저런 설명을 들어보니 <a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html</a> 요 product와 같은 설명이더라. 결론만 얘기하면, 지금 대부분의 시스템들이 저장 시스템 따로, Hadoop layer 따로, RDMBS layer 따로, 실제 application layer 따로, research layer 따로 BI 따로 진행하고 있는데 이 회사는 그 모든 것을 종합해주는 solution tool을 개발했다는 것이다.</p>


<p>그래서 talk 마지막에 스마트폰 예시를 들면서 이제 아무도 녹음기 따로 카메라 따로 전자 노트 따로 PDA 따로 저장장치 따로 안들고 다니고 스마트폰 하나만 들고 다니듯이 시스템도 나중에는 이런 종합 솔루션으로 통합될거라는 그런 talk이었다.</p>


<h5>Paper talk 3: BUbiNG: Massive Crawling for the Masses</h5>


<p>Open source crawler system 논문이었다. 내가 관심있는 주제는 아니었음</p>


<h5>Paper talk 4: Scalable Topic Change Detection in Social Posts</h5>


<p>노트가 잘 안되어있는걸보니 시스템 논문인 것 같다. 기본 아이디어는 소셜 데이터들이 마구 산개해있는것처럼 보여도 사실은 어떤 distibution을 가지고 있을 것이라는것, 그리고 변화 그 자체를 detection해서 시간에 따라 변하는 소셜 정보를 detect하자는 것. 그 정도였다.</p>


<h5>Invited talk: What do we learn from Kaggle machine learning competitions? - Kaggle</h5>


<p>가장 기대를 했던 talk인데, 스카이프 연결상태가 안좋아서 (온라인으로 talk을 했다) 내용도 잘 안들리고 PPT도 잘 안보였다. 하지만 그 중에서 기억나는 점을 꼽자면, 먼저 kaggle leader board 방식이 그냥 도입된 것이 아니라 상위 top player들의 performance를 높일 수 있는 optimal한 방법이라고 claim하는 것이었고, 그리고 실제 competition의 winner들의 algorithm들을 분석해서 얻은 결과였다. 크게 두 가지가 있었는데, 하나는 top 3 algorithm을 ansemble한 algorithm이 1등 algorithm보다 훨씬 좋았다는 것과 대부분의 top player들이 deep neural network based였다는 것. 그 두 가지가 꽤 흥미로운 결과였다고 할 수 있었다. 그만큼 neural network가 강력하다는 얘기이고, 또 하나는 지금까지 나온 그 어떤 모델들도 실제 현상을 잘 설명할 수 없다는 의미가 될테니까.</p>


<hr>


<p>나름 이틀 동안 들은 workshop이었는데, 뭐 그냥 그랬다. 재미있는 talk도 몇 개 있었고, 내가 전혀 관심없는 talk도 많았다. 특히 시스템 쪽이나 DB 쪽은 정말 재미가 없었다. 그래도 실제 real industry나 다른 연구자들이 어떤 focus로 데이터를 바라보고 있는지에 대해 알 수 있는 나름 의미있는 시간이었던 것 같다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BIG 2014 (2014/4/7 ~ 2014/4/8)]]></title>
    <link href="http://SanghyukChun.github.io/44/"/>
    <updated>2014-04-06T04:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/44</id>
    <content type="html"><![CDATA[<p>다음주 월요일부터 화요일 이틀간 <a href="http://big2014.org/">BIG 2014</a>라는 big data관련 workshop에 참여하게 되었다. 어떤 talk들이 주로 있는지 궁금해서 간략하게 프로그램을 훑어봤는데 이론적인 내용들보다는 실제 application level에서 일어나는 문제들이나 얘기들이 많이 있는 것 같다. 특히 시스템 쪽 얘기가 많이 나올 것 같은데 나는 이쪽 분야에 아직 어느 정도 관심이 있어서 들으면 재미있을 것 같다. 그리고 Visualization등과 관련되어 보이는 topic도 간간히 눈에 띄고, analysis에 대한 얘기도 눈에 띈다. 사실 새로운 이론적 깊이를 배우러간다기보다는 최근 real field에서 big data를 어떻게 생각하고 실제로 활용하고 있는지 보러간다는 것 자체가 나에게 큰 의미가 있을 것으로 기대된다. 무엇보다 나는 앞으로 이런 소위 말하는 빅데이터를 아이템 삼아서 벤처를 할 생각이니깐.. 특히 미국 쪽에서 어떤 움직임을 보이고 있는지 가서 잘 살펴보고 와야겠다. Invited speaker에 Kaggle engineer도 있고.. 관련 연구를 하는 교수님들도 있어서 이론적인 내용과 실제 apply하는 내용이 적절하게 잘 밸런스가 맞춰져 진행이 될 것 같다.</p>


<p>아마도 <a href="http://www2014.kr/">www2014</a>라는 코엑스에서 열리는 학회의 서브 프로그램인 것 같다. 서울에서 열린다는 점이 마음에 든다. 구글링해보니 작년 www2013은 브라질에서 한 모양이던데.. 아마도 인터넷 관련 학회인 것 같다. 논문도 내는 것 같고.. 일단 나와는 아주 상관이 있을 것 같지는 않다. 어쨌거나 코엑스에서 열려서 집에서 가기 좋다는건 참 괜찮은 것 같다.</p>


<p>해당 컨퍼런스의 프로그램은 아래와 같다. (프로그램은 홈페이지에서 따왔다.) <a href="http://ec2-50-112-76-239.us-west-2.compute.amazonaws.com/upload/big_agenda.pdf">PDF</a>로 받을 수도 있다.</p>




<h5>BIG 2014 Final Program</h5>


<table cellspacing="5" class="table table-bordered" style="width:600"><tbody><tr><td bgcolor="#4BACC6" colspan="2" class="white" style="text-align:center">
<strong>Monday, April 7th</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:15</strong>
</td>
<td bgcolor="#A5D5E2">
BIG'2014 Opening<strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:30</strong>
</td>
<td bgcolor="#D2EAF1">
<strong><u>Keynote Speaker</u></strong><strong>: </strong>Chris Volinsky, AVP AT&amp;T<br><strong>Shaping Cities of the Future using Mobile Data</strong><strong> </strong>
<ul><li>Introduced by Robin Chen</li>
</ul></td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>10:30</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Junlan Feng</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:00</strong>
</td>
<td bgcolor="#A5D5E2">
Neel Sundaresan and Jack Shen<br><strong>Visually:&nbsp; Telling Commerce Stories Through Pictures</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:25</strong>
</td>
<td bgcolor="#D2EAF1">
Jaegul Choo, Barry Drake and&nbsp;Haesun Park<br><strong>Visual Analytics for Interactive Exploration of Large-scale Document Data via Nonnegative Matrix Factorization</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:50</strong>
</td>
<td bgcolor="#A5D5E2">
Keyun Hu, Hongyan Yan and Junlan Feng<br><strong>mAnalytics: A Big Data Analytic Platform for Precision Marketing</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>12:15</strong>
</td>
<td bgcolor="#D2EAF1">
Frank Smadja<br><strong>The Big Data Challenges of Computational Market Research</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>12:40</strong>
</td>
<td bgcolor="#FBD4B4">
Lunch
</td>
</tr><tr><td align="right" bgcolor="#4BACC6" class="white">
<strong>13:45</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker:</u></strong> Rakesh Agrawal, Microsoft Technical Fellow<br><strong>Computational Education: A Big Data Opportunity?</strong><strong> </strong>
<ul><li>Introduced by Prabhakar Raghavan</li>
</ul></td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Frank Smadja</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:30</strong>
</td>
<td bgcolor="#D2EAF1">
Jinhyung Kim, Minhee Cho, Mikyoung Lee and Hanmin Jung<br><strong>Scholarly Big Data-based Prescriptive Analytics System Enhancing Research Capability</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:55</strong>
</td>
<td bgcolor="#A5D5E2">
Julien Masanes, Stanislav Barton and Philippe Rigaux<br><strong>Building an Analytic Platform for The Web</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>15:20</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Neel Sanduresan</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>15:50</strong>
</td>
<td bgcolor="#A5D5E2">
Patty Kostkova<br><strong>Integration, Cross-Verification, Participation and Open Data: Opportunities and Challenges for Public Health</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:15</strong>
</td>
<td bgcolor="#D2EAF1">
<strong>Panel:&nbsp;</strong><strong>Open Data: Holy Grail for Surveillance and Research - so what's the problem?</strong><br><strong>Moderator:</strong> Patty Kostkova<br><strong>Panelists:</strong> Philip Abdelmalik, Ciro Cattuto, Daniel Hulme and Hans Ossebaard
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>17:15</strong>
</td>
<td bgcolor="#FBD4B4">
End of technical program of Day 1
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>18:30</strong>
</td>
<td bgcolor="#A5D5E2">
<strong>BIG Dinner<em> Sponsored by AT&amp;T</em></strong><strong> </strong>
</td>
</tr></tbody></table>




<table cellspacing="5" class="table table-bordered"><tbody><tr><td bgcolor="#4BACC6" colspan="2" class="white" style="text-align:center">
<strong>Tuesday, April 8th</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:15</strong>
</td>
<td bgcolor="#A5D5E2">
Preview of today’s agenda<strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:30</strong>
</td>
<td bgcolor="#D2EAF1">
<strong><u>Keynote Speaker</u></strong><strong>: </strong>Prof. Sang Kyun Cha, Seoul National University<br><strong>In-Memory Real-Time Big Data Processing: What It Takes to Innovate and Change Industry</strong>
<ul><li>Introduced by Alessandro Panconesi</li>
</ul></td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>10:30</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Paolo Boldi</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:00</strong>
</td>
<td bgcolor="#A5D5E2">
Allan Hanbury,&nbsp;Georg Langs, Bjoern Menze and&nbsp;Henning Müller<br><strong>A Cloud-based Framework for Evaluation on Big Data</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:25</strong>
</td>
<td bgcolor="#D2EAF1">
Paul Ogilvie, Jonathan Traupman, Xiangrui Meng and Doris Xin<br><strong>Metronome: Building Blocks for Data Products</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:50</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker</u></strong><strong>: </strong>Prof. Thorsten Joachims, Cornell<br><strong>Big Data of the People, for the People: Understanding the Collective Wisdom of Users</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>12:35</strong>
</td>
<td bgcolor="#FBD4B4">
Lunch<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Andrei Broder</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>13:45</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Keynote Speaker</u></strong><strong>:</strong> Amr Awadallah, Cloudera CTO and Co-founder<br><strong>Evolution from Apache Hadoop to the Enterprise Data Hub: a new foundation for the Modern Information Architecture</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:45</strong>
</td>
<td bgcolor="#D2EAF1">
Sebastiano Vigna,&nbsp;Paolo Boldi, Andrea Marino and&nbsp;Massimo Santini<br><strong>BUbiNG: Massive Crawling for the Masses</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>15:10</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Ronny Lempl</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>15:40</strong>
</td>
<td style="background-color: rgb(210, 234, 241);">
Sofia Kleisarchaki,&nbsp;Vassilis Christophides, Sihem Amer-Yahia and<br>Ahlame Douzal-Chouakria<br><strong>Scalable Topic Change Detection in Social Posts</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:05</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker</u></strong><strong>:</strong> Ben Hamner, Director of Engineering, Kaggle<br><strong>What do we learn from Kaggle machine learning competitions?</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:50</strong>
</td>
<td bgcolor="#D2EAF1">
BIG’2014 Closing<strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>17:00</strong>
</td>
<td bgcolor="#FBD4B4">
End of technical program of Day 2<strong> </strong>
</td>
</tr></tbody></table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[인터넷 속의 수학 - How does Netflix recommend movies? (2/2)]]></title>
    <link href="http://SanghyukChun.github.io/31/"/>
    <updated>2013-12-02T21:43:00+09:00</updated>
    <id>http://SanghyukChun.github.io/31</id>
    <content type="html"><![CDATA[<p>본 포스팅은 <a href="http://SanghyukChun.github.io/29">단기강좌 인터넷 속의 수학</a>의 강의 들을 요약하는 포스트입니다.</p>


<h5>Recall: Machine Learning</h5>


<p><a href="http://SanghyukChun.github.io/21">이전의</a> <a href="http://SanghyukChun.github.io/30">많은</a> <a href="http://SanghyukChun.github.io/blog/categories/machine-learning">포스트 들</a>에서도 설명했듯이 Machine Learning은 데이터를 통해 새로운 시스템을 만드는 것을 의미한다. 그렇다면 굳이 사람이 아니라 기계가 이런 일을 해야하는 이유가 있을까? 무엇보다 기계는 사람보다 단순 계산을 훨씬 빠르게 할 수 있다. 간단한 예를 하나 들어보자. 페르마 숫자라는 문제가 있다.</p>


<p>$$ {F_n} = 2^{2^n} +1 $$</p>


<p>이 숫자는 위와 같이 표현이 되는데, 페르마는 모든 n에 대해서 이 숫자가 소수라는 주장을 하였다. 그러나 100년 뒤 오일러가 이의 반례를 찾아냈다.</p>


<p>$$ {F_5} = 2^{2^5} + 1 = 2^{32} +1 = 4294967297 = 641 * 6700417 $$</p>


<p>사람이 이를 증명하는 데에 100년이라는 시간이 걸렸지만, 컴퓨터를 사용하면 이 문제는 고작 몇 분안에 끝나는 간단한 문제이다. 이런 문제에서 컴퓨터 혹은 기계를 사용하는 것이 매우 효율적인 것이다. 다시 Machine Learning으로 돌아가보자. Machine Learing algorithm은 주어진 training data에서 특정한 시스템을 만들고 각종 model parameter들을 optimize하여 주어진 training data에 가장 잘 들어맞는 system을 만든다. 이런 과정을 위해서는 이런 optimize problem이 reasonable한 시간 안에 풀 수 있는 문제인지 그렇지 않은 문제인지 반드시 알아야만 한다. 만약 한 문제를 optimize하는데에 엄청 오랜 시간.. 예를 들어서 몇십만년 단위의 시간이 걸린다면 실전에서 사용할 수 없을 것이다.</p>


<p>과연 컴퓨터로 풀 수 있는 문제란 무엇이 있을까? 컴퓨터는 Turing에 의해 1936년에 처음 제시가 되었고 (Turing Machine) 이 덕분에 지금까지 하드웨어 문제에 불과했던 성능에 관련된 문제가 수학적인 문제로 치환될 수 있었다. 또한 1971년 Computational classes (NP complete) 가 Cook에 의해 define되었다. 여기에서 정의된 P와 NP problem을 사용하면 우리가 처음 제시한 질문: 이 문제를 컴퓨터로 풀 수 있는가? 에 대한 질문에 답을 할 수 있는 것이다.</p>


<p>다음에 대한 설명을 하기 전에 먼저 P와 NP problem에 대해 잠시 설명하도록 하겠다. 먼저 P는 금방 문제의 정답을 찾을 수 있는 문제이다. 또한 NP는 해답이 있을 때 이 해답이 맞는지 아닌지 verify할 수 있는 문제를 뜻한다. 예를 들어 어떤 주어진 여러 개의 Path 중에서 특정한 path를 찾는 문제는 P problem이다. 또한 NP problem은 path가 있을 때 그 path를 따라갈 수 있는가에 대한 문제가 되는 것이다. 이 두 개의 문제에 해당하지 않는 문제도 엄청나게 많으며, 재미삼아 말해주자면, P이면 NP인가? 라는 질문은 Seven Millennium Prize Problems 중 하나일 정도로 수학에서 상당히 중요한 영역을 차지하고 있다.</p>


<p>P problem의 대표적인 예는 Convex Optimization이다. Convex Optimization은 mimimum value를 찾는 문제 중에서 매우 특수한 경우를 의미하며, 함수가 convex하고 domain 역시 convex한 경우를 의미한다. 간단하게 생각하면 convex와 '볼록하다' 가 같은 말이며, convex function이란 모든 구간에서 볼록한 함수를 의미한다. (Convex Optimization에 대해서는 나중에 더 자세한 포스팅으로 설명을 할 수 있도록 하겠다.) 간단히 예를 들어보면</p>


<p><img class="<a" src="href="http://people.mech.kuleuven.be/~bdemeule/pics/convex.jpg">http://people.mech.kuleuven.be/~bdemeule/pics/convex.jpg</a>" width="400"></p>

<p>위의 그림에서 왼쪽 함수는 일부 구간에서 볼록하지 않기 때문에 convex하지 않고 오른쪽의 함수는 convex하다. 위의 그림을 보면 알 수 있듯, convex function에서는 local한 minimum value만 찾더라도 global한 mimimum값을 찾을 수 있다. 때문에 Convex optimization은 optimization 중에서도 매우 특수한 경우이며 P, NP problem 중에서 P에 속하는 문제이다. 이를 수식적으로 표현해보면</p>


<p>
$${minimize}\quad{f(x)}$$
$${subject}\,{to}\,{x} \in D \subseteq {R^n}$$
</p>


<p>으로 표현하는 것이 가능하다. Netflix 알고리듬에서 언급하게 될 3개의 알고리듬 중에서 Baseline predictor와 Matrix factorization 알고리듬에서 이런 Convex Optimization을 활용하게 된다.</p>


<h5>Recall: Netflix Recommendation Problem</h5>


<p>Netflix problem의 목적은 간단하다. Netflix Matrix라는 user와 movie의 조합으로 이루어진 Matrix에서 아직 알려지지 않은 부분의 값을 유추하는 것이다. 이 문제에 대한 설명은 지난번에 적은 글에 자세히 적혀있으니 생략하도록 하겠다. 그렇다면, 새로운 알고리듬이 더 좋은 알고리듬인지 아닌지 어떻게 판단할 수 있을까? 여러가지 방법이 있을 수 있지만, Netflix에서는 RMSE (Root Mean Squared Error) 를 정의한다. RMSE는 \(\sqrt{MSE} = \sqrt{\frac 1 n \sum_{i=1}^n ( \hat{X_i}-X_i )^2}\)로 표현이 가능하며, 쉽게 생각하면 예측치가 실제 값과 얼마나 차이가 나는지를 측정하는 역할을 한다고 생각하면 간단하다. 즉, Netflix의 Recommendation problem은 Netflix Matrix에서 알려져 있는 entry를 사용해 training set과 problem set을 만들고 RMSE를 계산해서 그 RMSE를 최대한 낮추는 문제인 것이다. 이 글에서는 이런 RMSE의 값을 10% 줄이기 위한 3가지 알고리듬: Baseline Predictor, Neighborhood method, Matrix Factorization에 대해 다루게 될 것이다.</p>


<h5>Algorithm 1: Baseline Predictor</h5>


<p>첫 번째 알고리듬은 Baseline Predictor이다. 이 알고리듬은 각각의 영화 혹은 사람마다 기본적으로 정해진 Baseline이 존재한다는 가정에서부터 시작된다. 즉, 각각 영화마다 평점이 높은 영화가 있을 수도 있으며. 또 평점을 잘 주는 사람이 있을 수도 있고 짜게 주는 사람도 있을 수 있다. 또한 비교적 popular 한 영화라면 rating이 높을 것이고, 이 사람이 이전에 준 rating의 값의 평균이 낮다면 앞으로 줄 rating의 값 또한 작을 것이라는 가설을 세울 수 있을 것이다. 그렇다면 이런 baseline을 사람에 대한 혹은 영화에 대해서 각각 만들 수 있을 것이며 이를 모으면 vector로 표현하는 것이 가능할 것이다. \(b_i\)를 movie에 대한 baseline, \(b_u\)를 user에 대한 baseline이라고 가정하고, 이 baseline이 높으면 rating을 잘 받는 영화 / 잘 주는 사람 이라고 생각하자. 그렇다면
$$\hat r_{ui} = {\overline r} + b_u + b_i$$
로 정의한다면, baseline을 찾는 문제는
$${minimize}\,\sum {(r_{ui} - \hat r_{ui})^2} $$
을 만족하는 \(b_u\)와 \(b_i\)를 찾는 문제로 바꿀 수 있다. 그리고 여기에서 가장 중요한 점은 이것이다. 이 문제는 Convex optimization으로 풀 수 있다는 것이다.</p>


<p>Baseline Predictor는 기존의 데이터를 가장 잘 설명할 수 있는 model parameter를 찾는 문제이며 성능이 아주 썩 좋은 편은 아니지만 random guessing보다는 훨씬 좋으며 어느 정도의 가중치를 줄 수 있다는 장점이 존재한다. 특히 temporal model과 결합하여 baseline predictor를 사용하면 꽤 강력한 결과를 얻을 수 있는데, Baseline Predictor with Temporal Models는 User의 rating은 day에 dependent할 수 있다는 가정을 깔고 movie의 trend가 시간에 따라 변한다고 가정한다. 그리고 이에 대한 적절한 변수를 시간마다 주고 \(b_u(t),\,b_i(t)\)를 가장 잘 설명할 수 있는 baseline의 값을 찾음으로써 시간에 대한 정보까지 고려할 수 있는 알고리듬을 설계하는 것이 가능한 것이다.</p>


<p>그러나, 기본적으로 parameter를 fitting하는 문제이기 때문에 Overfitting problem이 발생할 수 있다. Overfitting problem이란 현재 parameter들이 training data에 너무 optimization되어 오히려 future data에 대해서는 값이 제대로 맞지 않는 경우를 의미한다. 이는 전체 데이터가 아닌 일부의 데이터만 봤기에 생길 수도 있는 문제이며 data에 noise가 끼어 noise까지 fitting이 되었었을 수도 있다. 아무튼 overfitting problem은 현재에 너무 과도하게 집중하면 미래 data를 설명하는 데에 문제가 생길 수 있다는 것을 의미한다. Baseline Predictor에서 Model parameter를 너무 optimize시키면 지금까지의 known data에는 정말 잘 맞지만, test data에서는 error가 엄청 커질 수도 있는 것이다. 이를 막기 위해서 위에서 제시했던 minimzation problem을
$${minimize}\,\sum {(r_{ui} - \hat r_{ui})^2 + \lambda (\sum_u {b_u}^2 + \sum_i {b_i}^2)} $$
처럼 \(\lambda\)와 관련된 추가적인 term을 추가한 다음 풀게 된다면, overfitting문제가 어느 정도 해결된다. 여기에서 overfitting을 막기 위해 사용한 \(\lambda\)가 증가하게 되면 점점 test data error가 떨어지다가 어느 정도 지나면 test data error가 다시 increase 된다. 따라서 적절한 \(\lambda\)를 선택하는 것도 매우 중요하다는 것을 알 수 있다.</p>


<h5>Algorithm 2: Neighborhood Method</h5>


<p>지난 포스트에서도 설명했던 것 처럼 이 알고리듬에서는 각각의 movie마다 movie 간의 유사도 정보를 가지고 있다고 가정하고 각각의 movie i와 j마다 \(d_{ij}\)라는 distance term을 정의하여 그 distance를 통해 얼마나 유사한지를 판별하게 된다. 즉 이 아이디어는 rating을 user가 영화 i를 좋아했으면 j도 좋아하지 않겠느냐.. 라는 idea를 기반으로 measure를 하게 된다. 이 알고리듬에서 distance function은
$$ d_{ij} = \frac{({r_i} * {r_j})}{(|r_i| * |r_j|)} $$
위와 같이 정의한다. 이 때 \(r_i\)와 \(r_j\)는 모든 user의 movie rating을 모아둔 vector이다. 즉, \(r_i = [2, 1, 3, 4, ...]\) 등으로 표현된다는 것이다. 이때 임의의 두 vector사이 unknown factor가 다를 수 있으므로 두 vector에서 모두 알고 있는 값들을 모아 reduced form을 구해서 이 값을 계산하게 된다고 한다. distance가 두 벡터의 내적을 2-norm으로 나눈 것으로 정의가 되기 때문에 \(d_{ij}\)는 두 vector 사이 angle에 cosine을 취한 값이 된다. 즉, 두 벡터가 가까우면 가까울 수록 1에 근접해지고 멀어질 수록 값이 작아지게 된다. 즉, 이렇게 거리를 정의함으로써 두 벡터 간의 유사성이 얼마나 되느냐를 측정하는 척도가 될 수 있는 것이다.</p>


<p>NH method는 이 알고리듬 자체만 사용하게 되었을 때 결과가 그닥 좋지는 못하다. 그러나 Baseline Predictor랑 같이 결합해서 사용할 수 있으며 Baseline predictor를 계산하고 알고 있는 값과의 error를 계산하고 이 에러 값을 사용해서 NM을 사용하면 훨씬 결과가 좋게 나오게 된다. 이렇게 사용하기 위해서는 \(\hat r_{ui} = \sum \frac {(d_{ij} * r_{ij})} {\sum (d_{ij})}\) 와 같은 형태로 r을 정의하고 predict를 하게 된다. 이 경우 영화의 개수가 많아질수록 연산량이 어마어마하게 늘어나기 때문에 이 알고리듬은 모든 영화에 대해 전부 다 적용하는 것이 아니라 top 50 movie 중에서 i와 similar한 movie를 일부 골라서 적용한다고 한다.</p>


<h5>Algorithm 3: Matrix Factorization</h5>


<p>만약 알려진 거대한 Matrix가 있을 때 이를 더 작은 Matrix의 multiplication으로 표현할 수 있다면 우리는 더 적은 값을 measure해서 전체 값을 추측할 수 있을 것이다. 이것이 Matrix Factorization의 기본 아이디어이며, 이 알고리듬은 성능이 매우 뛰어나서 다른 알고리즘 없이도 8% 정도까지 개선이 가능하다고 한다.</p>


<p><img src="/images/post/30-1.png" width="400"></p>

<p>우리의 문제에서 각각의 Matrix를 R, P, Q라고 정의하자. 그리고 P와 Q 각각의 row의 개수와 column의 개수를 k라고 하자. 그렇다면 R은 480000 by 18000, P는 48000 by k, Q는 k by 18000 Matrix일 것이며, R = PQ가 될 것이다. 당연히 k의 값이 클 수록 낮은 에러로 원래의 데이터를 복구하기 쉬워지겠지만, k가 커질수록 overfitting issue가 존재하게 될 것이다. 실제로 Netflix에서는 약 20정도의 k를 사용한다고 한다. 당연한 얘기지만 실제로는 P, Q가 존재하지 않을 수도 있다. 따라서 이 문제는 아래와 같이 치환이 가능하다.
$${minimize_{PQ}}\quad{|R-PQ|^2} = {minimize_{PQ}}\quad{(r_{ui} - p_u q_i)^2} $$
이 문제는 P인가? 불행히도 이 문제는 함수 \(f(P,Q)=|R-RQ|^2\) 자체가 convex가 아니기 때문에 Convex optimization problem이 아니며, P역시 아니다. 대신 이 문제를 convex optimization으로 근사하는 방법이 가능하다.</p>


<p>첫 번째 방법은 \(minimize |R - PQ|\) 를 \(minimize |R - A|^2 \hskip 1em where \hskip 0.3em rank(A) = k… \) 로 바꾸는 것이다. \(|R-A|^2\)은 convex function이기 때문에 convex optimization으로 푸는 것이 가능해 보인다. 그런데 domain인 rank(A) = k가 convex set이 아니기 때문에 이 문제는 불행히도 convex optimization은 아니다. 따라서 이를 가장 유사한 convex optimization problem으로 바꾸면, rank(A) = k라는 조건 대신에 'sum of singular values of A is at most h' 라는 조건으로 문제를 풀면 된다. 이는 정확히 같은 조건은 아니고 거의 유사한 조건이다. 이렇게 문제를 non convex optimization에서 convex optimization으로 근사해서 원래 문제의 답을 추측하는 것이 가능한 것이다.</p>


<p>또 하나의 방법은 \(minimize_{P,Q} |R-PQ|^2\) 을 푸는 것이다. 이 때 \(f(P,Q) = |R-PQ|^2\)은 convex function은 아니지만, P를 constant로 두면 Q에 대해 convex하고 Q를 constant로 두면 P에 대해 convex해지게 된다. 이를 bi convex라고 하며 둘 모두에 대해 convex하면 joint convex라고 한다. 아무튼 이제 이 방법 두 개를 모두 사용해서 Q를 고정하고 가장 잘 설명하는 P를 찾고, P를 고정하고 가장 잘 설명하는 Q를 찾는 과정을 반복적으로 왔다갔다 하면서 값을 찾는다. 이 방법을 이론적으로 분석하는 것이 엄청 어렵고 힘들어서 논문으로 많이 나오지는 않았지만 실전에서 엄청 많이쓰는 방법이다. 앞서 설명한 방법보다 이 방법이 더 성능도 잘 나온다. 최근 [Sujay et al. 2013] 에서 앞서 언급한 approach보다 이 approach가 좋은지는 모르겠지만 최소한 나쁘지 않다라는 것을 증명하였다고 한다. (구체적으로는 global optima convergence condition for R을 증명하였다고 한다.)</p>


<h5>Summary and Questions</h5>


<p>마지막으로 <a class="red tip" title="Neighborhood method">NH</a>와 <a class="red tip" title="Matrix factoriztion">MF</a>에 대해 잠시 비교해보자. NM은 local structure를 찾아서 recommendation problem을 풀겠다는 컨셉이고 MF는 global structure를 찾아서 recommendation problem을 풀겠다는 컨셉이다. 당연히 local한 solution보다 global한 structure를 찾는 컨셉이 더 정확할 것이다. 실제로 다른 알고리듬 하나도 없이 MF만 적용을 해봐도 Cinematch에 비해 8% 정도 improved 된 결과를 취할 수가 있게 된다. 하지만 역시 맨 처음 제시되었던 10%를 달성하려면 <a class="red tip" title="Baseline predictor with temporal models">BP</a>를 적용한 NH와 MF 둘을 잘 combine해야만 달성이 가능하다.</p>


<p>이런 알고리듬들에 대해서 몇 가지 Further Questions이 있을 수 있을 것이다.</p>


<ul>
<li> R = PQ를 풀기 위한 R의 entries 숫자는 얼마나 될 것인가</li>
<li> MF를 더 빠르게 design할 수 있겠느냐, 더 나은 다른 algorithm도 있을 수 있겠느냐..</li>
<li> NM과 MF를 같이 조합했을 때 왜 결과가 좋은 이유가 무엇이냐, 이론적인, mathematical answer 를 줄 수 있느냐</li>
</ul>


<p>등의 question 들이 있을 수 있으며 이와 관련된 많은 연구가 활발하게 진행되고 있다고 한다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[인터넷 속의 수학 - How does Netflix recommend movies? (1/2)]]></title>
    <link href="http://SanghyukChun.github.io/30/"/>
    <updated>2013-11-27T15:17:00+09:00</updated>
    <id>http://SanghyukChun.github.io/30</id>
    <content type="html"><![CDATA[<p>본 포스팅은 <a href="http://SanghyukChun.github.io/29">단기강좌 인터넷 속의 수학</a>의 강의 들을 요약하는 포스트입니다.</p>


<h5>Introduction</h5>


<p>Netflix라는 미국의 DVD rental 업체가 있다. <a href="http://SanghyukChun.github.io/21">이전 포스트</a>에서 다뤘던 기업 중에 하나인데, 다시 한번 간략하게 설명을 하자면 Netflix는 미국의 온라인 DVD rental 업체이다. 1997년 시작한 DVD rental business이며 초기 BM은 간단했다. 한달에 고정적인 비용을 내고 video나 dvd를 빌릴 수 있도록 하며 번거로운 연체료가 없는 모델이었다. 이렇게 하면 return률이 감소하는 단점이 있는데 이런 단점을 새로운 dvd를 빌리려면 다시 return해야 빌릴 수 있는 rule을 만들어 크게 성공하였다. <a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788963708041&orderClick=LAH&Kc">디멘드</a>를 읽어보면 이에 대해서는 자세히 알 수 있을 것이다. 이후 2007년에 <a class="red tip" title="internet 상에서 VOD감상이 가능하도록 하는 서비스">VOD(video on demand)</a> service를 시작하였고, 2008년 9 million이던 user가 VOD 이후 30 million으로 폭증하였다. 현재 미국 traffic 25%는 Netflix VOD 때문에 발생할 정도로 거대한 기업이 되었다. 이렇게 Netflix가 크게 성장하게 된 배경에는 Recommendation system이 존재하는데, Netflix의 추천 시스템은 User prior video history를 기반으로 새 영화를 추천하고 사용자들이 더 다양하고 많은 비디오를 빌려볼 수 있도록 유도하고 있다. 이런 추천시스템은 Amazon, Youtube, GeoLife in MS 등도 적용하고 있는 많은 기업들에게 중요하게 인식되고 있는 시스템이다.</p>


<p>잠시 본 글로 넘어가기 이전에 Recommendation, 혹은 추천이라는 문제에 대해서 잠시 생각해보고 넘어가보자. 세상에는 정말 많은 Recommendation problem이 존재한다. 정말 간단한 현실 속의 예를 들어보자면 소개팅을 예를 들 수 있다. 소개팅을 주선해 줄 때 어떤 상대를 소개시켜주는 것이 가장 적절할까? 가만 생각해보면 소개팅을 상대방과 잘 맞을 것으로 예상되는 사람을 '추천' 해주는 문제로 변경해서 해결해 볼 수 있다. 예를 들어서 내가 소개팅을 시켜주려는 상대가 이전에 A라는 타입을 좋아했었다면 이번 소개팅에서도 A 타입을 추천해주는 방식으로 문제 해결이 가능한 것이다. 이런 것이 일종의 recommend question이다.</p>


<p>스포츠를 좋아하는 사람들을 위해 다른 예시를 들어보자면, 야구에서도 추천 문제로 생각할 수 있는 경우가 존재한다. 예를 들어서 현재 대타를 내세워야하는 상황이라고 생각해보자. 감독이 이런 중요한 순간에 A, B, C 타자 중 어떤 타자로 교체할지 decision making을 하는 것도 일종의 recommend question으로 생각이 가능하다. 예를 들어서 과거 대타 성공률을 기준으로 recommend를 하거나 아니면 출장 경기 기준 혹은 최근 몇 경기 실적 등으로 판단할 수가 있는 것이다.</p>


<p>이런 Recommendation problem은 Machine Learning 분야 중 굉장히 각광받고 주목받는 영역 중의 하나이다. 이 글에서는 ML의 컨셉이란 무엇인지에 대해 간략하게 다루고, 이런 recommend problem이 ML에서 어떤 positioning을 지니는지에 대해 얘기를 할 것이다.</p>


<h5>Machine Learning</h5>


<p>Machine Learning이란 무엇인가? 이 블로그에서 다뤘던 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning">수 많은 글들</a>이 Machine Learning에 대해 다루고 있지만, 역시 간략하게 다시 언급을 하자면 Machine Learning은 Data로 부터 system을 구성하는 것이라고 할 수 있다. 위키피디아의 설명을 참고하자면 <a href="http://en.wikipedia.org/wiki/Machine_learning">'Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data.'</a> 라고 한다.</p>


<p>이해를 돕기 위해 사람이 새로운 정보를 습득하는 과정에 대해서 생각해보자. 학교에서 새로운 지식을 배울 때, 우리는 수업과 책을 읽고 정보를 습득한다. 그리고 내가 제대로 배웠는지 판단하기 위해서 시험을 치고 그 결과에 따라서 이 정보를 잘 습득했다, 혹은 그렇지 못했다를 판단할 수 있는 것이다. 이때 공부를 위해서 sample exam을 계속 치면서 자신의 이해도를 판단하고 자신의 공부 방법을 개선해서 더 나은 학습을 하는 것이 가능하다. (시험을 수능, sample exam을 모의고사라고 생각하면 이해가 빠를 것이다)</p>


<p>Machine Learning도 크게 다르지 않다. 예를 들어 spam filter를 구성하는 ML algorithm을 작성한다고 생각해보면, 이 algorithm을 사용하는 system은 사용자가 spam mail이라고 report한 기존의 정보들을 습득하고 그 정보를 기반으로 새로운 email이 spam인지 아닌지 판별을 하게 된다. 이때 제대로 판단을 했느냐 하지 못했느냐로 해당 알고리듬이 얼마나 우수한지 판별할 수 있을 것이다. 사람으로 비유를 하자면 이 과정은 수능을 쳐서 자신이 얼마나 공부했는지를 판별하는 과정과 유사하다. 알고리듬의 개선을 위해서 지속적으로 test set을 통해 알고리듬의 변수들을 조정하여 더 나은 알고리듬을 만들어낼 수 있는데, 이 과정은 사람이 모의고사로 공부를 하는 과정과 유사하다.</p>


<p>이렇듯 Machine Learning에서 중요한 것은 system을 학습시키는 traing data가 존재하며, 해당 data를 기반으로 system이 구성이 된 이후 training data가 아닌 <a class="red tip" title="test data라고 한다">새로운 data</a>를 사용해 맞는 결과인지 아닌지를 확인하고 이 정보를 feedback해 현재 알고리듬을 개선한다. 즉, tranining data를 사용해서 ML algorithm에서 사용할 model과 rule을 만들고 test data를 사용해 해당 algorithm의 우수성을 판단하고 system을 개선시키는 것이 Machine Learning의 기본 컨셉이다.</p>


<p>그렇다면 왜 이제와서 Machine Learning인가? 최근 ML이 꽤 hot한 field로 주목받고 있지만, 사실 ML자체는 컨셉이 처음 나온지 벌써 <a class="red tip" title="처음에는 AI(인공지능)의 해결책으로 제시되었던 컨셉이었다.">2-30년이 된 생각보다 오래된 학문</a>이다. 이런 현상이 일어나게 된 것에는 흔히 말하는 Big data의 등장이 있다. Big data가 등장함으로 인해 ML에서 가장 중요한 data가 그야말로 엄청나게 많아지고 또한 접근성도 좋아지면서 이를 통해 의미있는 무언가를 찾아내기 용이해졌다. 이런 데이터를 통해 새로운 information을 도출할 수 있다면 분명 여러 분야에서 큰 도움이 될 것으로 예상할 수 있을 것이다. 이런 motivation으로 최근 ML이 크게 각광받고 있는 상황이며 흔히 말하는 빅데이터가 사실은 ML을 의미하는 경우도 많다. 이 글에서 얘기하게 될 Netflix는 ML과 Big Data의 가장 훌륭하고 성공적인 realistic한 BM example로 손꼽히고 있다.</p>


<h5>Recommendation Problem in Netflix</h5>


<p>사용자의 과거 영화 열람 기록을 기반으로 영화를 추천하기 위해서는 이 문제를 풀이가 가능한 형태로 바꾸는 과정이 먼저 필요할 것이다. 여러 방법이 있을 수 있겠지만, 여기에서는 간단한 하나의 Matrix로 문제를 바꾸어서 생각해보자.</p>


<p><a align="center" href="http://www.codecogs.com/eqnedit.php?latex=movie.&space;{\begin{matrix}&space;1&space;&&space;2&space;&&space;3&space;&&space;4&space;&&space;5&space;&6&space;&&space;7&space;&&space;8\end{matrix}}&space;\\&space;\left\{\begin{matrix}&space;user&space;1&space;\\&space;user&space;2&space;\\&space;user3&space;\\&space;user4&space;\\&space;user5&space;\\&space;user6&space;\\&space;user7&space;\\user8\,&space;\end{matrix}\right.&space;\begin{bmatrix}&space;3&space;&&space;5&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;*&space;&&space;2&space;\\&space;*&space;&&space;3&space;&&space;5&space;&&space;1&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;3&space;\\&space;4&space;&&space;1&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;3&space;&&space;2&space;\\&space;5&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;2&space;&&space;3&space;&&space;*&space;&&space;*&space;\\&space;*&space;&&space;2&space;&&space;4&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;1&space;&&space;2&space;\\&space;5&space;&&space;*&space;&&space;*&space;&&space;5&space;&&space;4&space;&*&space;&&space;*&space;&&space;4&space;\\&space;1&space;&&space;*&space;&&space;5&space;&&space;2&space;&&space;3&space;&1&space;&&space;5&space;&&space;3&space;\\&space;*&space;&&space;3&space;&&space;2&space;&&space;1&space;&&space;4&space;&&space;*&space;&&space;*&space;&&space;*&space;\\&space;\end{bmatrix}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?movie.&space;{\begin{matrix}&space;1&space;&&space;2&space;&&space;3&space;&&space;4&space;&&space;5&space;&6&space;&&space;7&space;&&space;8\end{matrix}}&space;\\&space;\left\{\begin{matrix}&space;user&space;1&space;\\&space;user&space;2&space;\\&space;user3&space;\\&space;user4&space;\\&space;user5&space;\\&space;user6&space;\\&space;user7&space;\\user8\,&space;\end{matrix}\right.&space;\begin{bmatrix}&space;3&space;&&space;5&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;*&space;&&space;2&space;\\&space;*&space;&&space;3&space;&&space;5&space;&&space;1&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;3&space;\\&space;4&space;&&space;1&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;3&space;&&space;2&space;\\&space;5&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;2&space;&&space;3&space;&&space;*&space;&&space;*&space;\\&space;*&space;&&space;2&space;&&space;4&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;1&space;&&space;2&space;\\&space;5&space;&&space;*&space;&&space;*&space;&&space;5&space;&&space;4&space;&*&space;&&space;*&space;&&space;4&space;\\&space;1&space;&&space;*&space;&&space;5&space;&&space;2&space;&&space;3&space;&1&space;&&space;5&space;&&space;3&space;\\&space;*&space;&&space;3&space;&&space;2&space;&&space;1&space;&&space;4&space;&&space;*&space;&&space;*&space;&&space;*&space;\\&space;\end{bmatrix}" title="movie. {\begin{matrix} 1 & 2 & 3 & 4 & 5 &6 & 7 & 8\end{matrix}} \\ \left\{\begin{matrix} user 1 \\ user 2 \\ user3 \\ user4 \\ user5 \\ user6 \\ user7 \\user8\, \end{matrix}\right. \begin{bmatrix} 3 & 5 & * & 4 & 1 &* & * & 2 \\ * & 3 & 5 & 1 & 2 & * & * & 3 \\ 4 & 1 & * & 4 & 1 &* & 3 & 2 \\ 5 & 2 & * & * & 2 & 3 & * & * \\ * & 2 & 4 & 2 & * & * & 1 & 2 \\ 5 & * & * & 5 & 4 &* & * & 4 \\ 1 & * & 5 & 2 & 3 &1 & 5 & 3 \\ * & 3 & 2 & 1 & 4 & * & * & * \\ \end{bmatrix}" /></a></p>

<p>위의 matrix에서 각각의 element는 user가 movie를 rating한 결과를 의미하고 각 column은 하나의 movie를 의미하고 각 raw는 user를 의미한다고 생각해보자. 다시 말해서 맨 처음 element는 1번 user가 1번 영화에 별점을 3점을 줬다는 것을 의미한다고 생각해보자. 마찬가지로 8번 user는 4번 영화에 1점을 준 것이라고 생각할 수 있을 것이다. *은 아직 영화에 평점을 주지 않았다는 것을 의미하고, 우리의 목표는 평점이 주어지지 않은 영화에 user가 과연 평점을 어떻게 매길까를 최대한 결과와 비슷하도록 예측을 하는 것이다. (이 Matrix를 Netflix Matrix라고 하자, 참고로 이 Matrix의 크기는 user가 480,000명, movie가 18,000개 존재하는 엄청나게 큰 Matrix이며 우리가 알고 있는 데이터는 이 중에서 1% 밖에 되지 않는다고 한다.)</p>


<p>즉, 영화 recommendation 문제는 일부 element가 소실되어 있는 matrix의 원본을 복원하는 recovery문제로 바꾸어서 생각할 수 있는 것이다. 이제 Unknown data를 알아내기 위해 Machine learning algorithm이 필요한 것이다. 주어진 data pattern에서 알려지지 않은 새로운 데이터를 추측하는 것이니 이 역시 ML문제라고 생각할 수 있는 것이다.</p>


<p>당연한 얘기지만 접근 방법은 무수히 많을 것이다. 그렇다면 여기에서 궁금증이 생기는데, 과연 그 수많은 알고리듬 중 어느 알고리듬이 우수한지 어떻게 평가할 수 있을까? 여러 algorithm 중에서 가장 좋은 system을 선택하기 위한 evaluation이 필요한 것이다. 실제 정보와 차이를 기반으로 평가를 할 수도 있지만, 일일이 그렇게 평가하는 것은 꽤 어렵기 떄문에 RMSE(Root Mean Squared Error)를 사용한다. RMSE는 \(\sqrt{MSE} = \sqrt{\frac 1 n \sum_{i=1}^n ( \hat{X_i}-X_i )^2}\) 로 표현이 되는데, 다시 말해서 전체 평균과 각각의 정보가 얼마나 많이 차이가 나는가를 평가하는 것이다. 당연히 RMSE는 작을수록 좋고 이 알고리듬을 evaluation하기 위해서 알고리듬의 결과로 나온 predicted result를 ground truth label과 비교하는 것이다.</p>


<h5>Netflix Prize</h5>


<p>Netflix는 자체 추천 알고리듬을 이미 가지고 있었지만, Netflix prize라는 것을 만들어서 이미 가지고 있는 알고리듬을 개선시키고자 하였다. 문제는 간단했다. 현재 Netflix의 recommend system의 RMSE를 10% 가량 개선시킬 수 있느냐? 문제가 2006년 10월에 공지되었으니 알고리듬은 그 당시를 기준으로 평가를 하였다. 이 prize를 위해서 1995 ~ 2005년 동안의 data를 사용해 Training set 100 million (책), probe set 1.4 million (문제집), quiz set 1.4 million (모의고사), test set 1.4 million (수능) 만큼의 data를 제공하였는데 이 정도의 데이터는 Personal Computer에서 돌릴 수 있을만큼 정도의 Data set이었다. 개발자들이 자신의 알고리듬을 평가하기 위해 하루에 한 번 정도 (Maximum 1번) Quiz set에 내 algorithm을 적용시켜 RMSE를 알 수도 있었다. ML 기법을 적용시키고 실제 RMSE를 개선시키는 것이 이 상의 목적이었고 상금은 한화로 약 10억원정도의 금액이었다.</p>


<p><p>그렇다면 과연 이 문제가 10억 이상의 가치가 있었는가라는 질문이 생길 수 있는데, 결론적으로만 말하면 그렇다고 대답할 수 있다. 일단 RMSE의 값을 0.01만 감소시켜도 top 10 recommendation이 달라질 정도로 이 값을 바꾸는 것은 실제 Netflix에 크게 영향을 미칠 수 있다. 또한 문제가 10억원을 내걸 정도로 꽤나 어려운 문제였는데, 이 문제가 완전히 풀리는데 3년이라는 시간이 걸렸으니 결코 쉬운 문제는 아니었던 것이다. 본래 Netflix가 가지고 있던 알고리듬은 Cinemath라는 고유 알고리듬이었는데, 이 알고리듬은 벌써 0.9514 RMSE를 가지고 있었다. 이 값의 10%를 개선하려면 RMSE가 0.8563보다 작은 알고리듬을 개발해야하는데, 아무 사전 지식이 없었던 참여자들이 Cinemath를 beating하는 데에 (따라잡는 데에) 겨우 1주일이 걸렸으며, 8.26%를 beating하는 데에도 겨우 10개월이라는 시간이 걸렸다. (team BellKor, 2007) 첫 해에 결국 8.43%의 결과를 달성했으며 이 값만 봐서는 정말 금방 마무리 될 것 처럼 보였으나&hellip; 0.8616까지 도달하는건 1년이 더 걸리고 (2008년) 결국 당시 leading team이던 BellKor와 BigChaos라는 팀이 결합해서 2009년 6월에 10%에 도달할 수 있었다. 여기까지 도달하는 데에 3년이라는 시간이 걸린 셈이다.
<p>Netflix prize는 총 5000개 이상의 팀이 도전했고 quiz set이 총 44,000 번 test되었다. 가장 먼저 10%를 달성한 팀은 방금전 설명한 BellKor, BigChaos 그리고 Pragmatic Theory 세 팀의 연합 팀이었는데, 10% 달성 이후 30일의 여유 기간에서 Ensemble이라는 다른 팀이 또 10%를 달성하게 되었다. 최종 평가를 위해 알고리듬을 돌려본 결과 두 팀의 RMSE값이 같았는데, BellKor, BigChaos, Pragmatic Thoery 연합팀의 알고리듬이 20분 더 빨라서 결국 상금은 이 팀이 가져가게 되었다.</p>
<h5>Key ideas in the winner of the prize</h5>
<p>사실 10%를 달성하기 위한 마지막 1%에는 정말 어마어마한 efforts를 들이부어서 algorithm을 tuning한 결과이다. 하지만 10%의 performance 중 8~9% 정도의 performance improvement를 위해서는 몇 개 안되는 key idea들만을 사용해서 충분히 그 결과를 얻어낼 수 있다. 이 글에서는 크게 두 개의 아이디어를 소개할 예정이다. 하나는 Neighborhood Method이고, 또 하나는 Matrix Factorization이다.</p>
<p>Neighborhood Method는 각각의 영화들이 얼마나 연관성이 있으며 user끼리는 어떤 연관성이 있는가에 대한 질문에서 시작된 알고리듬이며, 방금 말한 영화 혹은 사용자 간의 유사성을 통해 사용자의 결과를 예측한다. 만약 Machine Learning 중 Clustering에 관심이 있다면 collaborative filtering과 비슷하다고 느낄텐데, 사실 거의 같은 알고리듬이라고 생각하면 된다. 이 알고리듬은 사용자들을 그룹핑하고 (pair를 만들고) 그 그룹 안에 속한 유저가 내린 rating이 다른 user와 얼마나 비슷한지 혹은 다른지 (즉 상관도가 얼마나 있는지) 측정하고 그 측정 값을 기반으로 추천을 하는 알고리듬이다. 즉, 이전에 봤었던 Netflix Matrix에서 비어있는 entry를 그 주변 entry 들의 값을 보고 복구하는 방법이라고 생각할 수 있다.</p>
<p>컨셉만 놓고 비교하자면 Neighborhood Method가 훨씬 간단하지만 Matrix Factorization는 이보다 더 강력한 성능을 자랑한다. 다른 알고리듬 없이 이 알고리듬만 잘 구현한다면 기존 Netflix의 성능을 8-9% 정도 개선하는 것이 가능하다. 이 알고리듬의 기본 아이디어는 크게 어렵지 않다. 이 알고리듬이 기본적으로 사람들의 type 혹은 class가 생각보다 많지 않다는 것을 가정한다. 즉, drama를 얼마나 좋아하느냐, action을 얼마나 좋아하느냐 등등의 요소들이 각 유저들의 rating을 결정한다는 의미이다. 만약 전체 점수를 S라고 하고 각 factor를 fi 라고 하고 각각의 factor마다 개인이 가지는 가중치를 ai 라고 가정하자. 그리고 전체 n개의 factor가 있다고 한다면, 한 사람이 rating하게 될 점수의 예상치를</p>
$$ S = \sum_{i}^{n} {a_i * f_i} $$
<p>로 표현할 수 있을 것이다. 이런 몇 개의 basic classes의 combination으로 user의 rating이 표현이 될 수 있다면, Basic한 몇 개의 요소로 rating이 결정된다 라고 설명할 수도 있으며 약간 수학적으로 설명을 하자면 Netflix Matrix가 몇 안되는 factor들을 통해 표현할 수 있다는 의미가 되므로 Netflix Matrix가 low rank를 가지고 있다라고 표현할 수 있는 것이다. 이 statement가 Matrix Factorization algorithm의 기본 가정이다.</p>
<img src="/images/post/30-1.png" width="400">
<p>이 알고리듬의 목표는 각각의 &lsquo;class i of ratings&rsquo; 를 알아내는 것이다. 이 알고리듬이 실제 의미가 있기 위해서는 Netflix Matrix에서 알고 있는 데이터가 어느 정도 많아야 한다. 실제로 해당 decomposed 된 matrix에 들어있는 entry보다는 많은 데이터를 알고 있어야 하는데, 위의 그림에서 우리는 18,000개의 영화와 480,000명의 유저의 정보를 2개의 class로 표현했으므로 우리가 이미 알고 있는 1%의 정보를 사용하면, 총 48000 * 18000 * 0.01개, 약 8백만개 정도의 데이터를 사용해서 2 * 48000 + 18000 * 2 개, 즉 13만 2천개의 entry를 알아내야 한다. 8백만개의 정보에서 13만 2천개의 정보를 뽑아내는 것은 데이터의 양이 충분하다고 할 수 있을 것이다. 물론 지금은 rank가 2이라고 가정했으므로 2를 곱했고 13만 2천이라는 숫자를 얻게 되었지만, 실제로는 이보다는 더 많은 rank를 가졌다고 가정하고 때문에 더 많은 정보를 알아내야하기는 하지만, 그래도 8백만개에 비하면 충분히 작은 숫자가고 할 수 있을 것이다.</p>
<p>이 이외에도 다양한 아이디어가 있는데 예를 들어 Implicit feedback 아이디어는 사용자가 영화를 자주 봤음에도 불구하고 rating을 하지 않은 경우에 대해서 Netflix Matrix에 반영되지 않은 implicit한 data까지 사용해서 rating을 예측하는 아이디어이며, Temporal dynamics 아이디어는 Netflix Matrix가 불변하는 static한 Matrix가 아니라 실제로는 사람마다 영화 취향이 바뀔 수 있고 매번 유행하는 영화가 바뀌는 등 temporal하게 봤을 때 dynamic한 matrix라고 가정하고, 시간 축 상에서 entry들이 변화하는 양을 measure하여 이를 rating에 반영하는 아이디어이다. 이 밖에도 다양한 아이디어들이 있고 이런 추가적인 아이디어은 실제 8~9% 이상의 무언가를 달성하기 위한 알고리듬 튜닝에 쓰였다고 한다.</p>
<p>이상으로 Netflix prize와 실제로 그 목표를 달성한 알고리듬의 brief한 소개를 마치도록 하겠다. 이보다 더 자세한 내용은 두 번째 글에서 다루도록 하겠다</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[빅데이터 이야기: 데이터 수집에서 분석까지]]></title>
    <link href="http://SanghyukChun.github.io/21/"/>
    <updated>2013-11-19T18:06:00+09:00</updated>
    <id>http://SanghyukChun.github.io/21</id>
    <content type="html"><![CDATA[<p>빅데이터란 말 그대로 데이터의 양이 엄청나게 많은 것을 의미한다. 이때 빅데이터라고 불릴 정도로 데이터의 양이 많으려면 <a class="red tip" title="Peta Byte">PB</a> 정도의 데이터가 필요하다. 우리에게 친숙한 TB 단위는 빅데이터라고 언급하기에는 좀 작은 편이다. 이 PB라는 단위가 도대체 얼마나 큰 단위이냐하면.. 그야말로 어마어마하다. 1PB는 1,000TB이고, 1,000TB면 1,000,000GB이다. 이는 <a class="red tip" title="정확하게 말하면 Computer world에서는 10진법이 아니라 2진법을 사용하기 때문에 1000씩 곱해지는 것이 아니라 1024씩 곱해져야한다. 그러니깐 1024*1024*1024=1073741824MB가 된다.">1,000,000,000MB</a>라는 도대체 감도 오지 않을 정도로 거대한 값이 된다. 조금 친숙한 단위로 바꾸어서 생각해보자. 일반적으로 3분 남짓한 고음질 MP3 파일이 약 10MB정도 된다. 이를 1PB에 대해 동일한 비율로 계산해보면 300,000,000분 즉, 570년 이상의 재생 시간을 가지는 무지막지하게 거대한 음악파일이 된다. 이 정도면 조금 감이 잡히려나?</p>


<p>이제 데이터가 중요한 것은 누구나 알고 있다. <a href="http://forumblog.org/2012/02/the-2012-top-10-emerging-technologies/">세계 경제 포럼 2012년 Global Emerging Technology</a>에도 <a class="red tip" title="본문에는 Informatics for adding value to information라고 적혀있는데, 사실 Big Data라는 말은 그 출처가 불분명한 모호한 용어이다. 사실 이는 단순히 데이터가 많이 있음을 의미하는 말이며, 그 안에서 의미를 분석하는 기술은 informatics, data mining등으로 부른다. 이 글에서는 빅데이터를 앞에서 정의한대로 데이터의 양이 많음으로 정의하고, 빅데이터를 수집하는 방법과 분석하는 방법에 대해 설명할 것이다.">데이터 분석이 당당히 1위를 차지하고 있다.</a> 그러나 실제 그 중요한 데이터를 활용하는 사람들은 많지 않다. 데이터를 활용해 비즈니스 정보를 도출해내는 것이 기술적으로 굉장히 어렵고 고난이도의 도전을 요구하기 떄문이다. 구체적으로 말하자면 크게 두 가지 측면에서 문제가 된다. 하나는 그 많은 데이터를 어디에, 그리고 어떻게 저장할 것이냐라는 문제, 두 번째는 그렇게 모은 데이터를 어떻게 분석하고, 어떻게 더 빠르고 정확하게 분석하느냐라는 이슈가 있다. 지금 흔히 빅데이터라고 불리는 기술은 사실 빅데이터 저장 기술, 그리고 빅데이터 분석 기술 이렇게 크게 두 가지가 혼용되서 언급이 되고 있는 것이다. 이 글에서는 Big data storage technology와 Big data analysis technology란 무엇이고, 내가 연구 혹은 개발했던 경험들에 기반하여 내 개인적인 코멘트를 할 생각이다.</p>


<h3>Big Data Storage</h3>


<p>데이터 저장은 두 가지 측면에서 Challenge하다. 먼저, 들어오는 데이터가 너무 많은 경우에는 이 데이터를 저장하는 행위 자체가 어려울 수 있다. 또한 저장한 데이터의 분석을 용이하게 하기 위해서 어떤 Data structure를 가져갈 것인지에 대해서도 고민이 필요하. 둘 다 쉬운 문제는 아니며 최근 관련 연구가 활발하게 진행되고 있는 분야이다.</p>


<p>데이터가 엄청나게 많아져서 계속 저장해야하는 양 자체가 너무 늘어나게 되면, 그 데이터를 아무 preprocessing없이 저장하는 일 조차 엄청나게 어렵다. Facebook을 예를 들어보자. 현재 Facebook을 사용하는 유저는 세계적으로 10억명이 넘는다. 이 사람들 중에서 동시 접속을 10%만 한다고 생각해도 1억 명이고 이 사람들이 Timeline에서 like나 comment 혹은 post를 평균 1분에 한 번씩 한다고 해도 자그마치 1분에 1억 번의 traffic을 감내해야한다. 이게 얼마나 큰 값이냐하면, 만약 한 Packet당 크기를 편의상 1kByte라고 한다고 하면 매분 1천억Byte, 혹은 100TB의 Packet이 Facebook Data center로 흘러들어오는 것이다. 이렇게 되면 Server side에서 logic처리만 하더라도 전체 리소스가 부족한데 거기에 로깅까지 하려면 정말 엔지니어 입장에서는 그야말로 미쳐버릴 노릇이 되어버린다.</p>


<p>이렇게 Real-Time으로 어마어마한 데이터를 Capture하고 Storing하는 것이 가장 어려운 이유는, 실제 Disk에 작성을 할 때 Disk의 Bandwidth가 Network Bandwidth보다 훨씬 작기 때문이다. 네트워크가 maximum으로 들어온다고 가정했을 때 보통 <a class="red tip" title="1 Giga bit per s 즉, Byte로 환산하면 약 1-200 MB/s가 된다">1G</a> 정도를 상한선으로 잡으니 우리도 네트워크 인풋이 1Gbps라고 가정해보자. 1Gbps는 125MBps와 같은 값이니 1초에 125MB의 데이터가 들어오는 셈이다. 반면 일반적인 HDD의 쓰기 속도는 100MB/s가 되지 않는다. 결국 초당 1G의 data가 들어오게 된다면 하나의 하드디스크로만 사용하여 데이터를 저장하는 것이 어렵다. 이는 하드웨어의 한계이기 때문에, 더 좋은 하드웨어를 사용하거나 (예를 들어 SSD는 HDD보다 읽기 쓰기 속도가 훨씬 빠르다), 소프트웨어로 하드웨어를 여러 개 사용해 Scalability를 높이도록 설계하는 방법 밖에 없다. 당연히 아직 하드웨어를 업그레이드하는 것은 매우 비싸고, 데이터 속도가 더 빨라지면 다시 하드웨의 한계에 언젠가 부딪힌다는 문제가 있다. 물론 돈만 있다면 가장 간단한 해결방안이다. 그보다 더 일반적인 해결방안은 소프트웨어를 사용해 Scalability를 높이는 것이다. Multi-threading Programming을 통해 연산해야하는 계산량을 분산하고 굉장히 latency가 큰 disk I/O를 따로 처리할 수 있는 것이다. Multi Machine으로 확장하는 가장 쉬운 방법은 Map-Reduce를 사용하는 것이다. Map-Reduce는 functional language의 map operation과 reduce operation에서 따온 이름인데, 실제 functional language에서 사용하듯 operation을 여러 개의 작은 operation으로 나누어 각각의 머신에서 처리하고 다시 이 작은 operation들을 한 머신에서 취합하는 방식이다. 물론 실제로 이를 적용하기에는 많은 문제점이 있다. Overhead가 어디이며 얼마나 많은 데이터를 처리할 수 있을 것인가, multi-threading program은 보통 1개의 머신에서 작동하도록 설계가 되는데 더 분산 설계를 많이 고려해서 multiple machine에서도 동작할 수 있도록 설계를 할 수 있는가, 만약 각각의 연산 unit이 한 번에 처리할 수 있는 capacity가 달라서 load balancing이 성능 뿐 아니라 corectness에 또한 중요한 요소가 되는 상황에 과연 적절한 load balancing 설계가 가능한가 등의 문제가 생길 수 있을 것이다. 모두 절대 간단한 질문이 아니며 System에 대한 조예가 깊어야 해결할 수 있는 문제가 많다. 대부분의 경우 데이터를 계속 기록하다가 이렇게 하드웨어의 한계에 도달하는 경우 이를 확장하지 못해 더 이상 기록하는 것을 포기하거나 아니면 일부만 기록하는 경우가 허다하다.</p>


<p>단순히 데이터를 저장만 하는 것도 어렵지만 사실 데이터를 어떤 format으로 저장하느냐 역시 중요하다. 아무 의미 없어보이는 불규칙한 데이터를 분석하는 것과 이미 대략적으로 entry를 가지고 있고 분류가 되어있는 데이터를 분석하는 것은 큰 차이가 있다. 예를 들어서 'ㅇㅇㅇ 고객은 ㅁㅁ대학교에 재학 중인 대학생이며 한달 평균 지출액은 약 10만원 정도이며 이와 비슷한 다른 고객들은 대략 5만원을 사용하는 것을 고려해보았을 때 ㅇㅇㅇ 고객은 우리가 중요하게 생각해야하는 고객이다' 라는 raw text data를 저장하는 것과, {이름: ㅇㅇㅇ, 직업: 대학생, 직업_세부사항: ㅁㅁ대학교, 한달 지출액: 10만원} 의 key-value format으로 저장하는 것이 분석하기가 훨씬 편할 것이다. 여러 format을 사용할 수 있으나, 대표적으로 떠올릴 수 있는 형태는 예시로도 들었던 key-value storage일 것이다. 만약 우리가 분석해야하는 데이터가 정형화되어있고, 분석 역시 일부 데이터에서만 하게 된다면 key-value storage는 매우 좋은 선택이 될 것이다. 그러나 데이터 logging이라는 특성을 고려해본다면 항상 key-value가 좋기만 한 것은 아니다. logging은 상황에 따라 format이 바뀔 수 있다. 예를 들어서 {이름: ㄴㄴㄴ, 직업: 회사원, 한달 지출액: 20만원, 주 관심 카테고리: 차량} 등의 형태로 형태가 변화할 수도 있다. (이 경우는 주 관심 카테고리가 추가되었다) 만약 MySQL 등의 strict한 key-value storage라면 매번 entry를 추가해야한다. 만약 갑자기 로깅 skeme을 변경하겠다는 결정이 내려지면 MySQL을 사용하고 있던 서비스는 새로운 DB를 생성해야만 할 것이다. 이런 문제점을 방지하기 위해서 NoSQL이라는 것이 등장했고 MongoDB, Redis, Cassandra 등이 이에 속한다. 자세한 내용은 나중에 한번쯤 다뤄보도록 하겠다. NoSQL을 사용하면 SQL보다 유연하게 formatting을 할 수 있고 조금 더 분산환경에 적합하다. 아무튼 생각보다 데이터를 formatting하는 것이 어렵다. 가장 큰 이유는 데이터를 기록하기 시작할 때 부터 어떤 분석이 필요할 것이라고 예측하는 것이 쉽지 않기 떄문이다. 구글이 검색어 로그를 수집할 때 해당 검색어가 독감 지도가 되고 대선 지표가 될 것이라고 예측할 수 있었을까? 아마 아닐 것이다. 빅데이터의 가장 큰 아름다움은, 아무 의미없고 연관성 없어보이는 여러 사실들이 데이터를 통해서 얽히고 섥혀 한 순간에 모든 사실을 데이터가 꿰뚫는 순간인 것이다. 이런 순간을 미리 예측한다는 것은 거의 불가능에 가깝다. 이 분야는 내 주요 관심분야도 아니고, 내가 많이 해본 적은 없지만, 내가 알기로는 점점 strict한 format은 줄어들고 보다 유연하고 쉽게 바꾸기 쉬운 형태의 format으로 옮겨가고 있는 추세로 알고 있다. 그만큼 데이터가 어떤 방향으로 변할지 예측하는 것이 어렵다는 것이다.</p>


<p>우리는 이제 사방에 데이터가 넘쳐나는 환경에 살고 있어서 데이터라는 것을 얕잡아보기 쉽지만, 이 데이터의 양이 무지막지해지면 데이터를 저장하고 적절한 formatting을 하는 것만으로도 수 많은 기술적 challenge를 요구한다. 하지만 사실 우리가 관심있는 부분은 데이터를 저정하는 것이 아니라 데이터를 분석하는 것이다. 다음 paragraph부터는 실제 데이터를 분석하는 방법들과 예시들에 대해 얘기해보겠다.</p>


<h3>Big Data Analysis</h3>


<p>'구슬이 서말이어도 꿰어야 보배다' 빅데이터 분석과 가장 잘 어울리는 속담이 아닐까? 아무리 데이터를 많이 가지고 있어도 그 데이터를 분석하지 못한다면 아무런 의미가 없다. 데이터를 분석하는 여러가지 방법이 존재하지만, 이 글에서는 주로 내가 공부하고 있는 Machine Learning을 중심으로 설명하려고 한다. 다른 방법들에 대해서는 내가 잘 알지 못하고 데이터가 많은 경우에 잘 동작하지 않는 등 제한된 조건에서만 동작하는 방법이 많기 때문이다.</p>


<p>그러면 Machine Learning이란 무엇인가? 마치 사람이 새로운 학습 정보를 받아들여 자신의 행동을 개선하듯, 머신이 데이터를 통해 decision making을 더 개선해나가는 과정을 의미한다. <a href="http://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5" taget="new">위키</a>를 참고해서 말하자면 머신러닝은 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야를 말한다. 더 자세한 내용은 <a href="http://SanghyukChun.github.io/3">이전에 내가 정리한 글</a>을 참고하길 바란다. (참고로, Data mining이라는 용어도 존재하는데, 내가 앞으로 설명하려는 내용과 거의 일치한다. 사실 Data Mining은 데이터에서부터 의미를 캐내는-mining하는-모든 것을 통틀어서 말하는 것이며, 대부분의 경우 Machine Learning Technology를 사용해서 분석을 하는 경우가 많다. 때문에 이 글에서는 딱히 그 둘을 구분하지 않고 혼용해서 적도록 하겠다.) 그렇다면 머신러닝이 데이터 분석에 어떤 식으로 기여할 수 있을까? 머신러닝을 사용하면 데이터의 패턴을 파악하는 것이 가능해진다. 다시 말해서 내가 알고 있는 정보를 사용해서 내가 모르는 정보를 얻을 수 있다는 의미이다. 간단한 예를 들어보자, 만약에 아래와 같은 데이터가 있다고 가정해보자.</p>


<p><img src="/images/post/14-1.png" width="400"></p>

<p>음.. 쉽게 생각해서 가로축은 내가 게임에서 사용한 돈의 액수를 의미하고, 세로축을 그 게임에서 내가 가지고 있는 좋은 아이템의 개수라고 생각해보자. 아마 돈을 많이 쓸 수록 좋은 아이템을 많이 가지게 될 것이다. 그렇다면 내가 돈을 11만큼 썼을 때 내가 얼마나 많은 아이템을 얻을 수 있을까? 위의 데이터를 토대로 추정해보자면 아마 약 11에 근접한 값을 얻게 될 것이다. 머신러닝은 이런 방식으로 주어진 정보만을 토대로 아직 알고있지 못한 정보를 추정하게 된다. 자 이제 대략적인 설명은 충분한 것 같다. 그렇다면 실제 이렇게 머신러닝을 사용해서 데이터를 분석하고 있는 예시는 무엇이 있을까?</p>


<p></p>

<p>미국에 Netflix라는 기업이 있다. 이 기업에서 제공하는 주요 서비스는 영화와 관련된 서비스 들이다. 시작은 1997년 시작한 DVD 대여 사업에서부터 시작했지만, 지금은 북미에서 스트리밍되는 대부분의 영화를 독식하고 있는 어마어마하게 거대한 기업이다. 실제로 지금 미국에서 발생하는 트래픽의 25%가 Netflix에 의해서 생겨난 트래픽이라고 하니, 이 기업이 얼마나 사람들에게 영향을 미치고 있는지 알 수 있을 것이다. 이 기업이 거대하게 성장하게 된 배경에는 '추천'이라는 시스템이 자리잡고 있다. 이 추천 문제는 사실 Machine Learning에서 상당히 중요하게 다뤄지는 문제 중에 하나이다. 추천이라는 것을 사용자의 다른 정보들을 사용하여, 아직 밝혀지지 않은 사용자의 기호를 추정하는 문제로 생각하면 Machine Learning 기법을 사용하여 문제를 해결하는 것이 가능하기 때문이다. Netflix는 이런 접근 방식을 통해 실제 Big Data와 Machine Learning을 사용한 가장 성공적인 사례로 손꼽히는 기업이다. Netflix에서는 영화를 대여하거나 시청하게 되면 자동으로 관련되거나 사용자가 관심있어할만한 영화를 자동으로 추천해준다. 특히 Netflix에서는 영화를 scoring하는 것이 가능한데, 과거에 내가 점수를 매겼던 history를 기반으로 하여 새로운 영화에 내가 몇 점을 매길 것인지를 꽤 정확한 확률로 추천해준다. 내가 해리포터와 마법사의 돌, 해리포터와 비밀의 방, 그리고 해리포터와 아즈카반의 죄수라는 영화를 재미있게 봤다면 내가 해리포터와 불의 잔이라는 영화를 좋아할 가능성이 높다고 추천해주고, 이런 추천을 통해 사용자들의 추가적인 과금을 유도하는 형식이다. 이런 방식의 추천 시스템은 아마존 등에도 적용이 되어있으며 마찬가지로 매우 효과적인 결과를 얻고 있다고 한다. 이런 추천 시스템을 구축하기 위해서 Netflix는 엄청나게 복잡하고 많은 머신러닝 기법들을 사용한다. 이 시스템은 2006년 Netflix prize라는 이름으로 추천 시스템의 성능을 향상시키는 대회가 열렸는데, 이 대회에서 우승을 했던 시스템이 기반이 되어 만들어졌다. Netflix prize에 대해서는 조만간 글을 쓸 예정이므로 여기에서는 자세한 언급은 피하도록 하겠다. 마찬가지로 이 대회에서 우승을 한 알고리듬을 자세히 설명하는 것도 다른 글에서 다룰 수 있도록 하겠다. 중요한 것은 Netflix에서 상당히 높은 수준으로 추천이 되는 영화들이 사람이 인위적으로 작성한 리스트에 따라서 추천이 되거나 하는 것이 아니라, 온전히 Machine Learning Technology를 사용하여 추천을 하고 있다는 것이다. Netflix는 흩뿌려져 있는 것 처럼 보이는 데이터를 수집해서 그 데이터에서 의미있는 결과를 도출하고 그 결과를 통해 실제로 돈을 벌 수 있다는 것을 보여주는 아주 좋은 예시이다. (한국에서 <a href="http://watcha.net">왓챠</a>라는 유사한 서비스가 있다. 카이스트 선배님이 CEO로 계신 회사인데, 이 서비스도 상당히 좋은 서비스이니 관심있다면 한 번쯤 이용해보는 것도 재미있을 것이다.)</p>


<p>오바마 대통령의 재선 캠프에 있었던 데이터 분석팀 역시 빅데이터를 잘 활용한 좋은 예시로 손꼽힌다. 미국은 정말 많은 인종과 사람들이 존재하는 나라이다. 그리고 국토도 <a title="미국 9,826,675제곱킬로미터, 남한 99,720제곱킬로미터" class="red tip">남한의 98배가 넘을 정도로</a> 워낙 방대하기 때문에 한정된 시간동안 많은 사람들을 대상으로 하는 대선운동을 하는 것이 쉽지 않다. 우리나라에서 대선을 하는 동안에도 어느 지역에 어떤 자원을 투자하느냐가 이슈가 되는데, 하물며 우리보다 거의 100배나 거대한 미국에서는 어떠하겠는가? 우리나라도 지역별로 선호하는 후보나 당이 분명하게 구분이 되는 것처럼 미국도 지역별로 공화당 혹은 민주당 중에서 더 선호되는 당이 다르다. 하지만 남한 만큼 거대한 주가 51개 주 중 37개에 달하는 만큼, 우리나라처럼 지방 하나가 거의 동일한 색채를 띄고 있지 않다. 또한 우리나라는 인구 밀집도가 특정 지역이나 도시에 몰려있는 반면, 미국은 인구 밀집도가 많이 낮은 편이다. 때문에 직접 선거 운동을 할 수 있는 지역은 극히 제한된다. 오바마 선거 운동팀은 그 동안 근사적으로만 접근하던 모든 데이터를 수치화하고 그 값을 바탕으로 효율적인 자원분배를 이뤄냈다. 오바마 캠프의 데이터 분석팀이 가장 먼저 한 일은 현재 자세히 분류되어있지 않은 유권자들을 투표성향, 성별, 인종 등으로 나누어 새로운 유권자 명부를 작성한 것이다. 이렇게 작성된 명부를 바탕으로 하여 micro targeting이 이루어졌다. 강한 민주당 지지자이며 투표를 할 사람들, 강한 민주당 지지자이지만 투표를 하지 않을 사람들, 약한 민주당 지지자, 약한 공화당 지지자, 강한 공화당 지지자... 등의 분류를 하고, 각 분류별로 각기 다른 전략을 세워 유권자를 끌어모았다. TV광고도 단순히 사람들이 많이 볼 것으로 예상되는 시간에 편성하는 것이 아니라, 지역별로 사람별로 적당하다고 생각되는 채널에 광고를 편성하였다. 이전에는 사람들이 많이 보는 뉴스나 스포츠 경기에 광고를 하는 방식으로 홍보를 했다면, 이제는 특정 지역의 특정 계층의 사람들에게 타겟 광고를 하는 방식으로 바뀐 것이다. 예를 들어서 캘리포니아의 30대 여성들을 공략하기 위해 해당 계층이 가장 많이 보는 프로그램에 광고를 내보내는 것이다. 이는 선거 자금 모금 운동에도 동일하게 적용되었는데, 2012년 5월 조지 클루니의 자택에서 열린 선거 자금 모금 운동이나, 그로부터 한 달 뒤에 세라 제시카 파커(섹스 앤 더 시티의 출연자 중 하나)의 집에서 벌인 모금 운동 모두 40대 여성들을 겨냥한 데이터 팀의 분석 결과로부터 도출된 전략이었다. 또한 전화 선거 운동을 할 때에도 데이터 분석을 통해 사람들을 분류하고, 오바마를 지지할 가능성이 없는 사람들은 과감히 버리고 오바마를 지지할 가능성이 존재하거나, 오바마를 지지하지만 투표를 하지 않는 사람들을 중심으로 전화를 걸어서 맷 롬니 진영보다 훨씬 효율적으로 선거운동을 할 수 있었다. 이 밖에도 선거 운동을 하는 사람들을 위한 소프트웨어나 지속적인 모의 투표, 트래픽 폭주 상황에 대비한 Plan B 수립 등 데이터 팀이 오바마 캠프에 미친 영향은 지대하였다. 결국 오바마 캠프는 '데이터의 승리'라고 불릴 정도로 성공적인 데이터 분석 사례로 손꼽히게 되었다.</p>


<p>이렇듯 Machine Learning은 데이터를 분석하는 방법 중에서 가장 많이 쓰이고 가장 각광받는 방법이다. 하지만 이것은 어디까지나 접근 방법에 대한 문제이고, 이런 알고리듬을 구현하고 적용하기 위해서 필요로 하는 기술들은 정말 많다. 특히 가장 필요로 하는 기술 중 하나는 parallel programming 혹은 distributed programming인데, 쉽게 생각하면 한 연산을 하나의 머신에서 처리하는 것이 아니라 여러 개의 분산된 머신에서 병렬적으로 이를 처리하는 방식이다. 앞서 설명했던 것과 같이 이런 기술이 필요한 이유는 간단하다. 머신 하나의 연산 처리 속도가 우리가 요구하는 연산 처리 속도보다 훨씬 느리기 때문에 software를 사용해 hardware의 기능을 확장해서 사용하는 것이다. 데이터 분석을 위해 현재까지 가장 많이 쓰이는 기술을 꼽자면 Hadoop과 Map-Reduce를 꼽을 수 있을 것이다. Map-Reduce는 앞에서 잠깐 언급했듯 연산을 여러 개의 분산되어있는 머신에게 분할해서 각자 연산을 따로 하게 하고, 그 결과를 다시 합쳐주는 방식이다. 현재 Google에서 개발한 Google Map-Reduce가 가장 많이 쓰이고 있다. 하둡은 일종의 파일시스템인데, 오픈 소스이기 때문에 많은 개발자들이 즐겨 사용하고 있다. 하둡 분산 파일시스템이 만족하는 요구사항이 몇 가지가 있는데, 분산 환경에서는 여러 개의 컴퓨터가 연합하여 연산을 수행하기 떄문에, 이 중 몇 개의 머신이 고장이 나더라도 전체 시스템에 문제가 생기지 않도록 빠른 복구를 요구하고, 자료 자체는 저장이 된 이후 (일반적으로) 다른 머신으로 옮겨가지 않기 때문에 데이터가 옮겨지는 것 보다 연산이 옮겨가는 것이 더 빠르게 설계하는 등의 요구사항을 만족하고 있다. 자세한 내용은 <a href="http://ko.wikipedia.org/wiki/%ED%95%98%EB%91%A1">하둡 위키</a>를 참고하길 바란다.</p>


<p>데이터 분석은 데이터가 존재하는 모든 곳에서 적용가능한 발전 가능성이 무궁무진한 기술이라고 할 수 있다. Netflix라는 기업에서부터 미국 대선에 이르기 까지 실제로 이를 요구하는 기업이나 단체도 매우 많으며 또한 좋은 결과를 달성하고 있다. 이 밖에도 정말 많은 예시들이 있지만 이미 충분히 글이 길어진 것 같아서 이 쯤에서 줄이도록 하겠다.</p>


<p>이렇게 간략하게나마 빅데이터를 수집하는 방법과 분석하는 방법에 대해서 다루어보았다. 생각보다 글이 길어지기는 했지만, 내가 알고 있는 빅데이터에 대한 개념과 실제 적용을 할 때 생기는 문제점들 그리고 실제 해당 기술들을 적용해 성공적으로 운영하고 있는 예제들도 간략하게나마 살펴보았다. 분명 데이터는 미래에 점점 더 중요한 자원이 될 것이며, 이 자원을 확보하기 위한, 그리고 확보한 자원에서 정보를 뽑아내기 위한 싸움이 치열하게 벌어질 것이다. Machine Learning을 연구하고 있는 사람 중 하나로써 새로 맞이하게 될 데이터 시대에서 큰 역할을 할 수 있기를 희망한다.</p>

]]></content>
  </entry>
  
</feed>
