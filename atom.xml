<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[README]]></title>
  <link href="http://SanghyukChun.github.io/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2014-08-04T21:09:11+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (3) Overfitting]]></title>
    <link href="http://SanghyukChun.github.io/59/"/>
    <updated>2014-08-03T16:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/59</id>
		<content type="html"><![CDATA[<p>Machine Learning Algorithm을 design하고 실제 구현을 하다보면 Overfitting이라는 문제가 꽤나 머리를 아프게 하는 경우가 많다. 아니, Algorithm 자체가 문제 없이 실행이 된다면 대부분의 경우 overfitting이 가장 큰 문제라고 단언해도 좋을 것 같다. 물론 좋은 Algorithm을 design하고 못하고의 문제는 또 다른 문제이기는 하다. 다음 글은 Algorithm에 대해서 써볼까.. 아무튼 이번 글에서는 Overfitting에 대해 좀 다뤄보도록 하겠다.</p>


<h5>Overfitting</h5>


<p>overfitting이란 문자 그대로 너무 과도하게 데이터에 대해 모델을 learning을 한 경우를 의미한다. 지금 현재에 대해 잘 설명하면 되는 것 아닌가? 싶을 수 있지만, 우리가 사실 원하는 정보는 기존에 알고 있는 데이터에 대한 것들이 아니라 새롭게 우리가 알게되는 데이터에 대한 것들을 알고 싶은 것인데, 정작 새로운 데이터에 대해서는 하나도 못맞추고, 즉 제대로 설명할 수 없는 경우라면 그 시스템은 그야말로 무용지물이라고 할 수 있을 것이다. 조금 더 자세한 설명은 <a href="http://SanghyukChun.github.io/14#Overfitting" target="new">이전에 적은 포스트</a>로 대체하겠다.</p>


<h5>Regularization</h5>


<p>이런 문제를 해결하기위해 여러가지 시도를 할 수 있다. 먼저 Overfitting이 일어나는 이유는 무엇인가에 대해서 한 번 생각해보자. 먼저 overfitting의 가장 간단한 예시를 하나 생각해보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/59-1.png" width="500"></p>

<p>위 그림에서도 알 수 있 듯, 만약 우리가 주어진 데이터에 비해서 높은 complexity를 가지는 model을 learning하게 된다면 overfitting이 일어날 확률이 높다. 그렇다면 한 가지 가설을 세울 수 있는데, &#8216;complexity가 높을 수록 별로 좋은 모델이 아니다.&#8217; 라는 가설이다. 이는 <a href="http://en.wikipedia.org/wiki/Occam%27s_razor" target="new">Occam&#8217;s razor</a>, 오컴의 면도날이라 하여 문제의 solution은 간단하면 간단할수록 좋다라는 가설과 일맥상통하는 내용이다. 하지만 그렇다고해서 너무 complexity가 낮은 model을 사용한다면 역시 부정확한 결과를 얻게 될 것은 거의 자명해보인다. 그렇기 때문에 우리는 원래 cost function에 complexity와 관련된 penalty term을 추가하여, 어느 정도 &#8216;적당한&#8217; complexity를 찾을 수 있다. 이를 regularization이라 한다. 이 이외에도 다양한 설명이 있을 수 있기에 <a href="http://en.wikipedia.org/wiki/Regularization_(mathematics)" target="new">위키 링크</a>를 첨부한다.</p>


<p>그리고 이를 Bayesian 관점에서 설명을 할 수도 있다.</p>


<p>\[ p(Y|X) = {p(X|Y) p(Y) \over p(X)} \]</p>


<p><a href="58#Bayes" target="new">이전 글</a>에서도 설명했던 것 처럼, 만약 우리가 어떤 사전지식, 혹은 prior knowledge가 존재한다면 단순히 observation만 하는 것 보다는 훨씬 더 잘 할 수 있을 것이다. 그리고 다시 Overfitting 문제로 돌아와서, overfitting이 생기는 가장 큰 이유는 너무 지금 데이터, 즉 observation에만 충실했던 것이 그 원인이다. 그러면 당연히 prior가 있으면 이를 해결할 수 있지 않을까? 라는 질문을 던질 수 있을 것이다. 즉, 우리의 prior는 high complexity solution은 나오지 않을 것이다. 어느정도 complexity가 높아지는 것 까지는 용인하지만, (표현할 수 있는 영역이 더 넓어지니까) 하지만 너무 그 complexity가 높아지면 문제가 생길 수 있다. 즉, 그런 high complexity를 가지는 solution이 나올 확률 자체가 매우 낮다. 라는 prior가 있다면 이를 간단하게 해결 할 수 있을 것이며, 이것이 결국 앞에서 봤었던 penalty와 동일하다는 것을 알 수 있다.</p>


<h5>Model Selection</h5>


<p>그런데 제 아무리 우리가 좋은 prior를 넣고, 좋은 penalty term을 design하더라도 만약 우리가 제대로 되지 않은 데이터들을 이용해 learning을 한다면 문제가 생길 수 있다. 마치 장님 코끼리 만지듯 전체 데이터는 엄청나게 많이 분포해있는데 우리가 가진 데이터가 아주 일부분에 대한 정보라면, 혹은 갑자기 그 상황에서 갑자기 노이즈가 팍 튀어서 데이터가 통채로 잘못 들어온다면? 아마 그런 데이터로 learning을 했다가는 sample bias가 일어나게 되어 크게 성능이 저하되게 될 것이다. 사실 위에 complexity라는 말도 결국에는 &#8216;Data point 대비 높은 complexity&#8217;가 더 정확한 말이다. 이를 최대한 피하기 위하여 우리는 <a href="http://en.wikipedia.org/wiki/Generalization" target="new">generalization</a> 이라는걸 하게 되는데, 다시 말해서 우리의 solution이 specific한 결과만을 주는 것이 아니라 general한 결과를 주도록 하려는 것이다. 이를 위한 여러 방법이 있을 수 있지만, 가장 많이 사용하는 방법은 validation set이라는 것을 사용하는 것이다. 즉, 모든 data를 전부 training에 사용하는 것이 아니라, 일부만 training에 사용하고 나머지를 일종의 validation을 하는 용도로 확인하는 것이다. 만약 우리의 모델이 꽤 괜찮은 모델이고, validation set이 잘 선택이 되었다면 training set에서만큼 validation set에서도 좋은 결과가 나올 수 있을 것이다.</p>


<p>보통 전체 데이터 중에서 training과 validation의 비율은 8:2로 하는 것이 일반적이다. 하지만 만약 validation set이 너무 작다면, 이 마저도 좋은 결과를 내기에는 부족할 수 있다. 이를 해결할 수 있는 컨셉 중에서 cross-validation이라는 컨셉이 있는데, validation set을 하나만 가지는 것이 아니라 여러개의 validation set을 정해놓고 각각의 set에 대해서 learning을 하는 것이다. 예를 들어 우리가 데이터가 X={1,2,3,4,5,6,7,8,9,10} 이 있을 때, 첫 번째 learning에서는 {1,..,8}을 사용해 learning하고 그 다음에는 {2,..,9}까지 learning하는 식으로 모든 permutation에 대해서 learning을 할 수 있을 것이다. 그리고 당연히 이런 방식으로 여러번 learning을 하게되면 그 때 얻어지는 model parameter는 그때그때 달라질텐데, cross-validation은 그 값들을 적당히 사용하여 가장 적절한 parameter를 얻어내는 방식이다. average로 해도 되고, median으로 해도 되고, 여러 방법이 있을 수 있다. cross-validation의 단점은 algorithm의 running time이 데이터의 크기 뿐 아니라 validation을 하기 위한 그 여러 set들에 dependent하다는 것이다. 그리고 데이터가 많아지면 그런 validation set이 엄청나게 많아진다. 정확히는 exponential로 늘어나기 때문에 마냥 모든 데이터에 대해 cross-validation을 하는 것은 불가능하다.</p>


<p>그렇기 때문에 실제로 cross-validation을 할때는 모든 데이터를 사용하지는 않고, 적당히 몇 개의 set을 골라서 여러 번 model parameter를 &#8216;적당히&#8217; 구하는 방법을 사용한다. 물론 이론적으로 AIC, BIC 등의 개념이 존재하여 이에 맞춰서 모델을 고르는 방법도 존재하지만 (AIC는 Akaike Information Criterion이고 BIC는 Bayesian Information Criterion으로, 둘 다 어떤 &#8216;information criteria&#8217;를 사용하느냐에 대한 내용이다.) 지금 내가 다루고자 하는 내용에서 좀 벗어나기 때문에 나중에 여유가 되면 이에 대한 글을 작성해보도록하겠다.</p>


<h5>Curse of dimension</h5>


<p>그러나 이게 끝이 아니다. 우리가 싸워 이겨내야할 문제들은 complexity, number of data 뿐 아니라 dimension of data 역시 존재한다. 즉, 우리가 1차원의 데이터를 다루는 것과 10000차원의 데이터를 다루는 것과는 정말 어마어마한 차이가 존재한다는 것이다. 이렇게 차원이 높은 데이터를 다룰 일이 있을까? 하고 약간 막연하게 생각할 수 있지만, 가장 간단한 예로, 100px by 100px 그림은 각각의 픽셀이 하나의 차원이라고 했을 때 10000차원 벡터로 표현이 가능하다. 실제로 머신러닝 분야에서 이미지를 다룰 때는 이런 식으로 처리를 하게 된다. 이 이외에도 high dimensioanl space 상에 존재하는 데이터를 다룰 일은 매우 많이 존재한다.</p>


<p>그렇다면 이런 high dimensional data가 왜 우리가 learning한 system의 성능을 나쁘게 만들까? 정말 간단하게 생각해보자. 만약 우리가 주어진 공간을 regular cell로 나눴다고 가정해보자. 그리고 각각의 cell에 가장 많이 존재하는 class를 그 cell의 class로 생각하여 무조건 그 cell에 존재하는 데이터는 그 class라고 하는 logic을 생각해보자. 당연히 cell의 개수를 무한하게 가져가게 된다면, 그리고 데이터가 무한하다면 이 logic은 반드시 truth로 수렴하게 될 것이다. 이 알고리듬이 제대로 동작하려면 각각의 cell, 혹은 bin이 반드시 차있어야한다. 즉, 비어있는 empty cell이 존재해서는 안된다. 따라서 데이터는 아무리 적어도 cell의 개수만큼은 존재해야한다. 그런데 이렇게 cell을 만들게 될 경우 그 cell의 개수는 dimension이 증가함에 따라 exponential하게 늘어나게 되는 것이다. 예를 들어 우리가 1차원상에서 3개의 bin을 가지고 있다고 하면, 이는 2차원상에서는 9개, 3차원상에서는 27개.. 이렇게 exponentially grow하게 되는 것을 알 수 있다. 이 모델의 parameter들이 exponentially 증가하는 것이다. (아래 슬라이드(<a href="http://cssanalytics.wordpress.com/2013/10/06/random-subspace-optimization-rso/" target="new">출처</a>) 참고)</p>


<p><img src="http://SanghyukChun.github.io/images/post/59-2.png" width="600"></p>

<p>따라서 당연히 각각의 bin이 비어있지 않도록 ensure해줄 수 있는 data의 개수 역시 exponentially 하게 늘어나게 되고, 즉 차원이 증가하게 되면 필요한 데이터가 exponentailly하게 늘어나게 된다는 것을 의미한다. 그러나 당연히 우리가 3차원 데이터보다 100000차원 데이터를 exponential하게 더 많이 가지고 있으리라는 법은 없고, 이로 인해 문제가 발생하게 되는 것이다.</p>


<p></p>

<p>그리고 또 문제가 되는 것은 high dimensional space에서 정의되는 metric들로, 특히 2-norm 혹은 euclidean distance의 경우는 그 왜곡이 매우 심하여, 실제 멀리 떨어진 데이터보다 별로 멀리 떨어져있지 않고 각 dimension의 방향으로 약간의 noise가 섞여있는 데이터에 더 큰 distance를 부여하는 등의 문제가 존재한다.</p>


<p>조금 다른 예를 들어보자. 만약 D-dimensional space에서 엄청나게 얇은 구각을 만들었다고 생각해보자. 여기에서 &#8216;구&#8217; 라는 것은 한 점에 대해 거리가 동일하게 떨어져있는 모든 점 내부의 영역을 의미한다는 것은 당연한 것이고.. 이 구의 부피는 \(V_D(r)=K_D r^D\) 가 될 것이며, \(K_D\)는 그냥 D에 대한 상수라고 생각하면 된다. 그럼 반지름이 1이고 두께가 \(\epsilon\)인 구각의 부피와 반지름이 1인 구의 부피의 비율은 \({V_D(1) - V_D(1-\epsilon) \over V_D(1)} = 1-(1-\epsilon)^D\) 가 될 것이다. 놀랍게도, 만약 very very high dimension D에서는 이 값이 1로 수렴하게 된다. 즉, 매우 얇은 구각의 부피가 구의 부피와 같다는 의미. 혹은 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 그 shell에 존재한다는 희안한 의미가 된다.</p>


<p>즉, high dimension은 (1) model의 complexity도 증가시키며 (2) 필요한 데이터의 양도 exponentially 하게 늘어나게 하고 (3) 우리가 기존에 사용하던 metric이 제대로 동작하지 않는 그야말로 끔찍한 환경이라 할 수 있다. 그래서 이런 high dimensional space에서 일어나는 여러 문제점들을 통틀어 Curse of dimension이라 한다.</p>


<p>이를 해결하기 위해서는 결국 feature extraction 등의 기술을 사용하여 dimension을 가장 적절하게 낮추는 것이 바람직하다고 할 수 있다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (2) Probability Theory]]></title>
    <link href="http://SanghyukChun.github.io/58/"/>
    <updated>2014-08-03T14:18:00+09:00</updated>
    <id>http://SanghyukChun.github.io/58</id>
		<content type="html"><![CDATA[<p>지난 글에서 Machine Learning을 일종의 function 처럼 묘사했었는데, 사실 Machine Learning을 Probability density의 관점에서 보는 것도 가능하다. 이 얘기를 하려면 먼저 Probability density를 포함한 전반적인 probability theory에 대해 먼저 다뤄야할 것 같아서 그 주제로 먼저 글을 써보기로 했다. 일단 엄청 기본적인 내용들, 예를 들어서 조건부 확률이 무엇이고, 이런 얘기들은 일단 생략을 하도록 하겠다. 너무 basic한 얘기이고 \(p(X) = \sum_Y p(X,y) \) 이런거나 \(p(X,Y) = p(Y|X) p(X) \) 이런건 너무나도 기초적인 얘기들이니까. 만약 조건부확률을 잘 모른다면 구글링을 통해 먼저 간단한 지식을 습득하고 오기를 권한다.</p>


<p>다만 그 중에서도 약간 중요하게 다룰만한 주제가 있는데, 그게 바로 Bayes&#8217; theorem이다. 이 Theorem 자체는 그냥 간단하게 \(p(X|Y) \)와 \(p(Y|X) \)와의 관계를 표현한 것임에 불과하지만, 그 안에 숨겨진 의미가 매우매우 중요하다.</p>


<h5 id="Bayes">Bayes&#8217; Theorem</h5>


<p>\[ p(Y|X) = {p(X|Y) p(Y) \over p(X)} \]</p>


<p>식의 모양은 위와 같다. 이 식이 중요한 이유는 무엇이냐면, 실제 우리가 관측하는 데이터와 실제 일어나는 현상과의 관계를 이어주는 연결고리가 되기 때문이다.</p>


<p>우리가 실제로 관측할 수 있는 데이터와 현상의 관계는 어떻게 되느냐 하면 &#8216;이런 현상이 나타났을 때 데이터의 분포&#8217; 를 보는 것이다. 무슨 얘기냐, 만약 데이터를 X, 현상을 Y라고 해보자. 그럼 우리가 당연히 얻고 싶은 것은 주어진 데이터 X에 대한 현상 Y일 것이다. 즉 \( p(Y|X) \) 를 우리는 실제로 계산하고 싶은 것이다. 그런데 우리가 확인할 수 있는 데이터는 무엇이냐 하면, 어떤 주어진 현상 Y에 대해 존재하는 데이터 X들의 분포 즉, \( p(X|Y) \) 만을 관측할 수 있다. 예를 들어보자. 만약 어떤 질병에 대한 검사를 하는 상황이라고 가정해보자. 우리가 확인할 수 있는 것은 해당 검사에 대해 양성 판정을 받았는지 아닌지 밖에 확인할 수 없다. 만약 이 검사가 완벽하지 않다면 (즉, 정확도가 90% 정도라면) 실제 우리가 관측하는 검사 결과와 그 사람의 질병 보유 상황이 다를 수 있는 것이다. 즉, 검사 결과를 X라고 하고 실제 병에 걸렸는지 아닌지를 Y라고 한다면 우리가 최종적으로 확인하고 싶은 것은 \( p(Y|X) \), 즉 검사 결과를 보고 이 사람이 진짜 질병을 가질 확률을 알고 싶은 것이지만, 실제 우리가 확인할 수 있는 데이터는 \( p(X|Y) \), 즉 이 사람이 실제 병에 걸렸을 때 제대로 검사가 될 확률 (아까 90%라고 했었던) 뿐이 가지고 있지 않다. 더 자세한 것은 <a href="http://musicetc.wikidot.com/bayes-theorem#toc3" target="new">링크</a>를 참조하길 바란다.</p>


<p>중요한 것은 무엇이냐 하면, 진짜 우리가 관측할 수 있는 데이터만 가지고는 우리가 원하는 추론을 하는 것이 어렵다는 점이다. 이때, 주어진 현상에 대해 나타나는 데이터의 분포 \( p(X|Y) \), 즉 우리의 observation을 일컬어 Likelihood라고 한다. 만약 우리가 아무런 정보도 가지고 있는 상황이 아니라면 언제나 이 값을 최대로 만드는 작업을 통해 가장 optimized된 현상을 찾을 수 있는데 이를 maximum likelihood라 한다. (<a href="http://en.wikipedia.org/wiki/Maximum_likelihood" target="new">위키</a>) 즉, 우리가 관측한 정보만을 가지고, 그 정보가 전부라고 가정한 이후에 그 안에서 모든 optimization을 거쳐 가장 좋은 something을 얻어내는 과정이라 할 수 있다. 이를 Maximum Likelihood Estimation 혹은 MLE라 한다. 이 녀석은 Machine learning을 하면서 정말 많이 나오는 용어이고, 실제 이 방법을 사용해 풀어내는 문제들이 많기 때문에 반드시 숙지해야하는 개념이다.</p>


<p>그런데, 앞서 설명했던 예시와 같이, 항상 MLE가 능사는 아니다. 극단적으로 생각해서, 만약 우리가 동전 던지기를 해서 10번 던져서 10번 tail이 나오면 &#8216;이 동전은 tail이 100%로 나오는 동전이다&#8217; 라고 예측하는 것이 바로 MLE인 것이다. 이 방법이 잘 될 떄도 많지만, 방금처럼 데이터가 부족한 경우 등에는 좋은 결과를 얻지 못할 수 있다. 만약 우리가 &#8216;동전 던지기를 하면 head, tail이 50:50 으로 나온다.&#8217; 라는 것을 알고 있다면 조금 더 나은 추론을 하는 것이 가능하지 않을까? 이런 생각에서 나오는 것이 바로 Maximize a posterior, 혹은 MAP이다. Posterior는 앞에서 설명했던 주어진 데이터에 대한 현상의 확률, 즉 \( p(Y|X) \)이다. 간단히 생각해서 Likelihood는 내가 본 데이터에 대한 관측값 만을 의미하는 것이고, Posterior는 관측값과 다른 결과들을 조합하여 나온 조금 더 추론하기에 알맞은 형태? 라고 보면 될 것 같다.</p>


<p>앞서 Bayes&#8217; theorem에서 계산했듯, Observation, 혹은 Likelihood를 알고 있을 때 Posterior를 계산하기 위해서는 \(p(X), p(Y)\)가 필요하다. 이때 \(p(Y)\)는 어떤 현상에 대한 사전 정보이다. 즉, 아까 동전 던지기에서 동전을 던졌을 때 head tail이 나올 확률이 0.5라는 것에 대한 사전 정보이다. 이를 prior 라고 한다. 만약 이 값을 알고 있다면 observation이 잘못되어도 이 값이 약간의 보정을 해주는 역할을 할 수 있게 되는 것이다. 그리고 여기에서 데이터의 분포 \(p(X)\)는 일종의 normalization 을 해주는 역할을 하며, 실제 모든 데이터에 대해 \(p(X|Y) p(X)\) 를 계산한 뒤 그 값들을 noralization하는 것과 동일한 효과이기 때문에 이 값에 대해 알 필요는 없다.</p>


<p>정리하자면, Bayes&#8217; theorem은 observation(likelihood), 현상에 대한 사전정보 (prior), 주어진 데이터에 대한 현상의 확률 (posterior) 의 관계를 define하는 중요한 역할을 한다고 할 수 있겠다.</p>




<h5>Probability densities</h5>


<p>Probability density, 우리 말로 하면 확률밀도가 되겠다. 간단히 생각하면 주어진 domain에 대해 확률이 어떻게 분포하고 있는지를 나타내는 일종의 function이라 할 수 있겠다. 아마 이것도 고등학교 수학시간에 배우는 것으로 기억하는데.. 그만큼 아주 간단한 개념이다. 자세한건 위키를 <a href="http://en.wikipedia.org/wiki/Probability_density_function" target="new">참고</a>하면 될 것 같다. 그럼 이게 실제로 어떤 의미가 있으며 맨 처음에 probability density 관점에서 machine learning을 볼 수 있다는 것의 의미는 무엇인가?</p>


<p>Probaiblity density라는 녀석은 결국 주어진 데이터들이 어떤 방식으로, 어떤 확률로 분포해있는지를 알려주는 녀석이라 할 수 있다. 예를 들어보자. 만약 우리가 스팸필터를 만들었는데 &#8216;광고&#8217; 라는 단어가 포함이 되면 해당 메일이 스팸일 확률이 80%이고, &#8216;구매&#8217; 라는 단어가 포함이 되면 90%, &#8216;Machine Learning&#8217; 이라는 단어가 포함되면 스팸일 확률이 10%.. 이런식으로 모든 단어, 모든 domain에 대해 스팸일 확률을 알고 있다면, 혹은 그런 probability density를 찾을 수 있다면, 더 정확히 말하면 probability density function을 찾아낼 수 있다면 우리는 매우 좋은 inference를 할 수 있게 될 것이다.</p>


<p>이제 Machine Learning과의 연계를 지어보자. 어떤 우리가 모르는 probability density function을 가지는 데이터들에 대해, 그 데이터들을 사용해 probability density function을 찾아내는, estimate하는 과정을 일컬어 Density estimation이라 부른다. 이전에는 데이터들을 통해 &#8216;함수&#8217;를 찾아낸 것이고, 지금은 그 함수가 density function이라는 점이 다른 점이다.</p>




<p>Bayes&#8217; theorem 이나 probability density 등 이외에도 probability theory 쪽에서 언급해야할 얘기들이 없는 것은 아니다. 예를 들어서 expectation이라거나.. 하지만 내 생각에 이번 글에서 언급한 두 개는 약간 기본적인 probability theory와는 다르게 조금 머신러닝적인 insight가 필요한 부분이 아닐까해서 조금 강조해서 언급해보았다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (1) Machine Learning이란?]]></title>
    <link href="http://SanghyukChun.github.io/57/"/>
    <updated>2014-08-02T18:48:00+09:00</updated>
    <id>http://SanghyukChun.github.io/57</id>
		<content type="html"><![CDATA[<p>Machine learning이라는 것을 접한지 어느새 거의 1년 반이 넘는 시간이 지났다. 계속 여러 종류의 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning/" target="new">Machine learning과 관련된 글들</a>을 써왔지만, 항상 중구난방이고 제대로 정리가 되지 않은 느낌을 받아서 근래에 다시 머신러닝에 대해 공부를 하는 김에 싹 몰아서 정리해보기로 했다. 8월에 연구실에서 머신러닝 스터디를 하기로 했지만, 그것과는 조금 다르게 내가 생각했을 때 이런 순서로 정리를 하면 되겠다.. 하는 느낌으로 정리를 해볼 생각이다. Bishop 책을 많이 참고했으면 하고.. 특히 이번 ICML에서 느낀거지만, Machine Learning에는 정말 많은 분야가 있는데, 많은 Lecture들에서 어쩔 수 없이 그것들을 전부 커버하지 못하는 점이 너무 아쉬웠다. 그래서 일단 내가 할 수 있는데까지는 정리해보는게 좋지 않을까.. 하는 생각으로 글을 써보려 한다. 이런 글을 써야겠다는 생각은 오래전부터 했는데 정작 실천에 옮기는데에는 시간이 많이 걸렸네.. 그래 무튼 시작해보자.</p>


<p>Machine Learning&#8230; 혹은 우리 말로 하면 기계 학습. 이놈이 대체 뭐길래 너도나도 머신러닝 머신러닝 해대는 걸까. 내가 예전 <a href="http://SanghyukChun.github.io/3" target="new">Andrew Ng. 교수의 Machine Learning Lecture를 들으며 정리했던 글</a>에 정의했던바에 따르면, </p>


<blockquote>
    <p>머신러닝은 그 관계를 알 수 없는 수 많은 데이터들 사이에서 관계를 찾아내주는 마법과 같은 기술이다.</p>
</blockquote>


<p>라고 정의했었다. 마법과 같은 기술이라! 어디서 저런 중2병 스멜나는 용어를 가져왔는지 참.. 아무튼 내가 이쪽을 더 공부해보면서 느낀건, 결국에 이 Machine Learning이라는 것은 일종의 Modeling Problem이라는 것이다. 무슨 얘기냐? 머신러닝이란 <b>주어진 데이터</b>에 대해 <b>현상</b>을 <b>가장 잘 설명할 수 있는 관계</b>를 찾아내는 것이라 했었는데, 이 말을 조금 수학적으로 풀어내면 주어진 <b>데이터</b> \(X = (x_1, x_2, x_3, \ldots, x_n)\) 이 있을 때 이 데이터와 실제 <b>현상</b> \(Y = (y_1, y_2, \ldots, y_n) \)에 대한 <b>관계</b> function \(f\)를 찾아야한다. 당연히 우리가 정확한 함수 \(f\)를 찾아낼 수는 없으므로 <b>최대한 잘 설명할 수 있는</b>, 함수 \(f&#8217;\)을 찾아내야 한다. 이를 Hypothesis라고 한다. 즉, 이 모든 과정을 일종의 Modeling process로 생각이 가능하다는 뜻이다. 다만 우리의 새로운 process는 for given data에 대해 dependent한 model을 만들어낸다는 점이 독특한 것이다.</p>


<p>스팸필터를 생각해보자. 데이터 X는 메일들이다. 우리가 받는 메일 하나하나가 데이터가 될 수도 있고, 그 메일 안에 있는 단어 하나하나가 될 수도, 혹은 아예 알파벳 하나하나가 될 수도 있다. 그것은 우리가 정하기 나름이니까. 그럼 현상 Y는 무엇일까? 각각의 메일이 스팸인지, 아니면 일반 메일인지 구분하는 구분자, indicator, 혹은 Label, Class가 될 것이다. 마지막으로 가장 잘 설명할 수 있는 함수는 이런 데이터들을 가지고 어떻게 Learning을 할 것인지에 대한 얘기가 될 것이다. 예를 들어 Naïve Bayes라던가, KNN, SVM도 있고, 요새 핫한 Deep learning도 있을 수 있다.</p>


<p>그러면 이 함수로 우리는 무엇을 하고 싶은걸까? 단순히 데이터와 현상의 관계를 함수로 나타내는 것이 무슨 의미가 있지? 이것이 의미가 있는 이유는 우리가 새로 주어진 데이터, 혹은 test data에 대해 새로운 추론, inference를 하는 것이 가능하기 때문이다. 이 메일은 스팸인가? 이런 쇼핑 목록을 가진 사람은 또 뭘 사고 싶어할까? 이런 날씨에는 고속도로가 막힐까 막히지 않을까? 이런 질문들에 대해 앞서 우리가 정의한 방식대로 Machine Learning problem을 풀어 새로운 Hypothesis를 통해 우리는 inference를 내릴 수가 있다. 이런 inference가 맞을 수도 있고 틀릴 수도 있다. 그러나 우리는 최대한 좋은 방법으로 Learning을 한다면 좋은 inference를 할 수 있을 것이라고 생각할 수 있다. 이 &#8216;좋은 방법&#8217; 을 위해 도입되는 개념이 바로 cost function이다. 내가 원하는 결과와 실제 내가 찾아낸 가설과의 괴리가 얼마나 있는지를 function으로 정의하는 것이다. 가장 간단한 예로, 스팸 필터에서 &#8216;스팸이다 아니다&#8217;라는 질문이 틀리면 cost function은 1, 맞으면 0이라고 하자. 그렇게 정의하면 이 cost를 최소화 하는 방향으로 model을 learning하면 결국 가장 잘 맞추는 model을 얻을 수 있겠지.</p>


<p>즉, 머신러닝이란, 완전히 raw한 데이터를 다루는 과정, 예를 들면 Dimensionality reduction, Metric Learning 등의 과정에서부터 시작해서, 어떻게 Learning할 것이냐, model이 무엇이냐, 조금 더 구체적으로 얘기하면 어떤 Algorithm을 사용해 이 model을 learning할 것이냐라는 문제를 풀어내고, 마지막으로 어떻게 결정을 내릴 것이냐, decision making을 할 것이냐 하는 이 여러개의 과정으로 나뉘어지는 것이다. 내가 관심있는 부분은 주로 어떤 model을 사용할 것이냐, 또 어떤 방법으로, 어떤 algorithm으로 이 model을 learning할 것이냐에 관심이 있다. 또 추가로 관심이 있는 부분이라면, 어떤 문제가 있을 때 그 문제를 Machine Learning의 관점에서 풀어내는 것도 좋아하고.. 예를 들면 추천 알고리듬 같은게 있겠다.</p>


<p>사실 이렇게 머신러닝을 정의하기에는 조금 어정쩡한 부분이 있는 것이 사실이다. 그럼 Unsupervised Learning은?? Reinforcement Learning은? 그 이외에도 많은 것들을 이 정의만 가지고 설명하기에는 사실 무리가 있는 것이 맞지만, 그래도 이런 컨셉으로 이해를 하게 된다면, 즉 Machine Learning이 근본적으로 model, algorithm 문제라는 것을 이해하게 된다면 이 녀석들에 대한 이해도 빠르게 할 수 있지 않을까 생각된다. 위에 링크 건 <a href="http://SanghyukChun.github.io/3" target="new">예전 글</a>에 이와 관련된 글들이 있으니 궁금하면 읽어보면 될 것 같다.마찬가지 이유로, 요즘 유행하는 Big Data 역시 근본적으로는 Machine Learning의 일부라는 것을 알 수 있다. <a href="http://SanghyukChun.github.io/21" target="new">이에 대한 글</a>도 이전에 적은 적이 있으니 참고바란다. 달라지는 점이라면 algorithm이 굉장히 빨라야 한다하거나, Memory efficient 해야한다거나, 아니면 분산처리가 가능해야한다거나 하는 새로운 challenge들이 존재하기는 하지만, 근본적으로는 같다고 봐도 무방하다. 즉, 이 머신러닝이라는 것을 제대로 이해하고 다룰 수 있다면 정말 많은 문제들을 해결할 수 있다는 의미가 될 것 이다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2014 ICML 후기]]></title>
    <link href="http://SanghyukChun.github.io/55/"/>
    <updated>2014-06-26T16:16:00+09:00</updated>
    <id>http://SanghyukChun.github.io/55</id>
		<content type="html"><![CDATA[<p>이번에 <a href="icml.cc/2014/" target="new">ICML 2014</a>를 다녀왔다. 내 첫 해외 학회이기도 했고, 처음으로 Machine Learning과 관련된 연구를 하는 사람들의 talk을 듣고, 그 사람들이 직접 하는 일들을 많이 볼 수 있어서 개인적으로 많이 고무된 상태로 학회에 참여했었다. 첫 학회를 다녀온 기념으로 학회에서 내가 느낀 점들을 간단하게 정리해보고자 한다. 대충 보자면 아래 리스트 정도가 될 것 같다.</p>


<ul>
<li>항상 열정을 가지고 있고 열심인 사람들</li>
<li>어떤 식으로 연구를 해야겠다라는 생각</li>
<li>Machine Learning이라는 학문의 방대함</li>
<li>Neural Network와 Deep Learnig의 강세</li>
<li>Real Industry와 Machine Learning</li>
</ul>


<p>먼저 사람들. 개인적으로 내 자신에게 살짝 아쉬운 점이라면 학회에서 만난 사람들에게 먼저 다가가서 이야기를 걸거나 할 베짱이 많이 없었다는 것이다. 사실 내가 그렇게 사람들에게 먼저 다가가고 얘기를 나누기에는 내가 알고 있는 지식이 많이 부족함에도 원인이 있기는 하지만 그래도 다들 같은 분야에 관심을 가지고 (세부 관심사는 조금씩 다를 수 있지만) 나와 비슷한 입장을 가진 사람들도 있을 수 있었을텐데 내가 조금 적극적이지 못했던 부분이 있다. 학기 중에 윤준보교수님 수업에서 학회를 가면 그 사람들과 이야기를 많이 나누어보라는 얘기를 해주셨는데 막상 내게 그런 기회가 생기니 할 수 있는 말이 많이 없더라. 조금 더 정진해서 그런 대화에 두려움이 없을 정도의 지식을 쌓아야할텐데. 그런 개인적인 아쉬움은 잠시 넘겨서 생각을 해보자면, ICML에서 만난 사람들은 정말 열정있는 사람들이었다. 고작 15분에서 20분짜리 talk조차 이해하지 못하고 허덕이고 있는 나와는 다르게, 정말 발표자의 talk을 이해하고 그들과 communication하면서 질문하는 모습이 멋져보였다. 1시간 가까이 되는 invited talk이나 key note talk에서도 많은 부분을 놓치지 않으려 노력하는 모습도 멋졌다고 생각한다. 나도 그런 멋지다고 생각한 모습에 한 단계 더 다가가야할텐데.</p>


<p>그리고 그 다음은 역시 연구였다. 내가 대학원생이 되었고, 연구가 나에게 가장 중요한 비중을 차지하게 된 이상, 어떤 연구를 할 것이며 어떻게 연구를 할 것이며.. 혹은 왜 연구를 해야하는 것이냐 등의 물음은 나에게 굉장히 중요한 물음이다. 약간 어느 정도는 간접적으로 그 질문들에 대한 답을 얻을 수 있었던 것 같은데, 어떤 공명감으로 연구를 한다는 느낌보다는 이 문제를 풀어야하겠다는 그런 근본적인 호기심? 같은게 영향을 미치는게 아닌가 싶다. 사실 명확하게 이거다! 싶은 느낌은 잘 들지 않았지만 앞서 말했듯이 다들 열정적으로 임하고 질문 하나하나가 날카롭게 들어가는 모습을 보면서 이런 학회에 publish를 하는 사람들은 어떤 생각으로 다른 사람들의 talk을 듣는지 조금이나마 간접적으로라도 체험할 수 있지 않았나 싶다. 아무튼 진짜 열정적으로 해야한다. 그게 진짜 큰 것 같다.</p>


<p>또 학회에서 놀라웠던 점이라면 Machine Learning이라는 분야 자체가 생각보다도 훨씬 더 방대했다는 점이다. 전체 Track이 6개가 parallel 하게 돌아가면서 전체 다 합쳐서 거의 300개 가까이 되는 talk이 진행이 됐으니까.. (<a href="http://icml.cc/2014/index/article/12.htm" target="new">스케쥴</a>) 단순히 accept된 paper만 많은 것이 아니라 각 track의 주제 또한 너무나도 다양하였다. Networks and Graph-Based Learning, Reinforcement Learning, Bayesian Optimization and Gaussian Processes, Supervised Learning, Neural Networks and Deep Learning, Graphical Models, Bandits, Monte Carlo, Statistical Methods, Structured Prediction, Deep Learning and Vision, Matrix Completion and Graphs, Learning Theory, Clustering and Nonparametrics, Active Learning, Optimization, Large-Scale Learning, Latent Variable Models, Online Learning and Planning, Clustering, Metric Learning and Feature Selection, Optimization, Neural Language and Speech, Graphical Models and Approximate Inference, Online Learning, Monte Carlo and Approximate Inference, Method-Of-Moments and Spectral Methods, Boosting and Ensemble Methods, Matrix Factorization, Nonparametric Bayes, Manifolds, Kernel Methods, Unsupervised Learning and Detection, Crowd-Sourcing, Manifolds and Graphs, Regularization and Lasso, Nearest-Neighbors and Large-Scale Learning, Topic Models, Sparsity, Neural Theory and Spectral Methods, Features and Feature Selection, Time Series and Sequences&#8230;. 와 진짜 많다. 물론 이 전체를 또 잘 묶으면 더 줄어들 수 있겠지만 그래도 일단 각각의 Track들이 서로 다른 주제를 가지고 이렇게 많이 있다는 사실이 놀라웠다. 글쎄, 그래도 굳이 크게 나누자면, (1) Learning for Graphical Model (2) Traditional Machine Learning Problems (Bayesian, Supervised Learning&#8230;), (3) Optimization (4) Monte Carlo (5) Unsupervised Learning (Clustering, Metric Learning&#8230;) (6) Neural Network (7) Others 정도가 아닐까. 모르겠다 너무 많고 내가 모르는 분야가 너무 많아서. 아무튼 정말 Machine Learning이 어마어마하게 큰 분야라는 것을 다시 한 번 느끼게 되었다. 나는 저 많은 Track 중에서 어느 분야에 기여를 할 수 있을까?</p>


<p>꼭 그런건 아니었지만, 전반적으로 Deep learning 과 관련된 talk들. 심지어 &#8216;Deep&#8217; 이라는 이름이 들어가기만 해도 컨퍼런스 룸이 터질듯한 것을 볼 수 있었다. 정말 요즘 이게 핫하긴 핫하다. 근데 난 이상하게 정말 Deep learning이 싫은데.. 이유를 잘 모르겠다. 가장 practical하게 powerful해서 그렇겠지? 중국에서 해서 그런지는 모르겠지만 Deep learning세션은 중국인들이 바글바글 몰려서 진짜 산만했었다. 그만큼 가장 핫하다는 뜻이고, 중국인들이 이런 실용적인 것들에 무지 관심이 많다는 것을 느꼈다. 이론쪽보다는 확실히 그런 practical 한 세션에 중국인들이 압도적으로 많았다. Deep learning 관련 시스템 쪽도 사람 엄청 많았고.. 진짜 그야말로 Deep Learning의 시대라고 봐도 무방할 정도. 대단하더라.</p>


<p>마찬가지 맥락에서, 굉장히 많은 기업들이 ICML을 찾았다. 구글, Facebook, 아마존, MS, 야후 같은 글로벌 기업은 물론이고 바이두, 알리바바 같은 중국 기업들도 엄청 많았다. 그만큼 머신러닝을 전공한 사람들의 힘이 필요하다는, 그런 사람들에 대한 수요가 확실하구나.. 라는 그런 생각이 들더라. 기업 연구소 특히 MS나 구글 연구소 등에서도 많은 논문들이 나오는걸 보고, 저런 연구소에서 일하는 것도 생각보다는 나쁘지 않을 수도 있다는 그런 생각도 들고 그랬다.</p>


<p>ICML에서 여러모로 많은 자극을 받았다. 재미도 있었고. 내년 ICML은 내가 intivation 되서 갔으면 좋겠다! 나도 좋은 논문을 쓸 수 있었으면.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[나의 창작욕에 대하여]]></title>
    <link href="http://SanghyukChun.github.io/53/"/>
    <updated>2014-06-14T20:22:00+09:00</updated>
    <id>http://SanghyukChun.github.io/53</id>
		<content type="html"><![CDATA[<p>무지막지하게 오래간만에 블로그에 글을 써본다. 시험 기간이라 그런가, 평소에는 어떤 글들을 쓸지 생각만 잔뜩해두고 있다가 시간이 부족하거나 아니면 내가 해야겠다는 생각을 못하거나 해서 글을 못쓰는 경우가 엄청 많았는데, 이 글은 뭔가 지금 쓰고 싶다는 생각이 들었다. 어쩌면 지금 이 글도 내가 지금 시험 공부에 집중을 못하고 내면에 숨겨진 창작에 대한 욕구가 불타오르는 것을 우회적으로 억누르기 위해서 이 글을 쓰는 것일 수도 있다.</p>


<p></p>

<p>생각해보면, 난 어렸을 때 부터 무언가 &#8216;만드는 것&#8217;에 대한 욕구가 굉장히 많았다. 그림을 그리는 것이라거나, 색종이를 오려 붙여서 무언가 새로운 것을 만들거나.. 그래서 내가 엄청나게 많이 봤던 프로그램 중 하나가 EBS의 만들어보아요.. 혹은 참 쉽죠? 로 알려진 밥로스의 그림 교실.. 물론 밥로스의 그림 교실은 아무리 따라해도 따라할 수 없었지만 ㅋㅋㅋㅋ 내가 유치원 혹은 끽해야 초등학교 저학년 시절에 만들었던 것 중에서 가장 기억에 나는 것 중 하나는 신문지로 만든 집.. 집이라기는 그렇고, 그냥 신문지를 둘둘말아 기둥을 만들어서 일종의 직육면체 모양의 미로? 비슷한걸 만드는 거였다. 이런걸로 꽤 집에서 피곤할 만한 짓들을 많이했었지.</p>


<p>뭐, 그래서 어렸을 때는 사실 미술 쪽으로 진학을 할 생각도 많았다. 특히 디자인 쪽으로 해보고 싶은 생각이 굉장히 강했는데, 아마 내가 디자인 쪽으로 갔으면 캐리커쳐나 광고 등등 무언가 익살스럽게 만들 수 있는 쪽으로 가지 않았을까.. 아 물론 만약 디자인을 골랐다면 전공은 분명히 평면 디자인으로 갔을 것 같다. 아니면 만화라거나. 하지만 내가 미술에 재능이 아주 뛰어나게 부각되었던 것도 아니고, 사실 내가 부각을 보인 쪽은 수학, 과학 분야 혹은 사회 쪽이었기 때문에 나는 자연스럽게 공부 잘하는 학생들이 걷는 길을 걷게 되었다. 물론 그런 길을 걷게 된 계기는 내가 뚜렷한 목표가 있었기 때문이기도 했었지만.</p>


<p>내가 이미 굉장히 어렸을 때 부터 나는 &#8216;기업가&#8217;가 되고 싶었다. Entrepreneurship에서 말하는 그런 기업가. 즉, 창업가가 되고 싶었다. 정주영 회장과 유한일 회장의 일화를 읽고 또 읽으면서 그런 빛나고 또 당당한 사람이 되기를 원했고, 내가 재미있는 분야에서 그런 것들을 해보고 싶었다. 내가 당시에 꽂혀있던건 다양한 종류의 전자기기들. 컴퓨터나 휴대용 음악 재생 기기 등에 많이 꽂혀있었고, 나는 그런 것들을 만들고 싶고, 그런 것들로 성공하고 싶다는 생각을 했었다. 그리고 창업으로 성공한 많은 사람들이 공대 출신이었기에, 나는 우리나라에서 가장 좋은 공대라고 생각되었던 카이스트에 진학하고 싶다는 생각을 이미 초등학생 때, 믿기지 않겠지만 하고 있었다. 그게 무슨 의미를 뜻하는지도 모르는 채로 그런 생각을 하고 있었다는게 용하다고 해야할지 멍청했었다고 해야할지..</p>


<p>아무튼 나는 그렇게 수학 과학의 길을 걷게 되었고, 중고등학교 공부가 그렇듯 내가 할 수 있는 창조적인 일이라는건 존재하지 않았다. 내가 당시 재미있게 만들 수 있었던 것들은.. 글쎄 필기 노트라거나 음악 가사 적는거? 생각해보면 내가 적었던 가사들의 순수한 양만 따져본다면 그 시기에 가장 많은 가사를 적었던 것 같다. 어찌보면 글 쓰는 재미를 그때부터 알게 된 것일지도 모르고. 중학교 시절 동안 내가 할 수 있는건 거의 없었지. 그냥 내가 &#8216;만들 수 있는&#8217; 것들은 모나미 볼펜에다가 제도 샤프 넣어서 볼펜 샤프 만드는거 정도? 한 3개 정도 만들었던 것 같다.</p>


<p>대학교에 진학했다. 내가 가고 싶다고 생각했던 카이스트로. 아, 물론 고등학교 가서는 카이스트가 아니라 서울대를 가고 싶었지만 난 성적이 모자랐었고, 지금 보면 뭐 서울대보다는 지금 여기가 더 많은 가치를 안겨주었다고 생각이 든다. 세상은 언제나 어떻게 흐를지 모르는거라니깐.. 대학교에 와서 나에게 창조적인 행위를 하고자 하는 욕구를 분출 할 수 있었던 출구는 보고서나 발표 수업이었던 것 같다. 보고서 하나 쓰는 데에도 남들은 그냥 배끼고 별로 의미를 부여하지 않는 것들에도 나는 그 하나를 정교하게 만들기 위해서 많이 노력하고 엄청나게 많은 내가 아는 지식들을, 모른다면 그 지식들을 배워서라도 채워넣으려고 했었다. 그 결과 나는 물리 실험에서 A+를 받았고.. 그 보고서들은 지금 읽어봐도 진짜 열심히 했다는게 느껴진다. 뭔가 짠하네.. 그리고 또 하나의 분출구가 PPT. 디자인을 하고 싶었던 욕망이 PPT로 많이 분출된 것 같다. 어찌보면 내가 하고 싶었던 것은 스티브 잡스이지, 워즈니악이 아니었는데 나는 잡스를 꿈꾸면서 워즈니악이 되는 커리큘럼을 밟는다는 것이 큰 스트레스였다.</p>


<p>그러나 결국 그 창작욕들이 가장 크게 분출된 곳은 개발이 아니었을까. 4학년이 끝나고, 컨설팅에 대한 회의를 느끼고, 동시에 연구 역시 내 길이 아니라고 생각하고 있을 때 쯤, 이음에서 인턴을 할 기회가 생겼다. 물론 힘든 일도 많기는 했지만 결론적으로 말하자면 엄청나게 많은 것을 배운 것 같다. 그 중에서 가장 크게 배운 것은 내가 무엇을 할 수 있고 내가 무엇에 재미를 느끼고 내가 해야할 것이 무엇인지 배웠다는 것. 엄밀히 말하면 배운 것은 아니고 깨달은 것이라는게 맞을 것 같다. 누가 알려준 것이 아니라 내가 일을 하면서 그런 것들에 대해 진지하게 생각해보고 결론을 내릴 수 있었던 것이니까. 결국 내가 할 수 있는 것들 중에서 가장 내가 재미있게 할 수 있는 것은 무언가를 만드는 것이었다. 만드는 것. 얼마나 창조적인 단어인가. 근래에 정부가 계속 창조경제 창조경제 하지만, 그 의미가 무엇이건, 혹은 내가 그 정책에 대한 우호도가 어떻던지간에 분명 창조경제라고 일컬어지는 것의 근간은 만드는 것이다. 새로운 것을 만들어내고 그 안에서 새로운 가치를 창조해내는 것. 그리고 단순히 만들기만 하는 것은 재미가 없다. 남들이 만들었던 것을 그대로 다시 만든다거나 혹은 예전에 만들었던 것들을 그대로 다시 만드는 것은 재미가 없다. 그 동안 그 누구도 해보지 못했던 새로운 것을 만드는 것이 재미있는 것이지. 그리고 놀랍게도, 소프트웨어 영역에서 무언가를 만드는 일은 그 동안 해왔던 것을 반복하는 것도 아니며, 단순히 정해진 언어에 맞추어 내가 내 논리를 풀어내며 말을 하는 것에 불과했었다. 그래서 프로그래밍이 논리의 영역이라고 하는 사람들도 있지만, 그 논리가 어떤 것에 대한 논리인가를 생각해내는 것도 엄청 중요하다. 그런 것들이 내 안의 어떤 창조에 대한 욕구를 자극했고, 나는 개발에 내 창작열을 마구 불태웠다.</p>


<p>회사에서 누가 시킨 것도 아닌데 새로 통계 프로그램을 뜯어 고치고, 매번 말썽을 부리던 시스템을 뜯어고치고 내가 새로 어드민 페이지를 만들어내고.. 테스트 페이지가 필요하다해서 다른 페이지들에서 영감을 얻어서 새로운 테스트페이지를 만들어내고.. 사실 대단한 일들은 아니었다. 하지만 시키지 않은 일을 했다는 것에 대해서, 그리고 그 안에서나마 잠시라도 기획서나 남들이 시키는 일에서 해방될 수 있다는 점에서 참 행복했었다. 멍청하게 그때 그 화면들을 캡쳐를 해놨어야했는데 하나도 캡쳐를 안해놔서 다시 보고 싶어도 볼 수가 없다.. 물론 reproduce는 무지 빨리 할 수 있겠지만. 아무튼 그렇게 이음을 그만두고 나서도 나는 그런 무언가를 만드는 혹은 개발하는 일을 꾸준히 했던 것 같다. 그 중 하나가 바로 이 블로그였고, <a href="http://sanghyuk.kaist.ac.kr/aboutMe/" taget="new">aboutMe</a>였고, 그 이외에도 만들었던 수 많은 페이지들이 그런 것들을 뿜어내는 역할을 했던 것 같다.</p>


<p>사실 이 글을 쓰게 된 이유는 연구실 쇼파에 누워서 쉬던 중 갑자기 쇼파있는 쪽에 간단한 장판이나 매트를 깔면 엄청나게 좋은 휴식 공간이 될 것 같다는 아이디어가 불현듯 떠올랐었기 때문이다. 내 창작욕을 가장 크게 분출했던 곳 중 하나가 바로 연구실 디자인이었는데, 이미 완성이 되어있는 공간을 어떻게 활용해야 가장 아름답고 효율적이고 좋게 만들 수 있을까에 대해 정말 많이 고민했었고 고민 끝에 지금과 같은 내가 생각해도 참 만족스러운 배치가 나왔다. 물론 아직 내가 생각했었던 것들이 완전히 구현된 것은 아니다. 아직도 들어와야 할 가구나 비품들이 더 있지만 조금만 더 기다리고 방학 때 마무리를 할 생각이다. 아마 방학이 되면 내 창작욕을 불태울 새로운 장난감, 서버가 생길테니까 그런 일에 많이 시간을 못 쓸 수도 있겠지만.. ㅋㅋ</p>


<p>그러니깐 내가 하고 싶은 말은 어린 시절 불타오르는 창작에 대한 욕구를 조별과제나 보고서 같은 의외로 학구적인 일들에 쏟아왔었고, 그리고 고학년이 되어서는 개발을 하는 것에 그 욕구를 해소하고 남는 시간에는 연구실을 꾸미면서 그런 욕구를 해소했던 것 같다. 요리라거나 인테리어에 관심이 많았던 것도 어떻게보면 같은 맥락인 것 같고. 그래서 학기 중에 K가 나에게 새로운 것을 만드는 일을 같이 해보지 않겠냐고 물어봤을 때 굉장히 가슴이 뛰고 당장이라도 무언가를 만들어보고 싶은 욕구에 사로잡혀 고생을 조금 했었던 것 이기도 하고.. 근래 관심이 생기는 것들은 셀프 인테리어라거나 DIY 쪽이다. 사실 이 쪽에 관심이 없었던 것은 아니지만 내가 이런 것들을 할 수 있는 공간이 거의 없었지만, 이제 연구실이 생겼고, 심지어 연구실도 새 연구실이다! 문자 그래도 내가 하자는 대로 모든 것들이 이뤄지는 셈. 심지어 교수님께서도 협조적이시다. 나는 이런 것들을 하는 것이 재미있고 더 좋은 공간을 만드는 것에 엄청나게 관심이 있다보니 그런 것들이 시너지를 일으켜서 점점 더 괜찮은 공간을 만들어가고 있다는 그런 생각도 든다. 그런 것들의 공통점은 새로운 것이라는 점, 그리고 또 어떻게 보면 내가 살고 있는 공간을 바꾼다는 점이 아닐까 싶다. 그래서 내가 갑자기 배워보고 싶다는 생각이 든 것이 목공이다. 생각해보면 내가 목공을 잘 했다면 혹은 조금이라도 알았더라면 많은 것들을 만들 수 있었을 것 같다. 특히 그냥 간단한 선반이나 아니면 박스형 수납장, 심지어 내가 지금 쓰고 있는 해피해킹의 팜레스트 정도는 만들 수 있지 않을까? 팜레스트는 사실 별로 필요없기는 한데.. 아무튼 그런 새로운 것들을 만드는 작업들을 배워보고 싶다는 그런 생각이 들었다.</p>


<p>내가 대학원에 온 가장 큰 이유는 새로운 것, 그동안 남들이 하지 않았던 혹은 해보고 싶었어도 능력이 없어서 해보지 못했던 것들을 하기 위함이었다. 결국 그런 욕구에 충실하게 살아가다보면 뭐가 되더라도 되지 않을까? 그런 생각이 들었다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Scale Free Network (Barabasi-Albert Network)]]></title>
    <link href="http://SanghyukChun.github.io/52/"/>
    <updated>2014-04-23T15:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/52</id>
		<content type="html"><![CDATA[<h5 id="52-1-before">들어가기 전에</h5>


<p>이 글은 <a href="http://SanghyukChun.github.io/47" target="new">2014년 KAIST Network Science 수업</a> 중 Scale Free Network 내용을 요약한 글이다. 이 렉쳐에서는 Scale Free Network라는 concept에 대해 다루게 된다.</p>


<h5 id="52-2-scalefreenetwork">Scale Free Network</h5>


<p>이전 글들에서 <a class="red tip" title="따로 글로 정리를 하지는 않았지만, Watts-Strogatz를 설명하면서 다뤘던 부분이다.">Regular Network</a>, <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a>와 <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a>에 대해 다뤘던 것들 중 Path length, Clustering coefficient, 그리고 Degree distribution 부분을 정리해보자.</p>


<p>먼저 Path length이다. 우리가 관측하는 대부분의 network들은 Path length가 그 크기의 logarithm function으로 표현된다는 것을 알 수 있다. 조금 더 구체적으로 표현하자면 \(l_{rand} \approx {\log N \over \log \bar k}\)로 표현이 된다. 먼저 Regular Network의 path length는 \(l \approx N^{1/D}\)로 표현이 된다. 우리가 원하는 log 와는 다른 형태임을 알 수 있다. 그렇다면 Random Network와 Small world Network는 어떨까? 이전 결과들을 통해 확인할 수 있듯 \(l_{rand} \approx {\log N \over \log \bar k}\)로 표현이 된다는 사실을 알 수 있다. 즉, Erdös-Rényi Network와 Watts-Strogatz Network는 일반적으로 우리가 관측하는 네트워크와 비슷한 Path length를 지니고 있고 Regular Network는 그렇지 않음을 알 수 있다.</p>


<p>Clustering coefficient는 어떠한가? 대부분의 실제 네트워크의 clustering coefficient는 그 크기에 무관하게 항상 상수로 표현된다. 즉, \(C \sim const \)로 표현이 된다. Regular network와 small world network가 이 값이 상수임에 반해, Random network는 이 값이 \(C = p = {\bar k \over N}\)으로 표현이 된다. 즉, 크기가 커질수록 이 값이 감소하는 경향을 보이는데 이 부분은 실제 네트워크와 큰 차이가 있는 부분이다. 즉, Random network는 실제 네트워크보다 뭉침 현상이 덜 하고, Regular Network와 Small world network는 실제 네트워크와 그 뭉침 정도가 비슷하다는 것을 알 수 있다.</p>


<p>그렇다면 지금까지의 결론을 보면 Small world Network만 Path lenth, 그리고 Clustering의 두 가지 측면에서 실제 네트워크와 유사함을 알 수 있다. 그렇다면 마지막 Degree distribution은 어떠한가? 실제 네트워크에서 나타나는 degree distribution은 power law distribution으로 표현이 된다. 즉, \(P(k) \sim k^{-\gamma}\)로 표현이 된다. 그런데 Regular Network의 degree distribution은 \(P(k) = \delta (k-k_d)\)이며 Random Network는 \(P(k) = e^{-\bar k} {\bar k ^k \over k!}\)로 표현이 된다. 그리고 Small world network의 degree 역시 exponential function으로 표현이 된다. 즉, 지금까지 우리가 살펴본 그 어떤 네트워크도 실제 네트워크와 유사한 degree distribution을 보이지 않음을 알 수 있다.</p>


<p>이러한 문제점, 즉, degree distribution이 잘 맞지않는다는 문제점으로 인하여 새로운 Scale-Free network라는 개념이 등장하게 된다. Scale-Free network란 degree를 \(k\)라 했을 때 degree sequence \(g&#8217;\)이 power-law function \(h(k) \sim k^{-q}\)로 표현이 되는 네트워크를 의미한다. 이 때 exponent \(q\)의 값은 보통 2에서 3 사이로 결정이 된다. 수학적이지 않은 관점에서 바라본다면 scale-free network는 적은 숫자의 high degree node가 있고 그 이외의 많은 node들은 엄청 작은 degree를 가지는 네트워크를 의미한다. 그리고 이런 degree가 높은 node를 일컬어 hub라고 부르게 되며, 다시 말하자면 Scale-Free Network란 hub가 존재하는 네트워크를 의미하게 된다. 이 현상은 사실 생각해보면 우리 주변에도 많이 발생하는데, <a class="red tip" title="영어 단어의 분포는 그 단어의 빈도의 순위의 역수로 표현된다. 즉, 상위 일부가 전체 대다수를 차지한다.">Zipf의 법칙</a>나 <a class="red tip" title="상위 20%가 80%의 부를 가져간다는 법칙. 보통 2:8의 법칙으로 불린다">Pareto의 법칙</a> 등의 관측도 존재하고, 실제 social network에서도 친구가 엄청나게 많은 일부의 사람들이 존재하고 나머지 사람들은 그보다는 적은 사람의 친구를 가지는 등, 이미 hub라는 현상은 우리가 자연스럽게 받아들일 수 있는 개념이라는 것이다.</p>


<p>그렇다면 degree가 exponential인 것과 power-law인 것이 정말 크게 차이가 날까? 만약 그게 아니라면 우리는 충분히 Watts-Strogatz의 결과물을 사용할 수 있을 것이다. 아래 그 둘을 비교한 그림이 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/52-1.png" width="400"></p>

<p>이 그림을 통해 알 수 있듯, exponential과 power-law는 그 기울기의 감소 정도가 매우 많이 차이가 난다는 것을 알 수 있고, 우리는 degree distribution이 power-law를 가지는 새로운 network가 필요하다는 것을 알 수 있다. 이런 Scale-Free Network는 1999년 Alber, <a class="red tip" title="KAIST 물리과 정하웅 교수님">Jeong</a>, Barabasi에 의해서 처음 연구가 되었으며, 이런 네트워크를 만드는 과정을 Barabasi-Albert Procedure라고 부른다. 그렇다면 Scale-Free라는 이름은 왜 생긴 것일까? Small-world라는 말이 diameter의 증가 정도가 네트워크의 증가 속도보다 훨씬 느리기 때문에 붙은 알이라면, Scale-Free는 degree가 증가하는 정도와 실제 distribution이 같은 속도로 증가함을 의미한다. 수식으로 나타내자면 \(h( \alpha k = \beta h(k)\)로 표현이 된다. 즉, x-axis로 factor \(\alpha\) 만큼 scaling을 한 결과는 y-axis에 factor \(\beta\) 만큼 scaling을 한 것과 같다는 것이다. 따라서 이 power-law curve를 factor \(\alpha\)로 scaling을 하더라도 그 모양은 단순히 위아래로 움직이기만하는 형태로 표현이 된다는 것이다. 즉, 그 우리가 Scaling을 하더라도 그 형태가 변하지 않는 Scale-Free한 Network라는 것이다.</p>




<h5 id="conclusion">Conclusion</h5>


<p>Scale-Free Network를 한 번 정리하고 넘어가보자. 먼저 Scale-Free란 degree distribution이 power-law로 표현되는 network이며, 토폴로지 관점에서 봤을 때 Small-world와 Random network 사이 쯤에 존재하는 네트워크이다. 아래 그림을 보면 엔트로피와 Clustering Coefficient, Average path length, hub degree를 모두 비교해본 결과인데 이 결과를 보면 다른 네트워크와 비교했을 때 다른 값들은 대체로 높지만 상대적으로 뭉침 정도가 약함을 알 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/52-10.png" width="500"></p>

<p>Scale-free network의 entropy는 \(I(G) \sim O( \log_2 ( \Delta m) = O( \log_2 (density(n/2)))\)임을 알 수 있다. 이는 small-world network의 \(I(G) \sim O( \log_2 p )\)와 비슷한 결과이며, 따라서 엔트로피의 관점에서 봤을 때 random network보다는 small-world network에 가까움을 알 수 있다.</p>


<p>Path length는 fixed n에 대해 \(l = A - B k_{hub} \sim O({ \log (n) \over \log (n) + \log (density)}) \)으로 표현이 된다. 그리고 cost-effectiveness라는 것도 정의가 되는데, \(E = {1-\bar {l(density)} \over m}\)으로 표현이 되며 Density는 \(2 \frac {m} {n(n-1)}\)으로 표현이 된다. ========</p>


<p></p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Small World Network (Watts-Strogztz Network)]]></title>
    <link href="http://SanghyukChun.github.io/51/"/>
    <updated>2014-04-23T13:06:00+09:00</updated>
    <id>http://SanghyukChun.github.io/51</id>
		<content type="html"><![CDATA[<h5 id="51-1-before">들어가기 전에</h5>


<p>이 글은 <a href="http://SanghyukChun.github.io/47" target="new">2014년 KAIST Network Science 수업</a> 중 Small World Network 내용을 요약한 글이다. 이 렉쳐에서는 Small World Network라는 concept과 실제 그런 컨셉을 적용한 Network model 중 하나인 Watts-Strogztz Network에 대해 다루게 된다.</p>


<h5 id="51-2-problemofrandomnetwork">Prolem of Random Network</h5>


<p><a href="http://SanghyukChun.github.io/50#50-10-problems" target="new">이전 글</a>에서 다뤘 듯, Random Network는 실제 Network와 맞지 않는 부분이 많이 존재한다. 가장 큰 문제는 degree distribution과 clustering coefficient가 실제 network 분포와 크게 반하다는 점이다. 따라서 이 글에서는 그런 점들을 개선시킨 새로운 컨셉의 네트워크 모델링에 대해서 다루게 될 것이다. 사실 이 Small world network라는 것이 맨 처음 1998년도 논문으로 발표가 되었을 때 그 기반이 되는 Network가 <a href="http://en.wikipedia.org/wiki/Regular_graph" target="new">Regular Network</a>이기 때문에 이런 Regular Network에 대해서도 다룬 이후에 Small World Network를 다루는 것이 맞다고 생각하지만, 실제 강의 내용에서 Regular Network 생략되었기 때문에 나 역시 이 글에서 Regular Network에 대해서는 많이 다루지 않을 생각이다. 참고로 Regular Network는 모든 vertex가 같은 degree를 가지는 Network를 의미하며 k-regular network라고 하면 모든 vertex의 mean degree가 \(k\)라는 의미이다. 그런데 이 lecture에서는 local degree의 값이 \(k\)가 아니라 \(2k\)이며 \(k= {m \over n}\)으로 정의한다. 엄청나게 헷갈리기는 하지만.. 일단 lecture의 notation을 따르도록 하겠다. 즉, 2-regular network는 local degree가 4이며 즉, 각 vertex는 4개의 edge를 가진다. 실제 이 chapter 자체가 2-regular network를 small world network로 바꾸는 Watts-Strogztz Network에 대한 내용이므로 이 점을 꼭 숙지하고 넘어가야한다.</p>


<h5 id="51-3-smallworldnetwork">Small World Network</h5>


<p>Small world network란 예전 <a href="http://SanghyukChun.github.io/34#34-2-smallworld" target="new">인터넷 속의 수학</a>에서도 간략하게 다뤘던 내용이므로 예전 글을 참고해도 좋을 것 같다. Small World Network란 높은 clustering coefficient를 가지고 있고, 상대적으로 짧은 diameter를 가지고 있으며 entropy가 scalable한 sparse network를 의미한다. 여기에서 여러 가지 용어들이 나오는데, clustering coefficient는 넘어가도 되고, diameter는 network에서 가장 긴 shortest path, 그리고 entrophy는 이 graph를 나타내기 위한 information의 양을 의미한다. 마지막으로 sparse network라는 것은 모든 vertex pair사이에 edge가 존재하는 것이 아니라 edge가 존재하지 않는 vertex pair가 존재한다는 의미이다. Small world network는 regular network와 random network의 중간 정도 쯤 되는 network인데, 역시 이것도 <a href="http://SanghyukChun.github.io/34#34-3-poissonregularnetwork" target="new">이전 글</a>에서 다뤘던 내용이므로 관심이 있다면 간략하게 읽어보기를 권한다. 간단하게 설명하면 Samll world network는 regular network에 적당한 randomness를 추가하여 얻을 수 있기 때문에 이 두 개의 네트워크의 중간 정도라고 표현하는 것이다.</p>


<p>사실 이 Small world라는 단어는 Stanley Milgram의 6 degree 실험에서 처음 등장한 것인데, 이 실험에 대한 자세한 설명은 <a href="http://SanghyukChun.github.io/32#32-3-milgramexp" target="new">이전에 적은 글</a>을 참고하기를 바란다. 이 실험의 결론만 얘기하자면 실제 social 네트워크에서는 임의의 vertex pair를 선택했을 때 그 둘 사이를 지르는 가장 짧은 path의 길이가 network의 크기에 비해서 엄청나게 짧다는 것이다. 이 실험에서는 실제 미국 내의 네트워크에서 6개의 step이면 상대방에게 도달할 수 있다는 것을 알 수 있었다. (심지어 shortest path도 아니고 greedy search 였음에도 불구하고) 이 글에서 설명하게 될 Small world network 역시 상대적으로 짧은 diameter를 가지게 되고, clustering coefficient와 closeness centrality도 높다.</p>


<h5 id="51-4-wattsstrogatz">Watts-Strogatz Procedure</h5>


<p>그러면 이제 small world network를 generate하는 방법에 대해 생각해보자. 이 과정을 Watts-Strogatz Procedure라고 부른다. 이 algorithm은 k-regular network를 small world network로 만드는 알고리듬인데, 주어진 k-regular network의 임의의 \(pm\) 개의 edge를 random하게 rewire를 시킴으로써 얻을 수 있다. 즉, 내가 임의로 regualr network에서 p의 확률로 하나의 edge를 random 한 edge로 바꿔주는 과정을 계속 반복하기만 하면 된다. 이런 과정을 통해 우리는 regular network에 강제적으로 randomness를 주입할 수 있게 되며, \(p\)가 약 1~4% 정도가 되면 small-world effect가 나타난다고 한다. 이 \(p\)의 값 0.01 ~ 0.04를 transition threshold 혹은 crossover point라고 한다.</p>


<p>놀라운 사실은, 이렇게 간단한 algorithm을 적용하기만 해도 degree sequence distribution, diameter, average path length 등의 정보가 크게 달라지게 된다. 특히 diameter와 average path length는 아주 조금의 randomness만 주입하게 되어도 그 값이 크게 감소하게 되는데, 이런 극적인 거리 감소 현상을 곧 small-world effect라고 하는 것이다.</p>


<p>이런 과정을 통해 얻어지는 Small world network는 아래 그림과 같다. 이 그림은 vertex가 20개 있는 2-regular network를 WS procedure를 사용해 reconstruct한 것으로, p의 값이 점점 올라가면서 그 모양이 바뀌는 모습을 관측한 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/51-1.png" width="600"></p>

<p>자 그런데 지금까지 설명한 방법은 완전히 random procedure이기 때문에 connected graph를 보장할 수 없다. 어떻게 이것을 개선시킬 수 있을까? 간단하게 생각할 수 있는 방법으로는 만약 선택한 edge를 제거했을 때 isolated vertex가 생기는 경우 해당 edge를 바꾸지 않도록 하는 방법이 있을 수 있을 것이다. 2000년도 뉴먼이 제시한 NSWS model에서는 이런 문제를 해결한 새로운 알고리듬을 제시했다고 하는데 <a href="http://arxiv.org/pdf/cond-mat/0001118.pdf" target="new">논문</a>은 찾은 것 같은데 자세히 읽어보지는 못했다.</p>


<h5 id="51-5-degreedist">Degree Sequence Distribution</h5>


<p>Degree Sequence는 Graph \(G\)의 모든 \(n\) vertex들의 degree value 들의 sequence이며 \(g=[d_1, d_2, d_3, &#8230;, d_n ]\) 과 같이 표현된다. 그리고 이것의 distribution, 즉, degree가 1인 node 들의 비율, 2인 node들의 비율.. 등등을 표현하는 distribution은 \(g&#8217; = [h_1, h_2, h_3, &#8230; h_{max_d}]\) 라고 표현할 수 있다.</p>


<p>맨 처음 k-regular network의 모든 vertex들이 가지는 edge의 개수는 \(2k\)이다. 그런데 \(p\)의 확률로 edge가 변경되더라도 결국 평균 edge의 개수, 혹은 connectivity는 \(c=2k\)로 고정될 것이라는 사실을 알 수 있다. 자 이제 \(P_p (c)\)를 degree의 probability distribution이라고 해보자. vertex들의 2k connection 중 k개의 connection은 아직 still untouched 일 것이므로, 이런 상황에서 vertex i의 connectivity는 \(c_i = k+n_i \ n_i \geq 0 \)라는 것을 알 수 있다. 이제 \(n_i \)라는 것도 두 가지 부분으로 나눠 생각할 수 있는데, 먼저 \(n_i^1 \leq k \) 은 \(1-p\)의 확률로 rewire되지 않는 link e들의 개수이고, \(n_i^2 = n_i - n_i^1\) 은 그 반대로 vertex \(i\)에 reconnected된 edge들을 의미한다. 그리고 총 \(N\)개의 vertex가 있다고 했을 때, 임의의 edge가 vertex \(i\)로 rewire될 확률은 \(p \over N \)라는 것도 알 수 있다.</p>


<p>그렇다면 우리는 이 사실을 통해 small world network의 degree sequence distribution을 다음과 같은 과정을 통해 얻을 수 있게 된다.</p>


<p>$$ P_1 (n_i^1) = {k \choose n_i^1} (1-p)^{n_i^1} p^{k-n_i^1} $$</p>


<p>$$ P_2 (n_i^2) = {(kp)^{n_i^2} \over n_i^2 !} e^{-pk} \ for \ large \ N $$</p>


<p>$$ P_p (c) = \sum_{n=0}^{min(c-k, k)} {k \choose n} (1-p)^n p^{k-n} {(kp)^{c-k-n} \over (c-k-n)!} e^{-pk} \ c \geq k $$</p>


<p>아래 그림은 degree sequence distribution \(P_p (c)\)를 connectivity \(c=2k\)에 대해 표현한 것이다.</p>


<p>  <br/>
<img src="http://SanghyukChun.github.io/images/post/51-2.png" width="600"></p>

<p>이 그림을 통해 우리는 degree sequence distribution이 평균값이 \(\bar c = 2k\)인 Poisson 분포임을 알 수 있다.</p>


<p>이번에는 random network와 실제 network data, 그리고 앞서 서명한 과정을 사용해 만들어낸 network의 degree distribution을 비교한 그림을 살펴보자</p>


<p><img src="http://SanghyukChun.github.io/images/post/51-3.png" width="600"></p>

<p>이 그림을 통해 우리는 random network보다 small world network가 더 실제 네트워크와 더 비슷하다는 것을 알 수 있다.</p>


<h5 id="51-6-entropy">Entropy</h5>


<p>앞서 정의했었던 Degree sequence distribution을 사용하면 Graph의 entropy \(I(G)\)를 정의할 수 있는데 이 값은 곧 네트워크를 표현 하기 위한 information의 양을 의미한다. 당연히 Regular Network는 Entropy가 낮고 Random Network는 Entropy가 높다. 다시 말해서 Graph \(G\)의 Entropy는 그 graph의 randomness를 bit로 표현한 값이 되며, 이 randomness는 다시 말해서 degree sequence \(g&#8217;\)으로 다음과 같이 나타낼 수 있다.</p>


<p>$$ I(G) = - \sum_{i=1}^{max_d} h_i (\log_2 h_i), \ where \ g&#8217; = [h_1, h_2, h_3, &#8230; h_{max_d}] $$</p>


<p>이 값을 확률 2-regular network를 \(p\)의 확률로 edge를 rewire하는 경우에 대해 ploting을 해보면 다음과 같은 결과를 얻을 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/51-4.png" width="600"></p>

<p>즉, 엔트로피는 확률 \(p\)의 log scale로 증가하게 된다. 또한 entropy의 정의에 따라 이 값은 \(I_{WS} = -\sum_k h(d) \log_2 h(k) \)로도 표현이 된다. 이런 결과들을 통해 우리는 small world network가 scalable하다는 것을 알 수 있는데 왜냐하면 \(p\)가 0이면 엔트로피의 값은 regular network와 같은 0이지만 \(p\)가 증가함에 따라서 우리가 원하는 entropy를 randomness를 조절함으로써 얻을 수 있기 때문이다. 따라서 이런 Small world network는 scalable하다는 것을 알 수 있다. (일단 수업에서 배운대로 정리를 하기는 했지만 이부분은 너무나도 모호하다. 아무래도 네트워크의 scalable의 definition을 추가로 찾아보고 알아봐야 할 것 같다)</p>


<h5 id="51-7-entropyvsdensity">Entropy vs Density</h5>


<p>k-regular network의 density는 단순하게 \(density = 2 { k \over n }\)으로 계산할 수 있으며, 따라서 우리는 \(k = n {density \over 2}\) 라는 사실을 알 수 있다. 이 값을 통해서 우리는 small world network의 entropy와 density의 관계를 유추할 수 있다. 즉, parameter \(A, B, C\)를 주고 그 값에 대해 \(I_{WS(density)} = A log_2 B(density) - C \) 라는 엔트로피 식을 적을 수 있다. 이 때 \(A, B, C\)를 각각 0.5, 60, 0 이라고 한다면 이 식은 \(I_{WS(density)} = 0.5 log_2 (60(density)) = O(log_2 (density)) = O \left(log_2 \left( \sqrt {k \over n} \right) \right)\) 임을 알 수 있다. 즉, 우리는 entropy가 density에 \(O(\log_2 density)\)의 형태로 표현된다는 것을 알 수 있으며 이 값은 random network의 \(O(density)\)보다 훨씬 덜 가파른 증가율이라는 것을 알 수 있다. 죽, 조금 더 자세히 말하자면 Small world network의 density의 증가률 혹은 rewiring probability의 증가율에 대한 entropy의 증가율은 random network의 그것보다 훨씬 더디게 증가함을 알 수 있다. 이것을 Density에 대해 ploting하게 되면 아래와 같은 결과를 얻게 된다. (일단 3개의 term 중에서 entropy만 보면 된다)</p>


<p><img src="http://SanghyukChun.github.io/images/post/51-5.png" width="600"></p>

<p>다시 한번 정리하자면, small world network에서 density와 entropy는 logarithm relationship을 가진다.</p>


<h5 id="51-8-nmweq">Newman, Moore, and Watts Equation</h5>


<p>먼저 간단한 observation들을 나열해보자. 먼저 rewiring 이 없는 2-regular network는 average path가 \(n \over 4k\)로 표현이 된다. 그리고 바로 전 section에서 본 그림처럼 매우 작은 rewiring probability \(p\)에 대해서 average path length는 매우 빠르게 감소함을 알 수 있다. 그리고 마지막으로 어떤 early point \(p^*\)가 존재해서 이 값보다 작은 rewiring probability를 가진 small world network는 regular에 매우 가깝고 그보다 큰 값을 가지는 네트워크는 random에 더 가깝다. 이 지점을 우리는 crossover, 혹은 transition threshold라고 부르며 이런 현상을 네트워크의 phase transition이라 부른다.</p>


<p>Average path length는 \(p=0\)에서 \(n \over 4k\)의 값을 가지며, \(p\)가 증가함에 따라 감소한다. 이때 \(r=2pm\)이라는 값을 정의하면 \(r\)에 대한 path lenth scaling function을 아래와 같이 표현할 수 있다.</p>


<p>$$ f(r) = 4 {\tanh^{-1} {r \over \beta} \over \beta }; \ where \beta = \sqrt{r^2 + 4 r} $$</p>


<p>또한 average path length는 다음과 같이 표현이 된다.</p>


<p>$$ \bar L_{SW} = n {f(r) \over 2k} = {2n \over \beta k} tanh^{-1} {r \over \beta} $$</p>


<p>만약 \(n=100, m=200, k=2, density=0.04, p=0.04\)라는 조건을 넣고 이 값을 계산하면 average path length는 11.7이라는 값을 얻을 수 있다. 그런데 이 값은 ====</p>


<h5 id="51-9-avgpathlength">Average Path Length</h5>


<p>작성중</p>


<p>$$ log_2 \bar L(r) = \log_2 \left( {n \over 4k} \right) - q \log_2 r $$</p>


<p>$$ \bar L(r) = {n/4k \over r^q }, \ where \ r=pkn $$</p>


<p>즉, 이 값들을 통해 우리는 \(p\)가 0이 아닐 때 (0이면 regular network와 같은 값이 된다) 이 평균 path length는 \(\bar L(r) = {n/4k \over (pkn)^q }\) 로 표현된다는 것을 알 수 있다.</p>


<h5 id="51-10-clusteringcoefficient">Clustering Coefficient</h5>


<p>작성중</p>


<h5 id="51-11-closness">Closeness Centrality</h5>


<p>작성중</p>


<h5 id="51-12-search">Seach in Small World Network</h5>


<p>작성중</p>


<h5 id="51-13-conclusion">Conclusion</h5>


<p>자 이렇게 small world network에 대해 살펴보았다. Small world Network는 Regular Network에 확률 \(p\) 만큼 randomness를 부여해 만들어지는 graph이며, 이 randomness는 scalable하며, random network와 비교했을 때 훨씬 낮은 entropy를 가진다.또한 이런 small world network의 topology는 매우 높은 clustering coefficient와 closeness를 가지게 된다. 또한 그 크기에 비해 상대적으로 짧은 average path와 diameter를 가지게 된다. (그리고 이 특성 자체가 small world effect를 지칭하는 것이기도 하다) 그리고 small world network에서 가장 좋은 search algorithm은 max-degree search 알고리듬이라는 것도 알 수 있었다.</p>


<p>다만, 거의 대부분의 좋은 성질을 가지고 있음에도 불구하고 Small world network 혹은 Watts-Strogatz Model에는 real network와 반하는 특성이 하나 있는데, 바로 degree distribution이다. 일반적인 real network의 degree distribution이 \(P(k) \simeq k^{-\gamma}\)로 표현되는 것에 비해, WS model은 exponetial degree distribution을 가지게 된다. 따라서 이런 문제를 해결하기 위해서 우리는 degree distribution이 power law distribution으로 나타나는 새로운 형태의 network인 scale-free network에 대해 다루게 될 것이다.</p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Random Network (Erdös-Rényi Network)]]></title>
    <link href="http://SanghyukChun.github.io/50/"/>
    <updated>2014-04-22T11:37:00+09:00</updated>
    <id>http://SanghyukChun.github.io/50</id>
		<content type="html"><![CDATA[<h5 id="50-1-before">들어가기 전에</h5>


<p>이 글은 <a href="http://SanghyukChun.github.io/47" target="new">2014년 KAIST Network Science 수업</a> 중 Graph theory 내용을요약한 글이다. 이 렉쳐에서는 가장 기초적인 Network modeling 중 하나인 random network에 대해 다룬다.</p>


<h5 id="50-2-why">Why we need network model?</h5>


<p>이 글을 포함해 3개의 강의는 모두 네트워크 모델링과 관련된 내용이다. 이 글은 random network, 그리고 다음 글은 순서대로 small world network, 그리고 그 다음은 scale free network에 대해 다루게 된다. 이런 네트워크 모델링을 우리는 왜 알아야할까. 사실 이유는 간단하다. 그냥 네트워크를 사용해 무언가를 분석하는 것이 매우 어렵기 때문이다. 예를 들어서 우리는 실제 social network가 어떻게 구성되어있는지 알지못한다. 만약 우리가 실제 네트워크와 매우 유사한, 그러나 훨씬 간단한 형태의 모델을 만들게 된다면 해당 네트워크에서 발생하게 될 일들을 쉽게 예측할 수 있을 것이다. 이 글에서 다루게 되는 random network는 그러한 네트워크 모델링 중에서 가장 간단한 모델 중 하나이다.</p>


<h5 id="50-3-random-graph">Random Graph</h5>


<p>Random graph란 어떤 fixed parameter를 가지는 stochastic 모델이다. Notation은 \(G(n,p)\) 로 적게 되는데, n은 vertex의 개수, p는 각각의 vertex pair들이 서로 edge를 가질 independent probability를 의미한다. 즉, 이 network는 확률 p로 인해 edge가 생성되므로 매번 새로 generate할 때 마다 그 결과가 달라지는 stochastic model인 것이다. 따라서 우리가 n개의 vertex와 m개의 edge를 가지는 임의의 network \(G(n,m) \)를 얻게 될 확률은 \(P(G) = p^m (1-p) ^ { {n \choose 2} - m} \) 로 얻을 수 있다. 이러한 모델을 맨 처음 수학적으로 이를 분석했던 사람들의 이름을 따서 Erdös-Rényi model 이라고도 하고 그 degree와 edge의 distribution이 Poisson혹은 Bernoulli 분포를 따르기 때문에 Poisson random graph 혹은 Bernoulli random graph라고 하기도 한다.</p>


<h5 id="50-4-meanedgedegree">Mean Edge and Mean Degree in Random Graph</h5>


<p>아런 Random graph에서 n과 p가 주어졌을 때, graph가 generate가 될 때마다 항상 edge의 개수는 달라지지만, 이것이 특정 확률을 따르기 때문에 평균 edge의 개수를 계산하는 것이 가능하다. 계산하기 전에 간단하게 생각하면 n개의 vertex에 존재하는 모든 pair들 중 확률 p로 edge를 가지기 때문에 평균 edge의 개수는 모든 pair의 개수에서 edge가 존재할 확률인 p를 곱한 \( {n \choose k} p\) 가 될 것이다. 그렇다면 정말 그런지 확인해보도록 하자. 그러면 먼저 m개의 edge를 가지는 임의의 Graph \(G(m)\) 이 generate될 확률을 구해보자. 이 갚은 바로 위에서 edge가 m개인 graph \(G(m)\) 하나가 표현될 확률을 계산했으므로, 이런 graph의 개수를 count한 후 간단하게 이를 summation하면된다.</p>


<p>$$ P(m) = { {n \choose 2}  \choose m} p^m (1-p)^{ {n \choose 2} - m} $$</p>


<p>위에서 우리는 m에 대한 확률 분포를 구했기 때문에 m의 평균 값은 간단하게 \(\sum_m mP(m)\) 으로 계산할 수 있다. 그런데 \(m\)의 확률 분호 \(P(m)\)은 \(n\)이 \({n \choose m}\) 이고 \(k\)가 \(m\)인 binomial distribution이다. 또한 우리는 binomial distribution의 mean value가 단순하게 \(np\) 임을 알고 있으므로 평균 값은 아래와 같이 계산할 수 있다. 이에 대한 자세한 설명은 <a href="http://en.wikipedia.org/wiki/Binomial_distribution" target="new">위키피디아 링크</a>로 대체한다.</p>


<p>$$ \bar m = \sum_{m=0}^{n \choose 2} m P(m) = {n \choose 2} p $$</p>


<p>이 결과는 우리가 처음 예측한 값과 정확히 일치한다.</p>


<p>그러면 이제 degree의 평균 값을 구해보자. 그 이전에 이번에는 mean degree가 얼마일지 간단하게 생각해보자. 이때 mean degree라는 것은 결국 한 vertex에서 가지는 평균 edge의 개수를 의미하기 때문에 평균 edge개수인 \({n \choose k}p\)를 vertex pair 개수인 \(n \over 2\)로 나눠준 값인 \((n-1)p\) 가 될 것이라고 예측할 수 있다. 그렇다면 실제로 계산해보자. <a href="http://SanghyukChun.github.io/48#48-7-degree">이전 글</a>에서 이미 우리가 유도했던 것처럼 edge가 \(m\)개 있는 graph의 mean degree는 \(2m \over n\)으로 구할 수 있다. 따라서 평균 degree는 위에서 우리가 edge의 평균 개수를 구한 것 처럼 간단히 구할 수 있다.</p>


<p>$$ \bar k = \sum_{m=0}^{n \choose k} {2m \over n} P(m) = {2 \over n} {n \choose 2} p = (n-1)p $$</p>


<p>역시 처음에 예측한 값과 일치한다. 참고로 random graph에서 평균 degree는 \(c\)라는 notation으로 표기가 되며, 따라서 \(c=(n-1)p\)이다.</p>


<h5 id="50-5-degreedist">Degree Distribution</h5>


<p>그렇다면 이번에는 degree의 distribution을 알아보자. 평균 degree의 개수도 물론 중요하지만 실제 degree가 어떻게 분포하고 있는지를 아는 것도 매우 중요하다. 왜냐하면 결국 network의 특성을 이해하기 위해서는 개별 node들이 얼만큼 다른 node들과 연결되어있는지 알아야하며, 이 결과가 네트워크에 어떤 결과를 불러올지 파악하는 것이 매우 중요하기 때문이다. 그렇다면 먼저 간단하게 하나의 vertex가 \(k\)개의 다른 vertex와 연결되어있는 상황이 발생할 확률을 계산해보자. 이때, 당연히 전체 vertex 개수는 \(n\)개 이므로 \(k\)개의 vertex와 연결되어있는 vertex는 나머지 \(n-1-k\) vertex와 연결되어있지 않다. 따라서 임의의 vertex가 \(k\)개의 vertex와 연결되어있을 확률 \(p_k\)는 아래와 같이 계산된다.</p>


<p>$$ p_k = { n-1 \choose k } p^k (1-p)^{n-1-k} $$</p>


<p>이 결과는 또 binomial distribution이다. 즉, \(G(n,p)\)의 degree distribution은 binomial distribution이라는 것을 알 수 있다. 하지만 이 식은 간단한 근사식을 통해서 더 간단하게 표현하는 것이 가능하다. 대부분의 경우 우리가 관심이 있는 영역은 \(n\)이 엄청나게 큰 network이므로 이런 상황에서 앞 부분의 combination을 근사하고, 그 다음에는 뒷 부분의 \((1-p)^{n-1-k}\)의 log 값을 근사 시켜보자.</p>


<p>먼저 \(n-1 \choose k\)를 근사한 결과는 \({n-1 \choose k} = {(n-1)! \over (n-1-k)! k!} \simeq {(n-1)^k \over k!}\) 이다. 이 근사식은 그냥 매우 간단한 근사식이므로 설명을 생략하도록 하겠다. 그렇다면 이제 다음으로 확률의 맨 뒷부분을 근사해보자. 이 값은 먼저 log를 취한 후 log를 근사하여 그 값을 계산한다.</p>


<p>$$ \ln[(1-p)^{n-1-k}] = (n-1-k) \ln ( 1- \frac {c} {n-1}) \simeq -(n-1-k) { c \over n-1} \simeq -c $$</p>


<p>이 근사식에서 첫 번째 계산식은 \(p = { c \over n-1} \)이라는 이전의 결과를 사용한 것이고 근사하는 부분은 <a href="http://en.wikipedia.org/wiki/Natural_logarithm#Derivative.2C_Taylor_series" target="new">log의 taylor expansion</a>이 \(\ln(x) = (x-1) - {(x-1)^2 \over 2} + {(x-1)^3 \over 3} - &#8230;\) 라는 것을 사용한 것이다. 그 다음 근사는 당연히 n이 k보다 엄청 크다고 가정한 것이다. 자 그 결과는 놀랍게도 \(\ln[(1-p)^{n-1-k}] \simeq -c\) 이다. 즉, \((1-p)^{n-1-k} \simeq e^{-c}\) 라는 사실을 알 수 있다.</p>


<p>자 이제 모든 결과를 종합해보면 n이 엄청나게 큰 상황에서 다음과 같은 결과를 얻는다</p>


<p>$$ p_k \simeq {(n-1)^k \over k!} p^k e^{-c} ={ (n-1)^k \over k!} ({c \over n-1})^k e^{-c} \simeq e^{-c} {c^k \over k!}$$</p>


<p>자, 우리는 n이 매우 크다는 조건 하나를 사용해 근사를 한 결과 Poisson distribution을 얻게 되었다. 즉, n이 매우 큰 \(G(n,p)\)에서 degree distribution은 poisson distribution을 가진다는 사실을 알 수 있다. (사실 글의 처음에서도 말했던 것 처럼 이런 이유로 random network라는 이름은 사실 poisson random graph의 준말이다.)</p>


<h5 id="50-6-cluster">Cluster and giant Component</h5>


<p>자 그러면 이제 또 중요한 measurement 중 하나인 clustering coefficient를 계산해보자. Random Graph에서 \(c\)는 평균 degree를 의미하기 때문에 이 글에서 clustering coefficient는 \(C\)로 표기될 것이다. 이 clustering coefficent는 내가 이웃한 vertex 두 개가 서로 연결되어있을 확률을 의미한다. 그런데 우리의 graph \(G(n,p)\)의 임의의 vertex pair가 서로 연결되어있을 확률은 언제나 \(c \over n-1 \)이다. 따라서 clustering coefficient \(C\)는 \(c \over n-1 \) 라는 사실을 아주 간단하게 알 수 있다. 즉, 이 식에 따르면 \(n \to \infty\) 가 되고 \(c\)의 값이 fixed 되어있는 graph에서의 \(C\)는 0으로 수렴한다는 사실을 알 수 있다. 그러나 실제 네트워크에서 관찰되는 결과는 mean degree가 fixed되어있더라도 clustering coeficient의 값이 여전히 크게 유지가 된다는 것이다. 이런 특성 때문에 바로 다음 글에서 설명하게 될 small world network의 필요성이 대두된다.</p>


<p>아무튼 단순히 clustering coefficient만 계산하는 것은 크게 와닿지 않는다. 구체적으로 이 네트워크에서 giant component라는 것이 존재할 확률이나 실제 존재했을 때 어떤 형태로 존재할지를 예측해보자. 여기에서 giant component란 문자 그대로 네트워크 상에서 존재하는 가장 largest한 component라고 보면 된다. (Component에 대한 설명은 <a href="http://SanghyukChun.github.io/48#48-8-path" taget="new">이전 글</a> 참고) Simple하게 \(p=0\)이면 모든 graph가 disjoint되어있고 giant component라는 것은 존재하지 않는다. 반면 \(p=1\)이면 모든 vertex가 연결되어있고 graph는 vertex의 개수가 \(n\)인 오직 하나의 componet를 가지게 된다. 자 그러면 이제 \(p\)가 0에서 1사이의 값일 때를 생각해보자. 이런 상황에서 giant component가 존재한다고 가정하고, 이 component에 포함되는 vertex의 개수를 \(n_{gc}\) 라고 하자. 이렇다고 가정을 하게 되면 임의의 fraction of nodes가 giant component에 포함이 되지 않을 확률을 \(u\)라고 하면 우리는 이 값이 \(u=1-{n_{gc} \over n}\) 이라는 사실을 알 수 있다. (당연히 \(n_{gc} = n(1-u)\) 이다.) 그렇다면 이제 이 u를 통해 giant component를 분석해보도록 하자.</p>


<p>먼저 임의의 vertex \(i\)가 이런 GC에 포함된다고 가정해보자. 그렇다면 이 vertex \(i\)는 vertex \(j\)라는 vertex를 거쳐 GC에 연결되어야한다고 했을 때, \(i\)가 GC에 포함이 되지 않을 확률은 (1) \(i\)와 \(j\)가 연결되어있지 않다: \(1-p\) (2) \(i\)와 \(j\)가 연결되어있으나 \(j\)가 GC에 연결되어있지 않다: \(pu\) 이렇게 총 두 가지 경우 임을 알 수 있다. 따라서 임의의 vertex \(i\)가 임의의 다른 vertex \(j\)를 거쳐 GC에 포함되지 않을 확률은 \(1-p+pu\)이고, 다른 모든 vertex에 대해 이를 확장해보면 다른 vertex가 \(n-1\)개 있으므로 이 확률은 \({(1-p+pu)}^{n-1}\) 임을 알 수 있다. 그런데 이 값은 결국 한 vertex가 GC에 포함될 확률 \(u\)와 같다. 따라서 \(u={(1-p+pu)}^{n-1}\) 이라는 사실을 알 수 있다.</p>


<p>자, 그러면 \(p={c \over n-1}\) 이므로 위의 식에 대입하고, log를 근사시키면 아래와 같은 결과를 얻게 된다.</p>


<p>$$ \ln u= (n-1) \ln {\left(1-{c \over n-1} (1-u) \right)} \simeq -(n-1) {c \over n-1} (1-u) = -c (1-u) $$</p>


<p>위의 식을 통해 \(u=e^{-c(1-u)}\) 라는 사실을 알 수 있다. 이때 giant component 안에 들어있는 vertex의 개수의 비율을 \(S=1-u={n_{gc} \over n}\) 이라고 정의한다면 이 식은 아래와 같이 적을 수 있다.</p>


<p>$$ S=1-e^{-cS} $$</p>


<p>이로부터 우리는 Giant component의 크기 자체가 평균 degree에 의해 bound된다는 사실을 알 수 있다. 즉, 어떤 값 \(c\)에 의해 S가 결정된다는 것이다. 그러나 식이 한 번에 풀릴 수 있는 간단한 형태가 아니기 때문에 이 식에서 바로 정확한 S값을 구할 수는 없고, 그 대안으로 graphical solution이 제시된다. 즉, x축이 S의 값이고 y축이 S에 대한 함수의 값인 2차원 그래프를 그리고, \(y=S\)와 \(y=1-e^{-cS}\)의 교점을 구하는 것이다. 이런 두 함수를 그려보게 되면 아래와 같은 결과를 얻게 된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/50-1.png" width="500"></p>

<p>위의 그래프에서 알 수 있는 사실은, 모든 함수들이 원점에서부터 시작하고 계속 기울기가 감소하는 형태이기 때문에 만약 원점에서 함수의 기울기가 1보다 큰 값이 존재하는 함수가 존재한다면 그 함수는 반드시 원점이 아닌 교점이 존재한다는 사실이다. 이런 교점을 찾아보면, \({d \over dS} (1-e^-{-cS})=1\)이라는 식을 풀어야하며 이 식을 계산해보면 \(ce^{-cS} = 1\)이라는 식을 얻을 수 있다. 즉, S=0인 점에서 기울기는 무조건 \(c\)가 되므로 \(c\)의 값이 1보다 크면 반드시 giant component가 존재하며 그 보다 작으면 giant component가 존재하지 않는 다는 것을 알 수 있다.</p>


<h5 id="50-7-smallcomponent">Small Componets</h5>


<p>이번에는 Giant component에 포함되어있지 않은 Small component에 대해 알아보자. 만약 giant component가 네트워크에 딱 하나만 존재한다고 가정해보자. 그리고 각각의 크기가 \(S_1 n\) \(S_2 n\) 이라고 가정했을 때, \(i\)가 첫 번째 GC에 속하는 vertex, \(j\)가 두 번째 GC에 속하는 vertex일 때, 모든 distinct vertex pair \((i,j)\)의 개수는 간단하게 그 둘을 곱한 \(S_1 S_2 n^2\)일 것이다. 만약 각각의 pair는 \(p\)의 확률로 연결이 되어있거나 \(1-p\)의 확률로 연결되어져있지 않다. 따라서 이 두 개의 GC가 서로 완벽하게 분리되어있을 확률 \(q\)는 다음과 같이 계산할 수 있다</p>


<p>$$ q = (1-p)^{S_1 S_2 n^2} = \left( 1 - {c \over n-1} \right)^{S_1 S_2 n^2} $$</p>


<p>이때 만약 \(n \to \infty\) 가 된다면, 우리는 아래와 같은 근사식을 구할 수 있다.</p>


<p>$$ \ln q = S_1 S_2 \lim_{n \to \infty} \left[ n^2 \ln \left( 1 - {c \over n-1} \right) \right] = S_1 S_2 \left[ -c(n+1) + {1 \over 2} c^2 \right] = c S_1 S_2 [-n + ({1 \over 2} c -1) ]$$</p>


<p>그러면 우리는 남은 상수항을 \(q_0\)라 정의하고, \(q = q_0 e^{-c S_1 S_2 n}\) 이라는 식을 구할 수 있다. 이 식은 너무나 당연하게 \(n \to \infty\) 가 되면 값이 0이된다. 따라서 우리는 \(n\)이 매우 큰 상황에서 두 개의 GC가 존재할 확률이 0이므로 \(n\)이 큰 네트워크는 단 하나의 Giant component를 가지고 있다는 결론을 내릴 수 있다.</p>


<p>Random network는 단 하나의 Giant component만을 가지고 있기 때문에, 그에 포함되지 않은 나머지 component들을 모두 small component라고 정의할 수 있을 것이다. 이 때 각각의 small component의 크기를 \(\pi_s\)라고 정의하면 이 값들의 모든 합은 반드시 \(1-S\)이므로 \(\sum_s \pi_s = 1-S\) 라는 식을 얻을 수 있다.</p>


<p>본격적으로 small component에 대해 다루기 전에 먼저 small component가 어떤 형태로 구성이 되어있을지 생각해보도록 하자. 정답부터 말하자면, small component는 tree의 형태를 하고 있다. 각각의 small component를 \(s\)개의 vertex를 가지는 tree라고 해보자. 당연히 tree의 edge의 개수는 \(s-1\)일 것이며 이 값은 connected 되어있는 \(s\)의 vertex가 만들어낼 수 edge의 최소 개수이다. 만약 이 최소 개수보다 많은 edge가 단 하나라도 존재할 확률은 \(c \over n-1\)이며, 이런 graph는 반드시 cycle이 생기게 되므로 더 이상 tree가 아니게 되어버린다. 그렇다면 우리가 이 small component가 tree임을 입증하기 위해서는 이런 additional edge가 존재할 확률이 0라는 것을 보이면 된다. tree를 구성하는 edge이외에 추가가 될 수 있는 edge의 개수는 \({s \choose 2} - (s-1) = {1 \over 2} (s-1)(s-2)\)이다. 이를 통해 tree안에 이런 edge가 단 하나라도 존재할 확률은 \({1 \over 2} {c(s-1)(s-2) \over n-1}\) 라는 것을 알 수 있다. 따라서 \(n \to \infty\) 가 되면 이 확률은 0이 되어버린다. 따라서 이 component는 반드시 tree라는 사실을 알 수 있다.</p>


<p>이번에는 아래와 같은 상황을 한 번 상상해보자. 왼쪽과 오른쪽은 단 하나의 vertex만 제외하면 완벽하게 같은 Graph이다. 왼쪽 graph는 vertex \(i\)의 neighbor들이 모두 다른 small component에 속해있는 경우이며, 즉, 서로 분리되어있는 subgraph들을 하나로 이어주는 역할을 하고 있다. 오른쪽은 그 \(i\)는 없지만 여전히 서로 같은 확률을 그대로 유지하고 있고 여전히 확률 p를 가지는 random graph 이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/50-2.png" width="500"></p>

<p>이런 상황에서 만약 우리가 충분히 큰 \(n\)을 가정했을 때 각각의 subgraph 역시 큰 graph의 특성을 따라갈 것이기 때문에, \(i\)가 없는 오른쪽 그림에서\(i\)의 neighbor \(n_1\)이 size가 \(s_1\)인 small component에 속할 확률은 \(\pi_{s_1}\)로 주어진다는 것을 예측할 수 있다.</p>


<p>자 그렇다면 이런 vertex \(i\)가 \(k\)의 degree를 가지고 있었다고 가정해보자. 위의 결과로 인해 vertex \(i\)가 \(s\) 만큼의 크기를 갖는 small component에 속할 확률 \(P(s|k)\)는 이 vertex의 모든 neighbor, 즉 \(k\)개의 neighbor들이 각각 \(s_1\)부터 \(s_k\)까지 속할 확률과 같으며 이 값은 아래와 같이 계산 할 수 있을 것이다.</p>


<p>$$ P(S|k) = \sum_{s_1}^infty &#8230; \sum_{s_k}^{\infty} \left[ \Pi_{j=1}^k pi_{s_j} \right] \delta (s-1, \sum_j s_j) $$</p>


<p>이때 \(\delta (m,n)\)은 <a href="http://en.wikipedia.org/wiki/Kronecker_delta" target="new">Kronecker delta</a>를 의미하며 이 함수는 간단하게 두 값이 같으면 1 다르면 0을 반환한다. 즉, 이 수식에서 델타의 의미는 모든 \(s_j\)들을 더하면 \(s-1\)이 나와야한다는 것이다. =========== 추가 설명 작성 중 =============</p>


<p>============Small component에 대한 추가 설명 작성 중=================</p>


<h5 id="50-8-distofcompsize">Complete Distribution of Component Size</h5>


<p></p>


<h5 id="50-9-path">Path Length</h5>


<p>보통 네트워크, 혹은 그래프에서 Diameter, 혹은 지름이라 함은 가장 긴 longest geodesic distance를 의미한다. 이 값은 즉, 임의의 두 vertex를 골랐을 때 가장 짧은 경로의 길이를 의미한다고 보면 된다. Random graph에서 이 값은 어떻게 구할 수 있을까? 먼저 간단하게 임의의 vertex에서 \(s\) 번 이동했을 때 visit할 수 있는 평균 vertex의 개수는 매 진행마다 degree 만큼의 vertex를 추가로 더 갈 수 있으므로 평균 degree들을 \(s\)번 곱한 형태인 \(c^s\)일 것이다. 그리고 우리가 원하는 경로는 가장 짧은 경로를 찾는 것이므로 vertex travel step을 이 visit 가능한 vertex와 전체 vertex의 개수가 같아지는 순간 끝나하면 우리가 원하는 shortest path의 길이를 유추할 수 있게 될 것이다. (정확한 경로는 예측할 수 없지만) 따라서 \(c^s \simeq n\)이 될 것이며 따라서 \(s \simeq {\ln n \over \ln c}\)가 될 것이다. 즉, 놀랍게도 아무리 \(n\)의 값이 급격하게 커지더라도 random graph의 diameter가 증가하는 속도는 \(\log n\) 이라는 것이다. 이 값은 충분히 작은 값으로, 이런 움직임이 실제 네트워크의 움직임과 매우 흡사하다는 것을 실험을 통해서 밝혀졌다.</p>


<h5 id="50-10-problems">Random Graph의 문제점</h5>


<p>앞서 살펴본 바를 토대로 Random Graph가 가지고 있는 문제점들을 살펴보도록 하자. 먼저 Average path length는 \(\bar l \simeq {\log n \over \log c} \)로 표현이 되며 이것은 곧, random graph는 \(n\)의 값에 비례하여 path length가 늘어나는 것이 아니라 log scale로 증가한다는 것을 알 수 있다. 즉, \(n\)이 매우 크더라도 path는 그에 비해 매우 짧다는 것을 알 수 있다. 이것은 실제 대부분의 네트워크들에서도 나타나는 현상이다. 반면 clustering coefficient는 \(c \over n\)으로 표현이 되며 \(n\)의 값이 증가할 수록 떨어지는 것을 알 수 있는데, 실제 network들이 \(n\)이 커지더라도 높은 clustering coefficient를 유지한다는 점에서 현실과 잘 맞지 않는다는 것을 알 수 있다. 마지막으로 Degree distribution을 살펴보게 되면, random graph에서의 degree distribution은 \(P(k) \simeq e^{-c} {c^k \over k!}\)로 근사가 되는데, 실제 네트워크에서 관측되는 distribution은 \(P(k) \simeq k^{-\gamma}\) 와 같은 power distribution이다. 따라서 이 역시 잘 맞지 않는다는 것을 알 수 있다.</p>


<p>Random network는 가장 간단한 network모델이며 수학적으로 잘 증명되었고 쉽게 이해할 수 있는 network모델이다. 그러나 real network와 비교해 잘 맞지 않는 점들이 있기 때문에 우리는 결국 새로운 network model들을 계속 더 공부해야만 하는 것이다.</p>


<p>따라서 다음 글은 이와 같은 문제를 일부 해결한 Small world network에 대해 다루게 될 것이다.</p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Measures and Metric]]></title>
    <link href="http://SanghyukChun.github.io/49/"/>
    <updated>2014-04-17T01:59:00+09:00</updated>
    <id>http://SanghyukChun.github.io/49</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 <a href="http://SanghyukChun.github.io/47" target="new">2014년 KAIST Network Science 수업</a> 중 Graph theory 내용을요약한 글이다. 이 렉쳐에서는 기본적인 Graph theory에 대해 배운다. 어려운 내용은 아니고 정말 기초적인 부분들에 대해서 다루게 된다.</p>


<h5>Why we need measures and metric?</h5>


<p>이건 사실 lecture 내용에는 없는 내용이지만 엄청나게 중요한 내용이라고 내가 판단해서 집어넣은 part이다. 일단 이 lecture에서 다루기만 하는 metric이나 measure들이 엄청나게 많다. 대충 10개는 넘을텐데, 이런 수 많은 것들을 우리가 왜 알아야하느냐? 바로 주어진 임의의 graph를 측정하고 어떤 graph인지 판단할 수 있기 때문이다. 또한 진짜 중요한 목적 중 하나는 어느 vertex가 중요한지 알아내는 것이다. 근데 이 &#8216;중요함&#8217;이라는 것이 정의하기에 따라 달라지기 때문에 각각의 &#8216;중요함&#8217;이 무엇인지 정의하는 방법이 달라지게 되고 그렇기 때문에 이 lecture에서 cover하는 measure와 metric이 많은 것이다. 개인적으로는 그냥 closeness, betweeness, clustering coefficient 정도만 cover하고 끝내고 싶지만 나름 중요한 내용이 많아서 일단 최대한 많이 cover를 할 수 있도록 해야겠다. 아무튼 이런 수많은 metric들을 통해 우리가 알고싶은 것은 그래서 어느 vertex가 진짜 중요한 녀석이고, 나중에 다루게 될 dynamic process에서 어느 vertex를 주목해서 그 vertex에 처리를 해야하느냐 등을 하기 위해서 필요한 과정이다. 따라서 목적에 맞게 사용하는 것이 중요하고, 각각의 metric이 어떤 것을 측정하기 위함인지 이해하는 것이 매우 중요한 것이다.</p>


<h5>Degree Centrality</h5>


<p>가장 먼저 살펴볼 centrality는 degree centrality다. 그냥 이건 얼마나 각각의 node가 많은 vertex와 연결되어있느냐, 혹은 각각의 vertex의 degree는 얼마나 되느냐를 측정하는 centrality에 불과하다. Directed graph같은 경우에는 간단하게 indegree와 outdegree를 합한 형태가 되는데, 상황에 따라 indegree centrality와 outdegree centrality를 define하는 것이 가능하기는 하다. 이 metic은 얼마나 많은 vertex들과 연결이 되어있는지를 알아보는 것으로, 많이 연결되어있다면 혹은 degree가 높다면 그만큼 중요할 것이라는 가정에서부터 나온 centrality이다.</p>


<h5>Eigenvector Centrality</h5>


<p>또 다른 방법으로는 각각의 vertex가 다른 vertex와 결국 궁극적으로 얼마나 많은 connection을 가지는지를 확인하는 것으로, 간단하게 eigenvector로 표현할 수 있다. 증명과정은 매우 간단하다. 먼저 centrality를 \(x_i &#8216;\)으로 정의했을 때 \(x_i &#8217; = \sum_j A_{ij} x_j\) 이므로 \(x&#8217; = Ax\)이다. 이 때, 이 과정을 t번 반복하면 \(x(t) = A^t x(0) \)이 된다. 이제 \(x(0) = \sum c_i v_i \) 라고 나타냈을 때, \(x(t) = A^t \sum c_i v_i = \sum c_i j_i ^t v_i \)이고, 따라서 \(x_i &#8217; = k_i^t \sum c_i ( \frac {k_i} {k_1} )^t v_i \)가 된다. v는 eigenvector고 k는 eigenvalue이다. 이 과정을 무한하게 반복하면 \(x(t) \to c_1 k_1^t v_1 \)이 된다. 따라서 이 과정을 통해 우리는 \(Ax = k_1 x\)일 때, centrality를 \(x_i = k_1^{-1} \sum A_{ij} x_j\)로 정의할 수 있다.</p>


<h5>Katz Centrality</h5>


<p>앞서 살펴본 eigenvector centrality는 directed graph에서 outdegree가 0이고 indegree가 0보다 큰 값이더라도 eigenvector centrality를 계산하면 해당 vertex는 0이라는 값을 가지게 된다는 단점이 있다. Katz centrality는 \(x_i = \alpha \sum_j A_{ij} x_j + \beta\)로 정의된다. 방금 전 eigenvector centrality에서 상수만 곱하고 더해준 형태가 된다. 이때 더해주는 \(\beta\) term으로 인해 기본적으로 모든 vertex가 특정 값 이상의 centrality를 가지도록 bound시키는 효과가 있다.</p>


<h5>Page Rank</h5>


<p>그러나 결국 Katz centrality도 문제가 있다. 이 경우 many other vertex를 point하는 high katz centrality를 가지는 vertex가 point하는 vertex 역시 높은 katz cent를 가진다는 것이다. 이것이 왜 문제냐, 예를 들어서 구글은 아마 엄청나게 높은 Katz centrality를 가지고 있을 것이다. 그런데 내 블로그나 내 홈페이지는 아마 connection이 매우 적을 것이고 우리는 이 페이지들의 centrality가 낮기를 기대하지만 실제로는 google이 내 page로 향하는 edge를 가지고 있기 때문에 내 page도 centrality가 높아지게 되는 것이다. 이것은 우리가 원하는 결과가 아니기 때문에 수정이 필요하다.</p>


<p>Page Rank는 \(x_i = \alpha \sum_j A_{ij} x_j \frac {x_j} {k_j^{out}} + \beta\) 으로 정의된다. (사실 이렇게 정의되는 것은 아니며, page rank는 일종의 ranking algorithm이지만, 여기에서는 이것을 일종의 metric으로 삼으려는 것이므로 일단 이 lecture의 정의를 따라가기로 했다.) 아무튼 outdegree만큼 나눠줌으로써 아까 발생했던 문제를 해결할 수 있다. Google이 가진 outdegree는 엄청나게 높을 것이므로 나를 point하더라도 outdegree로 그 값이 나눠져서 매우 작은 값만 더해질 것이기 때문이다.</p>


<h5>Degree, Eigenvector, Katz and Page Rank</h5>


<p>간단하게 위의 네 개의 metric을 정리해보면 아래와 같은 결과를 얻게 된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/49-1.png" width="350"></p>

<h5>Hub and Authorities</h5>


<p>Authority란 무언가 유용한 정보를 가지고 있는 vertex를 뜻하며, Hub란 그런 vertex 중 best를 찾을 수 있는 곳이 어디인지 알려줄 수 있는 vertex들을 의미한다. 이 두가지는 directed network에만 존재하며, 이를 통해 새로운 형태의 metric을 design할 수 있다. 앞서 정의한 바에 따르면 Authorityu centrality x와 Hub centrality y는 다음과 같이 정의된다. \(x_i = \alpha \sum_j A_{ij} y_j\), \(y_i = \beta \sum_j A_{ji} x_j\) 그러면 이것을 matrix 형태로 표현할 수 있는데, \(x = \alpha A y\), \(y = \beta A^\top x\)가 된다. 따라서 \(A A^\top x = \lambda x\), \(A^\top A y = \lambda y\)가 된다. 즉, authority centrality x는 A의 singular vector가 되며 이는 곳 A의 cociation matric C의 eigenvector centrality가 된다.</p>


<h5>Closeness Centrality</h5>


<p>Closeness Centrality는 매우 중요한 measure 중 하나로, given vertex에서 다른 모든 vertex까지의 shortest path의 mean distance로 정의된다. 이때, distance는 geodesic path, 즉, i에서 j까지의 minimum hop 으로 정의가 된다. 이때 distance를 d라고 하게 되면, i에서 j로 가는 mean geodesic distance는 \(l_i = \frac {1} {n} \sum_j d_{ij}\) 가 되므로 closeness centrality는 \(C_i = \frac {1} {l_i} = \frac {n} {\sum_i d_{ij}}\) 가 된다. 이때, 자기 자신과의 거리는 0이므로 \(C_i &#8217; = \frac {n-1} {\sum_{i \neq j} d_{ij}}\)로 Harmonic mean closeness라는 것을 정의하기도 한다. (큰 차이는 없다.) distance의 평균 값이 높아지려면 vertex가 다른 vertex들과 가까이 모여있는 형태여야 하며, 즉, degree가 높다는 것과는 다른 의미로 중요한 vertex를 의미하는 지표가 된다. (당연히 멀리 떨어질수록 centrality는 작아지며, 연결이 안된 경우는 d가 무한대이므로 0이 된다.) 이 경우 small component에 속한 vertex는 높은 closeness centrality를 가지게 될 것이다.</p>


<h5>Betweenness Centrality</h5>


<p>Betweenness centrality는 graph에 존재하는 모든 shortest path들에 대해 vertex i가 얼마나 많이 그 path에 속하는지를 나타내는 지표이다. 이 지표는 매우 중요하게 생각할 수 있지만, 안타깝게도 그것을 찾아내는 방법이 매우 어렵다. (아마 NP-Complete로 알고 있다.) 아무튼, 이 값이 중요한 이유는, 실제 flow가 생기는 dynamics를 생각해봤을 때, 진짜 중요한 vertex는 많은 path에 속하는 vertex가 된다. 이런 vertex를 막아서 확산을 막을 수도 있고, 반대로 이 vertex에게 무언가를 확신시키게 하여 다양한 path로 뻗어나가게 하는 것이 가능하기 때문이다. \(x_i = \sum_{st} \frac {n_{st}^i } { g_{st} }  \) 으로 정의가 되며, \(x_i = \frac {1} {n^2} \sum_{st} \frac {n_{st}^i} {g_{st} } \) 으로 noramlized 된 betweenness를 정의할 수 있다.</p>


<p>또한 random-walk betweenness라는 것도 정의할 수 있는데, shortest path가 아니라 그냥 random walk를 무지하게 많이 만든 다음에 얼마나 많이 그 안에 count가 되었는지를 측정하는 것이다. 이는 betweenness centrality의 approximation algorithm이 되는데, 실제 network에서 움직이는 모양이 random walk인 경우가 많아서 꽤 괜찮은 근사법이라고 한다.</p>


<h5>k-plex and k-components</h5>


<p>Clique는 모든 vertex들이 fully connected 되어있는 vertex set을 의미한다. 모든 vertex들끼리 꼭 fully connected되는 것은 아니고 거의 fully connected되는, 구체적으로 얘기했을 때 n개의 vertex가 각각의 관점에서 최소한 n-k개 만큼 연결되어있는 vertex set을 생각할 수 있을 것이다. 이것이 바로 k-plex의 정의가 된다. clique도 좋은 성질을 가지고 있지만, k-plex는 일종의 approximated clique로, 실제 network에서 항상 fully connected된 vertex set만 의미가 있는 것이 아니기 때문에 나름의 의미를 가진다. 또한 k-core라는 것도 정의할 수 있는데, (n-k) plex와 같다. k-core는 간단하게 network pruning을 하는 greedy algorithm을 통해 찾아낼 수 있다.</p>


<p>모든 vertex들끼리 적어도 k개의 vertex independent path를 가지는 subset을 k-component라고 한다. 참고로 Component는 단순하게 어떤 path를 통해 다른 구성원에 도달할 수 있으면 그것을 일컬어 component라고 한다. 즉, 이 k-component 안에 있는 임의의 vertex는 마찬가지로 같은 k-component에 존재하는 다른 임의의 vertex로 반드시 갈 수 있다. 대부분의 network backbone은 매우 높은 k를 가지는 k-component라고 한다.</p>


<h5>Transitivity and Clustering Coefficient</h5>


<p>만약 vertex u와 v, 그리고 w가 서로 모두 연결되어 삼각형을 이루게 된다면 이것을 transitive라고 부른다. 이런 transitive가 정확히 몇 개나 있느냐를 재는 것은 perfect transitivity로 측정하게 되고, partial transitivity라는 것을 통해 u와 v가 연결되고 v와 w가 연결되어있을 때만을 카운트하는, 즉 삼각형에서 edge하나가 빠진형태일 때를 카운트하여 partial rate를 측정할 수도 있다.</p>


<p>자 이제 clustering coefficient라는 것을 정의해보자. <a href="http://SanghyukChun.github.io/34">이전에 썼던 글</a>에서 조금 자세히 다뤘던 것으로 기억하는데, 같은 컨셉이다. 얼마나 graph가 뭉쳐있는지를 알아보는 계수로, 정의는 간단하게 삼각형을 이룰 것 같은 vertex set 중에서 실제 삼각형을 이루는 vertex set의 비율로 정의된다. 수식으로 표현하면 다음과 같다.</p>


<p>$$ C = \frac {number \ of \ traiangles \times 6} {number \ of \ paths \ of \ length \ 2} $$</p>


<p>그런데 이 clustering coefficient를 local하게 아래와 같이 정의할 수 있다. </p>


<p>$$ C_i = \frac {number \ of \ pairs \ of \ neighbors \ of \ i \ that \ are \ connected} {number \ of \ pairs \ of \ neighbors \ of \ i} $$</p>


<p>즉, 이 local cc는 내 친구와 다른 친구가 서로 친구일 확률을 의미하게 된다. 따라서 degree가 높은 vertex는 낮은 local cc를 가지게 될 확률이 높아지게 된다.</p>


<p>또한 Redundancy라는 것을 정의할 수 있는데, 이는 i의 neighbor에서부터 다른 neighbor간의 connection의 평균이 된다. 따라서 이 값은 0보다 크거나 같고 \(k_i -1\)보다는 작거나 같다. Local cc를 redundancy를 사용해 표현할 수 있는데, 다음과 같은 방법으로 나타내게 된다. \(C_i = \frac {\frac {1} {2} k_i R_i} { \frac {1} {2} k_i (k_i - 1) } = \frac {R_i} {k_i -1} \) 따라서 global clustering coefficient는 그냥 local clustering coefficient의 summation이 된다.</p>


<h5>Reciprocity</h5>


<p>Reciprocity는 directed network에서 length 2짜리 loop이 얼마나 많은지를 의미한다. 즉, 서로가 서로를 point하는 vertex의 개수가 얼마나 되는지를 측정하는 것이다. 이는 곧, 서로가 서로에게 paht가 있다는 의미가 되므로 이전에 살펴봤던바와 같이 이 값을 수식으로 표현하게 되면 \(r = \frac 1 m \sum A_{ij} A_{ji} = \frac 1 m Tr A^2\)이 된다는 사실을 쉽게 알 수 있다.</p>


<h5>Structural Balance</h5>


<p>만약 edge를 -1과 1의 두 가지로 정의하고, -1은 서로 enemy, +1은 서로 friend라고 해보자. 이런 경우 어떤 loop이 있을 때 해당 loop에 -1인 edge가 짝수개 있으면 stable하고 홀수개 있으면 unstable해진다. (간단하게 삼각형을 그려서 확인해볼 수 있다) 아무튼 이럴 때 -1 인 edge가 짝수개 있는 상태를 structural balance라고 하며, Harary&#8217;s theorem은 이런 balanced network가 같은 그룹은 positive한 connection만 가지고 다른 그룹끼리는 negative한 connection만 가지는 group들로 divided된다는 것을 증명한다. 대부분의 social network는 이런 balanced한 상황인 경우가 많다고 한다.</p>


<h5>Cosine Similarity</h5>


<p>Similarity는 서로 다른 graph가 얼마나 비슷한가를 측정하는 척도이다. 즉, 하나의 graph에서 서로 다른 subgraph들끼리 많은 수의 neighbor를 공유하면 높은 값을 가지는 structurally equivalent를 정의할 수 있는데, 우리가 하고 싶은 것은 서로 꼭 같은 vertex를 공유하는 것은 아니더라도 비슷하게 생긴 neighbor를 가지는 상황에서 regularly equivalent를 정의하고 싶은 것이다.</p>


<p>그렇다면 문제는 이런 비슷한 정도를 측정하는 것인데, cosine similarity라는 것을 통해 이런 것을 측정할 수 있다. cosine은 간단하게 vector x와 y의 inner product의 normalization된 형태로 계산이 가능하다. 따라서 cosine similairy는 adjacency matrix의 column vector혹은 row vector들을 inner product하여 그 값을 비교하는 것이다. 수식으로 나타내면 아래와 같이 표현된다.</p>


<p>$$ \sigma_{ij} = cos \theta = \frac {\sum_k A_{jk} A_{kj} } {\sqrt {\sum_k A_{ik}^2} \sqrt {\sum_k A_{jk}^2} } $$</p>


<p>$$ \sigma_{ij} = \frac {\sum_k A_{jk} A_{kj} } {k_i k_j}  = \frac {n_ij} {k_i k_j} $$</p>


<p>이때, \(n_ij\)는 vertex i와 j의 common neighbor들의 숫자가 된다.</p>


<p>이런 cosine similarity를 통해 Pearson Coefficient라는 것도 정의할 수 있는데, \( r_{ij} = \frac {cov(A_i,A_j)} {\sigma_i \sigma_j} = \frac { \sum_k (A_{ik} - <A_i>) (A_{jk} - <A_j>) } {\sqrt {\sum_k (A_{ik} - <A_i> )^2} \sqrt {\sum_k ( A_{jk} - <A_j> )^2} } \) 가 되며, 이 값은 -1부터 1사이에 존재하게 된다.</p>


<p>그 밖에도 Normalized \(n_ij\), Euclidean distance, Normalized Euclidean distance 등의 structural equivalence를 측정하기 위한 방법 들이 존재한다.</p>


<p>Similar score \(sigma_{ij}\)는 \(sigma_{ij} = \alpha \sum_{kl} A_{ik} A_{jl} \sigma_{kl}\)로 정의된다. 이때, 자기 자신에게 높은 similarity를 주지 않는다는 문제 가있어서 \(\delta_{ij}\)를 추가하는 수정된 방식이 존재하며 이때 이 similiarity는 \(\sigma = \alpha A \sigma A + I\)로 표현이 된다. 하지만 이 경우도 even length를 가지는 path만 고려할 수 있다. 따라서 이를 또 개선시키기 위해서 \(sigma_{ij} = \alpha \sum_{k} A_{ik} \sigma_{kj} + \delta_{ij}\)로 수정이 가능하다. 이 값은 \(\sigma = \alpha A \sigma + I\)가 되며 iteration을 \(\sigma(0) = 0\)에서부터 시작하면 \(\sigma = \sum_m^\inf ( \alpha A )^m = ( I - \alpha A ) ^{-1}\)이 된다. 그런데 이 경우 degree가 높은 vertex에 높은 similiarity가 가는 상황이 발생하게 된다. 따라서 다시 degree로 나눠주는 term을 넣어서 마지막으로 수정식을 쓰면, \(sigma_{ij} = \frac {\alpha} {k_i} \sum_{k} A_{ik} \sigma_{kj} + \delta_{ij}\), \(\sigma = \alpha D^{-1} A \sigma + I\)가 된다.</p>


<h5>Homophily and Assortative Mixing</h5>


<p>작성 중</p>


<h5>Assotative Mixing by Enumerative Characteristics</h5>


<p>작성 중</p>


<h5>Assotative Mixing by Scalar Characteristics</h5>


<p>작성 중</p>


<h5>Assortative Mixing by Degree</h5>


<p>작성 중</p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Graph Theory]]></title>
    <link href="http://SanghyukChun.github.io/48/"/>
    <updated>2014-04-16T19:39:00+09:00</updated>
    <id>http://SanghyukChun.github.io/48</id>
		<content type="html"><![CDATA[<h5 id="48-1-before">들어가기 전에</h5>


<p>이 글은 <a href="http://SanghyukChun.github.io/47" target="new">2014년 KAIST Network Science 수업</a> 중 Graph theory 내용을요약한 글이다. 이 렉쳐에서는 기본적인 Graph theory에 대해 배운다. 어려운 내용은 아니고 정말 기초적인 부분들에 대해서 다루게 된다.</p>


<h5 id="48-2-networkandgraph">Network and Graph</h5>


<p>Network 혹은 Graph는 vertex와 그 vertex를 잇는 edge로 구성되어있는 일종의 collection이다. vertex와 edge는 node와 link라고 하기도 한다. 인터넷, 전력망, Citation network, social network 등을 예로 들 수 있으며, 요즘 내가 공부하고 있는 <a href="http://SanghyukChun.github.io/blog/categories/neural-network/">artificial neural network</a> 역시 neuron과 synapse로 이루어진 graph이다. 보통 Graph는 mathematical하게 \(G = (V,E)\)로 표현이 된다. V는 vertex를 의미하며 E는 edge를 의미한다. 또한 일반적으로 n과 m이라는 notation도 사용하는데, n은 vertex의 개수, m은 edge의 개수이다. 이때 graph의 종류는 크게 두 가지가 있다. 하나는 directed graph이며 또 하나는 undirected graph이다. directed graph는 edge에 direction이 존재하여 한 vertex에서 다른 vertex를 point하는 형태가 되며, undirected graph는 edge에 특정한 direction이 존재하지 않는다. 또한 directed graph 중 Acyclic directed network 혹은 directed acyclic graph (보통 DAG라고 부른다) 라는 graph가 있는데, 이 그래프는 이름 그대로 cycle이 없는 graph를 얘기하며, 즉, 한 vertex에서 edge를 따라 이동했을 때 그 어떤 vertex, 그 어떤 path를 선택하더라도 자기 자신으로 돌아오지 않는 graph를 의미한다. 또한 graph의 edge는 weight라는 것을 가지고 있는데, 이 값은 edge의 고유한 값으로, 상황에 따라 다르게 해석이 된다. 예를 들어서 internet edge에서는 data flow의 양을 의미하기도 하고, social network에서는 얼마나 두 사람 간의 contact가 많냐 등으로 해석할 수도 있고, 단순한 geometric graph에서는 weight가 두 vertex간의 거리를 뜻하기도 한다. 렉쳐노트에는 weight가 non-negative라고 표현이 되어있지만, 실제 문제들에서는 edge가 negative weight를 가지는 경우도 많다. 물론 이런 경우 계산이 좀 복잡해진다.</p>


<h5 id="48-3-adjacencymatrix">Adjacency Matrix</h5>


<p>그런데 이런 graph를 실제 분석하기 위해서는 어떻게 해야할까? 아래와 같은 그래프를 예로 들어보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/48-1.png" width="200"></p>

<p>이 그래프는 간단한 undirected graph이며, 1,2가 연결되어 있고 2,3이 연결되어 있고&#8230;. 등등등이 연결되어있는 그래프이다. 그런데 우리가 어떤 그래프를 서술할 때에 있어서 항상 이렇게 일일이 어느 node와 어느 node가 연결되어있는지 명시해야할까? 이런 표현을 간단히 하기 위하여 Adjacent matrix라는 것이 도입된다. 이 matrix는 \(A_{ij} = weight \ of \ edge \ from \ vertex \ j \ to \ i\) 로 표현이 된다. 당연히 두 vertex간에 edge가 없으면, 혹은 연결되어있지 않으면 그 값은 0이 된다. 즉, 위의 그래프는 아래와 같은 matrix로 간단하게 표현이 가능하다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/48-2.png" width="200"></p>

<p>당연히 undirected graph는 이 adjacent matrix가 symmetric하며, self-edge가 없는 graph는 \(A_{ii} = 0\) 이다.</p>


<p>또한 위에서 설명했었던 DAG를 adjacency matrix로 표현하면 재미있는 일이 벌어지는데, without loss of generality, 항상 DAG는 triangular matrix로 표현이 된다. Why? 왜냐하면, 아주 간단한 이유인데, DAG에서 어떤 path를 고르더라도 반드시 그 path는 cycle이 아니다. 즉, 다시 자기 자신으로 돌아오지 않기 때문에, 결국에는 outgoing edge가 없는 vertex에 도달하게 될 수 밖에 없다. (당연하다) 그러므로 반드시 최소한 하나 이상의 out degree가 0인 vertex에 도달하게 되고 (degree는 아래에서 더 자세히 설명) 이 말은 곧, column 중 하나는 반드시 zero-vector여야 한다는 소리이다. 이때 만약 out degree가 0인 vertex를 제거해보면 (이에 해당하는 edge도 제거하면) 그 zero vector를 pointing했던 vertex 역시 마찬가지 이유로 outdegree가 0인 vertex가 될 것이다. 이런 식으로 하나하나 제거해나가보면 결국, 반드시 triangular matrix로만 표현이 된다는 것을 알 수 있다. 또한 이런 triangular matrix의 eigenvalue는 diagonal element 들인데, DAG는 cycle이 없기 때문에 모든 diagonal element가 0이 되고 따라서 모든 eigenvalue는 0이 된다. 즉, 어떤 주어진 graph가 DAG인지 아닌지 여부를 판단하기 위해서는 간단하게 모든 eigenvalue들이 0이 되는지만 확인하면 되는 것이다.</p>


<h5 id="48-4-cocitation">Cocitation and Bibliographic Coupling</h5>


<p>Cocitation이란 directed graph의 vertex i와 j를 동시에 point하고 있는 vertex들의 개수를 의미한다. 간단하게 &#8216;co-citation&#8217;이라는 단어 자체가 같이 citation한다는 의미이므로 그 의미 그대로 이해하면 된다. 그런데 이런 경우, 앞서 설명했던 adjacency matrix로 cocitation matrix를 표현하는 것이 가능하다. 그 이유는 vertex i,j를 동시에 point하는 k는 \(A_{ik} A_{jk} = 1\) 이라는 식을 만족하기 때문이다. 따라서 모든 점들에 대해 이런 과정을 반복해보면 \(C_{ij} = \sum_k^n A_{ik} A_{jk} = \sum_k^n A_{ik} A_{kj}^\top \)라는 것을 알 수 있고, 이는 곧 \(C = A A^\top\) 라는 것을 알 수 있다. 이 경우 임의의 matrix와 그 matrix의 transpose를 곱하면 그 결과는 항상 symmetric하다는 것이 잘 알려져 있다. 따라서 이 matrix는 symmetric하다. 이 결과는 당연하다고 할 수 있는데, i와 j에 대해 그 순서가 바뀐다고 해서 citation하고 있는 vertex가 달라지지는 않을 것이기 때문에 항상 \(C_{ij} = C_{ji}\)라는 것을 알 수 있다. 그리고 graph가 symmetric하다는 것은 이 graph가 undirected graph라는 것을 의미하게 된다. 그리고 이 graph는 weighted graph인데, 이 weight는 얼마나 많은 co-citation이 존재하는지를 indicate하는 숫자가 된다. 그리고 \(C_{ii}\) 는 vertex i를 point하는 모든 edge의 개수를 의미한다.</p>


<p>Cociation이라는 것을 정의했으니 그 반대 방향을 생각할 수 있을 것이다. 즉, i와 j가 동시에 point하고 있는 vertex들의 개수를 생각해볼 수 있는데, 이 것을 Bibliographic Coupling이라 한다. 위와 같은 방법으로 유도를 해보면 \(B = A^\top A\) 라는 사실을 알 수 있다. 마찬가지로 이 graph도 symmetric하며 따라서 이 graph도 undirected graph다. 또한 마찬가지로 이 graph는 weighted graph이며 그 weight는 얼마나 많은 vertex를 동시에 같이 point하고 있느냐를 의미한다. 마지막으로 \(B_{ii}\) 는 vertex i가 point하는 모든 vertex의 개수를 의미한다.</p>


<p>이 두 가지 matrix는 directed graph에서의 search algorithm에 사용될 수 있다고 한다.</p>


<h5 id="48-5-bipartite">Bipartite Network</h5>


<p>Hypergraph라는 것이 존재한다. Hyperedge라는게 존재하는 graph를 의미하는데, 이 hyperedge는 하나의 edge가 2개 보다 많은 vertex에 연결되어있는 edge를 의미한다. 이런 hypergraph를 표현하는 방법 중 하나는 bipartite graph를 사용하는 것이다. Bipartite graph는 vertex를 두 그룹으로 나누어 그룹 안에는 edge가 존재하지 않고, 모든 edge가 그룹 사이에서만 존재하는 graph를 의미한다. 즉, 아래 그림에서 까만점을 원래 hypergraph의 vertex, 하연점을 edge로 생각하면 우리가 일반적으로 알고 있는 graph로 hypergraph를 표현할 수 있는 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/48-3.png" width="500"></p>

<p>이런 bipartite matrix에는 incidence matrix라는 것이 존재하는데, 아래 그림과 같이 정의된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/48-4.png" width="600"></p>

<p>이 incidence matrix B는 projection을 위해 사용하는 것이 가능하다. 이때 projection은 edge들끼리 같은 vertex를 가지고 있는지 여부, 혹은 그 반대로 vertex끼리 같은 edge를 가지고 있는지 여부로 새로운 형태의 그래프를 그리는 것을 의미하며 아래에 그 간단한 예시가 나와있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/48-5.png" width="300"></p>

<p>이때, i와 j가 같은 group k에 존재한다고 했을 때 우리는 \(B_{ki} B_{kj} = 1\)이라는 것을 알 수 있다. 따라서 i와 j가 몇 개의 같은 그룹에 속해있는지를 indicate하는 P는 \(P_{ij} = \sum B_{ki} B_{kj} = \sum B_{ik}^\top B_{kj} \) 이며, 따라서 \(P = B^\top B\) 라는 것을 알 수 있고, \(P_{ii} = \sum B_{ki}^2 = \sum B_{ki}\) 라는 것을 알 수 있다. 따라서 \(P_{ii}\)는 vertex i가 속해있는 모든 group의 개수를 의미하게 된다.</p>


<h5 id="48-6-tree">Tree and Planer Network</h5>


<p>Tree는 connected, undirected network의 일종으로, closed loop이 존재하지 않는 graph를 의미한다. 이 경우, 다시 말해서 parent가 단 하나만 존재하는 graph가 tree이며, 이 때문에 그 어떤 vertex pair를 고르더라도 그 사이를 연결하는 optimal path는 단 하나밖에 존재하지 않는다. 또한 n개의 vertex를 가지고 있는 tree의 edge개수는 반드시 n-1개이다.</p>


<p>Planar network는 서로 교차하는 edge가 하나도 없게 그릴 수 있는 graph를 의미한다. tree도 planar graph의 일종이다. 이런 planar graph는 반드시 4-coloring 보다 많은 색을 칠하는 것이 불가능하다는 것이 이론적으로 증명되어있다. Coloring problem은 graph를 any adjacent vertex pair가 서로 같은 색을 가지지 않도록 색을 칠해주는 문제인데, 4-coloring이라는 것은 4가지 색으로 색칠을 한다는 의미이다. 즉, planar graph에서 5-color problem은 반드시 풀 수 없는 문제이다.</p>


<p>그런데 일반적인 graph가 planar인지 아닌지 어떻게 판단할 수 있을까? <a href="http://en.wikipedia.org/wiki/Kuratowski's_theorem" target="new">Kuratowski&#8217;s theorem</a>이라는 것이 있다. 모든 non-planar graph는 적어도 한 개 이상의 subgraph가 K5혹은 UG의 expansion이라는 theorem인데, 이를 통해 우리는 간단히 graph의 subdivision을 구해서 이 graph가 planar인지 아닌지 여부를 판단할 수 있다.</p>


<h5 id="48-7-degree">Degree</h5>


<p>Degree는 vertex i와 연결된 모든 edge의 개수를 의미한다. notation은 \(k_i\)로 표현되며, \(k_i = \sum_j^n A_{ij}\) 이며, \(m = \frac {1} {2} \sum_i^n k_i = \frac {1} {2} \sum_{ij} A_{ij}\) 라는 것을 알 수 있다. degree의 평균은 c로 표현하는데, \(1 \over n \sum_i^n k_i\) 이며, 이 값은 곧 \(c = {2m \over n}\)이다.</p>


<p>이에 대해 density 혹은 connectance라는 것을 정의할 수 있는데, 이것은 전체 존재 가능한 edge와 실제 존재하는 edge의 비율을 의미한다. 이 때 n개의 vertex가 있을 때 존재 가능한 edge의 개수는 단순히 n choose 2이므로 density \(\rho\)는 \(\rho = \frac {m} {\binom {n}{2}} = \frac {2m} {n(n-1)} = \frac {c} {n-1}\)으로 정의할 수 있다. 따라서 만약 dense한 graph는 n이 발산했을때 density가 constant일 것이고, sparse graph는 n이 발산하면 이 값이 0이 될 것이다. 이 방법을 사용해서 우리에게 주어진 graph가 densy한지 혹은 sparse한지 알 수 있는 것이다.</p>


<p>그리고 마지막으로 indegree와 outdegree라는 것이 있는데, 이것은 edge의 방향이 존재하는 directed graph에서만 정의되는 값으로, 당연히 indegree는 vertex i가 point하는 vertex의 개수고 outdegree는 vertex i를 point하는 vertex의 개수를 의미한다. 앞서 값을 살펴봤던 k(degree), m(edge number), c(density)를 살펴보면 아래와 같은 결과를 얻게 된다.</p>


<p>$$ k_i^{in} = \sum_j^n A_{ij}, \ k_j^{out} = \sum_i^{out} A_{ij} $$</p>


<p>$$ m = \sum_i^n k_i^{in} =  \sum_j^n k_j^{out} = \sum_{ij} A_{ij} $$</p>


<p>$$ c_{in} = \frac {1} n \sum_i^n k_i^{in} = \frac 1 n \sum_j^n k_j^{out} = c_{out} $$</p>


<p>$$ c = \frac {m} {n} $$</p>


<h5 id="48-8-path">Path and component</h5>


<p>Path는 모든 consecutive pair가 서로 connected edge의 sequence로 나태어지는 vertex들의 sequence를 의미한다. 즉, 주어진 vertex set을 순서대로 살펴보게되면 각 consecutive vertex pair 사이에는 edge가 존재하는 vertex set을 path라고 하는 것이다. 그냥 우리가 알고 있는 그 path를 수학적으로 정의한 것에 불과하니 넘어가도 그만이다. Path의 길이는 이 lecture는 path기 traversed한 edge의 개수로 정의하는데, 개수가 아니라 그 edge들의 weight들의 summation으로 일반적으로 정의한다.</p>


<p>아무튼 이 lecture에서 정의한대로 path의 length를 정의했을 때 주어진 graph에서 length가 r인 path의 개수는 어떻게 찾을 수 있을까? Adjacent matrix에서 \(A_{ik} A_{kj}\)를 계산하게 되면 k를 경유하는 i에서 j로 가는 모든 path의 개수가 나오게 된다. 이 사실로 부터 다음과 같은 결론을 유추해낼 수 있다. 아래에서 U는 orthogonal matrix of eigenvector를 의미하고 \(k_i\)는 i번째로 큰 eigenvalue를 의미한다. (간단한 eigenvector decomposition이다)</p>


<p>$$ N_{ij}^r = |A^r|_{ij} $$</p>


<p>$$ L_r = \sum_i^n |A^r|_{ij} = Tr A^r = Tr(UK^r U^\top )$$</p>


<p>$$  = Tr(U^\top U K^r) = Tr K^r = \sum_i k_i^r $$</p>


<p>Geodesic path라는 것이 있는데, shortest path의 일종이다. 이때 shortest path를 weight의 sum이 아니라 단순히 hop의 개수만 세는 방식으로 계산하는 것이다. 일반적으로 graph의 diameter를 측정한다고 했을 때는 이 값을 사용한다.</p>


<p>Eulerian path라는 것도 있는데, 모든 edge를 정확히 한번만 visit할 수 있는 path를 의미하며, Hamiltonian path는 모든 vertex를 한 번씩 visit하는 path를 의미한다. Euler path는 이전에 <a href="http://SanghyukChun.github.io/blog/categories/math-in-internet" target="new">인터넷 속의 수학</a>에서 설명했었던 <a href="http://SanghyukChun.github.io/32">쾨르히스베르크 다리 문제</a>를 풀 때 사용할 수 있다. 이 path는 job sequencing, garbage collection, parallel programming등에 사용할 수 있지만 이 path를 찾는 방법은 NP-complete이기 때문에 빠른 시간 안에 찾을 수는 없고 보통 approximation algorithm을 사용하게 된다.</p>


<p>component는 graph의 subgraph를 의미한다. 이 때 component 안의 vertex를 모두 visit하는 path가 최소한 하나 이상 존재해야한다. out-component는 특정 vertex A에서 시작해 도달 가능한 vertex들의 set이며, in-component는 vertex A로 도달 가능한 path를 가지는 vertex들의 set이다.</p>


<p>Independent 혹은 disjoint path라는 것도 정의할 수 있는데, edge independent는 두 개의 path가 서로 공유하는 edge가 없는 경우이며, 당연히 vertex independent는 공유하는 vertex가 없는 경우를 의미한다. vertex indep이면 edge indep하지만 그 반대는 항상 성립하는 것은 아니다. 이 때 given vertex pair 사이에 존재하는 모든 independent path의 개수를 connectivity로 정의할 수 있다. 이 값은 두 vertex가 얼마나 많은 connectivity를 가지고 있는지, 혹은 그 둘 사이의 bottleneck이 얼마나 될지를 판단할 수 있는 지표가 된다.</p>


<h5 id="48-9-cutset">Cut sets</h5>


<p>cut set은 그에 속하는 vertex 혹은 edge등을 제거했을 때 특정한 vertex들의 pair가 disconnected되는 set을 의미한다. 또한 minimum cut set은 그런 cut set 중에서 가장 작은 element를 가지는 cut set을 의미한다.</p>


<p>Menger&#8217;s Theorem에 따르면, given pair of vertices의 minimum vertex cut set의 크기는 같은 pair의 connectivity와 같다고 한다.</p>


<p>또한 두 vertex사이의 max flow라는 것을 edge-independent path의 개수 곱하기 capacity로 정의할 수 있는데, 이런 max flow를 정의했을 때, max-flow/min-cut theorem에 따르면 vertex pair의 maximum flow는 항상 minimum cut set와 single path의 capacity의 곱과 같다고 한다. 이때 undirected graph에 대해서 edge connectivity와 minimum edge cut set의 size와 maximum flow의 크기가 같다는 것이 증명될 수 있다.</p>


<h5 id="48-10-laplacian">Graph Laplacian</h5>


<p>어떤 commodity 혹은 substance의 flow가 어떤 비율로 흘러가느냐에 따라 Diffusion이라는 것을 정의할 수 있다. 약간 복잡한 얘기인데, 약간 불필요해보이기도 하고 복잡해서 <a href="http://en.wikipedia.org/wiki/Laplacian_matrix#Random_walks_on_graphs" target="new">링크</a>로 대체하겠다. 링크에서도 나오듯 결국에는 Random walk를 정의하기 위해서 나온 개념으로 보이므로 생략하도록 하겠다.</p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Science - Introduction]]></title>
    <link href="http://SanghyukChun.github.io/47/"/>
    <updated>2014-04-16T13:47:00+09:00</updated>
    <id>http://SanghyukChun.github.io/47</id>
		<content type="html"><![CDATA[<p>Network Science에 처음 관심을 가지게 된 계기는 <a href="http://SanghyukChun.github.io/blog/categories/math-in-internet" target="new">인터넷 속의 수학</a>에서 들었던 lecture였다. 내가 예전에 관심있었던 부분 중 하나가 소셜 네트워크, 혹은 다른 형태의 네트워크를 이해하고 그 네트워크를 더 효율적으로 개선시키거나 혹은 그 네트워크에서 무언가 우리가 새로운 방식의 BM을 창출해내는 것이었는데, Network Science는 이런 네트워크들을 이론적으로 이해하고 다양한 모델링 방법과 분석 방법을 제시한다는 점에서 관심이 갔었다. 그러던 중 학교에서 이런 수업이 개설되었기에 수강하게 되었다. 이 수업은 크게 봤을 때 3부분인데, 첫 번째 부분은 네트워크에 대한 기초적인 얘기들과 기본적인 Graph theory 영역이고 두 번째는 기본적인 네트워크 모델링이며 마지막은 네트워크에서 발생하는 Dynamic process에 대한 내용이다. 텍스트북은 Newman의 Networks - An Introduction, Lewis의 Network Science, 그리고 마지막으로 Barrat의 Dynamical Processes on Complex networks 총 세 권이다.</p>


<p>네트워크의 종류는 정말 정말 많은데, 그냥 대부분의 관계도가 네트워크로 구성이 가능하고, network science를 사용해 분석이 가능하다. 예를 들어서 논문을 서로 refer하는 citation network를 분석할 수도 있고, 실제 사람들의 social network를 분석하거나 전력망 (power grid)을 네트워크로 생각해 문제를 푸는 것도 가능하다. Web도 hyper link와 page로 이루어진 network고, P2P역시 네트워크이다. 질병 역시 감염자와 비감염자로 판단할 수 있는 네트워크이고 Traffic 역시 네트워크이다.</p>


<p>이런 network는 결국 수학적으로 바라보게 된다면 graph이다. 네트워크란 무엇인가? 결국에는 어떤 포인트가 있고 그 포인트들이 서로 연결되어있는 형태가 아닌가, 이를 수학적으로 정의하면 vertex와 edge로 정의된 graph가 된다. 또한 graph를 이해하기 위해서는 수학적인 접근법이 필수적이다. 따라서 기초적인 graph theory에 대한 이해도가 필요하며, 임의의 graph가 아닌 특정한 네트워크를 modeling하게 되면 네트워크에 대한 설명을 보다 더 간단하게 하는 것이 가능하다. 이것이 우리가 network modeling을 배우는 이유이며, 이는 결국 우리가 하고 싶은 Network 상에서 어떤 dynamic process 역시 더 간단하게 분석할 수 있는 것이다.</p>




<h5>KAIST Network Science</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/network-science/" target="new">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/47" target="new">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/48" target="new">Graph Theory</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/49" target="new">Measures and Metric</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/50" target="new">Random Network</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/51" target="new">Small world Network</a></li>
    <li>Lecture 6: <a href="http://SanghyukChun.github.io/52" target="new">Scale free Network</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BIG 2014 세션들]]></title>
    <link href="http://SanghyukChun.github.io/45/"/>
    <updated>2014-04-09T10:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/45</id>
		<content type="html"><![CDATA[<h5>Introduction</h5>


<p>BIG 2014에 대한 설명은 <a href="http://SanghyukChun.github.io/44">이전 글</a>을 참고하길 바란다. 이 글은 BIG 2014 일정 4월 7일 월요일과 4월 8일 화요일 세션들에 대해 내가 기록한 것들을 간략하게 정리한 글이다. 첫 날 세션 중에서 흥미로운 세션은 Chris Volinsky의 mobile data analysis에 대한 talk과 Visualization talk 중에서 Matrix factorization을 사용해 진행한 paper도 나름 흥미로웠다. 그 밖에 나머지 세션들은 그냥 저냥 별로 interest하거나 inspiring하는 세션은 많이 없었다. 2일차 세션 역시 나에게 흥미가 가는 세션은 코넬 대학교의 Machine learning을 전공하는 교수님이 했던 talk과 kaggle engineer talk 정도였다.</p>


<hr>


<h4>1일차</h4>


<h5>Keynote talk 1: Shaping Cities of the Future using Mobile Data - Chris Volinsky</h5>


<p>이 talk을 진행한 <a href="http://www2.research.att.com/~volinsky/" target="new">Chris Volinsky</a>는 자그마치 Netflix prize의 winner 중 한 명이라고 한다. Machine Learning의 성공으로 꼽히는 대표적인 example 중 하나인 Netflix의 Algorithm을 만들어낸 사람의 talk을 듣게 될 줄이야. 내용도 흥미로운 내용이 많았다. 기본적인 아이디어는 우리가 살고 있는 장소, 더 구체적으로 말하자면 도시를 데이터를 기반으로 더 살기 좋은 장소로 만들자는 것이다. 다시 말해서 데이터 사이언스로 private한 data를 분석해 public good으로 만드려는 것이다. 이런 것의 예로 Netflix의 recommend system을 들었다. 즉, 그 자체로는 큰 의미가 없거나 private한 데이터들을 모아 big picture를 그리는 것이다. 이런 아이디어로 데이터를 공개하는 도시들이 많아지고 있는데, 뉴욕 시카고 텍사스 오스틴 등이 있다고 한다. 그리고 이런 데이터는 대부분 <a href="http://www.data.gov/" target="new">http://www.data.gov/</a>에서 확인가능하다고 한다. 나도 아직 제대로 본 적은 없는데 꽤 좋은 자료가 많은 듯. 실제로 이런 데이터를 사용해서 시카고의 범죄 데이터와 다른 데이터 간의 연관성 (뉴욕도 이런 실험을 했었다), 데이터 visuallization 등을 구축하고, 위에서 언급한 도시들은 API도 제공하고 portal도 제공한다고 한다. 이런 아이디어를 통해 더 나은 도시를 만들고자 하는 것이다. 이런 예는 정말정말 많다. 위에서 언급한 범죄 정보도 있고, street bump (문제가 있는 street), traffic, 날씨 등등.. 이런 접근을 통해 도시는 더 효율적이 되고 더 나은 movement를 장려하고 더 적은 전기, 물, traffic을 사용한다고 한다. </p>


<p>사실 최근 이런 움직임이 가속화되는 가장 큰 이유는 모바일 데이터인데, 실제 이 talk의 연사는 AT&T lab 소속으로, 모바일 데이터를 다룰 일이 많다고 한다. 모바일 데이터 중 하나는 위치정보인데, 안타깝게도 완전한 위치정보를 얻는 것은 불가능하다. GPS가 항상 켜져있는 것도 아니고 GPS가 항상 정보를 송신하는 것은 아니기 때문. 대신 서로 다른 전파탑과의 통신 기록이 남는데, 이 기록을 사용해 대략적인 위치를 추적하는 것이 가능하다고 한다.</p>


<p>Data access problem에는 다음과 같은 특성이 있는데 (1) No Content Ever, (2) Anonymize (always), (3) Aggregate (when possible), (4) Reduce granularity, (5) Principle of Least Privilege 가 그것이라고 한다. 아무튼 이 연사는 모바일 데이터를 사용해 사용자들의 움직임의 패턴을 분석해냈는데, 예를 들어 사람들이 아침 8시와 오후 6시에 각각 다른 장소 예를 들어 잁터와 집에 있을 것이다라는 가정을 하고 자도에 scatting을 해보면 실제 사람들의 traffic을 알 수 있다고 한다. 이때 쓰는 데이터는 사람들이 얼마나 많이 전화하고 문자를 하느냐 등의 정보로, 이를 사용하면 얼마나 많은 시간을 차 안에서 보내는지, 얼마나 많은 사람들이 대중교통을 쓰는지 등등을 알 수 있다고 한다. 실제로 이런 분석을 해보면 뉴욕보다 캘리포니아가 더 green하고 communication이 적다한다.</p>


<p>이 뿐 아니라 모바일 데이터를 응용하면 사람들의 움직임의 dynamics도 관측이 가능하다. 그렇다면 이런 질문이 가능한데, 만약 우리가 (통신사가) 데이터를 제공한다면 이를 얼마나 더 좋은 곳에 사용할 수 있을 것인가라는 궁금증이 생긴다. 실제 사람들의 문자와 전화 패턴만을 분석하여 (특정 사람의 정보가 아니라 특정 송신탑에 걸리는 network traffic을 분석한다) 사람들의 생활 양식을 알 수도 있고, 다음 버스가 언제 올 것이며 택시는 어디에 있을 것인가 등등을 inform하는 방식으로 우리 삶을 개선시킬 수 있다. 이런 예로 traffic의 흐름을 어떤 조건에 따라 예측할 수 있다면 우리의 삶은 크게 개선될 수 있다. 그래서 한 번 송신탑들이 받는 시간에 따른 데이터 시퀀스 정보를 사용해서 사람들이 움직이는 traffic을 그려봤는데, 대부분의 사람들이 중심에 살기는 하지만 엄청 멀리 사는 사람도 있고.. 하여간 엄청 복잡하단다. 그래서 이걸 supervised learning으로 learning하는데, label은 어떤 상황 (비가 오거나 주말, 주중, 낮, 밤 등등등) 에 데이터 시퀀스가 어떻게 변화할 것이냐를 learning하는 것이다. 이런 데이터 시퀀스로 traffic을 예상할 수 있기 때문이다. 즉, 우리가 알고싶은 정보를 기존의 데이터로 표현하고 기존의 데이터를 learning하는 것이다. 알고리듬은 simple nearest neighbor를 사용했는데, metric을 <a href="http://en.wikipedia.org/wiki/Earth_mover's_distance" target="new">earth mover&#8217;s distance</a>로 썼다고 한다. 이를 통해 learning해본 결과, 다양한 상황에 따라 어떻게 traffic data가 변화하는지를 learning할 수 있었고 실제 오차률도 작았다고 한다.</p>


<p>이 외에도 모바일 데이터를 (전화와 문자 사용 빈도) clustering한 결과 약 4개와 7개 cluster가 가장 optimal한 cluster인 것으로 나왔는데, 이런 clustering을 통해 요금제를 세분화하거나 마케팅을 세분화하는 등의 접근이 가능할 수 있다.</p>


<p>전반적으로 우리가 사용하기 힘들어보이는 데이터를 어떻게 의미있는 데이터로 만들어내느냐에 대한 얘기가 많았다. 매우 인상깊었다.</p>


<h5>Paper talk 1: Telling Commerce Stories Through Pictures, eBay Data Lab - eBay</h5>


<p>간단하게, eBay라는 엄청나게 거대한 big commerce data를 가진 업체가 자신들의 가정에 따라 데이터를 분석하고 이를 시각화하고 그에 대해 스토리를 풀어내는 talk이었다. 데이터 수집 및 분석 환경은 구글 페이스북 등 다른 인터넷 기업들과 별로 다를 것 없이 유사하고, 데이터 분석을 통해 5W1H (Who, What, When, Where, Why, How) storytelling을 이끌어내더라. 이때 문제라면 스케일이 너무 크기 때문에 이런 정보를 리얼 타임에 처리하고 이를 통해 실제 활용가능한 액션을 도출하는 것이 어렵다는 것. Visuallization을 하면 좋은 점이 이런 과정을 크게 줄일 수 있고, 비기술자도 쉽게 이해할 수 있다는 것이다. 그래서 간단한 분석으로 tax에 따른 seller와 buyer의 분포를 봤더니 tax가 싼 방향으로 시장이 형성된다. 예를 들어 CA는 밖으로 나가는 세금이 비싸서 대부분의 리테일러와 구매자가 CA 사람들이다. 이걸 Cross border로 확장할 수도 있고 global trading에도 쓸 수 있다고 한다.</p>


<p>Talk자체는 그냥 그랬고, 그냥 eBay에서 데이터 분석을 어떻게 쓰고 있는가 살펴볼 수 있는 talk이었다.</p>


<h5>Paper talk 2: Visual Analytics for interactive exploration of large-scale documents via nonnegative matrix factorization - 조지아텍</h5>


<p>이날 세션 중 두 번째로 흥미로웠다. <a href="http://www.cc.gatech.edu/~joyfull/" taget="new">Jaegul Choo</a>라는 분이 쓴 논문인데, Visuallization이라는 범주를 벗어나서, Matrix Factorization을 사용해 뭔가 Classification스럽게 사용했었다는 것, 그림이 굉장히 Deep Neural Network랑 유사하다는 점 두 가지가 흥미로웠다. 내용은 그닥 볼 것 없다. 그냥 예를 들어 갤탭과 아이패드 중 뭐가 나은가 보고 싶은데 리뷰가 각각 1300개 2000개가 있을 때 이걸 다 읽을 수는 없으니깐 데이터 마이닝을 적당히 하고 이를 시작화하면 의사결정에 도움이 된다, 그리고 이런 Visuallization을 nonnegative matrix factorization으로 풀어보겠다 라는 내용이다. NMF은 그냥 Topic Modeling으로만 사용한다는데, 내가 보기에는 단순한 Clustering으로 보였다. 즉, 이를 사용해 classification이나 clustering같은 general한 ML문제를 풀 수 있을 것 같다는 것이 나의 아이디어. 그리고 LDA보다 NMF가 엄청나게 빠르더라. Convergence도 빠르고 iteration도 적게 걸리고, 안정도 빨리 된다. (다 같은 얘기같지만..) 아무튼 이런 방법으로 visualization이 가능하다고 한다. <a href="http://www.cc.gatech.edu/~joyfull/resources/2014_big_vanmf.pdf" target="new">포스터</a>는 링크를 보면 되는데, 별거 없고 차라리 Visualization tool을 만든 <a href="http://www.cc.gatech.edu/~joyfull/resources/2013_tvcg_utopian.pdf" taget="new">논문</a>을 보는게 나은 것 같다. 제대로 읽어보지는 않았는데 NMF은 여기 나온다.</p>


<p>이 talk을 듣고 궁금해서 찾아봤는데 <a href="http://jmlr.org/proceedings/papers/v5/lee09a/lee09a.pdf" taget="new">EEG를 NMF로 Classification하는 논문</a>도 있더라. 여러모로 흥미로운 주제인 것 같다.</p>


<h5>Paper talk 3: mAnalytics: A Big Data Analytic Platform for Precision Marketing - China Mobile</h5>


<p>중국의 통신 기업 China Mobile이 어떻게 데이터를 분석하는가에 대한 내용인데.. 그냥 시스템이 어떻게 돌아가는지에 대한 내용이었다. Recommendation에 쓴다는 것 같은데 (mAnalytics의 m이 Marketing의 M) 내가 흥미를 가질만한 내용은 없었다.</p>


<h5>Invitation talk 1: Computational Education: A Big Data Opportunity? Electronic textbook, internet-based classes, new models of funding educations - MicroSoft</h5>


<p>MS의 엔지니어가 와서 했던 talk인데, 쉽게 생각해서 전자 textbook을 만들고 인터넷 베이스 클래스를 만들 때 기존에 존재하는 좋은 교육 시스템을 모아서 더 좋은 새로운 시스템을 만들자라는 내용이다. 그리고 그걸 데이터 기반으로 하는거지. &#8216;좋은&#8217; 시스템은 Algorithmically ML based로 분석하고 이를 모아서 일종의 앙상블처럼 취합하는 듯. 전반적으로 NLP의 내용이 많았다. 예를 들어 section의 난이도가 어떠냐를 분석하는건 syntatic complexity를 통해 결정하는데, 이건 완전 통짜 NLP.. 아무튼 이런식으로 good/bad를 labeling하고 구체적으로 probabilistic decision model을 만들어낸다고 한다 (이 경우는 good/bad binary class model). 이건 좀 졸아서 적은게 많이 없는데, 아무튼 Syntatic Complexity는 단어의 길이랑, 단어당 syllable의 개수, 문장 길이 등등으로 판별한다고 한다. 아무튼 결국 이렇게 새로운 textbook과 curriculum을 개발하는게 최종 목적인듯</p>


<p>Talk은 졸려서 많이 못들었는데, 일단 교육을 데이터로 접근한다는게 굉장히 신선했다.</p>


<h5>Paper talk 5: Scholarly Big Data-based Prescriptive Analytics System Enhancing Research Capability - KISTI</h5>


<p>text data (document) 분석하는 시스템 빌딩하는 것 같은데 발표 자료도 문제가 있고해서 뭔지 잘 모르겠더라. 시스템은 완성된 모양인데, 웹과 앱으로 deploy가 되어있다. 주소를 첨부한다. <a href="http://inscite-advisory.kisti.re.kr/search" taget="new">http://inscite-advisory.kisti.re.kr/search</a>, <a href="https://play.google.com/store/apps/details?id=net.xenix.inscite&hl=ko" taget="new">https://play.google.com/store/apps/details?id=net.xenix.inscite&hl=ko</a> 시스템은 어쨌거나 꽤 잘 만든 것 같다. UI도 그렇고 돌아가는 것도 그렇고..</p>


<p>추가: 웹에서 설명을 찾았다. 인사이트 어댑티브는 KISTI 소프트웨어연구센터. 컴퓨터 지능연구실에서 개발한 테크놀러지 인텔리전스 서비스입니다. 인사이트 어댑티브 서비스는 총 4개의 기술 심층 분석 서비스와 총3개의 기관(국가)심층 분석 서비스로 구성되며 최종적으로 기술 분석 보고서를 자동으로 생성하여 pdf 형태로 제공합니다. 인사이트 어댑티브 서비스는 논문, 특허, 웹의 다양한 정보를 기반으로 기술에 대한 심층적인 분석과 예측 결과를 제공할 뿐 아니라 사용자 의도를 지능적으로 인식하여 사용자에게 적응형, 맞춤형 서비스 또한 제공합니다.</p>


<h5>Paper talk 6: Building an Analytic Platform for The Web - Internet Memory</h5>


<p>데이터 분석용 시스템 논문이다. 기본 아이디어는 웹 데이터가 영구하지 않기 때문에 계속 보관해야하고, 또 엄청 크기때문에 분산 시스템으로 구축해야한다는 것이다. 그 이상은 잘 모르겠다. 내가 이해하기로는 이 talk은 web data가 시간이 지나면서 변하거나 없어지는 정보가 존재하는데 그 정보를 어떻게 잘 처리해서 그걸 잘 처리하는 시스템, 혹은 플랫폼을 만들었다는 것인거 같은데 talk은 영 별로더라. 아 그리고 preprocessing 얘기가 자꾸 나오는데 데이터를 처리하는 방법에 대해서도 다루는건가 잘 모르겠더라.</p>


<h5>Paper talk 7: Integration, Cross-Verification, Participation and Open Data: Opportunities and Challenges for Public Health</h5>


<p>Healthcare에 대한 talk이었는데, 정확히는 기억이 안나지만 노트해놓은 것을 보니 그냥 여러개의 데이터 소스를 섞어서 prediction을 하는 모양이다.</p>


<p>Challenges로는 new data sources integration & cross-correlation / citizens participation and data donors / open data가 있는데, 이것들을 cross-valdation, non-medical data sources for event-based surveillance 으로 해결한다고 한다.</p>


<hr>


<h4>2일차</h4>


<h5>Keynote talk: In-Memory Real-Time Big Data Processing: What It Takes to Innovate and Change Industry</h5>


<p>그냥 in-memory DB에 대한 talk이었다. 솔직히 이게 왜 여기에서 keynote talk으로 들어갔는지 이해가 안된다.</p>


<h5>Paper talk 1: A Cloud-based Framework for Evaluation on Big Data</h5>


<p>talk의 목표는 “Bring the algorithms to data, not data to algorithms&#8221; 인데, 그래서 정작 어떻게 하겠다는건지는 잘 모르겠더라. 그냥 데이터를 cloud로 저장하는 시스템을 만든 듯</p>


<h5>Paper talk 2: Metronome, Building Blocks for Data Products</h5>


<p>Dataset management system 논문이었다. 역시 딱히 흥미가 가지는 않았다.</p>


<h5>Invited talk: Big Data of the People, for the People: Understanding the Collective Wisdom of Users - Conell</h5>


<p>이 talk은 이날 talk 중에서 가장 흥미를 끄는 talk이었는데, 일단 발표자가 machine learning을 하는 사람이었어서 나랑 view point가 좀 맞는 편이었다.</p>


<p>이 talk의 motivation은 Human interaction data를 처리하는 것인데, 이게 무엇이냐 하면 그냥 사람이 interaction하면서 발생하는 data를 의미한다. 예를 들어 사람들의 클릭률 정보라거나 어느 페이지에 오래 있는지 등의 interation에서 발생하는 정보이다. 그런데 이런 정보의 문제가 무엇이냐 하면 내가 관측한 data가 실제 machine learning system에서 사용하는 training data와는 다르다는 것이다. 무슨 얘기냐하면, 사람들의 행동이 어떤 distribution을 따르는 것이 아니라 내가 준 상황 내에서 본인이 고를 수 있는 최선을 고르기 때문에 실제 general model의 training data로 사용할 수 없다는 것이다. 즉, 유저들의 decision process를 먼저 이해해야하는데, 이런 관점으로 바라보게 된다면 다음과 같은 새로운 접근 방법이 가능하다. Decision -> feedback -> learning algorithm. 무슨 얘기이냐 하면 사용자가 내린 결정에 대해 우리가 feedback을 주는 방식으로 learning algorithm을 만들 수 있다는 것이다.</p>


<p>간단한 예를 들어보자. 만약 우리가 두 개의 랭킹 function 중 하나를 선택해야하는 decision making problem이 있다고 하자. 대부분의 경우 real industry에서 하는 가장 합리적인 선택은 A/B test를 하는 것이다. Abandonment rate, reformulation rate, queries per session, click per query, click @1, max reciprocal rank, mean reciprocal rank, time to first click, time to last click 등의 정보들을 비교해 A와 B 중 어느 결정이 더 합리적인지를 밝혀내는 것이다. 그런데 <a href="ArXiv.org" target="new">ArXiv.org</a> 를 통해 case study를 해본 결과, 이런 여러가지 metric 중에서 그 어떤 metric도 expected order에 영향을 미치는 absolute metric이 없다는 결론이 나왔다고 한다. (이에 대해서 내 생각을 말해보자면, A/B test라는 것이 일종의 Maximum likelihood estimation 이기 때문에 발생하는 문제라고 생각한다. 우리가 봐야하는 정보는 엄청나게 많은데 매우 제한적인 정보만을 가지고 예측을 하기 때문에 정확하지 않은 결론으로 귀결되는 것이다.)</p>


<p>다시 말하지만 observed data와 training data는 다르다. Observed data는 user의 decision이고, 결국에 우리가 explicit feedback을 주면 해당 decision에 영향을 주게 된다. 즉, 이 decision 혹은 observed data는 training data와는 다르게 된다. 따라서 우리는 decision process를 개선할 수 있는 feedback function을 design해야하고, 우리가 machine learning으로 기여할 수 있는 부분은 이런 feedback function을 위한 learning algorithm을 만들고 feedback function을 개선시키는 것이다.</p>


<p>그래서 이 얘기를 하면서 Balanced interleaving라는 얘기가 나오는데 무슨 얘기인지 까먹었다. 아무튼 이런 문제를 dueling bandit problem으로 생각해 regret을 minimization시켜서 feedback function을 개선한다고 한다. 이때 retrieval function이 유한한 상황에서 dueling bandit로 인해 발생하는 reget은 theorically bounded된다고 한다. (그냥 쉽게 생각하면 이 알고리듬을 사용했을 때 기대되는 성능이 좋다는 의미이다)</p>


<p>그리고 또 하나는 coactive feedback model인데, unknown utility function algorithm/user interaction, relationship to other online learning models observe context x, learning algorithms presents y, user return y with utility function for different algorithms 라고 하며 이 과정이 일어날 때 마다 regret이 update 된다고 한다. 이런 feedback model에서는 interaction이 given x, feedback이 개선된 prediction y이며, 이 x와 y를 supervised learning으로 learning시킨다. language translate 등이 이런 방법으로 알고리듬을 개선시킨다고 한다.</p>


<p>이런 예로 발표자가 예전에 개발한 preference perceptron이라는 알고리듬을 소개하는데, 내용이 너무 빨리 지나가서 정확히 적지는 못하고 논문만 찾아봤는데 나중에 천천히 읽어봐야겠다.</p>


<p>결론적으로 이 talk에서 하고자하는 얘기를 정리해보면, 실제 service provider 입장에서 어떤 특정 decision을 내려야하는 경우가 많다. 예를 들어 search 알고리듬을 바꾸거나 하는 경우가 있는데, 어떤 algorithm을 선택해야할 것이냐, 혹은 바꾸는 것이 좋냐 나쁘냐를 결정해야하는 경우가 많이 있다. 그런데 이 decision making을 하는 과정에서 feedback function을 주고 이를 통해 decision을 개선해 decision의 질을 높인다. 약간 game theory 비슷한 느낌이었는데, 가장 적절한 feedback function을 고르겠다는 얘기도 조금 나온 것으로 보아 일종의 reinforcement learning이 아닐까 생각된다.</p>


<h5>Keynote talk: Evolution from Apache Hadoop to the Enterprise Data Hub: a new foundation for the Modern Information Architecture - Cloudera</h5>


<p><a href="http://www.cloudera.com/" target="new">cloudera</a>라는 기업의 product에 대한 설명이었다. 결국 이런저런 설명을 들어보니 <a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html</a> 요 product와 같은 설명이더라. 결론만 얘기하면, 지금 대부분의 시스템들이 저장 시스템 따로, Hadoop layer 따로, RDMBS layer 따로, 실제 application layer 따로, research layer 따로 BI 따로 진행하고 있는데 이 회사는 그 모든 것을 종합해주는 solution tool을 개발했다는 것이다.</p>


<p>그래서 talk 마지막에 스마트폰 예시를 들면서 이제 아무도 녹음기 따로 카메라 따로 전자 노트 따로 PDA 따로 저장장치 따로 안들고 다니고 스마트폰 하나만 들고 다니듯이 시스템도 나중에는 이런 종합 솔루션으로 통합될거라는 그런 talk이었다.</p>


<h5>Paper talk 3: BUbiNG: Massive Crawling for the Masses</h5>


<p>Open source crawler system 논문이었다. 내가 관심있는 주제는 아니었음</p>


<h5>Paper talk 4: Scalable Topic Change Detection in Social Posts</h5>


<p>노트가 잘 안되어있는걸보니 시스템 논문인 것 같다. 기본 아이디어는 소셜 데이터들이 마구 산개해있는것처럼 보여도 사실은 어떤 distibution을 가지고 있을 것이라는것, 그리고 변화 그 자체를 detection해서 시간에 따라 변하는 소셜 정보를 detect하자는 것. 그 정도였다.</p>


<h5>Invited talk: What do we learn from Kaggle machine learning competitions? - Kaggle</h5>


<p>가장 기대를 했던 talk인데, 스카이프 연결상태가 안좋아서 (온라인으로 talk을 했다) 내용도 잘 안들리고 PPT도 잘 안보였다. 하지만 그 중에서 기억나는 점을 꼽자면, 먼저 kaggle leader board 방식이 그냥 도입된 것이 아니라 상위 top player들의 performance를 높일 수 있는 optimal한 방법이라고 claim하는 것이었고, 그리고 실제 competition의 winner들의 algorithm들을 분석해서 얻은 결과였다. 크게 두 가지가 있었는데, 하나는 top 3 algorithm을 ansemble한 algorithm이 1등 algorithm보다 훨씬 좋았다는 것과 대부분의 top player들이 deep neural network based였다는 것. 그 두 가지가 꽤 흥미로운 결과였다고 할 수 있었다. 그만큼 neural network가 강력하다는 얘기이고, 또 하나는 지금까지 나온 그 어떤 모델들도 실제 현상을 잘 설명할 수 없다는 의미가 될테니까.</p>


<hr>


<p>나름 이틀 동안 들은 workshop이었는데, 뭐 그냥 그랬다. 재미있는 talk도 몇 개 있었고, 내가 전혀 관심없는 talk도 많았다. 특히 시스템 쪽이나 DB 쪽은 정말 재미가 없었다. 그래도 실제 real industry나 다른 연구자들이 어떤 focus로 데이터를 바라보고 있는지에 대해 알 수 있는 나름 의미있는 시간이었던 것 같다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BIG 2014 (2014/4/7 ~ 2014/4/8)]]></title>
    <link href="http://SanghyukChun.github.io/44/"/>
    <updated>2014-04-06T04:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/44</id>
		<content type="html"><![CDATA[<p>다음주 월요일부터 화요일 이틀간 <a href="http://big2014.org/" target="new">BIG 2014</a>라는 big data관련 workshop에 참여하게 되었다. 어떤 talk들이 주로 있는지 궁금해서 간략하게 프로그램을 훑어봤는데 이론적인 내용들보다는 실제 application level에서 일어나는 문제들이나 얘기들이 많이 있는 것 같다. 특히 시스템 쪽 얘기가 많이 나올 것 같은데 나는 이쪽 분야에 아직 어느 정도 관심이 있어서 들으면 재미있을 것 같다. 그리고 Visualization등과 관련되어 보이는 topic도 간간히 눈에 띄고, analysis에 대한 얘기도 눈에 띈다. 사실 새로운 이론적 깊이를 배우러간다기보다는 최근 real field에서 big data를 어떻게 생각하고 실제로 활용하고 있는지 보러간다는 것 자체가 나에게 큰 의미가 있을 것으로 기대된다. 무엇보다 나는 앞으로 이런 소위 말하는 빅데이터를 아이템 삼아서 벤처를 할 생각이니깐.. 특히 미국 쪽에서 어떤 움직임을 보이고 있는지 가서 잘 살펴보고 와야겠다. Invited speaker에 Kaggle engineer도 있고.. 관련 연구를 하는 교수님들도 있어서 이론적인 내용과 실제 apply하는 내용이 적절하게 잘 밸런스가 맞춰져 진행이 될 것 같다.</p>


<p>아마도 <a href="http://www2014.kr/" target="new">www2014</a>라는 코엑스에서 열리는 학회의 서브 프로그램인 것 같다. 서울에서 열린다는 점이 마음에 든다. 구글링해보니 작년 www2013은 브라질에서 한 모양이던데.. 아마도 인터넷 관련 학회인 것 같다. 논문도 내는 것 같고.. 일단 나와는 아주 상관이 있을 것 같지는 않다. 어쨌거나 코엑스에서 열려서 집에서 가기 좋다는건 참 괜찮은 것 같다.</p>


<p>해당 컨퍼런스의 프로그램은 아래와 같다. (프로그램은 홈페이지에서 따왔다.) <a href="http://ec2-50-112-76-239.us-west-2.compute.amazonaws.com/upload/big_agenda.pdf" target="new">PDF</a>로 받을 수도 있다.</p>




<h5>BIG 2014 Final Program</h5>


<table cellspacing="5" class="table table-bordered" style="width:600"><tbody><tr><td bgcolor="#4BACC6" colspan="2" class="white" style="text-align:center">
<strong>Monday, April 7th</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:15</strong>
</td>
<td bgcolor="#A5D5E2">
BIG&#8217;2014 Opening<strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:30</strong>
</td>
<td bgcolor="#D2EAF1">
<strong><u>Keynote Speaker</u></strong><strong>: </strong>Chris Volinsky, AVP AT&amp;T<br><strong>Shaping Cities of the Future using Mobile Data</strong><strong> </strong>
<ul><li>Introduced by Robin Chen</li>
</ul></td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>10:30</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Junlan Feng</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:00</strong>
</td>
<td bgcolor="#A5D5E2">
Neel Sundaresan and Jack Shen<br><strong>Visually:&nbsp; Telling Commerce Stories Through Pictures</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:25</strong>
</td>
<td bgcolor="#D2EAF1">
Jaegul Choo, Barry Drake and&nbsp;Haesun Park<br><strong>Visual Analytics for Interactive Exploration of Large-scale Document Data via Nonnegative Matrix Factorization</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:50</strong>
</td>
<td bgcolor="#A5D5E2">
Keyun Hu, Hongyan Yan and Junlan Feng<br><strong>mAnalytics: A Big Data Analytic Platform for Precision Marketing</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>12:15</strong>
</td>
<td bgcolor="#D2EAF1">
Frank Smadja<br><strong>The Big Data Challenges of Computational Market Research</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>12:40</strong>
</td>
<td bgcolor="#FBD4B4">
Lunch
</td>
</tr><tr><td align="right" bgcolor="#4BACC6" class="white">
<strong>13:45</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker:</u></strong> Rakesh Agrawal, Microsoft Technical Fellow<br><strong>Computational Education: A Big Data Opportunity?</strong><strong> </strong>
<ul><li>Introduced by Prabhakar Raghavan</li>
</ul></td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Frank Smadja</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:30</strong>
</td>
<td bgcolor="#D2EAF1">
Jinhyung Kim, Minhee Cho, Mikyoung Lee and Hanmin Jung<br><strong>Scholarly Big Data-based Prescriptive Analytics System Enhancing Research Capability</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:55</strong>
</td>
<td bgcolor="#A5D5E2">
Julien Masanes, Stanislav Barton and Philippe Rigaux<br><strong>Building an Analytic Platform for The Web</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>15:20</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Neel Sanduresan</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>15:50</strong>
</td>
<td bgcolor="#A5D5E2">
Patty Kostkova<br><strong>Integration, Cross-Verification, Participation and Open Data: Opportunities and Challenges for Public Health</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:15</strong>
</td>
<td bgcolor="#D2EAF1">
<strong>Panel:&nbsp;</strong><strong>Open Data: Holy Grail for Surveillance and Research - so what&#8217;s the problem?</strong><br><strong>Moderator:</strong> Patty Kostkova<br><strong>Panelists:</strong> Philip Abdelmalik, Ciro Cattuto, Daniel Hulme and Hans Ossebaard
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>17:15</strong>
</td>
<td bgcolor="#FBD4B4">
End of technical program of Day 1
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>18:30</strong>
</td>
<td bgcolor="#A5D5E2">
<strong>BIG Dinner<em> Sponsored by AT&amp;T</em></strong><strong> </strong>
</td>
</tr></tbody></table>




<table cellspacing="5" class="table table-bordered"><tbody><tr><td bgcolor="#4BACC6" colspan="2" class="white" style="text-align:center">
<strong>Tuesday, April 8th</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:15</strong>
</td>
<td bgcolor="#A5D5E2">
Preview of today’s agenda<strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>9:30</strong>
</td>
<td bgcolor="#D2EAF1">
<strong><u>Keynote Speaker</u></strong><strong>: </strong>Prof. Sang Kyun Cha, Seoul National University<br><strong>In-Memory Real-Time Big Data Processing: What It Takes to Innovate and Change Industry</strong>
<ul><li>Introduced by Alessandro Panconesi</li>
</ul></td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>10:30</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Paolo Boldi</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:00</strong>
</td>
<td bgcolor="#A5D5E2">
Allan Hanbury,&nbsp;Georg Langs, Bjoern Menze and&nbsp;Henning Müller<br><strong>A Cloud-based Framework for Evaluation on Big Data</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:25</strong>
</td>
<td bgcolor="#D2EAF1">
Paul Ogilvie, Jonathan Traupman, Xiangrui Meng and Doris Xin<br><strong>Metronome: Building Blocks for Data Products</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>11:50</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker</u></strong><strong>: </strong>Prof. Thorsten Joachims, Cornell<br><strong>Big Data of the People, for the People: Understanding the Collective Wisdom of Users</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>12:35</strong>
</td>
<td bgcolor="#FBD4B4">
Lunch<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Andrei Broder</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>13:45</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Keynote Speaker</u></strong><strong>:</strong> Amr Awadallah, Cloudera CTO and Co-founder<br><strong>Evolution from Apache Hadoop to the Enterprise Data Hub: a new foundation for the Modern Information Architecture</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>14:45</strong>
</td>
<td bgcolor="#D2EAF1">
Sebastiano Vigna,&nbsp;Paolo Boldi, Andrea Marino and&nbsp;Massimo Santini<br><strong>BUbiNG: Massive Crawling for the Masses</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>15:10</strong>
</td>
<td bgcolor="#FBD4B4">
Coffee Break<strong> </strong>
</td>
</tr><tr><td bgcolor="#31849B" colspan="2" class="white">
<strong>Session chair: Ronny Lempl</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>15:40</strong>
</td>
<td style="background-color: rgb(210, 234, 241);">
Sofia Kleisarchaki,&nbsp;Vassilis Christophides, Sihem Amer-Yahia and<br>Ahlame Douzal-Chouakria<br><strong>Scalable Topic Change Detection in Social Posts</strong><strong> </strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:05</strong>
</td>
<td bgcolor="#A5D5E2">
<strong><u>Invited Speaker</u></strong><strong>:</strong> Ben Hamner, Director of Engineering, Kaggle<br><strong>What do we learn from Kaggle machine learning competitions?</strong>
</td>
</tr><tr><td bgcolor="#4BACC6" class="white">
<strong>16:50</strong>
</td>
<td bgcolor="#D2EAF1">
BIG’2014 Closing<strong> </strong>
</td>
</tr><tr><td bgcolor="#E36C0A" class="white">
<strong>17:00</strong>
</td>
<td bgcolor="#FBD4B4">
End of technical program of Day 2<strong> </strong>
</td>
</tr></tbody></table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week4 & 5 - Applications]]></title>
    <link href="http://SanghyukChun.github.io/43/"/>
    <updated>2014-04-05T21:38:00+09:00</updated>
    <id>http://SanghyukChun.github.io/43</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 4주차와 5주차 강의를 요약한 글이다. 이 강의에서는 이론적인 형태의 무언가를 배운다기보다는 real application의 예로 Neural Network를 사용해 word prediction (NLP에서 쓰이는) 그리고 object recognition를 하는 방법에 대해 다룬다. 내가 관심이 있는 부분은 실제 이 NN을 어떻게 적용하냐라기 보다는 NN이란 무엇이며 어떤 이론적 배경이 있는 것이며 그 알고리듬에 더 관심이 있기 때문에 대부분이 engineerning issue인 이번 두 강의들은 대략적으로 어떤 내용에 대해서 다루는지만 아주 간략하게 기술하였다.</p>


<h5>Learning feature vectors for words</h5>


<p>Speech recoginition에서 가장 중요한 문제 중 하나는 바로 제대로 인식되지 않은 단어가 무엇일지 추측하는 것이다. 아무리 기술이 좋아지더라도 speech를 완전하게 복원하는 것은 불가능하다. 살제 사람들도 완전하게 speech를 인식하는 것은 아니고 때에 따라 적당히 추측해서 이해하는 것처럼, 이런 문제가 음성 인식에서 많이 중요한 문제로 대두되고 있다. 나도 seri를 사용하다보면 음성인식률이 생각보다 좋지않아서 실망하고는 하는데, 이런 문제점을 해결하기 위해 다양한 방법들이 제시되고 있다. 이 강의는 이런 것들을 개선시키기 위해 neural network를 도입했었던 연구들에 대해 다룬다. 참고로 여기에서 사용하는 모든 NN들은 softmax function neuron을 사용하고 error function은 cross entropy error를 사용한다. 이유는 sigmoid function을 사용했을 때 error가 rmse라면 제대로 우리가 원하는 방향으로 학습하기가 어렵기 때문이다. 왜냐하면 sigmoid function은 양 쪽 끝 부분이 거의 평평하기 때문에 만약 우리가 정 반대쪽 방향에서 rmse의 gradient 방향을 취하게 된다면 거의 변화가 없다고 판단할 수도 있기 때문이다. 따라서 이런 방법을 개선하기 위해서 cross entropy error를 사용하였고, 이 방법을 사용하기 위하여 sigmoid를 softmax로 바꿔서 probability distribution으로 만들어준 것이다. 아무튼 이 강의에서 설명하는 방법들은 이런 방법들에 기반해서 NN을 만든다. 아무튼 우리가 처음에 풀려고 했었던 단어를 추측하는 고전적인 방법 중에 <a href="http://en.wikipedia.org/wiki/N-gram" target="new">N-gram</a>문제라는 것이 있는데, n개의 단어 배열들을 학습하여 임의의 n-1개의 단어가 주어졌을 때 그 다음 단어가 무엇일지 예측하는 문제이다. 실제로 자연언어처리에서 많이 사용하는 기법 중 하나인데, 보통 trigram을 많이 사용한다 (3개의 단어 시퀀스를 학습) 그런데 이 경우 우리가 모든 단어를 학습할 수도 없고, 우리가 관측하지 못한 단어배열이라고 해서 세상에 존재하지 않는 단어 배열이라 확신할 수가 없기 때문에 당연히 성능 역시 좋지 않을 것이라고 예측할 수 있을 것이다. 대신 input을 앞의 2개의 단어를 취하고 그 output을 세 번째 단어로 하는 neural network를 학습할 수도 있을 것이다. 하지만 이렇게 할 경우 output의 양이 너무 많아지므로 대신 3번째 단어의 후보군들의 집합을 같이 넣어서 결과를 얻는 방식을 취할 수도 있을 것이다. 더 성능을 높이기 위해서 엄청 긴 word seqeunce를 통채로 학습하고, 임의의 단어의 앞의 n개 단어 뒤의 m개 단어를 보고 추측하는 것이 가능할 것이다. 이 경우, 지금 단어가 random인지 제대로 된 단어인지 일부러 섞어서 learning을 하게 되면 output을 binary로 받는 것이 가능해져서 엄청나게 빠른 test time을 가질 수가 있게 된다는 장점이 있다.</p>


<h5>Object recognition with neural nets</h5>


<p>우리가 물체를 인식하는 것을 매우 자연스러운 일로 생각하지만 실제로 이것을 구현하는 것은 절대로 쉬운 일이 아니다. 우리는 물체가 살짝 가려져 있어도 구분이 가능하고, 또한 해당 물체를 유동적으로 인식한다. 무슨 말인가하면, 인식 알고리듬을 디자인하는데 있어서 문제점은 (1) 물체가 다른 물체 혹은 주변 환경에 의해서 가려진 상태일 때 (2) 밝기와 조명에 따른 해당 물체의 색 변화 (pixel 정보가 변한다) (3) 같은 클래스에 속해도 조금씩 다른 모습 - 예를 들어서 손글씨 숫자는 비록 같은 숫자이나 전부 필체가 달라서 variation이 있다. (4) 물체가 정의되는 것은 모양이 아니라 다른 방법으로 결정되는 경우도 많다 - 예를 들어 의자는 모습으로 구분하는 것이 아니라 그 사용처가 어디인가에 따라 분류해야 구분할 수 있을 것이다.</p>


<p>그 밖에도 결국 우리가 사용할 수 있는 정보는 image 정보이고, 디지털 input image는 단순한 pixel map이다. 예를 들어서 1024 by 720 pixel의 사진이라고 한다면 총 737280개의 pixel이 input이 될 것이다. 각 pixel은 RGB정보를 가지고 있으므로 값은 (0,0,0) ~ (255,255,255) 사이의 값으로 정해질 것이다. 색상이 중요하지 않은 경우에는 흑백 사진으로 바꾸어 단순히 밝기로만 판단하기도 한다. 아무튼 그렇기 때문에 약간의 물체의 이동도 성능을 크게 바꿔버릴 수 있다. 단순히 옆으로 이동한 것 뿐 아니라 회전된 정보나 뒤집힌 정보는 우리가 인식 알고리듬을 작성하는데에 매우 어려운 부분으로 작용한다. 따라서 우리는 모든 정보를 normalization시켜야할 필요가 있으며 align해야 할 필요가 있다. 이런 처리 없이는 올바른 정보를 학습하기가 매우 어려워질 것이다.</p>


<p>이 강의에서는 이런 문제점들을 가지고 있는 object recognition을 neural network로 접근한다. 이런 경우 determistic method보다 훨씬 더 좋은 성능을 낼 수 있으리라는 것은 자명할 것이다. Input은 pixel map이고, output은 어떤 object인지 알려주는 label 혹은 class가 될 것이다. 이런 Hidden layer가 포함된 neural network를 backpropagation 방법을 사용하여 learning한다. 실제로 많은 image recoginition에서 neural network를 사용하고 있으며, 앞에서 설명했었던 여러 문제점들을 해결해주는 경우가 많다. 또한 아마 이런 목적을 가지고 설계된 알고리듬 중에서는 neural network가 가장 성능이 좋을 것이다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week3 - Backpropagation]]></title>
    <link href="http://SanghyukChun.github.io/42/"/>
    <updated>2014-03-26T00:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/42</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 3주차 강의를 요약한 글이다. 이 렉쳐에서는 Perceptron의 한계를 극복하기 위해 도입된 multi-layer feed forward network를 learning하는 algorithm인 backpropagation algorithm에 대해서 다룬다.</p>


<h5>Learning the weights of a linear neuron</h5>


<p><a href="http://SanghyukChun.github.io/40#what-perceptrons-cant-do">Lecture 2의 마지막</a>에서 다뤘던 perceptron의 가장 큰 문제점은 문제가 조금만 복잡해지거나, linear하게 표현되지 않는 문제는 올바른 결과로 수렴할 수 없다는 것이었다. 예를 들어서 엄청나게 간단한 로직인 xor은 perceptron으로 learning될 수 없다. 이런 문제를 해결하기 위해 <a class="red tip" title="이 렉쳐에서는 multi-layer perceptron이라고 지칭한다.">multi-layer feed forwad neural network</a>의 필요성이 대두된다. 일단 우리는 기존의 perceptron algorithm으로는 해당 문제를 해결할 수 없다는 것을 알고있으므로, 무언가 다른 알고리듬을 한 번 고안해보도록 하자. 가장 간단하게 생각할 수 있는 알고리듬은 어떤 error function을 정의하고 그 error를 minimize시키는 network를 learning하는 것일 것이다. 그렇다면 어떤 error function을 minimize해야할까? 간단하게 neural network의 output을 계산해서 expected value (target value)와 actual output value의 차이를 error로 정의하면 어떨까? 즉, neural network의 output이 우리가 원하는 output과 가장 근접한 weight를 learning하는 방법을 취하는 것이다. 근데 여기에서 문제가 하나 생기게 되는데, 바로 이 error function이 weight에 대해서 convex하지가 않다는 것이다. 당연히 관련되는 weight set도 엄청나게 많고, network자체가 convex한 form이 아니기 때문이다. 즉, 실제 좋은 performance를 내는 weight들의 여러 set을 취해 그 중간 값을 취해 얻은 weight가 좋은 weight가 아닐 수도 있다는 것이다. perceptron algorithm의 가장 attractive한 점은 algorithm이 convex하기 때문에 언제나 같은 값으로 converge한다는 점인데, 이런 방식으로는 그런 convergence가 보장이 안되는 것이다.</p>


<p>일단, 어쨌거나 perceptron이 적용이 안되는 상황이니깐, 위에서 정의한 error minimization의 측면에서 문제를 접근해보도록하자. 가장 간단한 예시인 Linear neuron에 대해서 살펴보자. 일단 y를 neuron을 통해서 얻어진 estimated output, w를 weight vector, x를 input vector라고 하면 다음과 같은 식을 세울 수 있다.</p>


<p>$$ y = \sum_i w_i x_i = \mathbf w^\top \mathbf x $$</p>


<p></p>

<p>이 상황에서 input vector x의 real output을 t (target output)이라고 해보자. 이런 상황에서 가장 간단한 error는 actual output과 desired output의 squared difference이다. 즉, 이를 수식으로 나타내면 \(error = \sqrt{t-y}\)로 표현할 수 있을 것이다. 우리의 목표는 이런 상황에서 weight를 iterative method를 사용하여 구하고 싶은 것이다. Iterative method라는 것은 어떤 특정한 반복적인 알고리듬을 사용하여 (예를 들어 gradient descent나 perceptron처럼) 계속 값을 update시켜나가면서 가장 적절한 것으로 보이는 값을 찾아내는 방법이다. 즉, \(w_{t+1} = f(w_t) \)로 표현이 가능하다. \(w_t\)는 t번 째 loop에서 w의 값이고, f는 w를 update하는 rule이다. 그렇다면 여기에서 잠시 궁금한 점이 생길 수 있다. 만약 우리가 target vector를 알고 있다면, 왜 문제를 analytically하게 해결하지 않을까? 즉, 우리가 이미 x와 y를 안다면 이를 가장 최적화시키는 w를 계산으로 단 한 번에 구할 수 있을 것인데, 왜 하필 iterative method를 사용하여 계속 값을 update하는 것일까? 훨씬 비효율적이지 않을까? 이 질문에 대한 알고리듬 관점에서 바라봤을 때의 답을 간략하게 말해주자면, 그런 형태의 analytic solution은 반드시 문제가 linear해야하고 또 squared error measure에 대해서만 working하기 때문인 것이 하나, 그리고 Iterative method가 조금 비효율적으로 보일지는 몰라도 더 복잡한 네트워크에 대해서 generalize하기가 더 간단힌 이유 하나를 들 수 있을 것이다.</p>


<p>이런 iterative method는 맨 처음 모든 weight를 random하게 guess하고 <a class="red tip" title="조건은 바뀔 수 있다. 예를 들어 input 3개를 보고 update하는 것도 가능하다. 뒤에서 조금 더 자세히 다루도록 하겠다.">매 input마다</a> 적절하게 weight를 update시킨다. 이 방법은 weight가 어떤 특정한 value로 converge할 때까지 계속된다. 그렇다면 이런 방법의 예를 하나 들어보자. 이 강의에서는 다음과 같은 function을 정의한다. \(price = x_{fish} w_{fish} + x_{chip} w_{chip} + x_{ketchup} w_{ketchup}\) 즉, 내가 식당에서 <a class="red tip" title="fish and chips라고 하는 요리.. 생선튀김이랑 감자튀김 같이 먹는거랑 똑같다">생선과 칩과 케첩</a>을 먹었을 때 내가 지불해야하는 금액을 내가 먹은 양 (x), 그리고 각 item들의 가격 (w)으로 나타낸 것이다. 내가 알고 있는 값은 input x (내가 시킨 양) 그리고 계산서를 통해 얻은 값이다. 하지만 나는 w를 모르며, 이 w를 찾는 것이 목적이다. 그렇다면 처음에는 random하게 w를 guess할 수 있을 것이다. 이때, (120, 50, 100)이 true weight라고 해보자. 즉, 현재 input이 2,5,3일 때 price는 850일 것이다. 현재 우리는 weight에 대한 정보가 없으므로 모두 50이라고 가정하면 내가 estimate한 price는 500이고, error의 값은 350이 된다. 이때, \(\triangle w_i = \epsilon x_i (t-y)\)라는 learning rule이 있다고 해보자. (이 learning rule은 delta-rule이라는 규칙으로, 바로 다음 단락에서 자세히 다루도록 하겠다.) 이 수식을 적용하면 다음 weight는 70, 100, 80이 되고 error는 30으로 줄어들게 된다 (esitimated price = 880, true = 850) 이런 식으로 각 iteration마다 error의 값을 줄여나가면서 true weight를 찾는 것이 iterative method의 작동원리인 것이다.</p>


<p>그렇다면 이런 방법에서 가장 중요한 개념은 아마 learning rule일 것이다. 이 렉쳐에서는 &#8216;Delta Rule&#8217;이라는 rule을 소개하고 있다. 이 방법은 일종의 Gradient Descent method인데, single layer neural network에서 주로 사용하는 방법이라고 한다. 자세한 설명은 <a href="http://en.wikipedia.org/wiki/Delta_rule" target="new">wiki</a>를 참고. 그렇다면 왜 delta-rule은 \(\triangle w_i = \epsilon x_i (t-y)\) 의 꼴을 띄고 있는 것일까? 증명은 간단하다. error를 squared residuals summation error로 정의하고 차근차근 수식을 전개하면 해당 꼴을 얻을 수 있다. wiki에도 언급이 되어 있으므로 설명이 미진하다면 wiki를 참고하면 될 것 같다. 먼저 \(E = \sum_j \frac 1 2 (t_j - y_j)^2\)이라하자. (notation은 wiki의 notation을 사용하겠다.) 이 error는 convex function이고 domain도 convex하므로 gradient descent method를 사용하면 error의 global minimum값을 반드시 찾을 수 있다. 따라서 만약 우리가 &#8220;weight space&#8221;에 대해서 이 error를 최소화하게 된다면 매 순간 minimize하기 위해 내려가는 방향 즉, 이 함수의 gradient 값은 \(\frac {\partial E} {\partial w_{ji}}\)이 될 것이다. 이때, 이 gradient descent는 error를 줄이기 위해서 필요한 weight들의 change이고, 방향은 반대이므로 \(\triangle w_{ij} = - \epsilon \frac \partial E \partial w_{ji}\)라고 할 수 있는 것이다. 그리고 뒤의 미분항을 간단하게 chain rule을 사용하여 정리하면 이전의 식은 결국 다음과 같은 수식으로 표현이 가능하다.</p>


<p>$$ \triangle w_{ij} = \epsilon (t_j - y_j)x_i $$</p>


<p>wiki에서는 active function의 미분항까지 들어가게 되는데, 이 경우는 일단 생략하였다.</p>


<p>이제 update rule을 만들었으니 필연적으로 생기는 question들을 점검해보자. (1) 이 알고리듬은 반드시 global한 값으로 converge하는가? - convex optimization이기 때문에 global truth로 converge하긴한다. 적절한 step size가 필요한데 이것은 이론적으로 구할 수 있으므로 큰 상관이 없다. (2) converge rate는 얼마나 될 것인가? - gradient descent method들이 대부분 그러하듯 많이 느릴 것이다. 이를 개선하기 위해 steepest descent method를 적용하는 등의 방법이 있는 것으로 보인다. 마지막으로 perceptron과 비교해보자. perceptron은 &#8216;error가 발생해야만&#8217; update가 일어났으며, error는 binary error였기 때문에 update가 일어나지 않을 수도 있었다. 하지만 지금은 error가 real function이므로 error는 거의 항상 non zero value가 되고 update도 지속적으로 일어난다. 또한 perceptron이 아무런 parameter tuning이 없던 것과 비교해 (margin은 일단 예외로 하자) learning rate를 골라야하는 귀찮은 문제가 하나 생기게 되었다.</p>


<h5>The error surface for a linear neuron</h5>


<p>이 소강의는 거의 언급할 내용이 없다. 앞에서 이미 이 문제가 convex임을 밝혔으며, 또한 weight space라는 concept역시 이미 언급했다. 언급되고 있는 문제는 거의 gradient descent method의 문제점들이다. 특히 convergence rate가 느린 경우, zig-zag하게 수렴하는 경우는 어떻게 해야할 것인가? 등에 대한 question만 던지는 강의이기 때문에 과감하게 생략하도록 하겠다.</p>


<h5>Learning the weights of a logistic output neuron</h5>


<p>delta rule을 logistic neuron에 대해 적용하는 것인데, 결론만 얘기하면</p>


<p>$$ \triangle w_{ij} = \epsilon (t_j - y_j) y_i (1-y_i) x_i $$</p>


<p>의 꼴이 된다. 즉, 앞에서 언급했던 activate function의 미분값인 \(y_i (1-y_i)\)가 포함되는 형태라는 것만 알아두면 된다. 다만, 이 경우에 binary threshold neuron이 아니라 logistic neuron을 쓰는 이유는 binary threshold neuron은 error가 항상 0아니면 1이기 때문에 gradient descent method를 사용할 수 없기 때문이다. 이제 간단한 배경지식을 갖추었으니 이번 렉쳐의 메인인 backpropagation으로 넘어가보자.</p>


<h5>The backpropagation algorithm</h5>


<p>자, 사실 앞에서 이런저런 얘기를 주절주절 했던 이유는 바로 backpropagation algorithm에 대해 설명하기 위해서였다. 이 algorithm은 당연히 iterative method이며, logistic neuron에 대해서 delta-rule (gradient descent method)를 적용하여 최적의 weight를 계산해낸다. 이 알고리듬은 hidden layer가 존재하는 neural network를 learning하기 위해 사용이 되는 알고리듬이며, <a class="red tip" title="backpropagation은 역전파, 즉 반대 쪽으로 영향을 미친다는 뜻이다, 이 경우는 결과를 통해 weight를 학습하기 때문에 역전파라고 부른다">이름에서 알 수 있듯</a> network의 output value에서부터 역으로 weight를 learning하게 된다. 왜 우리는 hidden layer가 존재하는 neural network를 learning해야할까? 이런 방법을 쓰지 않으면 network가 항상 linear하기 때문에 real problem을 풀 수가 없기 때문이다. 그리고 또한, hidden layer를 사용한다는 의미는 우리가 임의의 feature를 정하고, 각 feature들의 weight가 얼마나 되는지 학습을 한다는 의미와 같다. 무슨 얘기이냐하면, 만약 엄청나게 dimension이 큰 input이 있을 때 (예 - 해상도 높은 사진) 실제 algorithm을 돌릴 때 모든 input을 사용해 learning하는 것은 거의 의미가 없고 (특히 high dimension, samll input인 경우는 overfitting issue가 크게 작용한다.) 해당 알고리듬에 대입해서 실행시킬 feature를 뽑아내는 과정을 필요로 하는 경우가 많다. 그런데 대부분의 경우 우리는 이런 feature를 heuristic하게 찾는다. 즉, 사진에서 눈, 코, 입을 feature로 삼아야한다고 우리의 heuristic으로 결정하고, masking을 손으로 하고 그 결과를 알고리듬에 대입하는 것이다. 그런데 hidden layer를 사용하게 되면 그런 불필요한 행동을 줄일 수 있다. 만약 hidden unit각각이 머리카락, 눈, 입술, 코, 귀 등등을 의미하고 있다면 적절한 weight를 learning함으로써 feature에 대한 weight를 결정할 수 있고, 우리가 일일이 손으로 하던 것들을 자동화시킬 수 있는 것이다. 이렇기 때문에 hidden layer가 포함된 neural network가 powerful하고 meaningful하다. 그리고 backpropagation을 사용하는 이유는 그것이 가장 효율적이고 빠른 학습 방법 중 하나이기 때문이다.</p>


<p>Backpropagation이 아닌 다른 예를 하나 생각해보자. 예를 들어서 output을 사용하지 않고 initial weight를 주고 weight를 조금씩 변화시키면서 적절한 값을 찾을 수도 있을 것이다. (Learning using perturbations) 즉, 원하는 target value를 고정해두고 해당 value에 가장 가깝도록 weight를 하나하나 강제로 조정하면서 전체 weight를 찾아가는 다소 reinforcement learning과 비슷한 방법으로 접근하는 것이 가능할 수도 있다. 그러나 이런 방법은 큰 문제가 있다. 먼저 weight가 많아질수록 찾아야하는 값이 많아지고 computation time이 엄청나게 빠르게 증가할 것이다. 또한 이런 방법은 weight에 대해 network가 convex하다면 의미가 있을 수 있지만 당연히 hidden layer가 포함된 network는 convex하지 않다. 결국 이 방법은 우리가 상상도 하지 못할 만큼 많은 양의 computation time을 필요로 하는 좋지 못한 방법인 것이다. 심지어 아주 적은 수의 neuron만 있더라도 바로 뒤에서 설명하게 될 backpropagation이 더 성능이 우수하기 때문에 이런 방법 자체를 사용하지 않는 것이다.</p>


<p>그렇다면 이제 backpropagation algorithm에 대해 discribe해보자. backpropagation의 기본 아이디어는 우리가 hidden unit들 그 자체에 대해서 알 필요가 하나도 없고 (알 수도 없을 뿐더러), 대신 hidden unit들로 인해 생성되는 error change를 관측하는 것이 더 낫다는 것이다. 즉, hidden unit 그 자체의 activity를 learning하는 것이 아니라, hidden unit들로 인해서 생겨나는 error derivatives를 사용하자는 것이다. 이 방법은 ouput layer에서 아래 layer로 정보를 backpropagation하여 (역으로 보내어) lower layer에서 그 값을 기준으로 다시 weight를 update시킨다. input pattern은 hidden layer에 전달이 되고, 다시 hidden layer가 output layer로 전달을 시키므로 (hidden layer가 하나일 때) 이런 방법으로 현재 weight에 대한 expected value와 estimated value 사이의 error를 구할 수 있고 이것을 최소화 하는 방향으로 weight를 learning하는 것이다. weight를 learning할 때는 앞에서 우리가 이미 살펴보았던 delta-rule을 사용하여 output layer에서의 각 neuron들의 error를 사용해 weight들을 update한다.</p>


<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network" target="new">링크</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/42-1.png" width="600"></p>

<p>Backpropagation은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>


<h6>Phase 1: Propagation</h6>


<ol>
    <li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 &#8216;forward&#8217; propagation이라 한다.)</li>
    <li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 &#8216;back&#8217; propagation이라 한다.)</li>
</ol>


<h6>Phase 2: Weight update</h6>


<ol>
    <li>Delta rule을 사용해 weight를 update한다. update rule은 다음과 같다. (delta rule for logistic neuron)<br>
        \( \triangle w_{ij} = \epsilon (t_j - y_j) y_i (1-y_i) x_i \)</li>
</ol>


<p>위의 과정은 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -> hidden k, hidden k -> hidden k-1, &#8230; hidden 2 -> hidden 1, hidden 1 -> input의 과정을 거치면서 계속 weight가 update되는 것이다. 그리고 이 cycle자체가 converge했다고 판단될 때 까지 계속 반복된다.</p>


<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 &#8216;역으로&#8217; 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>


<h5>How to use the derivatives computed by the backpropagation algorithm</h5>


<p>Overfitting과 Optimization issue가 나오는데, Online, batch update 중 무엇을 고르느냐, 어떻게 overfitting을 줄이냐 등등, 이미 예전에 많이 다뤘거나 앞으로 다시 다뤄질 주제들이라 판단되어 생략하도록 하겠다.</p>


<p>다만, backpropagation에 대해 중요한 언급이 빠져있어서 첨언을 하자면, backpropagation 은 항상 global optimum으로 converge하지 않기 때문에 언제나 local minimum으로 converge할 가능성이 존재한다. 이는 특히 hidden layer가 많아지면, 혹은 네트워크가 deep해지면 deep해질 수록 더 심해진다. 따라서 initial value를 어떻게 설정하느냐가 매우 민감하다. initial value에 따라 수렴하는 방향이 달라질 수 있기 때문인데, 나중에 배울 Deep belif network에서는 initial value를 미리 pre-training하는 방법으로 이를 극복해낸다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[행복에 대한 단상]]></title>
    <link href="http://SanghyukChun.github.io/41/"/>
    <updated>2014-03-22T17:02:00+09:00</updated>
    <id>http://SanghyukChun.github.io/41</id>
		<content type="html"><![CDATA[<h5>#1</h5>


<p>사실 나에게 있어 &#8216;행복&#8217;이라는 단어는 막연하고 추상적인 단어에 불과했다. &#8216;행복해지기 위해서 살아간다&#8217; 라는 이야기를 들어도 당장 내가 행복한 것도 아니었고, 나에게 있어서 중요한 것은 당장 행복해지거나 미래에 행복을 향해서 달려가는 것이 아니라 나에게 주어진 일을 해내는 것이나 혹은 내가 목표로 하는 무언가에 도달하는 것에 더 급급했었기 때문이다. 그 때문인지는 몰라도 그런 &#8216;달려나가는 것&#8217;과는 연관없는 연애 등에도 무심했던 것일지도 모른다.</p>


<h5>#2</h5>


<p>생각해보면 그렇다. 내 인생에 있어서 많은 (부정적인) 영향을 미쳤던 초등학교를 졸업한 이후로 나는 줄곧 &#8216;경쟁&#8217;이라는 시스템 안에서 살아왔고 그에 어느새 자연스럽게 적응하고 순응해버렸다. 중학교부터 과학고등학교 입시 경쟁에 뛰어들면서 내 하루하루는 나를 위한 것이라기 보다는 입시를 위한 무한한 경쟁 속을 살아가는 것에 불과했을지도 모른다. 그런 와중에 내 행복에 대한 감각은 서서히 무뎌지고 끝내 퇴화해버렸을지도 모른다.</p>


<h5>#3</h5>


<p>대학에 들어와도 크게 달라지지 않았다. &#8216;징벌적 수업료&#8217;, &#8216;연차초과자에 대한 불이익&#8217;, &#8216;강제적 영어수업&#8217; 등은 나를 더 초조하고 불안하게 만들었으며 하루 종일 도서관에 앉아 공부를 하지 않고서는 도저히 안심할 수 없도록 만들었다. 그 누구의 탓도 아니었지만 나는 결국 내 자신이 선택하여 다시 그런 삶을 살기로 결정했던 것이다. 그렇게 카이스트에서 3년이라는 시간을, 나는 친구를 사귀거나 추억을 만드는 일보다 치열하게 사는 일에 더 집중했었다.</p>


<h5>#4</h5>


<p>그러던 중 카이스트에 짧은 시간 동안 큰 일이 터지게 되었고, 학교에서는 이에 대한 근본적인 해결법을 모색하기 시작했고, 그 결과 2012년 봄에 &#8216;행복론&#8217;이라는 수업을 개설하게 되었다. 나는 그 당시 내가 좋아하던 사람의 같이 수강하자는 꾀임에 빠져 그 수업을 수강하게 되었다. 수업은 그야말로 최악이었다. 카이스트 안의 그 어떤 강의실에도 수용할 수 없는 대규모 인원은 창의관에서 가장 많은 인원이 들어갈 수 있는 터만홀에서조차 바닥에 앉아야하는 상황이 발생했다.</p>


<h5>#5</h5>


<p>당시 나는 MSK에서 팀장을 하고 있었고, 그 수업이 1시 수업이었음에도 불구하고 12시 이전에 일찍 자리를 맡으러 가는 것이 너무 버거웠고 매일 바닥에 앉아 수업을 들어야했다. 거기에 수업 내용도 &#8216;행복론&#8217;이라기보다는 소소한 영역에서도 행복함을 찾았던 교수 본인의 인생사를 늘어놓는 것에 불과했다. 나는 마이클 샌델이 강의했던 행복론 등의 강의를 기대했었지만 점점 나는 그 강의에 대해 짜증이 나기 시작했고 나는 그 강의를 들을 때 마다 불행해졌다.</p>


<h5>#6</h5>


<p>그러던 중 어느날 그런 생각이 들었다. 이 많은 사람들 중에서 나처럼 모든 것이 불행하고 힘들고 짜증나는 사람도 있지만 교수처럼 모든 일에서 행복에 대해 논하고, 같은 환경에서도 더 즐거운 삶을 살면서 자신의 삶에 대해 만족감이 높은 사람들이 있는 것은 왜일까? 저 사람은 그 동안 어떤 삶을 살았기에 저렇게 행복한 삶이라고 본인이 당당하게 말할 수 있을까? 그런 생각을 하는 순간 나는 나를 불행하게 만드는 주체가 다름아닌 나였다는 사실을 깨닫게 되었다.</p>


<h5>#7</h5>


<p>학기가 끝나고 나는 이음에서 인턴을 하게 되었다. 처음에는 정말 행복했다. 내가 꼭 하고 싶었던 벤처에서의 생활, 모두에게서 배울 점이 있다는 사실은 나를 흥분시키기에 충분했고, 작은 것이나마 기여하기 위해 정말 열심히 노력했다. 처음 1주 정도는 정말 행복하고 내가 벤처를 해야겠다는 생각을 했었다는 것을 마음 속 깊숙히 다시 새기게 하는 좋은 계기가 되었다</p>


<h5>#8</h5>


<p>그러나 거기까지였다. 나는 엄청나게 간단한 일조차 제대로 처리하지 못헀고, 같이 들어온 동기와 계속해서 비교받고 다른 사람들이 나를 무시하고 경멸하는 것을 느끼면서 회사를 다녀야만 했었다. 회사를 나가는 하루하루가 지옥같고 나를 인턴으로 데려온 형과 인연을 끊고 그냥 빨리 퇴사를 해야겠다는 생각까지 진지하게 하고는 했었다. 그때는 정말 내가 쓸모없고 제대로 할 수 있는 일도 없고 모든 사람들이 나를 싫어하는 것만 같아서 너무나 힘들었다.</p>


<h5>#9</h5>


<p>그러던 내가 결국에는 남들에게 인정받고, 어느 정도는 기여를 할 수 있는 사람이 될 수 있었던 이유는 다른 것이 아니라 내가 내 일을 즐기고, 내가 무언가 도움이 될 수 있다는 것이 행복했기 때문이었던 것 같다. 물론 나를 대놓고 무시하고 경멸하던 사람들이 퇴사를 했던 것도 컸지만, 그보다 더 중요한 것은 내가 내 일에 대해 주인의식을 가지고 그 일을 처리하는 것에서 행복감을 느끼기 시작했다는 것이 더 중요했던 일이었다. 그 이전에 내가 하던 일은 남이 하던 일을 받아서 유지보수를 하며 남이 만들어놓은 버그를 내 힘으로 고치면서 문제가 생길 때 마다 내 몸으로 막으면서 내가 하는 일도 없이 하루하루 지쳐가는 것에 불과했지만, 어느 순간부터 내가 하는 일은 문제를 주도적으로 고쳐나가고 새로운 것을 만드는 일이 되었다. 그럴 수 있었던 이유는 내가 너무 하기 싫고 힘들었던 일을 인정하고 그 일을 고치고 새롭게 시작하면서부터였다.</p>


<h5>#10</h5>


<p>퇴사를 하고 나서 지구가 한 바퀴 돌고, 나는 카이스트에 석사생이 되어 Machine Learning을 공부하고 있다. 나는 내가 하는 일에 만족하고 행복하지만 가끔 그런 행복에 대한 것을 잊을 때가 있다. 가끔 나는 내가 불행하다고 느끼는데, 그럴 때 마다 나에게 아이러니하게 비춰지는 모습은 카이스트로 피크닉을 나온 행복한 가족과 연인들의 모습이다. 같은 장소에서 나는 이렇게 불행하다고 느끼고 있고 버거워하고 있는데, 저 사람들은 지금 이 순간이 너무나 행복하고 미소를 짓고 있다는 사실이, 나에게는 다시 한 번 행복에 대해서 생각을 하게 하는 좋은 동기가 된다.</p>


<h5>마치며</h5>


<p>행복이라는 것은 결국 마음먹기 나름인 것인 것 같다. 감정에 있어서만큼은 사람에게는 양성 피드백 루프가 형성되어있는 것 같다. 내가 지금 하는 일이 그저 힘들기만하고 괴롭기만하다고 느끼면 나는 점점 더 힘들어진다. 반면 내가 지금 행복하고 일이 마음에 든다면 점점 나는 행복해지고 그 행복감에 넘쳐 생산성도 덩달아 올라가게 된다. 내가 행복에 대해 고민하게 된 것은 사실 근 2-3년 정도밖에 안되긴 했지만, 분명 행복이라는 것이 사람에게 얼마나 중요한 것인지 다시 한 번 깨닫게 된다. 가끔씩 행복이라는 것을 망각하고, 나를 다시 절망의 구렁텅이로 밀어넣고는 하는데, 그럴 때 마다 나를 끌어올릴 수 있는 것은 오직 나 밖에 없는 것이다. 그런 점에 있어서 최근 싸우자 귀신아의 내용에 많이 공감한다. 그 웹툰에서는 자존감으로 표현하지만, 결국에는 일맥상통하는 개념인 것 같다는 생각이 든다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week2 - Perceptron]]></title>
    <link href="http://SanghyukChun.github.io/40/"/>
    <updated>2014-03-21T07:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/40</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 2주차 강의를 요약한 글이다. 첫 주 강의에서 Neural Network란 무엇이며 어떤 종류의 Neural Network들이 있는지 등에 대해 간략하게 다뤘다면, 이 강의에서는 가장 오래된 Neural Network 중 하나인 Perceptron을 설명하는 내용이 주가 된다.</p>


<h5>An overview of the main types of neural network architecture</h5>


<p><a href="http://SanghyukChun.github.io/39" target="new">이전 글</a>에서 Neuron들에는 어떤 종류가 있을 수 있는가 다뤘었다. 대충 linear neuron, linear threshold neuron, binary neuron, binary threshold neuron, sigmod neuron 등이 있었다. 그렇다면 neuron들로 구성된 neural network에는 어떤 type들로 구분되는가도 간략하게 알아보도록 해보자.</p>


<p>일단 가장 간단한 형태의 network로 Feed-forward neural network가 존재한다. 가장 일반적으로 쓰이고 실제 어플리케이션에 적용되는 neural network들도 대부분이 feed-forward라고 한다. 이 네트워크는 상당히 간단한 구조인데, 첫 번째 layer는 input이며 가장 마지막 layer는 output이다. 그리고 중간의 input과 output으로 관찰되지 않는 영역을 &#8220;hidden&#8221; layer라고 하는데, 당연히 visuable하지 않으므로 (우리가 직접 관측하는 영역이 아니므로) hidden이라고 불리는 것이다. 만약 hidden layer가 하나보다 많이 존재한다면 이 network는 &#8220;deep&#8221; neural network라고 불린다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-1.png" width="350"></p>

<p>위의 그림이 Feed-forward neural network의 간단한 예시이다. (이 그림은 Hinton 교수의 slide에서 가져왔다.)</p>


<p>이보다 조금 더 복잡한 network로는 Recurrent network라는 것이 존재한다. &#8220;Recurrent&#8221;라는 이름이 붙은 이유는 graph에 cycle이 존재하기 때문인데, 이 말인 즉슨, 이 network에서는 arrow를 계속 따라가다보면 어느 순간 같은 장소를 계속 돌고 있을 수도 있다는 의미이다. 당연히 일반적인 방법으로 이것을 학습하는 것은 매우 복잡한 일이고 어려운 일이다. 그럼에도 일단 이 네트워크는 가장 &#8220;biologically&#8221; 현실적인 네트워크라고 한다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-2.png" width="200"></p>

<p>위와 같이 directed cycle이 존재하는 경우 recurrent network라고 하는데, 이 방법을 사용해서 sequential data를 modeling할 수 있다고 한다. 그런 행위가 가능한 근본적인 이유는 이 방법 자체가 일종의 시간 축으로 very deep한 network로 치환이 가능하기 때문이다. 그림으로 보면 아래와 같은 형태가 된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-3.png" width="250"></p>

<p>자 다시 위의 그림을 보면서 차근차근 설명하자면, 위의 그림은 매 시간마다 하나의 hidden layer를 가지는 네트워크이며, 각 hidden layer는 그 다음 hidden layer에 무언가 information을 주는 형태이다. 즉, 자기 자신이 자기 자신에게 정보를 주는 cycle이 존재하는 형태이며, 매 시간마다 input과 output이 존재한다고 생각할 수 있다. 이런 이유로 recurrent network를 이런 형태의 network로 치환하여 생각할 수 있는 것이다. 당연히 실제로 학습하기는 무지하게 어렵지만, 실제 이런 network가 계속 연구가 되고 있으며 2011년 Ilya Sutskever의 연구에서 이런 형태의 network를 사용해 wikipedia의 단어들을 학습해 자동으로 sentence를 generate하는 모듈을 만들어서 실행시킨 결과, 다음과 같은 문장을 얻었다고 한다.</p>


<p>In 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.</p>


<p>물론, 완전한 형태의 영어는 아니지만, 매 순간 단 하나의 단어만을 generate한 결과임에도 불구하고 엄청나게 뛰어난 성능을 보이고 있음을 알 수 있다. 일반적으로 이런 sentance generate을 위한 모델은 무지무지 복잡하고 여러 단어를 동시에 학습하거나 생성하거나 하는 등의 과정을 거치는데 이 논문에서는 오직 단어를 하나씩만 생성했음에도 꽤 그럴듯한 영어가 나왔다는 점이 고무적이라는 것이다.</p>


<p>마지막으로 Symmetrically connected network가 있다. 이 network는 recurrent network의 special한 case라고 보아도 무방한데, 간단히 말하자면 이전의 neural network들은 모두 directed graph였지만, 이 symmetrically connected network는 undirected graph이다. 즉, 각 layer간에 symmetric한 edge, 다시 말하자면 양 방향으로 서로 같은 weight를 가지게 된다는 의미이다. 이런 network는 energy function이라는 것을 도입하면 recurrent network보다 훨씬 분석하기가 용이하며, performance도 powerful하다. 만약 hidden unit이 없다면 Hopfield network라고 부르며, hidden layer가 존재하면 Boltzmann machine 이라 부르는데, 이 녀석은 나중에 언젠가 다루게 될 Deep network에서 이 Boltzmann machine을 restrict시킨 형태인 Restricted Boltzmann Machine (RBM)을 설명할 때 다시 한 번 자세하게 다룰 예정이다. (Coursera lecture로 따지면 거의 맨 끝 즈음이다.)</p>


<h5>Perceptrons: The first generation of neural networks</h5>


<p>자, 어쨌거나 2주차 강의의 핵심은 바로 perceptron이다. 이 녀석은 가장 오래된 neural network 중 하나이며, 특정 상황에서는 정말 outperform한 결과를 보여주지만 그 한계가 분명한 알고리듬이다. 1690년대 Frank Rosenblatt에 의해 제안된 알고리듬으로 Artificial neural network을 태동하게 한 알고리듬이지만, 그 한계가 너무나 명백하여 한 동안 neural network 연구 자체가 이뤄지지 않게 한 원인이 되기도 한다. 1969년 Minsky가 perceptron이 linear가 아니면 아무것도 할 수 없다는 것을 증명했는데 (단적인 예로, xor조차 학습하지 못한다) 당시 multi layer perceptron에도 이 방식이 적용될 것이라 다소 과도한 추측을 하는 바람에 neural network 연구 자체가 한 동안 메일 스트림이 아니었다. 아무튼, perceptron은 엄청 간단한 feed-forward network의 일종이다. 무지무지 간단하게 그림 하나로 표현하면 아래와 같다 (그림은 google image에서 찾은 그림..)</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-4.png" width="600"></p>

<p>하나하나 간단하게 설명해보자. 일단 input layer가 있다. 맨 아래 \(x_o\)는 \(x_n\)의 오타로 추정된다. 맨 위의 1은 bias를 위한 term이다. 이전 글에서 bias에 대해 설명한 것을 기억하는지? input과 weight를 linear combination 형태로 정리하고 나서 거기에 상수 항으로 더해지는 값이 bias이다. 즉, input과 상관없이 늘 더해지는 값으로, \(b = 1 \times w_o\) 라고 봐도 무방한 것이다. 아무튼, 지금은 간단하게 input layer에서 원래 input vector x와 bias term 1을 weight vector와 곱한 형태인 \(z = \sum_i w_i x_i\)를 계산했다고 간단하게 생각해보자. perceptron의 decision rule은 간단한데, 방금 계산한 값이 어떤 threshold를 넘으면 값을 activate, 넘지 못하면 값을 deactive 시키는 것이다. 간단하게 얘기하면 perceptron에서는 binary threshold neuron을 사용하는 것이다. 이 threshold를 결정하는 것은? 바로 bias가 그 역할을 하게 된다. 그러므로 이 알고리듬에서 &#8220;learning&#8221;하는 것은 weight와 bias가 될 것이다. 음.. 뭔가 간단하게 bias는 무시하고 weight만 학습하는 방법은 없을까? 앞에서 bias를 weight로 간단하게 치환한 방법을 사용하면 이렇게 문제를 간단하게 만드는 것이 가능해진다. 원래 input vector에 value 1을 추가하여 마치 input vector가 하나 더 있고, 그 component에 대한 weight가 존재하는 것처럼 trick을 쓰는 것이 가능해진다. 따라서 bias도 weight와 같은 방법으로 자연스럽게 learning할 수 있게 되고, 더 이상 threshold에 대해 고민할 필요가 없어진다!</p>


<p>perceptron이 weight를 학습하는 방법도 매우 간단하다. input vector가 들어왔을 때, 현재 weight로 맞는 값이 나온다면 weight는 update되지 않는다. (\(w_{t+1} = w_t\)) 만약 1이 나와야하는데 0이 나온다면 weight vector에 input vector를 더해준다. (\(w_{t+1} = w_t + v\)) 만약 0이 나와야하는데 1이 나온다면 weight vector에서 input vector를 더해주는 방식으로 weight를 update한다. (\(w_{t+1} = w_t - v\))</p>


<p>조금 더 잘 describe해보자면, input x에 대해서 output(label) y는 다음과 같은 수식으로 표현된다 -아래 수식에서는 편의를 위해 y = {-1,1} 이라고 하자-</p>


<p>$$ y = sign( \sum_{i=0}^n w_i x_i ) \hskip 1em where, x_0 = 1 \hskip 0.3em and \hskip 0.3em w_0 = -b$$</p>


<p>즉, label y는 vector w와 x의 inner product로 나타낼 수 있으며 이 때 bias b는 \(x_0 = 1\), \(w_0 = -b\)라는 형태로 간단한 weight vector와 input vector의 linear combination으로 표현할 수 있게 되는 것이다. 이 때 update rule은 다음과 같다</p>


<p>$$ w_{t+1} = w_t + y_n x_n, \hskip 1em when \hskip 0.3em misclassified $$</p>


<p>misclassified가 발생했을 때만 update가 일어나며, update rule은 원래 y와 x를 곱해서 원래 vector에 더해주는 것이다. 즉, 1이 나와야하는데 -1이 나왔다면 w에 +x를 취해주고, -1이 나와야하는데 1이 나왔다면 w에 -x를 취해주는 것이다. 그리고 step을 진행시키면서 (t가 점점 증가하면서) misclassified point가 발견될 때 마다 이 알고리듬을 반복한다. 이렇게 설명하면 조금 더 깔끔하게 수식적으로 설명이 가능해진다.</p>


<h5>A geometrical view of perceptrons</h5>


<p>위와 같은 update rule이 선택되는 이유는 무엇인가? 왜 하필이면 input vector를 합해야할까? 이런 질문들은 모두 geometric하게 해석할 수 있다. feature가 n개일 때, input vector와 weight vector는 some n-dimensional vectors이므로, 이 vector들이 존재하는 vector space를 정의하는 것이 가능해지기 때문이다. 여기에서는 weight space라는 새로운 형태의 space를 정의해서 perceptron을 해석할 것이다. 따라서 원래 수식과 대조하여 생각해보면 우리가 궁극적으로 찾고자하는 truth weight vector는 올바른 answer에 대한 어떤 hyperplane일 것이라는 것도 충분히 추측할 수 있다. 무슨 소리냐하면, input vector와 weight vector의 inner product의 sign이 y를 결정한다는 의미는, 곧 그 내각이 90도보다 크냐 작으냐로 생각할 수 있고 (물론 n-dimensional vector에서는 각도 개념이 정의하기 나름이지만) 아마도 대부분의 input vector들에 대해서 올바른 label을 가지게 하는 어떤 hyperplane이 우리가 찾고자하는 궁극적인 weight vector들이라는 것이다. 그림으로 설명해보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-5.png" width="600"></p>

<p>위의 그림에서 correct answer가 1이라면 input vector와 weight vector의 inner product를 구했을 때 올바른 값이 나오기 위해서는 당연히 초록색 vector이어야한다는 사실을 알 수 있을 것이다. 이유는 위에서 언급했듯 사이각이 90도 보다 작은 두 벡터의 inner product는 언제나 0보다 크기 때문이다. 따라서 input vector에 orthogonal한 plane을 그리고, 그 plane을 기준으로 weight vector가 올바른 곳에 존재하는지 그렇지 않은지 간단하게 알 수 있을 것이다. 다음에는 correct answer가 0인 경우 (-1인 경우)를 살펴보자. 이 경우에는 두 벡터의 사이 각이 90도보다 커야하므로, input vector에 orthogonal한 plane의 반대 부분이 올바른 weight vector의 위치가 됨을 알 수 있다. 그렇다면 올바르지 않은 (misclassified된) weight vector를 올바른 영역으로 옮기기 위해서 어떤 행동을 취할 수 있을까? 조금만 생각해보면 정말 간단한 vector sum으로 hyperplane의 반대쪽으로 보낼 수 있다는 것을 알 수 있다. 왼쪽 상황에서는 빨간 벡터를 초록 벡터로 만들기 위해서 간단하게 빨간 벡터에 파란 벡터를 대해주면 되고 (\(w_{t+1} = w_t + v\)) 오른쪽 경우는 빼주면 된다 (\(w_{t+1} = w_t - v\)). 이런 이유로 벡터를 더하고 빼는 것 만으로 weight가 &#8216;개선&#8217;되었다고 할 수 있는 것이다. 만약 weight들이 올바르게 learning되었다면 우리는 아래와 같은 결과를 얻게 될 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-6.png" width="300"></p>

<p>즉, 올바른 weight는 서로 다른 input vector들이 모두 well-classified되게하는 어떤 vector임을 알 수 있다. Training을 하면서, 우리가 찾아내는 값은 바로 가장 올바른 weight vector를 찾는 것이며, 위의 그림에서 볼 수 있듯 우리는 space 위에 여러 hyperplane을 그릴 수 있고, 이를 이용하여 good weight들이 위치하는 hypercone을 그릴 수 있다. 재미있는 점은, 이 cone위의 vector는 convex하다는 것이다 (그 어떤 벡터 두 개를 골라도 그 중간에 존재하는 모든 벡터들이 cone안에 존재한다) 즉, 우리가 만약 이 문제를 convex하게 해결한다면 항상 우리는 global optimum값을 찾을 수 있게 되는 것이다.</p>


<h5>Why the learning works</h5>


<p>위에서 geometric view로 perceptron을 서술하였으니, 이번에는 도대체 왜 이런 알고리듬이 작동하는지 알아보도록 해보자. 사실 엄밀한 수학적 증명이 강의에 나오지 않기 때문에 복잡한 증명은 생략하고, 간단하게 그림으로 설명해보도록 하겠다. 일단 아래 그림을 보면서 진행해보도록하자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-7.png" width="500"></p>

<p>아래 그림의 상황은 current weight vector와 any feasible한 weight vector 사이의 거리 \(d_a^2+d_b^2\)을 고려해보도록 하자. 만약 이런 상황에서 perceptron이 misclassified된다면, learning 알고리듬이 current vector를 조금 더 feasible한 weight vecotr에 가까워지도록 움직여줄 것이다. 하지만 문제가 생기는데, 거의 plane에 근접하게 있는 point를 생각해보자. 이 그림에서는 노란색 점이 그것이다. 이 점은 분명 조금 더 &#8220;feasible vector&#8221;에 가깝게 움직여질 필요성이 있지만, 노란색 점은 이미 feasible region 위에 위치하기 때문에 아무리 알고리듬이 running하더라도 절대로 feasible point 근처로 옮겨지지 않는 것이다. 이런 문제점을 해결하기 위해서 &#8216;margin&#8217;이라는 컨셉이 도입된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-8.png" width="300"></p>

<p>위의 그림에는 margin이라는 것이 표현되어 있는데, 이 margin은 feasible한 weight vector를 조금 더 strict하게 정해주는 역할이다. 즉, feasible region을 plane에서 margin 보다 더 멀리 떨어진 위치로 정의하고, 이 region안에 존재하는 vector를  &#8220;generously feasible&#8221;한 weight vecotr로 정의하는 것이다. 즉, 이제는 노란색 vector가 margin보다 더 조금 떨어져 있기 때문에 더 이상 &#8220;feasible&#8221;한 vector가 아니므로 perceptron algorithm을 사용하여 이 벡터를 옮기는 것이 가능해지는 것이다.</p>


<p>이런 가정하에, 이 알고리듬이 converge한다는 것이 증명가능하다고 하는데, 구체적인 증명과정은 강의에 설명되어있지는 않고, 간단한 아이디어만 서술되어있다. 그 아이디어는 크게 세 개인데, perceptron이 feasible region에 존재하지 않는 weight vector를 update하고, update마다 missclassified vector와 feasible vector사이의 distnace가 감소되는 방향으로 update가 될 것이다. 또한 이 거리는 매 번 최소한 input vector의 lenght의 제곱근만큼은 감소한다는 것이다. 따라서 유한한 숫자의 iteration안에 weight vector가 반드시 feasible region안에 위치하게 된다는 것이다. 물론 이 모든 것은 그러한 feasible region이 존재하는 경우에만 동작하는 것은 당연할 것이다.</p>


<h5 id="what-perceptrons-cant-do">What perceptrons can&#8217;t do</h5>


<p>하지만 perceptron은 너무나도 명확한 한계점이 존재한다. input vector가 binary이기 때문에 모든 input을 binary feature로 바꾸어야한다는 점도 문제이지만, 가장 큰 문제는 linearly separable하지 않은 dataset들은 learning할 수가 없다는 것이다. 엄청나게 간단한 예를 살펴보도록하자. xor은 binary 연산의 가장 기본적인 연산 중 하나이다. 두 값이 같으면 0, 다르면 1을 return하는 것인데, 이를 2차원 평면에 포함하면 아래와 같은 상황이 되어버린다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-9.png" width="300"></p>

<p>초록색 label이 된 점들이 output이 0인 점들, 빨간색 점들은 ouput이 1인 점들이다. 당연하게도, 이 점들을 구분할 수 있는 &#8216;단 하나의&#8217; plane은 존재하지 않는다. 단순히 이 결과만 보더라도 perceptron이 얼마나 제한적인 상황에 대해서만 동작하는지 분명하게 알 수 있다. 또한 perceptron의 decision making은 summation으로 이루어지기 때문에, 만약 n 차원 벡터의 패턴이 아래와 같으면 구분이 불가능한 것이다</p>


<p><img src="http://SanghyukChun.github.io/images/post/40-10.png" width="300"></p>

<p>pattern A는 점들의 set이 1, 1, 2로 존재해야하고, pattern B는 2, 2로 존재해야하는데 둘 다 합이 4이기 때문에 perceptron으로는 이를 구분하는 것이 불가능하다.</p>


<p>이렇듯 perceptron은 그 한계가 너무나 명확하다. 그러나 이는 single layer perceptron에 한정된 문제이지 neural network 전체의 문제는 아니다. 이를 해결하는 방법은 생각보다 간단한데, 바로 hidden unit을 learning하는 것이다. multiple hidden layer는 neural network가 더 이상 linear하지 않고 non-linear하게 해주는 역할을 하는데, non-linear해지기 때문에 learning하기가 힘들어지지만, 만약 learning이 가능하다면 그 만큼 powerful해지는 것이다. 그렇다면 이런 net을 learning하는 것은 가능할까? 결론부터 얘기하자면 엄청나게 어렵다. 때문에 이에 대한 연구가 활발히 이루어지고 있으며 꽤 성공적인 결과들이 존재한다. 또한 hidden layer의 weights를 learning하는 것은 feature를 learning하는 것과 같아지기 때문에 더 이상 feature에 대한 문제도 없어지고, 여러모로 hidden unit을 learning하면 그 한계를 깰 수 있는 network가 될 수 있는 것이다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week1 - Neural Network and Machine Learning]]></title>
    <link href="http://SanghyukChun.github.io/39/"/>
    <updated>2014-03-17T23:23:00+09:00</updated>
    <id>http://SanghyukChun.github.io/39</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>약 반 년 전에 <a href="https://www.coursera.org/" target="new">Coursera</a>에서 <a href="https://class.coursera.org/ml-003/lecture" target="new">Andrew Ng 교수의 Machin Learning Class</a>를 수강한 적이 있다. 사실 당시에 이 course를 수강할 때, 이 course는 introduction course로만 듣고, Geoffrey Hinton 교수의 Neural Network 강의를 들을 생각이었는데, 시간에 쫓기다보니 어느새 나는 석사생이 되었고, 아직도 이 강의를 듣지 못한 상태였다. 그러다가 최근 우연하게 이 강의를 다시 들여다 볼 일이 생기게 되었고, 약 2-3주 동안 이 강의를 듣고 요약글을 꾸준하게 올려 볼 생각이다. <a href="http://SanghyukChun.github.io/10" target="new">예전 글</a>에서 언급했지만, 내가 너무 쉽다고 생각되면 과감하게 중간부터 요약을 관둘 생각이다.</p>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 첫 주차 강의를 요약한 글이다. 첫 주차이기 때문에 아주 간단한 introduction course이며, 주로 machine learning과 neural network는 무엇인지 아주 간략하게 설명하는 내용이 주가 된다.</p>


<h5>Why do we need machine learning?</h5>


<p>사실 이 질문은 물론이고, machine learning이란 무엇인지 내가 <a href="http://SanghyukChun.github.io/3" target="new" class="red tip" title="Machine Learning Week1 - What Is Machine Learning">아주</a> <a href="http://SanghyukChun.github.io/21" target="new" class="red tip" title="빅데이터 이야기: 데이터 수집에서 분석까지">많은</a> <a href="http://SanghyukChun.github.io/30" target="new" class="red tip" title="인터넷 속의 수학 - How Does Netflix Recommend Movies?">글</a>에서 다뤘었기에 자세한 언급은 되도록 피하도록 하겠다. 다만 이 lecture에서는 주로 patterns recognition, anomalies recognition, 그리고 prediction 등의 문제에 집중을 하고 있으며, 특히 image를 classification하는 문제에 focus가 되어있다. 이런 문제의 대표적인 예는 MNIST (hand write letter data base), Face recognition 등이 있다. 실제로 내가 예전에 공부했었던 Neural Network의 대부분은 이런 image process에 focus되어있었다.</p>


<h5>What are neural network?</h5>


<p>그렇다면 neural network란 무엇인가? 이 질문에 대답하기 이전에 먼저 인간의 뇌가 어떻게 동작하는가에 대해 간략하게 알아보자. 인간의 뇌는 아주 많은 neuron(신경)들로 이루어져 있다. 각 neuron들은 synapse라는 통로를 이용하여 information을 전달하게 되는데, 이런 real human neural network 구조를 아주아주 simplify하면, graph의 형태로 표현이 가능해진다! 즉, 각각의 neuron을 graph의 node, 그리고 synapse를 그 node들을 연결하는 edge로 표현하는 것이다. 여기에서 조금 더 real-likely한 modeling을 하기 위해서 두 가지 factor가 추가된다. 하나는 weight이며 또 하나는 bias이다. 먼저 weight에 대해서 설명을 해보자. 실제 neural network 사이에서 information은 ion이 pumping이 되거나 하는 방식으로 이동하게 된다. 그런데 이 information이 모든 상황에 똑같이 전달되는 것이 아니라, 적절한 학습을 통해서 그 양이 조절이 된다. 즉, 우리가 &#8216;컴퓨터&#8217;라는 물체가 무엇인지 인지하는 과정에서 우리의 뇌로 들어오는 시각정보를 처리하기 위해서 각각의 신경세포들이 서로 다른 양의 information을 전달하게 된다는 것이다. 예를 들어서 우리가 컴퓨터를 봤을 때 모든 시각 정보를 총 동원해서 이것이 컴퓨터다! 라고 판단하는 것이 아니라 일부 특정한 feature들 (예를 들어서 모니터와 키보드 마우스가 있는 모습)을 보고 내가 지금 보고 있는 것이 컴퓨터라는 결론을 내리 듯, 우리의 neural network는 자연스럽게 synatic weight를 학습함으로써 더 정확하고 빠른 연산 및 분류가 가능하도록 설계가 되어있는 것이다. 이런 synaptic weight는 우리가 &#8216;학습&#8217;이라고 부른 과정 동안 계속 update가 된다. 그리고 또 하나 bias에 대해 생각해보자. 만약 우리가 데이터 센터에서 근무를 한다면 아마도 상당히 많은 컴퓨터를 보게 될 것이며, 아마도 대충 네모네모하게 생긴 물건들은 컴퓨터일 가능성이 높지 않을까? 반면 내가 지금 등산 중이라면 아마도 내가 본 물체가 컴퓨터일 가능성은 극히 낮을 것이다. 즉, &#8216;input이 어떤 특정 결과에 가까울 것이다&#8217;를 indicate하는 factor일 뿐 아니라, 그 정도를 조절하기 위한 값이라고 할 수 있는 것이다. 그렇다면 이런 구조의 장점은 무엇일까? 사람의 뇌에는 자그마치 \(10^{11}\)개의 neuron이 존재한다고 한다. 또한 그 neuron들을 연결하는 link는 약 \(10^{14}\)개가 존재하게 된다. 그야말로 어마어마한 숫자의 신경들이 비록 하나의 computation power는 떨어질지 몰라도 이것들이 하나의 network를 형성하면서 엄청나게 빠른 parellel computation이 가능해지고 엄청나게 빠른 연산이 가능해지는 것이다. 거기에 각 neuron들이 information을 저장하고 있기 때문에 단순히 RAM으로 binary bit를 저장하는 것과는 차원이 다른 용량을 저장할 수 있게 되는 것이다.</p>


<p>자 그러면 이제 human neural network가 어떻게 동작하는지 살펴보았다. 그렇다면 이런 뛰어난 model을 어떻게 real field적용할 수 있을까? 우리의 뇌가 그야말로 컴퓨터에 비해 outperformance를 보이는 분야에 이런 아이디어를 적용하면 좀 그 성능이 개선되지 않을까? 그야말로 많은 사람들이 얘기하듯 컴퓨터는 멍청하다. 인간이 만든 system에 정해진 input이 들어는 상황에서는 무엇보다 빠르고 정확한 computation을 보여주지만, 스스로 무언가를 &#8216;판단&#8217;할 수 없으며, 사람에 비해서 그 유연성이 매우 떨어진다. 때문에 AI를 연구하는 사람들에게 스스로 &#8216;학습&#8217;하는 machine learning이 새로운 대안으로 제시되고 이 분야가 AI에서부터 시작되었다는 점이 전혀 놀랍지 않은 것이다. 잠시 얘기가 샛길로 빠졌는데, 결국 사람이 컴퓨터에 비해서 엄청 잘 할수 있으며 실제 real field에서 수요가 많은 대표적인 문제가 바로 image processing이다. 컴퓨터는 image를 pixel map으로 밖에 인식할 수가 없다. 즉, 가장 많이 쓰이는 example인 MNIST handwrite database를 보면, 각 이미지는 28 by 28 pixel map이며, 다시 말해서 이미지 하나에 총 784개의 information이 존재한다는 것을 알 수 있다. 이 database는 흑백 사진이니깐 그냥 간단하게 까만 것과 하얀 것으로 구분하면, 총 784개의 binary 값을 component로 가지는 vector로 생각할 수 있을 것이다. 하지만 내가 위에서도 잠깐 언급했던 것 처럼 우리는 절대로 그 시각정보를 전부 활용하여 물체를 인지하지 않는다. 일부 &#8216;feature&#8217;를 인식해서 내가 지금 보고 있는 것이 무엇인지 판단하게 되는데, 안타깝게도 컴퓨터는 그런 작업이 불가능한 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/39-1.png" width="300"></p>

<p>위의 사진이 바로 MNIST dataset의 일부분인데, 우리는 바로 각 글씨가 무엇을 의미하는지 바로 인지할 수 있지만, 멍청한 컴퓨터는 이 글씨들을 10개의 digit으로 바로 인지하는 것이 아니라 784 차원의 vector로 인식하게 되는 것이다. 앗 잠깐, 그런데 우리가 &#8216;바로&#8217; 인지하는 것도 사실 뇌가 연산을 한 결과가 아닌가? 그렇다면 뇌가 어떻게 동작하는지를 &#8216;모방&#8217;하면 기존의 방법들보다 더 나은 새로운 방법이 나올 수 있지 않을까? 그렇다! 이것이 바로 artifitial neural network의 motivation이다. 인간의 뇌는 엄청나게 빠르고 엄청나게 많은 연산을 자그마치 &#8216;parellel&#8217;하게 처리한다! 이는 정말 optimal한 system이 아닐 수 없다. 때문에 neural network의 application의 대다수는 이런 vision 문제를 해결하기 위해 사용이 된다.</p>


<h5>Some simple models of neurons</h5>


<p>이제 neural network의 필요성과 기본적인 구조는 알았으니, 구체적으로 우리가 그것을 구현하기 위한 모델을 만들어보자. 앞서 얘기했듯 우리의 artifitial neural network는 input이 들어오고, 각 graph의 weight와 맨 처음 설정한 bias를 통해 output을 얻어내는 구조이다. 즉, input을 x, weight를 w, bias를 b, output을 y라고 한다면, </p>


<p>$$ y = b + \sum_i x_i w_i $$</p>


<p>와 같은 식을 얻을 수 있을 것이다. 여기에서 \(x_i\)는 i번 째 input을 의미한다. 즉, MNIST에서 24 by 24, 784개의 input들에 대해서 모든 component들 (각 pixel들)의 값에 weight를 곱하고 그걸 모두 더한 다음 bias를 더해준 결과가 output인 것이다. 매우 간단한 시스템이다. 그렇다면 소제목인 &#8216;Some simple models of neurons&#8217;은 무슨 의미란 말인가?? 별건 아니고, output을 바로 사용할 것이냐 아니면 무언가 다른 형태로 modeling하여 사용할 것이냐에 대한 문제이다. 앞서 설명한 수식은 neuron들을 계산한 결과가 바로 최종 output이 된다. 그러나 실제로는 이것 말고도 많은 모델들이 존재하는데, 예를 들어서 \(z = b + \sum_i x_i w_i\) 라고 했을 때 y의 값을 z가 0보다 크면 z값을 그대로 사용하고 0보다 작으면 0이라고 할 수도 있을 것이다. 이런 모델을 Rectified Linear Neurons이라고 하며 linear threshold neuron이라고 하기도 한다. 또한 0보다 작으면 0, 0보다 크면 1이 되도록 하는 binary threshold neuron도 생각할 수 있다. 실제로 우리가 사용하게 될 model은 바로 sigmoid neuron이다. Sigmoid function은 매우 간단한데, 다음과 같은 모양이다. \(y = \frac 1 {1+e^{-z}}\) 이런 형태가 되면, z가 양의 방향으로 무한하게 커진다면 아래 항이 1이 되므로 값이 1이 되고, z가 무한하게 음의 방향으로 커진다면 아래 항이 무한하게 발산하게 되어 전체 식의 값이 0이 되는 것이다. 즉, 아래와 같은 모양을 띄게 되는 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/39-2.png" width="320"></p>

<p>대부분의 경우 우리가 필요한 output은 binary이므로 (0또는 1이므로) 이 함수의 결과가 output의 확률을 나타내는 stochastic binary neuron을 생각하는 것이 가능하다. 즉, \(p(y=1) = \frac 1 {1+e^{-z}}\) 로 표현하고 output의 값을 stochastic하게 예측하는 방법을 사용할 수 있는 것이다. 아마 앞으로 neural network라 하면 이런 stochastic model이 중심이 된다고 생각하면 될 것이다.</p>


<h5>A simple example of learning</h5>


<p>이 부분은 사실 크게 설명할 것은 많이 없고, 그렇다면 이런 neural network를 실제 이미지 recognition에 어떻게 사용할 것이냐.. 에 대한 부분이다. MNIST를 예로 들면 임의의 784 pixel map이 들어왔을 때 10개의 class (0~9) 중에서 어느 class에 해당하는지 어떻게 예측할 것이고 어떻게 decision을 내릴 것인가! 에 대한 실제 예시를 다루는 것이다. 이미 class가 정해진 이미지들을 가지고 neural network의 weight들을 학습하고, 그 결과를 통해 class를 구분하는 것이다. 한 가지 방법은, neural network를 layer처럼 쌓는다고 생각했을 때 (아래의 첫 번째 그림) 만약 이 network에서 맨 마지막 layer에서 어떤 특정한 shape으로 수렴하도록 만들었을 때 그 수렴한 결과를 이용해 class를 구분할 수 있을 것이다 (마찬가지 아래 두 번째 그림).</p>


<p><img src="http://SanghyukChun.github.io/images/post/39-3.png" width="400">
<img src="http://SanghyukChun.github.io/images/post/39-4.png" width="600"></p>

<p>이렇게 복잡하게 해야하는 이유는 몇 개의 간단한 알고리듬, 예를 들어서 아래 삐침 글자가 오른쪽으로 뻗으면 &#8216;2&#8217; 라고 하는 등의 간단한 rule을 각각의 class에 대해 만들어서 이 rule에 의해 determistic하게 결정하는 무지무지 간단한 heuristic algorithm이 아니라 neural network을 쓰는 이유는, 실제 우리가 생각할 수 있는 것보다 엄청나게 많은 variation이 존재하고 (심지어 숫자임에도 불구하고!) 이 때문에 이런 heuristic한 방법으로는 좋은 performance가 나오기 힘들기 때문이다. 특히 MNIST에는 갈겨 쓴 글씨가 많아서 더 그럴지도..</p>


<h5>Three types of learning</h5>


<p>machine learnig에는 supervised learning, reinforcement learning, unsupervised learning 총 세 가지 큰 범주가 존재한다. 각각에 대한 설명은.. 워낙 많이 했기에 생략하고 (reinforcement learning은 한 적은 없지만, neural network의 main interest가 아니다) 간단하게 설명하면, neural network로 supervised learning을 하는 것이 앞의 절반, 그리고 unsupervised learning을 하는 것이 뒤의 절반이 될 예정이다. 특히 엄청나게 오래되고 old한 neural network가 재조명을 받고 연구가 활발하게 된 가장 큰 이유가 Deep learning 등의 unsupervised learning임을 감안해봤을 때, 매우 기대가 되는 부분이다. (대부분의 교재는 supervised learning에 대해서만 다룬다.)</p>


<h5>Conclusion</h5>


<p>이 렉쳐는 워낙 intro level이고.. 예전에 중복해서 다룬 개념이 너무 많아서 생략한 내용이 좀 많다. 최대한 자세하게 적으려 노력했지만, 의아한 부분이 있으면 위키피디아 등에 자세히 설명이 되어있으니 그 글들을 참고해주길 바란다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LMNN(Large Margin Nearest Neighbors) LMCA(Large Margin Component Anaylsis)]]></title>
    <link href="http://SanghyukChun.github.io/38/"/>
    <updated>2014-03-03T15:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/38</id>
		<content type="html"><![CDATA[<p>KNN은 machine learning에서 general하게 많이 쓰이는 알고리듬이다. 이 알고리듬은 아이디어도 매우 간단하고 구현하기도 간단하고 성능도 어느 정도 이상 나오는 꽤나 훌륭한 알고리듬이기 때문이다. <a href="http://SanghyukChun.github.io/37" target="new">이전 글</a>에서 distance metric learning의 대략적인 컨셉을 설명했었고, 그 중에서도 optimization을 통해 metric을 learning하는 category에 대해 간략하게 언급했었다. 이 글에서는 그런 알고리듬 중에서 LMNN (Large Margin Nearest Neighbors) 그리고 이 방법의 단점을 보완한 LMCA (Large Margin Component Analysis) 라는 알고리듬을 소개할 것이다.</p>


<h5>LMNN - Introduction</h5>


<p>먼저 LMNN이라는 아이디어는 2006년 <a class="red tip" title="Advances in Neural Information Processing Systems">NIPS</a>에 발표된 Distance metric learning for large margin nearest neighbor classification이라는 논문에 소개된 기법이다. 이 알고리듬은 distance metric learning에 대해 설명했던 <a href="http://SanghyukChun.github.io/37" target="new">이전 글</a>에 잠깐 언급했던 Mahalanobis Metric을 직접 학습하는 알고리듬이다. 이 Metric은 지난 번에 설명했기 때문에 자세한 설명은 생략하도록 하겠다.</p>


<p>이 논문에서는 거리가 제곱근 형태가 아니라 제곱 들의 합으로 표현을 했다. 즉, Mahalanobis Metric을 \(D(\vec x_i , \vec x_j )=(\vec x_i - \vec x_j )^\top \mathbf M (\vec x_i - \vec x_j )\) 꼴로 표현하게 된다. 혹은 \(D(\vec x_i , \vec x_j )= ||L(\vec x_i - \vec x_j)||^2 \)으로 표현된다.</p>


<h5>LMNN - Cost function</h5>


<p>이 방법의 핵심 아이디어는, 위에서 표현한 Metric을 평가하는 Cost function을 design하고 이 function을 minimize시키는 Metric을 찾아내는 것이다. 매우 간단한 컨셉이고, 만약 운이 좋아서 optimization 문제가 반드시 하나의 global solution으로 수렴한다는 것이 보장만 된다면 가장 최고의 성능을 낼 수 있을 것이라는 것은 자명한 일이다. (<a href="http://SanghyukChun.github.io/31/" target="new">이전에 작성한 글</a>에서 이러한 좋은 문제 중 하나인 convex optimization에 대해 간략하게 언급했었다.) 자, 그러면 이 논문에서 Cost function을 어떻게 정의했는지 한 번 알아보자.</p>


<p>$$ \varepsilon (\mathbf L ) = \sum_{ij} \eta_{ij} ||L(\vec x_i - \vec x_j)||^2 + c \sum_{ijl} \eta_{ij} (1-y_{il}) h[ 1 + ||L(\vec x_i - \vec x_j)||^2 - ||L(\vec x_i - \vec x_l)||^2 ] $$</p>


<p>이때, 각 notation이 의미하는 바는 아래와 같다</p>


<p></p>

<ol>
    <li>\({(\vec x_i , y_i )}_{i=1}^n\): training set을 의미한다. 벡터 x는 input data를, scalar y는 label을 의미한다. (binary class가 아니어도 상관없다.)</li>
    <li>\(\eta_{ij}\): \(\vec x_j\)가 \(\vec x_i\)의 target neighbor인가 아닌가를 나타내는 binary variable. 맨 처음 learning할 때 고정되는 값이며 알고리듬이 돌아가는 동안 변하지 않는 값이다.</li>
    <li>\(y_{ij}\): label \(y_i\)와 \(y_j\)가 서로 일치하는가 하지 않는가를 나타내는 binary variable이다. 역시 변하지 않는다.</li>
    <li>h(x): hinge function으로, 간단하게 표현하면 \(h(x) = max(0,x) \)이다. 즉, 0보다 작으면 0, 아니면 원래 값을 취하는 함수이다.</li>
    <li>c: 0보다 큰 임의의 상수로, 끌어당기는 term과 밀어내는 term사이의 trade-off를 조정한다. 보통 cross validation으로 결정한다.</li>
    <li>Target neighbor: 임의의 \(x_i\)와 같은 label을 가진 데이터들 중에서 가장 가까운 k개의 데이터들을 의미하며 k는 사용자가 세팅할 수 있다</li>
</ol>


<p>뭔가 복잡해보이지만, 일단 간단하게 설명하자면 앞의 항은 같은 label끼리 서로 끌어오는 term이고, 뒷 항은 서로 다른 label끼리 밀어내는 term이다. 이유는 간단한데, 먼저 앞과 뒷항 모두 포함되어있는 \(\eta_{ij}\)는 i와 j가 서로 target data일 때만 해당 항을 남기고, 아니면 0으로 만들어버리기 때문에 이 모든 연산은 target neighbor들에 대해서만 진행이 되게 된다. 따라서 앞의 항은 target neighbor들끼리의 거리를 의미하므로, 이 값을 minimization한다는 것은 서로 같은 label들끼리 최대한 가깝게 모아준다는 의미와 같게 되는 것이다. 그럼 오른쪽 항은? 이 항은 잘 보면 summation factor가 i,j,l인데, 일단 먼저 target neighbor i와 j에 대해서 이와는 다른 label을 가진 (\((1-y_{il})\)가 0이 되지 않는) l들에 대해서 최대한 그 거리를 멀어지게 하도록 하는 항이다. 이 값은 사실 그냥 나온 값이 아니라 아래 식을 통해서 나오게 된 값이다.</p>


<p>$$ d(\vec x_i , \vec x_j) + 1 \le d(\vec x_i , \vec x_l) $$</p>


<h5>LMNN - Optimization</h5>


<p>위의 식에 대해서 간단히 언급을 하자면, 모든 i와 j들에 대해서, label이 다른 l과의 거리보다 label이 같은 데이터들끼리의 거리가 무조건 1만큼은 작아야한다는 식이다. 이 식을 살짝 전개하면 원래 cost function의 오른쪽 항과 같은 모양을 얻을 수 있을 것이다.</p>


<p>자! 이제 cost function을 정의했으니 optimize를 해보자. 근데 문제가 하나 있는데, 이 cost function은 <a class="red tip" title="Convex Optimization은 solution이 무조건 하나다. 나중에 블로그에서 자세하게 다뤄보도록 하겠다.">convex</a>가 아니다. 때문에 L에 대해 문제를 해결했을 때 정확한 global minimum을 찾을 수가 없게 된다. 하지만 이 논문은 아주 간단하게 이 문제를 convex 문제로 바꾸게 된다. convex 문제 중에서 semidefinite programming이라는 문제가 있는데 (간단하게 SDP라고 한다) 이 문제는 &#8216;어떤 조건&#8217;을 가장 잘 만족하는 positive semidefinite matrix를 찾는 문제이다. 이 문제에 대해 언급하면 포스트가 너무 길어지니 <a href="http://en.wikipedia.org/wiki/Semidefinite_programming" target="new">위키 링크</a>로 대체하도록 하겠다.</p>


<p>그러면 이 문제를 어떻게 SDP로 바꿀 수 있을까? 해결법은 Metric을 L로 표현하는 대신에 M으로 표현하고, 이 M에 대해 문제를 푸는 것이다. 이렇게 표현하게 되면 문제가 아래와 같이 변하게 되며 이는 SDP로 간단하게 해결할 수 있는 문제가 된다.</p>


<p>$$ \mathbf {Minimize} \sum_{ij} \eta_{ij} (\vec x_i - \vec x_j )^\top \mathbf M (\vec x_i - \vec x_j ) + c \sum_{ij} \eta_{ij} (1-y_{il}) \xi_{ijl} \ \mathbf {subject} \ \mathbf {to:} $$</p>


<p style="margin-left:15%"> (1) \( (\vec x_i - \vec x_j )^\top \mathbf M (\vec x_i - \vec x_j ) - (\vec x_i - \vec x_j )^\top \mathbf M (\vec x_i - \vec x_j ) \geq 1- \xi_{ijl} \)</p>


<p style="margin-left:15%"> (2) \( \xi_{ijl} \geq 0 \)</p>


<p style="margin-left:15%"> (3) \( \mathbf M \succeq 0 \)</p>


<p>여기에서 \(\xi_{ij}\)는 slack variable로, 이전 식의 hinge function과 완전히 같은 동작을 하도록 &#8220;mimick&#8221;을 하는 변수이다. 이 문제는 앞에서 언급한 SDP로 해결할 수 있는 format이기 때문에 이제 이 문제를 해결해서 적절한 \(\mathbf M\)을 찾아내면 우리가 찾고자하는 적절한 Metric을 찾을 수 있게 되는 것이다.</p>


<h5>LMNN - Result</h5>


<p>자 이제 LMNN의 실제 performance를 measure해보자. 참고로 이 알고리듬은 저자가 직접 버전관리하는 소스코드가 존재한다. <a href="http://www.cse.wustl.edu/~kilian/code/code.html" target="new">링크</a>에서 간단하게 다운로드 받을 수 있다. 이 Metric learning이 well-working하는지 판단하기 위해서 이 논문에서는 총 4개의 알고리듬을 비교한다. (1) Euclidean distance를 사용하는 기존의 KNN (2) Optimization을 통해 얻은 Metric을 사용해 Mahaloanobis distance를 사용한 KNN (3) 앞에서 얻은 Metric을 계산할 때 사용한 Cost function을 가장 최소화시키는 label을 고르는 Energy-based classification (4) Multiclass SVM 이렇게 총 네가지 알고리듬을 사용한다. 그런데, 만약 dimension이 높은 경우에는 위의 Optimization식이 Overfitting이 될 위험성이 존재한다. 따라서 이를 방지하기 위하여 feature가 많은 문제는 PCA를 사용하여 dimension을 낮추는 작업을 하게 되는데, 이 문제가 결국 다음에 설명할 LMCA의 Motive가 된다. 아무튼 이런 방법을 사용하여 얻은 결과는 아래 표와 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/38-1.png" width="600"></p>

<p>대체로 Energy-based classification이 가장 좋은 결과를 보이는 것을 알 수 있으며, 이 논문의 방법을 통해 계산한 Metric이 분명 기존의 다른 방법들보다 더 나은 방법을 제시한다고 할 수 있을 것이다. 그러나 이 방법의 근본적인 문제점이라면 Optimization으로 Metric을 구하기 때문에 Overfitting문제에 매우 취약하다는 것이며, 특히 dimension이 높고 sample개수가 적으면 이 문제가 매우매우 심각해진다. 다만, face, hand-write letter, spoken letter 등등 매우 다양한 데이터셋에 전부 개선된 performance를 보이는 것은 충분히 고무적인 결과라고 할 수 있을 것이다.</p>


<h5>LMCA - Motivation</h5>


<p>하지만, Feature가 1000단위가 넘어가는 high dimension 상황에서는 문제가 발생할 수 있다. 기본적으로 Optimization 문제라는 것은 언제나 Overfitting issue에서 벗어날 수 없다. 특히 LMNN이 정의한 Optimization문제는 정사각행렬 M을 학습해야하므로, dimension이 높아질수록 Optimization을 통해 찾아내야하는 항이 제곱 스케일로 늘어난다. 즉, 당장 차원이 100단위만 넘어도 찾아내야하는 항의 수가 10000개가 넘어가게 된다는 의미이다. 따라서 우리가 실제 이 문제를 적용하는 경우에, 어쩔 수 없이 dimension reduction technology를 사용할 수 밖에 없어진다. LMNN 논문에서는 PCA를 사용하여 dimension을 낮춘 이후에 Optimization문제를 풀게 되는데, 이 PCA라는 것이 물론 좋고 많은 사람들이 사용하는 dimesion reduction 방법이지만, 이 방법으로 인해 발생하는 오차가 매우 크고 실제로 더 좋은 performance를 낼 수 있음에도 불구하고 그 성능이 크게 저하되는 요인이 된다는 것이 LMCA의 Motivation이다. 실제로 PCA를 사용하게되면 Dominant한 term을 뽑아내기는 하지만 그 dimension이 낮아지거나 혹은 기존에 가지고 있는 input vector들이 bais가 된 경우에는 좋은 결과를 얻지 못할수도 있기 때문에 이 문제는 꽤나 큰 문제가 될 수 있다.</p>


<h5>LMCA - Idea</h5>


<p>그렇다면 어떻게 PCA등의 별다른 dimension reduction technology없이 Overfitting 문제를 해결할 수 있을까? 사실 이 논문에서 주장하는 내용은 매우 간단하다. 이전 논문인 LMNN에서 찾고자하는 Metric인 L이 dimension을 변화시키기 않는 transformation이었던 것에 반해, LMCA에서는 L을 원래 차원 D에서 더 낮은 차원 d로 보내는 L을 찾겠다는 것이다. 하지만 여기에서 문제가 생긴다. Full rank가 아닌 \( \mathbf M = \mathbf L^top \mathbf L\) 은 이제 더 이상 Semidefinite programming문제가 아니게 된다. 이유는 원래 SDP 문제에서 rank = d라는 조건이 추가되기 때문인데, 이렇게 되면 M이 convex domain이 아니게 되기 때문에 더 이상 이 문제가 convex problem이 아니게 되고, 따라서 이 문제는 더 이상 global optimum으로 수렴하지 않는다!</p>


<p>그렇다면 해결책은 없는 것일까? 이 논문에서는 그냥 원래 non convex인 L에 대한 cost function을 그냥 gradient descent method를 사용하여 optimize시킨다. 물론 이렇게 계산된 값은 local optimum이다. 때문에 LMNN이 무조건 global solution을 찾았던 것과 비해서 매우 performance가 떨어질 것 같지만, 저자들은 다음과 같은 2가지 장점이 있기 때문에 오히려 이 방법이 더 performance가 높다고 주장한다. 첫째, 원래 Full rank M을 찾을 때는 unknown component들이 D by D만큼 존재했었지만, 지금은 차원을 더 낮추었기 때문에 찾아야하는 값이 더 적어진다. 마치 Matrix completion의 장점과 비슷한 것이다. 둘째, 원래 LMNN은 Optimization을 할 때 parameter들이 굉장히 많은데 이런 여러 요소 없이 바로 Optimization이 가능해진다는 것이다. 물론 당연히 이 논리의 기본 가정은 high dimension data에 PCA를 사용해 low dimension으로 만들었을 때 이미 information loss가 많이 발생하거나 이미 tranining data에 overfitting되기 때문에 성능에 무조건적인 저하를 불러일으키게 되기 때문에 Optimization을 PCA를 사용하지 않은 Full dimension에 대해서 실행했다는 가정 하에 성립할 것이다.</p>


<h5>LMCA - Results</h5>


<p>그렇다면 결과를 한 번 확인해보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/38-2.png" width="600"></p>

<p>위의 그림은 high dimension dataset에 대한 것이고 아래 결과는 low dimension dataset에 대한 결과이다. 아무래도 high dimension에서는 저자들이 주장한 대로 LMNN에 비해 결과가 많이 개선된 것을 확인할 수 있다. (이 그림에서는 kernelized된&#8230; 즉 non-linear method 역시 함께 evaluation된 결과이기 때문에 LMNN과 비교해야할 대상은 linear method이다) 하지만 low dimension에 대해서는 항상 더 높은 것 만은 아니며, 일부 경우에 대해서는 LMNN이 더 좋은 결과를 보임을 알 수 있다. 즉, 이 방법은 overfitting issue가 발생했을 때 global optimum은 아니지만 그와 유사한 (그러나 절대 같다고 하거나 그와 유사하다고 할 수도 없는) local optimum을 찾는 방법이기 때문에, overfitting issue가 적은 low dimension에서는 LMNN보다 성능이 떨어질 수도 있는 것이다.</p>


<h5>Non-linear LMNN, LMCA</h5>


<p>지금까지 언급한 방법들은 모두 &#8216;linear&#8217;한 transformation을 찾는 문제였다. 하지만 세상에는 엄청나게 많은 non-linear metric이 존재하며, 분명 linear보다 성능이 더 좋은 non-linear metric을 찾을 수 있을 것이라고 생각할 수 있다. 그렇다면 이 논문들에서 과연 그런 방법을 다루지 않을까? 일단 LMNN은 NIPS에 제출된 원래 논문에는 non-linear problem이 언급이 되어있지않지만, 나중에 GB-LMNN (Gradient Boost LMNN)이라는 방법을 소개하며, 이 방법의 powerful함은 LMNN code에서 직접 확인할 수 있을 것이다. 이 방법은 Gradient Boost라는 방법을 사용하여 non-linear metric을 찾아내는데, 문제는 이 방법이 non-convex하다. 따라서 초기값에 따라서 그 결과가 상이하게 달라지게 되는데, 해당 논문에서는 LMNN을 통해 학습한 L을 초기값으로 사용하여 Optimum값을 찾는 아이디어를 제시해 L의 성능을 개선시킨다고 명시되어있다. 분명 Optimize를 시키기 때문에 본래 값보다는 더 좋은 값으로 수렴할 것이며 성능도 어느정도 올라갈 것이라고 예측이 가능할 것이다. Gradient boost는 regression tree라는 것을 학습하여 non-linear transformation을 찾아내는데, 이 tree의 node개수나 level등등을 어떻게 학습시킬 것이냐에 따라 그 running time과 overfitting issue가 결정되는 듯 하다. 더 자세한 점은 해당 논문을 읽어보기를 권한다.</p>


<p>또한 LMCA는 원 논문에 non-linear method까지 언급이 되어있다. 본래 아이디어 자체가 그냥 gradient descent를 사용해서 local optimum L을 찾는 문제이기 때문에 kernel에 대해서도 이 문제를 동일하게 풀 수 있는 듯하다. 다만 그 update rule을 어떻게 결정하느냐의 문제가 있는지 논문에서 cost function의 gradient방향으로 내려가는 것이 올바른 update rule이라는 것을 Lemma를 증명해놓았다. 아무튼 당연한 얘기지만 이 방법이 linear method보다 그 결과가 좋다. 자세한 점은 마찬가지로 해당 논문을 참고하길 바란다.</p>


<h5>Conclusion</h5>


<p>KNN은 엄청 직관적인 method이지만 분명 powerful하고 easy to implement한 방법이다. 또한 이론적으로 그 bound가 가장 optimal한 case에 bayes risk와 같다는 것이 증명이 되어있기 때문에 사실 굉장히 좋은 방법이라고 할 수 있다. 그러나 실제 우리가 이 방법을 적용하는 대부분의 상황에서는 metric learning이 performance에 크게 영향을 끼칠 수 밖에 없다. LMNN과 LMCA는 Optimization problem을 solve함으로써 상당히 좋은 결과를 얻어낼 수 있는 좋은 Metric learning알고리듬이라고 할 수 있다. 물론 이 방법들에는 overfitting issue가 존재하고, 이 때문에 적절한 상황이 아닌 경우에 특히 high dimension, low sample problem에서 well working하지 않는다는 단점이 존재하기는 한다. 하지만 저자가 구현한 implement하기 좋은 matlab code도 존재하고, 여러모로 괜찮은 방법이 아닌가 하는 생각이 든다.</p>


<p>References</p>


<ul>
    <li>K.Q.Weinberger,J.Blitzer,andL.K.Saul(2006) .InY.Weiss,B.Schoelkopf, and J. Platt (eds.), Distance Metric Learning for Large Margin Nearest Neighbor Classification, Advances in Neural Information Processing Systems 18 (NIPS-18). MIT Press: Cambridge, MA.</li>
    <li>Torresani, L., & Lee, K. (2007). Large margin component analysis. Advances in Neural Information Processing</li>
    <li><a href="http://www1.cse.wustl.edu/~xuzx/research/publications/gb-lmnn.pdf" target="new">Kedem, D., Xu, Z., & Weinberger, K. (n.d.). Gradient Boosted Large Margin Nearest Neighbors</a></li>   
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distance Metric Learning]]></title>
    <link href="http://SanghyukChun.github.io/37/"/>
    <updated>2014-03-02T20:24:00+09:00</updated>
    <id>http://SanghyukChun.github.io/37</id>
		<content type="html"><![CDATA[<p>Machine Learning 분야에는 KNN 등의 input data의 <a class="red tip" title="간단하게 생각해서 distance function이라 생각하면 된다. 자세한 설명은 뒤에서 계속">distance metric</a>을 어떻게 설정하냐 따라에 크게 영향을 받는 알고리듬들이 종종 존재한다. 그런데, 대부분 이런 method들에서 주로 사용하는 distance metric은 Euclidean distance로, 이 metric은 근본적으로 데이터 하나와 다른 데이터 하나와의 관계만을 나타내기 때문에 실제 distribution으로 존재하는 데이터에는 적합하지 않은 경우가 많다. 때문에 데이터들의 분포 등을 고려하여 이런 &#8216;거리&#8217;를 새로 정의하는 분야가 존재하는데 이를 일컬어 Distance Metric Learning이라 한다.</p>


<p>그렇다면 distance metric이란 무엇인가부터 간단하게 짚고 넘어가자. Distance metric은 쉽게 생각하면 distance를 정의하는 방법이라고 할 수 있다. 몇 가지 규칙이 존재하는데, 자세한 내용은 <a href="http://en.wikipedia.org/wiki/Metric_(mathematics)" target="new">위키피디아 페이지</a>를 참고하길 바란다. 역시 가장 간단한 예시는 Euclidean distance로, 우리가 가장 많이 알고 있는 거리를 측정하는 방법일 것이다. 이 함수는 간단하게 \(d(p,q)=\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+&#8230;}\)로 정의된다. 그 밖에도 두 점의 값이 정확히 일치하면 1, 일치하지 않는다면 0으로 표시하는 binary distance등도 존재한다.</p>


<p>이 밖에도 중요한 distance metric으로는 Mahalanobis Distance Metric이라는 것이 있다. 이 distance metric은 Euclidean distance metric이 data set의 correlation을 하나도 고려하지 않은 문제점을 해결할 수 있고, 또한 scale-invariant한 특성을 가지고 있다. 이 metric은 \(d(p,q)=\sqrt{(\vec p - \vec q)^\top \Omega (\vec p - \vec q)}\)로 정의된다. 이 때 \(\Omega\)는 semidefinite matrix이다. <a href="http://en.wikipedia.org/wiki/Mahalanobis_distance" target="new">위키피디아</a>에서 발췌한 보다 정확한 정의는 앞서 나왔던 수식에서 \(\Omega\)가 covariance matrix인 metric이다. 따라서 이 metric이 data set의 correlation을 포함하여 거리를 표현할 수 있는 것이다. 하지만 실제 분포를 알 수 없는 임의의 데이터들에 대해서 올바른 covariance matrix를 계산하는 것은 매우 어렵다. 따라서 이 Mahalanobis metric의 \(\Omega\)를 learning하는 method들도 존재하는데, 대표적으로 LMNN(Large Margin Nearest Neighbor) classification이 있다. 이 논문에 대해서는 추후에 따로 포스트를 하도록 하겠다.</p>


<p>아무튼, distance metric learning은 input data space에서 data들에 가장 적합한 형태의 어떤 metric을 learning하는 알고리듬이다. 여기에서 data는 각 pair 별로 similar/dissimilar가 정의되어 있는 형태의 데이터이다. 즉, metric learning은 similar한 point끼리는 더 가까운 거리로 판단하게 하고, dissimilar한 point는 더 먼 거리로 판단하게 하는 어떤 metric을 학습하는 것이다. 당연히 KNN 등의 알고리듬들은 그 성능이 크게 개선될 수 있다.</p>


<p>아래는 distance metric learning을 간략하게 그림으로 나타낸 것이다. 그림은 Bellet, A., Habrard, A., and Sebban, M. A Survey on Metric Learning for Feature Vectors and Structured Data, 2013 에서 발췌하였다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/37-1.png" width="500"></p>

<p>즉, 우리가 metric learning을 하는 가장 큰 이유는 KNN 등의 metric에 크게 좌우되는 algorithm들의 성능을 개선시키기 위함인 것이다.</p>


<p>그렇다면 distance metric learning의 종류는 어떻게 되는가? 일반적인 machine learning 분류처럼 supervised/unsupervised learning이 존재한다. 먼저 supervised learning은 constraints나 label이 이미 주어진 상태에서 metric을 학습하게 된다. 즉, 이미 우리는 모든 데이터들의 관계를 알 고 있고, 이 관계에서 가장 적합한 distance metric을 찾는 알고리듬인 것이다. 대표적으로 NCA, RCA 등의 알고리듬 들이 존재한다고 한다. 이에 반해 unsupervised learning은 아무런 사전지식없이 metric을 learning하는데, 주로 dimension reduction technique으로 많이 사용한다. 예를 들어서 PCA가 이 범주에 들어가게 된다.</p>


<p>내가 읽은 두 개의 survery에서는 (Liu Yang, Distance Metric Learning: A Comprehensive Survey, 2005 그리고 Liu Yang, An Overview of Distance Metric Learning, 2007) 이 두 가지 분류 뿐 아니라 두 가지 분류를 더 추가하였다. 하나는 Maximum margin based distance learning이고, 또 하나는 kernel method이다. 일단 kernel 쪽은 내가 잘 모르기도 하고, 내 관심사는 maximum margin based distance learning이므로, 이 부분에 조금 더 집중해서 설명하도록 하겠다.</p>


<p>위의 survey에서 정의하는 Maximum margin based learning은 다음과 같다. <a class="red">&#8220;Formulate distance metric learning as a constrained convex programming problem, and attempt to learn complete distance metric from training data&#8221;</a> 즉, Convex optimization을 통해서 가장 최적의 metric을 찾아내는 method라는 것이다. 여기에서 convex optimization은 이전에 블로그에서 다룬 적이 없기 때문에 나중에 이에 대한 글을 쓰게 되면 여기에 추가 랑크를 달도록 하고 지금은 일단 위키피디아 링크로 설명을 대체하도록 하겠다. <a href="http://en.wikipedia.org/wiki/Convex_optimization" target="new">링크</a></p>


<p>이 방법은 주어진 input에 대해서 가장 최고의 performance를 내는 metric을 찾아내기 때문에 가장 성능이 좋아보일 것 같지만, 실제로는 몇 가지 문제점들을 가지고 있다. 하나, convex optimization은 대부분 gradient descent method를 사용하여 그 계산하는데, 이 계산량이 다른 method들에 비해서 많이 비싸다. 둘째, input training data들에 대해서 optimize한 결과로 metric을 정의하기 때문에 overfitting 문제가 발생할 수 있다. 특히 이 overfitting은 dimesion이 높아질 수록, traing sample의 숫자가 줄어들 수록 더더욱 문제가 된다. 때문에 이런 문제점을 해결하기 위해서 dimension을 reduction한 이후에 metric을 learning하는 등의 technique들이 사용되고 있다. 하지만 이 역시 문제가 있는데, 이 문제에 대해서는 나중에 포스팅하게 될 LMCA 논문에서 다루도록 하겠다.</p>


<p>아무튼 maximum margin based learning의 대표적인 예는 LMNN method로, 이 method는 위에서 설명했던 Mahalanobis metric을 직접 learning하며, non-convex problem을 Semidefinite problem으로 바꾸어 global optimum을 찾는 문제로 바꾸어서 계산을 하게 된다. 이 논문에 대해서는 나중에 다시 포스팅하도록 하겠다.</p>


<p>혹시 이 부분에 대해서 더 자세히 알고 싶다면, 아래에 링크해놓은 tutorial들을 읽어보길 바란다.</p>


<p>Tutorials</p>


<ul>
    <li><a href="http://www.iip.ist.i.kyoto-u.ac.jp/member/cuturi/Teaching/KAIST/kaist_2013.pdf" target="new">Marco Cuturi. KAIST Machine Learning Tutorial Metrics and Kernels A few recent topics, 2013</a></li>
    <li><a href="http://cseweb.ucsd.edu/~naverma/talks/metric_learning_tutorial_verma.pdf" target="new">Nakul Verma, A tutorial on Metric Learning with some recent advances</a></li>
    <li><a href="http://www-bcf.usc.edu/~bellet/misc/metric_learning_tutorial.pdf" target="new">Aurelien Ballet, Tutorial on Metric Learning, 2013</a></li>
    <li><a href="http://compscicenter.ru/sites/default/files/materials/2012_05_03_MachineLearning_lecture_09.pdf" target="new">Brian Kulis. Tutorial on Metric Learning. International Conference on Machine Learning (ICML) 2010</a></li>
</ul>


<p>References</p>


<ul><li>Liu Yang, Distance Metric Learning: A Comprehensive Survey, 2005</li><li>Liu Yang, An Overview of Distance Metric Learning, 2007</li><li>Bellet, A., Habrard, A., and Sebban, M. A Survey on Metric Learning for Feature Vectors and Structured Data, 2013</li><li>K.Q.Weinberger,J.Blitzer,andL.K.Saul(2006).InY.Weiss,B.Schoelkopf, and J. Platt (eds.), Distance Metric Learning for Large Margin Nearest Neighbor Classification, Advances in Neural Information Processing Systems 18 (NIPS-18). MIT Press: Cambridge, MA.</li></ul>

]]></content>
  </entry>
  
</feed>
