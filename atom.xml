<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[README]]></title>
  <link href="http://SanghyukChun.github.io/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2016-08-16T00:58:37+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Practical Bayesian Optimization of Machine Learning Algorithms (NIPS 2012)]]></title>
    <link href="http://SanghyukChun.github.io/99/"/>
    <updated>2016-08-16T00:58:00+09:00</updated>
    <id>http://SanghyukChun.github.io/99</id>
		<content type="html"><![CDATA[<ul class="no-float" id="markdown-toc">
  <li><a href="#section">들어가며</a></li>
  <li><a href="#hyperparameter-tuning-as-optimization-problem">Hyperparameter Tuning as Optimization Problem</a></li>
  <li><a href="#bayesian-optimization-for-black-box-function">Bayesian Optimization for “Black-box” function</a>    <ul>
      <li><a href="#stochastic-process">Stochastic Process</a></li>
      <li><a href="#gaussian-process">Gaussian Process</a>        <ul>
          <li><a href="#gp-with-noisy-data">GP with Noisy data</a></li>
        </ul>
      </li>
      <li><a href="#acquisition-function">Acquisition Function</a>        <ul>
          <li><a href="#probability-of-improvement">Probability of Improvement</a></li>
          <li><a href="#expected-improvement">Expected Improvement</a></li>
          <li><a href="#ucb">UCB</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#limitation-of-bayesian-optimization">Limitation of Bayesian Optimization</a></li>
  <li><a href="#practical-bayesian-optimization">Practical Bayesian Optimization</a>    <ul>
      <li><a href="#expected-improvement-and-matern-52-kernel-function">Expected Improvement and Matern 5/2 Kernel function</a></li>
      <li><a href="#integrated-acquisition-function-marginalize-hyperparameter">Integrated Acquisition Function (marginalize hyperparameter)</a></li>
      <li><a href="#expected-improvement-per-second">Expected Improvement per second</a></li>
      <li><a href="#monte-carlo-acquisition-for-parallelizing-bayesian-optimization">Monte Carlo Acquisition for Parallelizing Bayesian Optimization</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#section-1">변경 이력</a></li>
</ul>

<h3 id="section">들어가며</h3>
<p>Machine Learning 모델을 만들다보면 Hyperparameter 라는 녀석을 다뤄야 하는 일이 종종 발생한다.
예를 들면 Random forest의 forest 개수라거나, Neural network의 layer 개수, learning rate, momentum 값 등등..
문제는 이런 hyperparameter들을 어떻게 설정하느냐에 따라 그 결과가 크게 바뀌기 때문에 소위 말하는 ‘튜닝’에 시간을 매우 많이 쏟아야한다는 점이다.</p>

<p>이 논문은 hyperparameter tuning 문제를 Bayesian optimization을 사용해여 해결하는 방법을 제안한다.
Bayesian optimization은 알려지지 않은 “black-box” function을 optimization할 때 많이 사용되는 방법이다.
그러나 Bayesian optimization은 몇 가지 이유로 practical하게 쓰기 어려운데,
1) (kernel function, acquisition function 등) 모델을 어떤 것을 고르냐에 따라 성능이 크게 바뀐다,
2) Baysian optimization 자체도 hyperparameter가 있어서 이 hyperparameter들을 튜닝해야한다,
3) Sequential update를 해야하기 때문에 parallelization이 되지 않는다
등의 이슈가 있다.</p>

<p>이 논문은
1) empirical하게 좋은 성능을 보이는 적절한 kernel function과 acquisition function을 제안하고,
2) Baysian optimization의 hyperparameter를 (MCMC로 풀 수 있는) fully baysian approach를 통해 전체 optimization에서 한 번에 계산할 수 있는 방법을 제안할 뿐 아니라,
3) MCMC를 사용해 풀 수 있는 theoretically tractable parallelized Bayesian optimization을 제안한다.</p>

<p>사실 이 논문을 제대로 이해하기 위해서는 아래 개념들에 대해 이미 잘 알고 있어야한다.</p>

<ul>
  <li>Stochastic process (Random process라고도 부른다)</li>
  <li>Gaussian process (GP) &amp; kernel function of GP</li>
  <li>Bayesian optimization &amp; acquisition function</li>
  <li>Markov chain Monte Carlo (MCMC)</li>
</ul>

<p>마지막 MCMC는 이 글에서는 다루지 않기로 하고, 나머지들에 대해서는 차근차근 정리하면서 내용을 진행해보도록 하겠다.</p>

<h3 id="hyperparameter-tuning-as-optimization-problem">Hyperparameter Tuning as Optimization Problem</h3>
<p>보통 hyperparameter를 찾기 위해 사용되는 방법들로는 Grid search, Random search 등의 방법들이 있다.
Random forest 모델 하나를 예로 들어서 위 방법들에 대해 자세히 살펴보자.</p>

<p>Random forest에서 사용하는 hyperparameter는, tree의 개수 (n_estimators), split criteria (criterion), max depth (max_depth), leaf 당 최소 샘플 개수 (min_samples_leaf), … 등등이 있다. (자세한건 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Scikit learn의 Random Forest 코드 참고</a>)</p>

<p>우리가 찾고 싶은 hyperparameter는,</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr> <td class="code"><pre><code class=""><span class="line">n_estimators = [10, 50, 100, 200]
</span><span class="line">criterion = ['gini', 'entropy']
</span><span class="line">max_depth = [None, 100, 10]
</span><span class="line">min_samples_leaf = [1, 5, 10, 20]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>의 조합 중에 하나라고 가정해보자.</p>

<p>먼저 Grid search는 모든 parameter의 경우의 수에 대해 cross-validation 결과가 가장 좋은 parameter를 고르는 방법이다.
즉, 위에 나열된 hyperparameter의 모든 가능한 경우의 수는 4 x 2 x 3 x 4 = 96개인데, 모든 96개의 parameter들에 대해서
training data를 80:20으로 나누어 (꼭 80:20일 필요는 없다) 80으로 train을 하고, 20으로 test을 했을 때, test 결과가 제일 좋은 parameter를 고르는 것이다.</p>

<p>이 방법은, 주어진 공간 내에서 가장 좋은 결과를 얻을 수 있다는 장점이 있지만, 시간이 정말 정말 오래걸린다는 단점이 존재한다.
또한, 예시에서도 볼 수 있었듯, parameter의 candidate을 늘릴 때 마다 그 만큼의 시간이 더 필요하기 때문에,
정해진 시간 안에 parameter를 찾기 위해서는 어쩔 수 없이 hyperparameter의 candidate을 더 늘리지 못하고, candidate set이 제한된다는 단점이 존재한다.</p>

<p>이런 단점을 피하기 위해 나온 방법이 바로 random search이다.
Random search는 모든 grid를 전부 search하는 대신, random하게 일부의 parameter들만 관측한 후, 그 중에서 가장 좋은 parameter를 고른다.
Bengio 연구팀이 2012년에 발표한 논문 <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">2</a>에 따르면,
high dimensional hyperparameter optimization에서는, grid search를 하는 것 보다, random search를 했을 때 성능이 더 좋을 수 있다고 주장하고 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-1.png" width="600" /></p>

<p>서로 importance가 다른 두 개의 parameter가 있다고 가정해보자. Grid search는 중요하지 않은 parameter와 중요한 parameter를 동일하게 관측해야하기 때문에 정작 중요한 parameter를 다양하게 시도해볼 수 있는 기회가 적지만, random search는 grid로 제한되지 않기 때문에 확률적으로 중요한 parameter를 더 살펴볼 수 있는 기회를 더 받게 된다. 출처: <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">[2]</a></p>

<p>Machine Learning에서 hyperparameter search가 어려운 이유 중 하나는, hyperparameter가 바뀔 때 마다 모델이 바뀌게 되므로 다시 모델을 새로 learning해야한다는 점이다.
만약 실험 하나하나가 시간이 엄청 오래걸린다면, full grid search를 시도하기는 어려울 것이다. 그렇기에 random search가 꽤 유용하게 사용될 수 있는 여지가 더 많고,
실제로 나도 hyperparameter를 튜닝해야겠다싶으면 random search를 사용한다. (<a href="http://scikit-learn.org/stable/modules/grid_search.html">Scikit learn에서도 관련 패키지를 제공한다.</a>)</p>

<p>Grid search와 random search의 좋은 점 중 하나는, 모든 trial들이 independent하므로 parallelization이 자연스럽게 이루어진다는 점이다.</p>

<h3 id="bayesian-optimization-for-black-box-function">Bayesian Optimization for “Black-box” function</h3>
<p>Bayesian optimization은 다음과 같은 아주 무난한 optimization을 푸는 방법론 중 하나이다.</p>

<script type="math/tex; mode=display"> x^* = \arg\min_{x \in X} f(x). </script>

<p>이때, $X$는 bounded domain이고, $f(x)$는 그 모양을 모르는, 즉 input을 넣었을 때 output이 무엇인지만 알 수 있는 black box function이라 가정하자.
Optimization에는 여러 form이 있지만, minimization을 다루는 것이 일반적이기 때문에 여기에서도 minimization 꼴을 사용하도록 하겠다.
Bayesian optimization은 $f(x)$가 expensive black-box function일 때, 즉 한 번 input을 넣어서 output을 확인하는 것 자체가 cost가 많이 드는 function일 때 많이 사용하는 optimization method이다.</p>

<p>Bayesian optimization은 다음과 같은 방식으로 작동한다.</p>

<ol>
  <li>먼저 지금까지 관측된 데이터들 $D = [(x<em>1, f(x</em>1)), (x<em>2, f(x</em>2)), \ldots]$ 를 통해, 전체 function f(x)를 <strong>어떤 방식을 사용해 estimate한다.</strong></li>
  <li>Function f(x)를 더 정밀하게 예측하기 위해 다음으로 관측할 지점 $(x<em>{n+1}, f(x</em>{n+1}))$ 을 <strong>어떤 decision rule을 통해 선택한다.</strong></li>
  <li>새로 관측한 $(x<em>{n+1}, f(x</em>{n+1}))$ 을 $D$에 추가하고, 적절한 stopping criteria에 도달할 때 까지 다시 1로 돌아가 반복한다.</li>
</ol>

<p>1에서 언급한 estimation을 할 때에는 $f(x)$가 Gaussian process prior를 가진다고 가정한 다음, posterior를 계산하여 function을 estimate한다.
2에서는 acquisition function $a(x | D)$를 디자인해서 $\arg\max_x a(x | D)$ 를 계산해 다음 지점을 고른다.</p>

<p>간단한 예시를 통해서 이게 무슨 말인지 조금 더 자세히 살펴보자.</p>

<p>아래 그림에서 빨간색 점선은 우리가 찾으려고 하는 unknown black box function $f(x)$ 를 나타내고,
까만색 실선은 지금까지 관측한 데이터를 바탕으로 우리가 예측한 estimated function $\widehat f(x)$ 의 expectation을 의미한다.
까만선 주변에 있는 회색 영역은, function f(x)가 존재할 confidence bound이고 (쉽게 말해서 function의 variance이다),
밑에 있는 $EI(x)$는 위에서 언급한 acquisition function을 의미한다. (어떻게 구하는지는 아직 신경쓰지 말자)
출처: <a href="Practical Bayesian Optimization of Machine Learning Algorithms">[3]</a></p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-2.png" width="600" />
지금까지 관측한 데이터를 바탕으로, (acquisition function 값이 제일 큰) 파란색 점이 찍힌 부분을 관측하는 것이 가장 좋다는 것을 알 수 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-3.png" width="600" />
위에서 acquisition function 값이 제일 컸던 지점의 function 값을 관측하고 estimatation을 update한다. 함수의 uncertainty를 의미하는 회색 영역이 크게 감소했음을 알 수 있다. 그러나 여전히 좌측 부분과 우측 부분의 uncertainty가 꽤 큼을 알 수 있다. 다시 한 번 다음 관측할 point를 acquisition function을 통해 고른다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-4.png" width="600" />
계속 update를 진행한 결과, estimation과 실제 function이 거의 흡사해졌다. 이제 여태까지 관측한 지점 중 best point를 argmin f(x) 로 선택한다.</p>

<p>이것이 Bayesian optimization의 대략적인 procedure이다. 여기까지 설명한 내용은 완전히 새로운 것이 아니라, 오래 전에 이미 제안되었었고, 계속 쓰이던 방법이다.
이 내용을 완전히 이해하고 있어야 이 글에서 다루는 논문을 이해할 수 있다.
앞에서 Bayesian optimization을 위해서는 두 가지가 필요하다는 언급을 했었다. 하나는 function을 estimate하는 방법과, 또 하나는 다음 관측 지점을 고를 acquisition function이다.
이 두 가지 개념들이 대해 다음 두 subsection에서 조금 더 자세히 살펴보도록 하자.</p>

<h4 id="stochastic-process">Stochastic Process</h4>
<p>Stochastic process가 무엇인지 설명하기에 앞서, Random variable이란 무엇인지 알고 있어야한다.
특히 ‘Random variable은 함수이다’ 라는 개념을 이해하고 있어야하는데, 이 개념에 대해 살펴보도록 하자.</p>

<p>Random variable은 probability space에서 어떤 real value R로 가는 function으로 정의가 된다.
이때 이 real value R이 pdf나 cdf를 의미하는 것이 아니라, random variable의 값 그 자체가 된다.
Probability space란, 확률 값이 정의가 되는 공간이고, random variable이란 그 공간에서 실제 real value로 가는 function인 것이다.</p>

<p>주사위를 예로 들어보자. 먼저 주사위의 probability space는 <code>{1, 2, 3, 4, 5, 6}</code> 으로 정의가 되며,
각각의 값이 나올 확률은 동일하게 $Pr(X=1) = Pr(X=2) = \ldots = Pr(X=6) = 1/6$ 으로 정의가 된다.
여기에서 주사위의 random variable X는 다음과 같이 정의된다.</p>

<script type="math/tex; mode=display"> X= \begin{cases} 1 \mbox{ with probability } 1/6,\\ 2 \mbox{ with probability } 1/6,\\ 3 \mbox{ with probability } 1/6, \\ 4 \mbox{ with probability } 1/6, \\ 5 \mbox{ with probability } 1/6, \\ 6 \mbox{ with probability } 1/6\end{cases} </script>

<p>이 개념을 조금 더 확장시킨 것이 stochastic process이다.
Stochastic process는 어떤 ordered set T로 indexed된 random variable들의 collection으로 정의된다.</p>

<script type="math/tex; mode=display"> \{ X_t : t \in T \} </script>

<p>Ordered set T는 보통 시간이나 공간 등의 개념과 대응된다. Stochastic process라는 것 자체가, 시간에 따른 어떤 값의 변화를 추정하기 위해 도입된 개념이다보니,
(자그마치 아인슈타인이 브라운 운동 증명할 때 썼던 개념이라고 한다) 일반적으로는 이 ordered set은 시간으로 생각해도 충분하다.
앞서 설명한 random variable의 collection을 조금 더 간단하게 이야기하면, stochastic process는 probability space와 시간 T에 대한 function이라고 생각할 수 있다.
즉, 똑같은 probability space에서 한 지점을 sample했을 때, random varible은 값이 나오고 (주사위의 눈금이 나오고),
stochastic process는 t에 대한 함수가 (random variable $X_t$가) 나오게 된다. 그림으로 보면 아래와 같다.</p>

<div class="caption">
<img class="center" src="http://SanghyukChun.github.io/images/post/99-5.png" width="400" />
<p>Random variable $X$에서 값을 sample하면 real value R을 가지는 특정 값을 얻게 된다. 즉, X는 probability space에서 R로 가는 함수라고 할 수 있다.</p>
</div>

<div class="caption">
<img class="center" src="http://SanghyukChun.github.io/images/post/99-6.png" width="400" />
<p>Stochastic process $X_t$에서 값을 sample하면 시간 t에 대한 서로 다른 함수를 얻게 된다. 즉, $X_t$는 probability space에서 다른 function space로 가는 함수라고 할 수 있다.</p>
</div>

<p>Random process와 stochastic process에 대한 (그리고 뒤에서 설명할 Gaussian process 역시) 조금 더 자세한 내용은 reference에 추가한 블로그 글 <a href="http://enginius.tistory.com/489">[4]</a>을 참고하면 좋을 것 같다. (위 그림의 출처 역시 같은 블로그이다.)</p>

<h4 id="gaussian-process">Gaussian Process</h4>
<p>Gaussian process, 줄여서 GP는 continuous domain에 대해 정의되는 statistical distribution이다.
이때, input domain에 있는 모든 point들은 normal distribution random variable이 되며,
아무 finite한 GP sample들을 뽑더라도, 그 sample들은 multivariate normal distribution을 가지게 된다.</p>

<p>GP의 개념은 이 정도로만 설명을 마무리하고, formulation에 대해 살펴보자.
GP 하나를 정의하기 위해서는 mean function과 kernel function 두 가지 함수가 먼저 정의되어야 한다.</p>

<p>먼저 mean function $m(x)$는 이름에서도 쉽게 유추할 수 있듯 point x에서의 mean value를 나타내는 x에 대한 함수이다.
보통은 constant value m을 많이 선택하며, 그마저도 선택하지 않고 그냥 zero-mean을 고르는 경우도 많다고 한다.</p>

<p>Kernel function이 상당히 중요한데, kernel function은 주어진 GP sample들이 서로 어떤 relationship을 가지는지, 어떤 covariance matrix를 형성하게 되는지 정의하는 함수이다.
Kernel function $k(x, x^\prime)$은 점 두 개에 대해 정의가 되는데, 일반적으로 점 사이의 거리가 가까우면 relationship이 크고, 멀먼 작을 것이라는 가정을 하게 된다.
가장 간단한 kernel function인 squared-exponential kernel function은 다음과 같다. 이때, $x_d$는 $x$의 d 차원 value이고, $\alpha, \theta_d$는 hyperparameter이다.
($\theta_d$는 1부터 D까지 총 D개 존재한다.)</p>

<script type="math/tex; mode=display"> k_{sqe}(x, x^\prime) = \alpha \exp \left\{ -\frac{1}{2} \sum_{d=1}^D \left( \frac{x_d - x_d^\prime}{\theta_d} \right) \right\}. </script>

<p>Kernel function을 사용해 두 점 사이의 relation을 정의하고 나면, GP sample collection이 주어졌을 때, 해당 sample들의 covariance matrix를 다음과 같이 정의할 수 있다.
이 경우는 sample이 총 n개가 있고, $k_{ij} := k(x_i, x_j)$ 라고 정의하도록 하겠다.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
 K = \begin{pmatrix} k_{1,1} & k_{1,2} & \cdots & k_{1,n} \\ k_{2,1} & k_{2,2} & \cdots & k_{2,n} \\ \vdots  & \vdots  & \ddots & \vdots  \\ k_{n,1} & k_{n,2} & \cdots & k_{n,n} \end{pmatrix}.  %]]&gt;</script>

<p>Mean function $m(x)$와 kernel function $k(x, x^\prime)$이 정의가 되었으므로, 어떤 point x가 주어졌을 때, (앞에서 모든 GP의 sample은 normal ditribution r.v.라고 했음을 기억하자)
그 점의 mean과 variance를 계산할 수 있으므로, 이 둘만 정의가 된다면 아무 임의의 지점에 대해 Gaussian distribution r.v. 을 얻을 수 있다.</p>

<p>이 글에서는 function f(x)가 GP prior를 가진다고 가정했을 때, likelihood가 주어졌을 때 posterior가 어떻게 update되는지까지는 다루지 않을 것이다.
조금만 찾아보면 잘 정리된 내용들을 찾을 수 있을 것이다.</p>

<h5 id="gp-with-noisy-data">GP with Noisy data</h5>
<p>모든 함수가 항상 deterministic output을 가지지는 않는다. 오히려 거의 대부분의 real world function들은 관측할 때 마다 그 값이 바뀌게 된다.
이를 보통 우리는 noise라는 현상으로 설명하고는 한다. 조금 더 formal하게 적어보자.</p>

<p>다음과 같은 observation pair ${x_i, y_i}$ 가 있다고 가정해보자. 이 값은, input $x_i$와, 그 때 관측된 함수값 $y_i$의 pair로,
만약 noise가 없다면 $y_i = f(x_i)$ 라고 바로 쓸 수 있지만, 대부분의 경우는 noise가 있어서 그렇게 표현할 수 없다.
가장 많이 쓰이는 방법은 white Gaussian noise를 추가하는 방법이다. 따라서 이런 경우에 y는 다음과 같이 표현된다.</p>

<script type="math/tex; mode=display"> y_i \sim \mathcal N(f(x_i), \nu). </script>

<p>이때, $\nu$는 noise의 세기를 나타내는 hyperparameter가 된다.
이렇게 표현할 경우, noise가 없을 때와 있을 때 GP를 fit한 결과는 아래와 같은 차이가 나게 된다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-12.png" width="600" /></p>

<h4 id="acquisition-function">Acquisition Function</h4>
<p>Function f(x)가 GP prior를 가지는 Bayesian optimization을 진행 중이라고 가정해보자.
f(x)의 모든 point x에 대해, 우리는 mean과 variance를 계산할 수 있다 (위에 언급되었던 그림 중 까만 선과 회색 영역).
이때 다음으로 관측해야할 부분이 어디인지 어떻게 알 수 있을까?</p>

<p>한 가지 방법은 estimated mean의 값이 가장 작은 지점은 관측하여 현재까지 관측된 값들을 기준으로 가장 좋은 점을 찾아보는 것이다.
또 다른 방법은 variance의 값이 가장 큰 지점을 관측하여, 함수의 모양을 더 정교하게 탐색하는 방법이 있다.
즉, 다음에 어떤 점을 탐색하느냐를 결정하는 문제는 explore-exploit 문제가 된다. Explore는 high variance point를 관측하는 것, exploit은 low mean point를 관측하는 것이 되겠다.
Acquisition function이란 explore와 exploit을 적절하게 균형을 잡아주는 역할을 하며, 여러 종류가 있지만, 여기에서는 세 가지만 다루도록 하겠다
(Probability of Improvement, Expected Improvement, UCB).</p>

<p>이 섹션의 남은 부분에서, $f^\prime$ 이란, 지금까지 관측한 function 값 중에서 가장 minimum 값을 지칭하도록 하겠다.</p>

<h5 id="probability-of-improvement">Probability of Improvement</h5>
<p>Probability of improvement (PI)는, 특정 지점의 함수 값이 지금 best 함수 값인 f’ 보다 작을 확률을 사용한다.
즉, PI의 utility function은 다음과 같다.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
 u(x) = \begin{cases} 0 ~ & ~f(x) > f^\prime \\ 1 ~ & f(x) \leq f^\prime \end{cases}.  %]]&gt;</script>

<p>Estimated function f(x)의 값은 정해진 값이 아니라 확률 값이기 때문에, PI는 x에서의 u(x)의 expectation으로 표현된다.</p>

<script type="math/tex; mode=display"> a_{PI} (x) = \mathbb E [u(x) \| x, D] = \int_{-\infty}^{f^\prime} \mathcal N (f; \mu(x), k(x,x))df = \Phi (f^\prime; \mu(x), k(x,x)). </script>

<p>이때 $\mathcal N(f;\mu(x), k(x,x))$는 mean function $\mu(x)$와 kernel function $k(x, x)$로 표현되는 normal distribution이고, $\Phi(\cdot)$은 cdf를 의미한다.
PI를 그림으로 나타내면 아래와 같다. 아래 그림에서 이미 explore가 많이 된 지점이 PI가 높은 것에 주목하라.
(밑에 나올 그림들의 출처는 모두 <a href="http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf">[5]</a> 이다.)</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-7.png" width="500" /></p>

<h5 id="expected-improvement">Expected Improvement</h5>
<p>PI의 가장 큰 문제점 중 하나는, ‘improvement’ 될 수 있는 확률만 보기 때문에, 확률이 조금 더 낫을지라도, 궁극적으로는 더 큰 improvement가 가능한 point를 고를 수 없다는 점이다.
다시 말하면 exploit에 집중하느라 explore에 취약하다는 단점이 있다.
Expected improvement (EI)는 utility function을 0, 1이 아니라, linear 꼴로 정의하기 때문에 그 차이를 반영할 수 있다. (Step function과 ReLU의 차이라고 보면 된다)
EI의 utility function은 다음과 같다.</p>

<script type="math/tex; mode=display"> u(x) = \max(0, f^\prime - f(x)). </script>

<p>주의할 점은, EI가 PI의 expectation이 아니라는 점이다. 그냥 이름만 비슷한거고 완전히 다른 function이라고 생각하면 된다.
PI와 마찬가지로 EI역시 u(x)의 expectation을 계산해야 한다.</p>

<script type="math/tex; mode=display"> a_{EI} (x) = \mathbb E [u(x) \| x, D] = \int_{-\infty}^{f^\prime} \mathcal N (f; \mu(x), k(x,x))df = (f^\prime - \mu(x))\Phi(f^\prime;\mu(x),k(x,x)) + k(x,x) \mathcal N (f^\prime;\mu(x),k(x,x)). </script>

<p>EI를 그림으로 나타내면 다음과 같다. PI처럼 이미 explore가 많이 된 곳을 또 찾는 실수는 덜 저지른다는 것을 볼 수 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-8.png" width="500" /></p>

<h5 id="ucb">UCB</h5>

<p>UCB는 우리가 이미 잘 알고 있는 그 UCB이며, acquisition function은 다음과 같다.</p>

<script type="math/tex; mode=display"> a_{UCB}(x;\beta) = \mu(x) - \beta\sigma(x). </script>

<p>UCB의 문제점이라면, explore-exploit trade-off parameter인 $\beta$의 존재이다.
Form도 간단하고, 조절하기 쉽기도 하지만, hyperparameter를 또 조정해야한다는 문제 때문에 이 논문에서는 다루지 않는다.
UCB 역시 그림으로 나타내면 다음과 같다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-9.png" width="500" /></p>

<p>이 이외에도 Entropy search, Thompson sampling 등의 다양한 acquisition function이 있지만 이 글에서는 다루지 않도록 하겠다.</p>

<h3 id="limitation-of-bayesian-optimization">Limitation of Bayesian Optimization</h3>
<p>지금까지 Bayesian optimization (BO)에 대해 ‘간략히’ 알아봤다. 여기까지 글을 읽으면서 느꼈겠지만, Bayesian optimization은 굉장히 impractical하다.
여러가지 이유가 있는데, 크게는 다음과 같은 이유들이 있다.</p>

<ul>
  <li>Hyperparameter search를 하기 위해 BO를 사용하는데, BO를 사용하기 위해서는 GP의 hyperparameter들을 튜닝해야한다 (kernel function의 parameter 등)</li>
  <li>어떤 stochastic assumption을 하느냐에 따라 (어떤 kernel function을 사용해야할지 등) 결과가 천차만별로 바뀌는데, (model selection에 민감한데) 어떤 선택이 가장 좋은지에 대한 가이드가 전혀 없다.</li>
  <li>Acquisition function을 사용해 다음 지점을 찾는 과정 자체가 sequential하기 때문에 grid search나 random search와는 다르게 parallelization이 불가능하다.</li>
  <li>위에 대한 문제점들이 전부 해결된다고 하더라도 software implementation이 쉽지 않다.</li>
</ul>

<p>이런 문제점들을 해결하기 위해 이 논문은 (그렇다 이제서야 이 논문이 어떤 일을 했는지 얘기할 수 있게 되었다) 먼저 kernel function을 여러 실험적 결과 등을 통해
Matern 5/2 kernel이 가장 실험적으로 좋은 결과를 낸다는 결론을 내린다 (즉, kernel function은 언제나 Matern 5/2를 쓰면 된다). 또한 acquisition function도 EI로 고정한다.
다음으로 GP의 hyperparameter들을 Bayesian approach를 통해 acquisition function을 hyperparameter에 대해 marginalize한다.
이 marginalized acquisition function은 (integrated acquisition function이라고 한다) MCMC로 풀 수 있는데, 자세한 얘기는 뒤에서 이어서 하도록 하겠다.
마지막으로 이 논문은 이론적으로 tractable한 Bayesian optimization의 parallelized version을 (MCMC estimation이다) 제안한다.</p>

<p>저자들이 작성한 코드 역시 GitHub에 공개가 되어있다. (HIPS repo에 있는 코드가 최신이다. 둘이 라이센스가 다르기 때문에 상황에 맞춰 쓰면 된다.)</p>

<ul>
  <li><a href="https://github.com/JasperSnoek/spearmint">https://github.com/JasperSnoek/spearmint</a> (Out-dated, Fully open source)</li>
  <li><a href="https://github.com/HIPS/Spearmint">https://github.com/HIPS/Spearmint</a> (Up-to-dated, non-commercial use, academic use only)</li>
</ul>

<p>그 밖에도 최근 다른 곳에서도 이 내용을 implement한 것 같다.</p>

<ul>
  <li><a href="https://github.com/fdiehl/apsis">https://github.com/fdiehl/apsis</a></li>
</ul>

<h3 id="practical-bayesian-optimization">Practical Bayesian Optimization</h3>

<h4 id="expected-improvement-and-matern-52-kernel-function">Expected Improvement and Matern 5/2 Kernel function</h4>

<p>앞에서도 설명했듯, 이 논문은 먼저 acquisition function으로는 EI를 사용하고, kernel function으로는 Matern 5/2를 사용한다.
Kernel function을 무엇을 고르냐에 따라 어떤 변화가 나타나는지 보여주는 좋은 그림이 하나 있어 첨부한다. 출처: <a href="http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf">[5]</a></p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-10.png" width="500" /></p>

<p>가장 많이 쓰이는 Squared-exponential function의 가장 큰 문제는 ‘smoothness’로, 복잡한 모델을 표현하기에는 너무 ‘smooth’한 function만 estimate할 수 있다는 단점이 있다.
이를 해결하기 위해 이 논문에서는 Matern kernel function을 사용하며, 특히 그 hyperparameter로 5와 2를 사용하는 Matern 5/2를 사용하고 있다.
이 결과는 아무 값이나 고른건 아니고, 실제로 structured SVM의 hyperparameter를 찾을 때 여러 kernel function 중에서 가장 좋은 kernel이 무엇인지 아래와 같은 실험들 끝에 얻은 결과이다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-11.png" width="500" /></p>

<p>Matern 5/2 kernel의 구체적인 식은 다음과 같다.</p>

<script type="math/tex; mode=display"> K_{M52}(x, x^\prime) = \theta_0 \left( 1 + \sqrt{5 r^2(x, x^\prime)} + \frac{5}{3} r^2(x, x^\prime) \right) \exp \left\{ -\sqrt{5 r^2 (x, x^\prime)} \right\}. </script>

<script type="math/tex; mode=display"> r^2 (x, x^\prime) = \sum_{d=1}^D (x_d - x_d^\prime)^2 / \theta_d^2. </script>

<p>따라서 이 GP의 hyperparameter는 $\theta_0, \theta_d$로, d가 1부터 D까지 있으니 총 D+1 개의 hyperparameter를 필요로 한다.</p>

<p>앞으로 별 다른 언급이 없다면 kernel function은 Matern 5/2, acquisition function으로는 EI를 사용한다.</p>

<h4 id="integrated-acquisition-function-marginalize-hyperparameter">Integrated Acquisition Function (marginalize hyperparameter)</h4>

<p>이제 covariance의 형태를 결정했으니, GP의 hyperparameter를 없애는 일이 남았다.
우리가 optimize하고 싶은 hyperparameter의 dimension이 D라고 해보자 (위에서 언급했던 random forest의 경우, hyperparameter는 n_estimators, criterion, max_depth, min_samples_leaf로 D=4다). 이때 GP의 hyperparameter의 개수는 D+3개가 된다. 바로 앞에서 언급한 D+1개와, constant mean function의 값 m, 그리고 noise $\nu$가 그것이다.</p>

<p>이 논문에서는 hyperparameter를 완전하게 Bayesian으로 처리하기 위하여 모든 hyperparameter $\theta$ (D+3 dimensional vector)에 대해
acquisition function을 marginalize한 다음에, 다음과 같은 integrated acquisition function을 계산하는 방법을 제안한다.</p>

<script type="math/tex; mode=display"> \widehat a (x; \{x_n, y_n\}) = \int a(x; \{x_n, y_n\}, \theta) p(\theta \| \{x_n, y_n\})_{n=1}^N) d\theta. </script>

<p>PI와 EI에 대해서는 이 integrated acquisition function을 계산하기 위해 다양한 GP hyperparameter에 대한 GP posterior를 계산한 다음,
integrated acquisition function의 Monte Carlo estimatation을 구하는 것이 가능하다. 이 논문에서는 slide sampling을 사용해 구할 수 있다고 언급되어있다.
말이 조금 어려운데, 그냥 쉽게 생각해보면, sampling을 통해 얻은 여러 hyperparameter들에 대해 EI를 전부 구한 다음, 그것들을 사용해 expectation 계산을 하면 integrated EI를 구할 수 있다.
그림으로 표현하면 아래와 같다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-13.png" width="400" /></p>

<h4 id="expected-improvement-per-second">Expected Improvement per second</h4>
<p>위에서 구한 integrated EI를 사용한다고 하더라도, 아직 몇 가지 문제점들이 남아있다. 그 중 하나는, 모든 hyperparameter에 대해 실험 시간이 똑같지 않다는 점이다.
예를 들어 deep learning layer가 2인 것과 500인 것은 실험 시간의 차이가 어마어마하다.
따라서 실제로는 가장 최소한의 시행을 통해 optimization을 진행한다고 하더라도, 실제 소요 시간은 엄청 클 수도 있는 것이다.
이 논문은 그런 문제를 해결하기 위해, 필요한 경우 EI per second 라는 새로운 acquisition function을 제안한다.</p>

<p>아마도 NIPS 논문이 page limitation이 빡빡해서 그런지 정확한 formulation은 나와있지 않지만, 요점은 objective function f(x) 말고도,
duration function c(x) 라는 것을 따로 정의한 다음, 이 함수를 사용해 ‘cost’를 모델링하는 것이다.
c(x)도 GP라고 assume하는 것 같은데, c(x)와 f(x)가 independent하다고 가정하면 쉽게 acquisition function을 구할 수 있는 모양이다.
아래 실험결과에서도 볼 수 있듯, 오히려 실제 실행 시간의 관점에서는 EI per second가 더 빠른 것을 알 수 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-15.png" width="500" /></p>

<h4 id="monte-carlo-acquisition-for-parallelizing-bayesian-optimization">Monte Carlo Acquisition for Parallelizing Bayesian Optimization</h4>
<p>이제 이 논문의 마지막 하이라이트만 남았다. Acquisition function을 optimize하면서 다음 point를 고르는 방식은 parallelization하기가 쉽지 않다.
매 번 포인트를 고를 때 마다 이 function이 바뀌기 때문인데, 여러 heuristic을 사용할 수는 있지만, theoretically tractable한 결과를 얻기는 쉽지 않다.</p>

<p>다음과 같은 문제 상황을 가정해보자. N개의 데이터의 evaluation이 끝난 상황이고 $(\{x_n, y_n\}_{n=1}^N)$ J개의 point들에서 $(\{x_j\}_{j=1}^J)$ 실험을 진행 중이라고 가정해보자. (아직 결과는 나오지 않았다)
이론상 지금까지 진행한 실험과 $(\{x_n, y_n\}_{n=1}^N)$ 현재 진행 중인 실험 $(\{x_j\}_{j=1}^J)$ 을 모두 고려하여 다음 point를 고르기 위해서는,
acquisition function의 J개의 아직 결과가 나오지 않은 point들에 대한 expectation을 구한 다음, 그 결과를 acquisition function으로 사용하면 된다.</p>

<script type="math/tex; mode=display"> \widehat a (x; \{x_n, y_n\}, \theta, \{x_j\}) = \int a (x; \{x_n, y_n\}, \theta, \{x_j, y_j\}) p(\{y_j\}_{j=1}^J \| \{x_j\}_{j=1}^J, \{x_n, y_n\}_{n=1}^N)dy_1, \ldots, dy_J. </script>

<p>다행스럽게도, y가 Gaussian distribution이기 때문에 이 expectation은 쉽게 계산할 수 있으며,
단순히 동시에 진행하는 실험의 숫자를 늘리는 것으로 parallelization을 할 수 있기 때문에 parallelization 역시 간단하게 할 수 있다.
이 방법론을 GP EI MCMC라고 하며, 그림으로 나타내면 아래와 같다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/99-14.png" width="400" /></p>

<h4 id="conclusion">Conclusion</h4>
<p>이 논문의 꽃이라 할 수 있는 실험 결과는 스킵하도록 하겠다. 그냥 “압도적으로 좋다” 정도로 이해하고 넘어가자.
사실 또 하나 언급하지 않은 점은, supplimentary material에 있는 구체적인 acquisition function optimization 방법이다.
이 논문은 개념적으로 알고 있어야하는 내용이 안그래도 많은데, 이 얘기를 하려면 여기에서 더 많은 얘기를 해야해서 넘기기로 하였다.
나중에 여유가 있을 때 추가 포스트를 쓰던가 해야겠다.</p>

<p>이 논문은 잘 쓰기만하면 굉장히 outperform한 성능을 낼 수 있는 Bayesian optimization 기반 hyperparameter search 알고리즘을 제안한다.
핵심은 어떻게 다음 point를 고를 것인지 설정하는 acquisition function을 design하느냐인데,
이 논문은 GP의 hyperparameter도 acquisition function에 녹이고, parallelization을 하기 위해 아직 진행 중인 실험의 expectation또한
이 acquisition function에 녹임으로써 원래 Bayesian optimization이 가지고 있었던 한계를 극복한다.
그뿐 아니라 실험적으로 우수한 kernel function인 Matern 5/2를 기본 kernel function제안함으로써 model selection 이슈도 피해간다.</p>

<p>실제 구현해서 사용하기는 어려운 내용이지만, 잘 숙지해두면 분명 도움이 될 수 있는 아이디어라 생각한다.</p>

<h3 id="references">References</h3>
<ol class="reference">
	<li><a href="http://papers.nips.cc/paper/4522-practical">[NIPS] Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. &#8220;Practical bayesian optimization of machine learning algorithms.&#8221;, 2012.</a></li>
	<li><a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">[JMLR] Bergstra, James, and Yoshua Bengio. &#8220;Random search for hyper-parameter optimization.&#8221;, 2012</a></li>
	<li><a href="http://becs.aalto.fi/en/research/bayes/courses/4613/Vik_Kamath_Presentation.pdf">http://becs.aalto.fi/en/research/bayes/courses/4613/Vik_Kamath_Presentation.pdf</a></li>
	<li><a href="http://enginius.tistory.com/489">http://enginius.tistory.com/489</a></li>
	<li><a href="http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf">http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf</a></li>
	<li><a href="http://www.dmi.usherb.ca/~larocheh/publications/gpopt_nips_appendix.pdf">[NIPS] Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. &#8220;Practical bayesian optimization of machine learning algorithms.&#8221; Supplimentary material, 2012.</a></li>
</ol>

<h3 id="section-1">변경 이력</h3>
<ul>
  <li>2016년 8월 16일: 글 등록</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (10) PAC Learning & Statistical Learning Theory]]></title>
    <link href="http://SanghyukChun.github.io/66/"/>
    <updated>2016-05-06T00:26:00+09:00</updated>
    <id>http://SanghyukChun.github.io/66</id>
		<content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#section">들어가며</a></li>
  <li><a href="#motivation">Motivation</a></li>
  <li><a href="#revisit-machine-learning-in-function-approximation-view">Revisit: Machine Learning in function approximation view</a></li>
  <li><a href="#overfitting-revisited">Overfitting: revisited</a></li>
  <li><a href="#pac-learning-with-finite-hypothesis-space">PAC Learning with Finite Hypothesis Space</a></li>
  <li><a href="#pac-leanring-with-infinite-hypothesis-space">PAC Leanring with Infinite Hypothesis Space</a>    <ul>
      <li><a href="#vc-dimension">VC Dimension</a></li>
    </ul>
  </li>
  <li><a href="#structure-risk-minimization">Structure Risk Minimization</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#section-1">변경 이력</a></li>
  <li><a href="#machine-learning---">Machine Learning 스터디의 다른 글들</a></li>
</ul>

<h3 id="section">들어가며</h3>
<p>어떤 머신러닝 모델이 있다고 가정해보자. 이 모델이 다른 모델에 비해 뛰어나다고 주장하려면 어떤 것들이 필요할까? 알고리즘이 얼마나 더 뛰어난지 주장하기 위해서는 알고리즘의 수렴성이나 complexity 등에 대해 논하면 되지만, 모델이 얼마나 뛰어난지는 어떻게 설명할 수 있을까? 이번 글에서 설명할 PAC (Probably Approximately Correct) 라는 개념은 이렇듯 모델의 성능을 이론적으로 측정하는 방법이라고 할 수 있다. 이 글에서는 PAC의 기본 개념을 설명하고, 핵심이라 할 수 있는 PAC bound에 대해 설명할 것이다. 또한 모델의 hypothesis space가 infinite한 경우 (거의 모든 continous parameter를 가지는 모델의 경우) VC dimension이라는 것을 통해 PAC bound를 어떻게 다시 bound 시킬 수 있는지 등에 대해 설명할 것이다.</p>

<h3 id="motivation">Motivation</h3>
<p>맨 앞 글에서도 잠시 언급했듯, 우리의 motivation은 특정한 모델이 얼마나 뛰어난지를 측정하는 것이다. 모델이 ‘뛰어나다’ 라고 말하기 위해서는, 다른 모델에 비해 learning하는 데에 필요한 데이터의 수가 적어야할 것이고, learning한 이후에 inference를 했을 때 그 결과가 좋아야할 것이고, 마지막으로 learning이 가능한 상황이 많이 있어야한다. 예를 들어서 어떤 모델을 만들었는데, 이 모델이 너무 복잡해서 100번 learning시켰을 때 (overfitting등의 이슈로 인해) 오직 3번 정도만 제대로 learning이 된다고 하면 이 모델은 쓸모없는 모델일 것이다.
따라서 ‘좋은 모델’ 인지 여부를 판단하기 위해서는 다음과 같은 질문들에 대답을 해야한다.</p>

<ul>
  <li>“Seccessful” learning을 할 확률</li>
  <li>Learning에 필요한 training example의 개수</li>
  <li>ML algorithm에 의해 구해진 approximated target function의 정확도</li>
</ul>

<p>이런 질문들에 답을 하기 위해 등장하게 된 분야가 바로 <a href="https://en.wikipedia.org/wiki/Computational_learning_theory">Computational learning theory</a>으로, machine learning algorithm의 분석을 위한 수학적이고 이론적인 분야라고 생각하면 될 것 같다. 오늘 다루게 될 PAC는 이 computational learning theory의 한 부분으로, 가장 간단한 이론 중 하나이지만, 그 만큼 이론적으로 시사하는 바가 많은 이론이기 때문에 많은 machine learning course에서 한 번은 언급하고 넘어가는 경우가 많다.</p>

<h3 id="revisit-machine-learning-in-function-approximation-view">Revisit: Machine Learning in function approximation view</h3>
<p><a href="http://SanghyukChun.github.io/57">맨 처음 글</a>에서 잠시 다뤘던 내용이지만, machine learning이라는 것이 어떤 것인지에 대해 다시 살펴보도록하자. 그때도 언급했지만, machine learning이라는 것을 구성하는 것은 크게 다음과 같다. 이 글에서는 이 글에 맞는 용어로 재구성했다. 참고로 이 글에서는 machine learning의 역할이 특정 데이터의 결과를 예측하는 function을 찾는 것이라고 가정한다. 즉, 특정 데이터가 어느 class에 속하는지 판단하는 function을 찾는 문제라고 생각할 것이다. 따라서 error는 항상 정확하게 정의된다 (함수 값이 정확하게 찾아진 데이터의 수와, 그렇지 않은 수에 대해 바로 error가 0에서 1사이의 값으로 정의된다).</p>

<ul>
  <li>Instance $X$: 모든 데이터의 공간이라 할 수 있다. 즉, 모든 픽셀이 0 또는 1인 28 by 28 흑백 이미지라고 한다면 $2^{784}$ 크기의 set이 될 것이다.</li>
  <li>Target concept $c$: 혹은 우리에게 조금 더 익숙한 용어로 표현하면 ‘target function’이다. Instance space의 subset으로 정의가 되며, 주어진 데이터가 어떤 값을 가지는지 판단하는 함수라고 생각하면 된다. 예를 들어 MNIST classification 문제에서는 주어진 데이터 x가 [0-9] 사이의 데이터 중에서 어디에 속하는지 판단하는 함수가 될 것이다.</li>
  <li>Hypothesis space $H$: 주어진 $c$와 최대한 비슷한 approximated function (hypothesis) $h$가 속하는 function space이다. 예를 들어 함수가 linear라 가정한다면 Hypothesis space는 모든 linear function의 function space가 된다.</li>
  <li>Training Data $D$: 모든 instance space를 다 볼수는 없으니 그 중 일부의 데이터만이 training data로 주어진다.</li>
  <li>Machine learning 모델은 위의 4가지가 주어졌을 때, 다음과 같은 두 가지 질문을 판단해야한다.
    <ol>
      <li>주어진 training data $x \in D$에 대해 Hypothesis $h \in H$가 $c(x)$와 얼마나 비슷한가?</li>
      <li>모든 data $x \in X$에 대해 Hypothesis $h \in H$가 $c(x)$와 얼마나 비슷한가?</li>
    </ol>
  </li>
</ul>

<p>좀 더 자세히 설명하기 이전에 …. There is no free lunch 라고 한다.</p>

<p>여기에서 한 가지 재미있는 점이 있는데,</p>

<script type="math/tex; mode=display"> Pr[ \mbox{error}_{true} (h) \leq \mbox{error}_{train}(h) + \varepsilon ] \leq \| H \| exp(-2m\varepsilon^2). </script>

<h3 id="overfitting-revisited">Overfitting: revisited</h3>
<p>앞선 섹션의 마지막 두 질문, 주어진 training data와 모든 data에 대해 hypothesis $h$와 $c$가 얼마나 비슷한지에 대한 질문은 각각 training error와 true error의 값이 어떻게 되는가를 물어보는 질문과 동일하며, 이 둘은 각각 traning data set $D$와 전체 데이터셋 $X$에서의 learning된 hypothesis $h$의 error로 정의된다. 수식으로 표현하면 다음과 같다.</p>

<p><script type="math/tex"> error_{true}(h) := \mbox{Pr}_{x\in X} [c(x) \neq h(x)] </script>
<script type="math/tex"> error_{train}(h) := \mbox{Pr}_{x\in D} [c(x) \neq h(x)] </script></p>

<p>당연한 얘기지만, 모든 데이터에 대해 true error를 측정하는 것은 불가능하다. 그러나 만약 우리가 test data를 모든 데이터셋에서 uniformly random하게 (i.i.d하게) 뽑는다면 test error는 true error의 unbiased estimator라고 할 수 있다. 이 글에서는 반드시 test data가 i.i.d하게 뽑혔다고 가정하고 test error를 true error의 unbiased estimation으로 취급할 것이다. 즉, 앞으로 test error라고 언급하는 것들은 전부 true error와 같다고 생각해도 된다.</p>

<p>우리가 model을 learning하기 위해, 혹은 적절한 $h$를 찾기 위해 할 수 있는 가장 간단한 방법은 바로 현재 주어진 training data에서 가장 error가 낮은 hypothesis를 찾는 것이다. 수식으로 쓰면, $h = \arg\min_h error_{train}(h)$가 될 것이다. 여기에서 $h$와 training error가 서로 dependent하다는 사실을 기억해야한다. 이 방법은 주어진 데이터에 대해서 risk를 minimization하는 문제를 푸는 것이기 때문에 우리는 이를 empirical risk minimization이라고 부른다. 그러나 이 방법에는 한 가지 문제가 있는데, 앞서 설명한 test error와는 다르게 training error는 true error의 unbiased estimator가 아니기 때문이다. 왜냐하면 앞에서 설명했듯 $h$가 training data에서부터 골라진 값이기 때문에 training error가 $h$에 dependent하게 되고, 따라서 training error는 true error의 unbiased estimator가 아니라는 결론을 내릴 수 있다.</p>

<p>따라서 우리가 만든 $h$는 항상 true error보다 training error가 좋은 결과를 얻을 것이라는 생각을 할 수 있다.</p>

<h3 id="pac-learning-with-finite-hypothesis-space">PAC Learning with Finite Hypothesis Space</h3>

<script type="math/tex; mode=display"> m \geq \frac{1}{2\varepsilon^2} (\ln \|H\| + \ln(1/\delta)). </script>

<script type="math/tex; mode=display">\mbox{error}_{true}(h) \leq \mbox{error}_{train}(h) + \sqrt{\frac{\ln \|H\| + \ln \frac{1}{\delta}}{m}}.</script>

<h3 id="pac-leanring-with-infinite-hypothesis-space">PAC Leanring with Infinite Hypothesis Space</h3>

<h4 id="vc-dimension">VC Dimension</h4>

<h3 id="structure-risk-minimization">Structure Risk Minimization</h3>

<h3 id="references">References</h3>
<ol class="reference">
  <li>hi</li>
  <li>hello</li>
</ol>

<h3 id="section-1">변경 이력</h3>
<ul>
  <li>2016년 3월 : 글 등록</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Octopress Markdown Kramdown으로 이전하기]]></title>
    <link href="http://SanghyukChun.github.io/98/"/>
    <updated>2016-03-21T00:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/98</id>
		<content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#html--markdown-">HTML 코드에서 Markdown 코드로</a></li>
  <li><a href="#markdown--mathjax">기존 Markdown 디버그 (MathJax)</a></li>
  <li><a href="#kramdown-markdown">Kramdown Markdown</a>    <ul>
      <li><a href="#table-of-contents-toc">Table of Contents (ToC)</a></li>
      <li><a href="#inline-attribute-lists-ial">Inline Attribute Lists (IAL)</a></li>
      <li><a href="#section">개인적으로 불편한 점들</a></li>
    </ul>
  </li>
  <li><a href="#todo">TODO</a></li>
</ul>

<h3 id="html--markdown-">HTML 코드에서 Markdown 코드로</h3>
<p>이 블로그를 시작한 이래로, 나는 줄곧 markdown code가 아니라 진짜 pure html code로 블로깅을 해왔다.
별 다른 이유가 있다기보다는, 이 블로그를 맨 처음 시작할 때만 하더라도 markdown에 대한 개념도 많이 없었고,
어느 정도 시간이 흐른 뒤에는 markdown으로 작업하려니 몇 가지 충돌사항들이 발생하는데, 이걸 일일이 처리하기도 귀찮고,
그걸 처리하자고 일부는 markdown으로 쓰고 일부는 html로 작성하려니 정말 끔찍한 코드가 나와서, markdown을 버렸다.</p>

<p>Markdown과 충돌을 일으키는 대표적인 녀석은 바로 <code>MathJax</code>.
알 수 없는 이유로, markdown 안에 MathJax 문법을 쓰면 제대로 동작하지 않았다 (정확히는 escape됐다).
사실 조금만 알아보면 고칠 수 있었는데, 그때는 당췌 원인도 잘 모르겠고, 그냥 힘드니까 버려두었다가,
오늘 갑자기 삘이 꽂혀서 막 고치다가 정신을 차려보니 전체 소스 코드를 뜯어고치고 있더라.</p>

<p>결론적으로 지금은 only markdown 문법만 사용해도 블로깅을 할 수 있도록 소스코드를 뜯어고친 상태이고,
지금 이 글 역시 markdown으로만 작성하고 있다. Markdown을 사용했을 때의 장점이라면,</p>

<ol>
  <li>편집기 상에서 각각의 포스트들이 훨씬 더 readable하다.
    <ul>
      <li>아무래도 html tag가 난잡하게 섞여있으면 편집기로 코드를 읽기가 벅차다. 그럴 일은 없지만, 만약 html syntax highlighting을 못해주는 편집기라도 썼다가는….</li>
      <li>사실 이런 이유로, 일일이 html로 코딩할 수 없는 수업, 세미나, 아이디어 정리 등을 포스팅할 때 아래같은 문제가 생긴다.</li>
    </ul>
  </li>
  <li>포스팅을 할 때에 덜 번거롭다.
    <ul>
      <li>HTML을 생으로 코딩한다는 얘기는, id, class 같은 attribute라거나, a tag의 내부도 일일이 쳐줘야하고.. 여러모로 귀찮다.</li>
      <li>HTML로 코딩하는 것 보다는 확실히 생산성이 증가한다. 특히 내가 html 코딩용으로 맞춰둔 sublime text를 쓸 수 없는 환경에서 텍스트로 적은 글을 다시 html로 포팅하는 일은 시간만 엄청 잡아먹고 전혀 생산적이지 않은 일인데 그런 시간을 많이 줄일 수 있을 것 같다.</li>
    </ul>
  </li>
  <li>전체적인 포스팅 시간이 감소한다.
    <ul>
      <li>앞에서 얘기한 덜 번거롭다는 점도 물론 크게 작용한다.</li>
      <li>지금 내가 사용 중인 octopress의 경우, rails 기반이라 글을 수정하면 다시 컴파일을 하지 않아도 알아서 글이 rebuild되긴 하지만, 어쨌거나 이것도 시간이 든다.</li>
      <li>근데 markdown으로만 코딩을 할 수 있다면 그냥 빠르고 간단한 markdown editor로 글을 쓴 다음 붙여넣기만 하면 된다. 수식이나 그림같은 몇 가지 markdown으로만 지원되지 않는 방식들은 나중에 한 번에 확인하면 되니까 훨씬 시간이 감소한다.</li>
    </ul>
  </li>
</ol>

<p>항상 최고일 수는 없듯, markdown을 썼을 때의 단점도 있다.</p>

<ol>
  <li>명시적으로 코드를 쓰는 것이 아니기 때문에 원하는대로 동작하지 않을 때 디버깅하기가 많이 귀찮다
    <ul>
      <li>대표적인 예로 뒤에서 설명할 <code>MathJax</code> 디버깅할때 진짜..</li>
    </ul>
  </li>
  <li>어쩔 수 없이 html componenet를 하나하나 작업해야할 때가 있는데, 이때 마크다운만 쓰면 어쩔 수 없이 한계가 온다.
    <ul>
      <li>지금 image caption을 div tag를 써서 div &gt; img, p로 구성한 상태인데, 이걸 마크다운으로…. 적당히 플러그인으로 코딩하면 어떻게든 할 수 있지 않을까 고민 중이다.</li>
    </ul>
  </li>
</ol>

<p>하지만 이래저래 markdown을 썼을 때의 생산성만 못하기 때문에 markdown으로 제대로 쓸 수 있도록 몇 가지 작업을 시작했다. (마치 C가 memory를 명시적으로 다루기에 빠워하지만, 코드 생산성은 파이썬이나 루비를 못따라가는 것과 비슷한 이치)</p>

<h3 id="markdown--mathjax">기존 Markdown 디버그 (MathJax)</h3>
<p>기존 markdown기반 코드의 가장 큰 문제는 <code>MathJax</code>가 전혀 동작하지를 않는다는 것이였다. 예를 들어서</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr> <td class="code"><pre><code class=""><span class="line">* example 1: $\alpha$.
</span><span class="line">* example 2: \(\alpha\).</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>이런 코드를 작성했다고 했을 때, 둘 다 제대로 된 $\alpha$를 보여주는 녀석이 없었다.
뭐, 일단 여러 이유가 있었는데, 먼저 원래대로라면 위에 있는 <code>$</code> 쓴 녀석은 잘 동작해야한다.
하지만 제대로 동작하지 않았는데, 그건 내가 설정을 좀 잘못했던거고,</p>

<p>문제는 두 번째 녀석이었는데, 이 녀석은 다른게 문제가 아니라 <code>\</code> 가 escape되어버리는 문제가 있었다.
그러니까 저 코드를 돌리면, <code>example 2: (alpha)</code> 같은 모양이 나와서 속은 터지고 원인은 모르는 상황이 발생.
그래도 이게 markdown이 escape을 해버리는 거라서 html로 explict하게 적어주면 잘 동작했다.
그래서 결국은 저 간단한 코드가 이런 모양새가 되어버렸다</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="html"><span class="line"><span class="nt">&lt;ul&gt;</span>
</span><span class="line">	<span class="nt">&lt;li&gt;</span>example 1: \(\alpha\)<span class="nt">&lt;/li&gt;</span>
</span><span class="line">	<span class="nt">&lt;li&gt;</span>example 2: \(\alpha\)<span class="nt">&lt;/li&gt;</span>
</span><span class="line"><span class="nt">&lt;/ul&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>음.. 딱 봐도 지저분하지 않은가? 문제점을 해결하려고 몇 번 시도하다가,
초반에 <code>MathJax</code>에서 많이 고생했던 터라, 일단 동작하는 상태로 방치해둔게 벌써 거의 2년이 지난 셈이다.</p>

<p>결국, 지금은 <code>MathJax</code>의 설정을 다음과 같이 고쳤다.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="js"><span class="line"><span class="o">&lt;</span><span class="nx">script</span> <span class="nx">type</span><span class="o">=</span><span class="s2">&quot;text/x-mathjax-config&quot;</span><span class="o">&gt;</span>
</span><span class="line">    <span class="nx">MathJax</span><span class="p">.</span><span class="nx">Hub</span><span class="p">.</span><span class="nx">Config</span><span class="p">({</span>
</span><span class="line">      <span class="nx">jax</span><span class="o">:</span> <span class="p">[</span><span class="s2">&quot;input/TeX&quot;</span><span class="p">,</span> <span class="s2">&quot;output/HTML-CSS&quot;</span><span class="p">],</span>
</span><span class="line">      <span class="nx">tex2jax</span><span class="o">:</span> <span class="p">{</span>
</span><span class="line">        <span class="nx">inlineMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span><span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span><span class="p">]</span> <span class="p">],</span>
</span><span class="line">        <span class="nx">displayMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span><span class="s1">&#39;$$&#39;</span><span class="p">,</span> <span class="s1">&#39;$$&#39;</span><span class="p">]</span> <span class="p">],</span>
</span><span class="line">        <span class="nx">processEscapes</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class="line">        <span class="nx">skipTags</span><span class="o">:</span> <span class="p">[</span><span class="s1">&#39;script&#39;</span><span class="p">,</span> <span class="s1">&#39;noscript&#39;</span><span class="p">,</span> <span class="s1">&#39;style&#39;</span><span class="p">,</span> <span class="s1">&#39;textarea&#39;</span><span class="p">,</span> <span class="s1">&#39;pre&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">]</span>
</span><span class="line">      <span class="p">}</span>
</span><span class="line">    <span class="p">});</span>
</span><span class="line"><span class="o">&lt;</span><span class="err">/script&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>내가 앞에서 <code>$</code>가 제대로 동작하지 않은 이유는, 이 설정에서 제대로 명시가 되지 않아서 였던 모양이다.
원래 기본옵션으로 되어야 맞는데, 왜인지는 모르겠지만 잘 되지 않았던 모양.</p>

<p>그리고 지금은 <code>\(\)</code>와 <code>\[\]</code>를 아예 패턴에서 빼버린 상황인데,
먼저 <code>\(\)</code>나 <code>\[\]</code>같은 방식으로 수식을 쓰면 가독성이 떨어진다는 점 (처음 쓸 때는 LaTex 초보라 <code>$</code>같은 좋은걸 몰랐다)
그리고 앞에서 말한대로 자꾸 escape해버리는 경우가 발생해서 아예 없애버렸다.
이게 언제 심각해지냐하면, 다음과 같은 방식의 markdown을 쓸 때에 결과 값이 이상해진다</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="js"><span class="line"><span class="err">다음</span> <span class="err">링크</span> <span class="p">[[</span><span class="err">링크</span><span class="p">]](</span><span class="nx">http</span><span class="o">:</span><span class="c1">//sanghyukchun.github.io)를 누르면 제 블로그로 갑니다.</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>내가 이해하기로는 octopress에서 <code>MathJax</code>를 쓰면,</p>

<ul>
  <li>markdown (컴파일) → (public에 생성된) html → <code>MathJax</code>로 parsing</li>
</ul>

<p>이런 현상이 일어나는데, 위와 같은 방식으로 컴파일을 하고나면 경우에 따라 <code>[]</code>가 <code>\[\]</code>로 인식이 되어서
‘링크’가 display math로 떠버리는 황당한 경우가 생기더라.
결국 보기도 편하고 쓰기도 편한 달러로 통일.</p>

<ul>
  <li><a href="https://www.lucypark.kr/blog/2013/02/25/mathjax-kramdown-and-octopress/">https://www.lucypark.kr/blog/2013/02/25/mathjax-kramdown-and-octopress/</a> 같은 경우에도 보면 markdown 문법이랑 충돌하거나, markdown이 escape하는 녀석을 MathJax에서 쓰면 좀 귀찮아진다.</li>
</ul>

<h3 id="kramdown-markdown">Kramdown Markdown</h3>

<p>원래 내 Octopress는 <a href="http://dafoster.net/projects/rdiscount/">RDiscount</a> markdown을 쓰고 있었다.
괜찮은 녀석이긴 한데, 내가 결정적으로 원하는 결정적인 기능인 Heading tag에 자동으로 id 추가기능을 전혀 지원하지 않아서 다른 markdown으로 이전하기로 결정했다.
내가 후보로 살펴본 markdown은 <a href="http://kramdown.gettalong.org/">Kramdown</a>과 <a href="https://github.com/vmg/redcarpet">Redcarpet2</a>이었는데, 이 중 Redcarpet2는 GitHub의 markdown의 베이스가 되는 녀석이라서 <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> 좀 많이 탐을 냈는데, octopress와는 궁합이 잘 맞지 않아서 포기했다.</p>

<p>Octopress에서 Redcarpet을 쓰는 방법<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>에 대해 포스팅을 한 사람도 있어서 따라서 해봤는데,
나는 자꾸 <code>atom.xml</code>에서 <code>Liquid Exception</code>이 발생하더라.
내용은 <code>render</code>라는 tag가 없다는건데, 뭔가 redcarpet을 쓰면 내가 octopress에서 남발하고 있는 render가 제대로 동작하지 않는 모양. 결국 포기하고 Kramdown으로 넘어가기로 했다.</p>

<p>Kramdown으로 바꾸기 위해서는 <code>_config.yml</code>에서 markdown부분을 <code>markdown: kramdown</code>으로 수정하기만 하면 된다.
몇 가지 옵션을 줄 수 있는데, 아직 옵션은 안줘봤다.</p>

<h4 id="table-of-contents-toc">Table of Contents (ToC)</h4>

<p>내가 RDiscount를 버린 가장 큰 이유는 바로 Table of Contents가 없다는 건데,
꼭 필요한건 아니지만, 가끔 ToC가 있었으면 할 때가 있는데도 만들 수가 없어서 좀 번거로울 때가 있었다.
그리고 또 다른 포스트에서 특정 포스트의 Heading을 refer 해야할 때에도 id auto generation이 안되기 때문에
내가 일일이 다시 id tag를 달아줘야하는 끔찍함이 있었는데, 그런 불편함도 사라졌다.</p>

<p><a href="http://kramdown.gettalong.org/converter/html.html#toc">링크</a>를 타고 들어가면 ToC를 만드는 법에 대해 나와있으니 참고하면 좋을 것 같다. 아무튼 지금은 맨 앞에 이렇게만 쓰면 된다. 엄청 편함.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="js"><span class="line"><span class="o">*</span> <span class="nx">TOC</span>
</span><span class="line"><span class="p">{</span><span class="o">:</span><span class="nx">toc</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>아 물론 css를 좀 고쳐야한다. <a href="http://blog.riemann.cc/2013/04/10/table-of-contents-in-octopress/">http://blog.riemann.cc/2013/04/10/table-of-contents-in-octopress/</a> 를 참고하시길.</p>

<h4 id="inline-attribute-lists-ial">Inline Attribute Lists (IAL)</h4>

<p>그리고 <code>Kramdown</code>의 또 하나의 장점이라면 markdown의 여러 component들에게 inline으로 attribute를 지정할 수 있다는 것이다.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>
이게 무슨 말이냐 하면, 내가 지금 사용 중인 reference list는 다음과 같은 코드로 작성된다.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="html"><span class="line"><span class="nt">&lt;ol</span> <span class="na">class=</span><span class="s">&quot;reference&quot;</span><span class="nt">&gt;</span>
</span><span class="line">	<span class="nt">&lt;li&gt;</span>referenece 1<span class="nt">&lt;/li&gt;</span>
</span><span class="line">	<span class="nt">&lt;li&gt;</span>referenece 2<span class="nt">&lt;/li&gt;</span>
</span><span class="line"><span class="nt">&lt;/ol&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>리스트에 class attribute가 있어서 어쩔 수 없이 html 코드를 직접 써야하는데, <code>Kramdown</code>을 쓰면 다음과 같이 쓸 수 있다.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr> <td class="code"><pre><code class="html"><span class="line">1. reference 1
</span><span class="line">2. reference 2
</span><span class="line">{: .reference}
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>당연히 <code>#</code>을 쓰면 id도 가능하다!</p>

<h4 id="section">개인적으로 불편한 점들</h4>
<p>일단 <code>\|</code> 이 무조건 table로 사용이 된다. 그래서 <code>MathJax</code>에서 <code>\| A \|</code> 같은걸 쓰려면 <code>\\\| A \\\|</code> 처럼 escape이 된 형태로 써야한다 (심지어 지금도 그렇게 쓰고있다!). 내가 table을 쓰면 상관이 없지만, table을 안쓰는 입장에서 자꾸 이렇게 되니까 답답해 미칠 것 같다. 하다하다 짜증나면 p tag쓰면 된다지만… 그래도 ㅠㅠ..
그리고 링크도 <code>&lt;  &gt;</code>로 묶어줘야지만 auto link가 되는 것도 좀 귀찮다. 근데 이건 장단이 있으니까 익숙해지면 될 것 같다.</p>

<p>그걸 제외하면 아직까지는 엄청 불편한건 없어보인다.
사실 그 동안 markdown으로 글을 안쓰다보니, 갑자기 markdown을 쓰는 것 만으로 글이 엄청 편해져서, 이런 단점들은 아직까지는 그닥 크지 않다.</p>

<h3 id="todo">TODO</h3>
<p>오늘 하루 종일 이 작업뿐 아니라 블로그 공사에만 집중 했음에도 불구하고 아직도 할 일이 많다.
오늘 한 대표적인 일 중 하나라면 전체 CSS랑 HTML layout을 살짝 손봤다. 디테일한 부분을 손봤기 때문에 사실 크게 티는 안나지만, 그래도 뭐, 꽤나 만족한다.
이제 남은 녀석들은 예전 html로 작업한 녀석들 중에 필요한 녀석들은 markdown으로 포팅하는 무시무시한 작업을 제외한다면,
그 이외에는 다 루비코드를 직접 건드려야하는 녀석들이다. 대표적으로 남은 놈들이라면..</p>

<ul>
  <li>Syntax Highlighting 복구하기
    <ul>
      <li>이건 원래 되던 기능인데, 내가 bootstrap붙이면서 충돌이 너무 일어나서 겨우겨우 depreciate시켜놨더니 이제 무슨 수를 써도 highlighting이 안된다 (..)</li>
      <li>이건 js로 되는 문제가 아니라 원래 하이라이팅을 해주는 octopress 코드를 충돌이 일어나지 않도록 뜯어고쳐야할듯..</li>
      <li>오늘 오전부터 한 2-3시까지 이것만 했는데 도대체 왜 안되는건지 갑갑해서 돌아버리겠다.</li>
    </ul>
  </li>
  <li>Image Caption
    <ul>
      <li>지금 image caption은 만들어는 놨는데, markdown으로 하기에는 코드가 좀 끔찍하다.</li>
      <li><code>&lt;div class="caption"&gt;&lt;img src="img"&gt;&lt;p&gt;caption&lt;/p&gt;&lt;/div&gt;</code> 이런 식의 코드라서, 뭔가 루비 코드를 직접 건드려야 할 것 같다.
        <ul>
          <li><a href="http://blog.zerosharp.com/image-captions-for-octopress/">http://blog.zerosharp.com/image-captions-for-octopress/</a> 응용하면?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Github style heading 링크
    <ul>
      <li>이건 루비코드를 건드릴 필요는 없지만, 그래도 할게 좀 많아보인다.</li>
      <li><a href="http://ben.balter.com/2014/03/13/pages-anchor-links/">http://ben.balter.com/2014/03/13/pages-anchor-links/</a> 시간날 때 해보자.</li>
    </ul>
  </li>
</ul>

<p>조만간 시간있을 때 Syntax 부분이랑 anchor link는 다시 덤벼봐야겠다.
Caption은 일이 좀 커질 것 같아서..</p>

<p>아무튼 markdown으로 포스팅하니까 진짜 편하네.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://github.com/blog/832-rolling-out-the-redcarpet">https://github.com/blog/832-rolling-out-the-redcarpet</a><a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://yangsu.github.io/blog/2012/10/11/using-octopress-with-github-flavored-markdown-redcarpet/">http://yangsu.github.io/blog/2012/10/11/using-octopress-with-github-flavored-markdown-redcarpet/</a><a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="http://kramdown.gettalong.org/syntax.html#inline-attribute-lists">http://kramdown.gettalong.org/syntax.html#inline-attribute-lists</a><a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AlphaGo의 알고리즘과 모델]]></title>
    <link href="http://SanghyukChun.github.io/97/"/>
    <updated>2016-03-15T02:28:00+09:00</updated>
    <id>http://SanghyukChun.github.io/97</id>
		<content type="html"><![CDATA[<ul class="no-float" id="markdown-toc">
  <li><a href="#section">들어가며</a></li>
  <li><a href="#section-1">왜 바둑이 체스보다 어려운가?</a></li>
  <li><a href="#monte-carlo-tree-search">Monte-Carlo Tree search</a></li>
  <li><a href="#approaches-by-alphago">Approaches by AlphaGo</a>    <ul>
      <li><a href="#supervised-learning-of-policy-network-sl-policy-network">Supervised Learning of Policy Network (SL Policy Network)</a></li>
      <li><a href="#reinforcement-learning-of-policy-networks-rl-policy-network">Reinforcement Learning of Policy Networks (RL Policy Network)</a></li>
      <li><a href="#reinforcement-learning-of-value-networks">Reinforcement Learning of Value Networks</a></li>
      <li><a href="#searching-with-policy-and-value-networks">Searching with Policy and Value Networks</a></li>
    </ul>
  </li>
  <li><a href="#section-2">정리</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#section-3">변경 이력</a></li>
</ul>

<h3 id="section">들어가며</h3>
<p>근 일주일 동안 가장 뜨거웠던 주제를 하나 꼽으라면 누가 뭐래도 “알파고”일 것이다. 글을 쓰고 있는 2016년 3월 13일 기준으로 알파고와 이세돌 사범과의 경기에서 최종 승리를 확정지은 상태이며, 오늘 이세돌 사범이 첫 승을 거둠으로써 5국 중 3대 1의 상황이 되었다. 내일 모레 있을 경기에서 3-2가 될지 4-1이 될지가 최종 결정이 될 것이며, 어느 결과가 나오더라도 AI 분야에서는 기념비적인 사건이 될 것이다.</p>

<p>덕분에 AI (정확하게는 딥러닝)이 사람들의 이목을 집중적으로 받게되면서 불분명한 정보가 마구 흘러다니는 것 같아서 제대로 딥마인드에서 어떤 모델과 알고리즘을 사용한 것인지 직접 알아보고 정리해보기로 했다. 이 글에서는 deep learning, CNN <a href="75/#75-cnn">[7]</a>, deep Q-learning <a href="http://SanghyukChun.github.io/90">[9]</a>등의 용어들을 설명없이 사용할 예정이므로, 해당 개념들에 관심이 있다면 내가 쓴 이전 글들 <a href="75/#75-cnn">[7]</a>, <a href="http://SanghyukChun.github.io/90">[9]</a> 이나 다른 외부 자료들을 참고하면 좋을 것 같다.
이 글은 Google Deep Mind에서 2016년 Nature에 발표한 matering the Game of Go with Deep Neural Network and Tree Search <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[1]</a>를 기반으로 작성되었다.&lt;/p&gt;</p>

<h3 id="section-1">왜 바둑이 체스보다 어려운가?</h3>
<p>글을 본격적으로 시작하기 전에, 왜 바둑이 체스나 다른 게임들보다 어려운지부터 이야기해보자. 체스 (1997년 IBM Deeper blue), 체커 (1994년 CHINOOK <a href="https://www.aaai.org/ojs/index.php/aimagazine/article/viewFile/1208/1109">[6]</a>), tic tac toe (1952년) 등의 게임들은 이미 오래오래 전에 컴퓨터가 인간을 박살낸 분야이다. 게다가 최근 연구들을 통해 Atari 에뮬레이터를 기반으로 한 비디오 게임에서도 굉장히 뛰어난 성능을 내는 모델이 제안된 바 있다 <a href="http://SanghyukChun.github.io/90">[9]</a>. 그런데 바둑은 왜 아직까지 인간을 뛰어넘기가 어려웠을까? 그 이유는 다른 데에 있는 것이 아니라, 바둑에서 발생할 수 있는 경우의 수가 너무나 많기 때문이다. 체스, 바둑 등의 턴 방식의 게임들은 현 상황에서 앞으로 발생할 수 있는 모든 경우의 수에 대한 search tree를 만들고 가장 최적의 path를 찾아가는 방식으로 게임을 플레이하게 되는데, 이 tree의 size는 각 위치 별로 가능한 가능한 모든 movement의 개수 b와 게임의 길이 d의 조합인, $b^d$로 표현된다. 참고로 b는 breadth이고, d는 depth이다. 체스의 경우 b는 약 35, d는 약 80이지만, 바둑은 b가 약 250 d는 약 150이나 된다 (착수 가능한 경우의 수가 평균 250개, 평균 게임 길이가 150수). 10진수로 바꾸면 $250^{150} \simeq 10^{360}$이 되는데, 이게 얼마나 대단한 숫자이냐 하면 우주의 모든 원자의 숫자가 약 $10^{80}$으로 예상되니까, 각 경우 수마다 원소 하나 씩을 대응시키면 $10^{280}$개의 우주가 필요한 셈이다.</p>
<p>&#8230;사이즈가 너무 커서 도저히 감도 오지 않는다. 아무튼 체스는 그 search space의 크기가 20년 전의 (슈퍼) 컴퓨터로 exact tree search가 가능할 정도의 크기였고 지금은 개인 PC에서도 exact search가 가능할 정도이나, 바둑은 그와는 비교도 할 수 없는 무시무시한 크기를 보여주는 것이다. (정확히는 약 googol배는 더 크다고 한다. 맞다, 구글의 이름의 기원인 세상에서 가장 큰 단위인 그 구골이다..) 때문에 AI 쪽에서는 바둑을 정복하는 것이 최대 과제 중 하나였다. 어떻게 이 어마어마한 search space를 감당할 수 있을것인가?</p>
<p>보다 더 자세한 설명을 하기 이전에, 바둑, 체스, 체커 등의 턴제 게임을 어떻게 tree search로 해결한다는 것인지 설명을 하고 넘어가도록 하자. 모든 턴제 게임은 일종의 트리 travel로 생각할 수 있다. 즉, 맨 처음 시작에서 각자가 한 턴을 소비할 때마다 트리의 노드로 이동하고, 결국 맨 마지막에 누군가가 승리하는 위치로 도달하면 게임이 끝나는 것이다. 체스는 그것이 체크메이크가 되는 것이고, 바둑은 돌을 더 이상 둘 곳이 없을 때 집의 개수가 더 많은 쪽이 되는 것이다. 특히 바둑에서는 tree가 다음처럼 표현된다.</p>

<div class="caption">
<img class="center" src="http://SanghyukChun.github.io/images/post/97-1.png" width="600" />
<p>바둑의 Search Tree. 출처: <a href="http://spri.kr/post/14725">[3]</a></p>
</div>

<p>그럼 이제 tree를 만들었으니, 매 순간마다 맨 끝까지 tree를 진행한다음 제일 좋은 결과를 보이는 node를 고르면 된다. 끝! &#8230;이라고 하고 싶지만 계속 반복해서 언급했듯 tree size가 우주 원자보다 많기 때문에 brute force는 불가능하다. 모 IT 변호사님 말처럼 모든 경우의 수를 세는 brute force를 하기 때문에 불공평하다는 얘기는 말이 안되는 셈. 때문에 알파고에서 가장 핵심이 되는 부분 중 하나는 바로 어떻게 tree search를 하느냐이다. Exact search가 힘들기 때문에 search space를 줄이는 적절한 approximation algorithm을 사용해야하는데 그 방법을 어떤 것을 취하는지가 문제가 되는 것이다.</p>
<p>이건 사족인데, 방금 위에서 얼렁뚱땅 대충 tree로 좋은 node를 고른다고 했지만, 사실은 Minimax algorithm이라는 것을 사용해서, 나의 이득은 최대화하고, 상대방의 이득은 최소화하는 방향을 계속 반복하면서 아래로 value를 propagate하면서 계속 sub tree를 만들고&#8230; 뭐 그런 알고리즘을 써야한다. 결국 모든 node에 저런 계산을 해야하기 때문에 바둑같은 무식하게 큰 tree에서는 이 방법을 쓸 수 없는 것이 문제가 되는 것. Minimax에 대해서는 한국어로 된 좋은 자료 <a href="http://spri.kr/post/14725">[3]</a>가 있으니 참고하면 좋다.</p>

<h3 id="monte-carlo-tree-search">Monte-Carlo Tree search</h3>
<p>바둑의 search space의 크기는 착수 가능한 경우의 수를 밑으로하고 250의 평균 바둑 한 판의 길이인 150수를 지수로 하는 무식하게 큰 숫자이다. 따라서 search space를 줄이기 위해서는 (1) tree의 breadth search를 줄이는 방법 (착수 위치를 exact하게 전부 고려하는 대신, 좀 더 작은 숫자로 줄이는 방법), (2) tree의 depth를 줄이는 방법 (매 번 tree를 exact하게 끝까지 보지않는 방법) 이 두 가지가 필요하다. 현재 (AlphaGo이전의) state-of-art 바둑 system들은 이 방법을 해결하기 위하여 Monte-Carlo Tree search (MCTS)라는 방법을 사용하고 있다. MCTS의 이름에 Monte-Carlo가 들어가는 것을 보면 알 수 있듯, 이 방법론은 tree search를 exact tree traversal을 하는 대신, random하게 node를 하나 고르고 (sampling하고) 그것을 통해 확률적인 방법으로 approximate tree search를 하는 방법론이다. 당연히 계속 반복하면 asymptotic하게 optimal value function으로 converge하는 것 역시 증명되어있다.</p>
<p>MCTS를 완전 high level로만 설명하면, 다음과 그림과 같은 4개의 seqeunce를 계속 반복하는 과정이라 할 수 있다.</p>

<div class="caption">
<img class="center" src="http://SanghyukChun.github.io/images/post/97-2.png" width="600" />
<p>High level description of MCTS. 출처: <a href="http://www.cameronius.com/cv/mcts-survey-master.pdf">[4]</a></p>
</div>
<p>각 step에 대한 설명은 다음과 같다. 여기에서 핵심은 <b>Tree Policy</b>, 그리고 <b>Default Policy</b>이다.</p>

<ol>
  <li>Selection: root node에서부터 __Tree Policy (child selection policy)__를 recursive하게 적용해서 leaf node L까지 도달한 후 L을 select한다.</li>
  <li>Expansion: 만약 도달한 leaf L이 terminate state가 아니라면 (즉 L에서 게임이 끝나지 않았다면) __Tree Policy (leaf create policy)__에 의해 새로운 child node 한 개 혹은 여러 개를 더 만들어서 tree를 exapnd한다.</li>
  <li>Simulation: 새 node 에서 __Default Policy__에 따른 outcome을 계산한다.</li>
  <li>Backpropagation: Simulation 결과를 사용해 selection에서 사용하는 statistic들을 update한다.</li>
</ol>

<p>이때 Tree Policy와 Default Policy에 대한 설명은 각각 다음과 같다.</p>

<ul>
  <li>Tree Policy: 이미 존재하는 search tree에서 leaf node를 select하거나 create하는 policy
    <ul>
      <li>바둑의 경우에는 특정 시점에서 가능한 모든 수 중에서 가장 승률이 높은 수를 예측하는 policy라고 생각하면 된다.</li>
    </ul>
  </li>
  <li>Default Policy: 주어진 non-terminal state에서의 (얼마나 좋은 state인지를 측정하는) value를 estimation을 하는 policy
    <ul>
      <li>바둑의 경우에는 현재 상황에서 얼마나 승리할 수 있을지를 measure하는 policy라고 생각하면 된다.</li>
    </ul>
  </li>
</ul>

<p>Backpropagation step 자체는 둘 중 어떤 policy도 사용하지 않지만, 대신 backpropagation을 통해 각 policy들의 parameter들이 update된다. 이 4개의 step이 한 iteration으로, MCTS는 시간이 허락하는 한도 내에서 이 과정을 계속 반복하고, 그 중에서 가장 좋은 결과를 자신의 다음 action으로 삼는다. 다음은 가장 &#8216;좋은&#8217; node를 고르는 criteria의 4가지 예시이다.</p>

<ol>
  <li>Max child: 가장 높은 reward 값을 가지고 있는 node를 고른다.</li>
  <li>Robust child: root node에서부터 가장 많이 visit된 node를 고른다.</li>
  <li>Max-Robust child: 1, 2를 동시에 만족하는 node를 고르며, 그런 node가 없다면 계속 반복해서 그런 node를 찾아낸다.</li>
  <li>Secure node: 가장 lower confidence bound를 maximize하는 node를 고른다.</li>
</ol>

<p>딥마인드에서 쓴 논문 <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[1]</a>이나 포스트 <a href="http://googleresearch.blogspot.kr/2016/01/alphago-mastering-ancient-game-of-go.html">[2]</a>를 보면 single machine과 multi machine에서의 성능 차이에 대해 언급하는데, 아마도 iteration을 더 많이 시도해볼 수 있기 때문에 결과가 더 좋은 것이 아닐까싶다.</p>
<p>MCTS의 일반적인 알고리즘을 정리하면 다음과 같이 적을 수 있다. (1) Tree Policy, (2) Default Policy, (3) Best Child Selection 이 세가지를 어떻게 정하느냐에 따라서 알고리즘의 종류가 바뀐다고 보면 된다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-3.png" width="400" /></p>
<p>알고리즘을 보면 알 수 있듯, 굉장히 reinforcement learning스러운 방식을 취하기 때문에 (1) Aheuristic, 즉 뭔가 이유가 있고 reasonable한 decision making을 할 수 있으며 (2) Asymmetric, 아래 그림처럼 tree를 symmetric하지 않게, 더 relevant한 부분만 집중적으로 서치할 수 있다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-4.png" width="500" /></p>
<p>Survey paper <a href="http://www.cameronius.com/cv/mcts-survey-master.pdf">[4]</a>를 보면 MCTS family 중에서 몇 가지 유명한 알고리즘들에 대해 설명하고 있는데 bandit 기반의 알고리즘들이 많이 있다. UCB를 기반으로 한 UCT (Upper Confidence Bounds for Trees), 그것을 좀 발전시킨 BAST (Bandit Algorithm for Smooth Trees).. 그것 이외에도 정말 수 많은 family들에 대해 설명하고 있으니 더 관심이 있으면 해당 논문을 읽어보면 좋을 것 같고, survey paper가 너무 길다면, MCTS에 대해 잘 정리되어있는 사이트 <a href="http://mcts.ai/about/index.html">[5]</a>가 있으니 이 사이트를 참고하면 좋겠다.</p>

<h3 id="approaches-by-alphago">Approaches by AlphaGo</h3>
<p>앞서 설명한 MCTS가 비록 full search를 하지 않아도 된다지만, 결국 바둑에 적용하기 위해서는 breadth와 depth를 줄이는 과정이 필요하다. AlphaGo에서 이 둘을 줄이기 위하여 사용한 것이 바로 deep learning technique으로, 먼저 착수하는 지점을 평가하기 위한 value network, 그리고 샘플링을 하는 distribution을 만들기 위한 policy network 두 가지 network를 사용하게 된다.</p>
<p>결국 AlphaGo가 한 것을 한 마디로 요약하자면 Monte-Carlo Tree search이며, tree의 search space를 줄이기 위하여 value network와 policy network 두 가지 (사실은 세 가지) network를 한 번에 learning할 수 있는 architecture를 만들고 이를 사용해 MCTS의 성능을 끌어올린 것이다. 따라서 이 방법은 비단 바둑에서만 사용할 수 있는 방법이 아니라, MCTS를 사용할 수 있는 거의 모든 방법론에 적용하는 것이 가능하다. 지금 AlphaGo가 input으로 흰 돌과 검은 돌들이 놓여져 있는 바둑판 그림을 사용하고 있기에 바둑을 학습하는 것이고, 그 이외에 search space가 너무 넓어서 exact tree search가 불가능한 model에서 전부 AlphaGo의 방법론을 사용할 수 있는 것이다.</p>
<p>다시 본론으로 돌아와서, 더 자세한 설명을 하기 이전에 먼저 General MCTS에서 사용하는 네 가지 step (selection, expansion, simulation, backpropagation)을 AlphaGo는 어떻게 적용했는지 살펴보자.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-5.png" width="600" /></p>

<ol>
  <li>Selection: 현재 상태에서 Q + u가 가장 큰 지점을 고른다.
    <ul>
      <li>Q: MCTS의 action-value 값, 클 수록 승리 확률이 높아짐 (Q function에 대해서는 이전에 쓴 reinforcement 글 <a href="http://SanghyukChun.github.io/76">[8]</a> 참고)</li>
      <li>u(P): __Policy network (SL)__과 node 방문 횟수 등에 의해 결정되는 값</li>
    </ul>
  </li>
  <li>Expansion: 방문 횟수가 40회가 넘는 경우 child를 하나 expand한다.</li>
  <li>Simulation: __Value network__와 __Fast rollout__이라는 두 가지 방법을 사용해 reward를 계산한다.
    <ul>
      <li>Value network는 __Policy network (RL)__을 사용해서 learning한다.</li>
    </ul>
  </li>
  <li>Backpropagation: 시작 지점부터 마지막 leaf node까지 모든 edge의 parameter를 갱신한다.</li>
  <li>1-4를 (시간이 허락하는 한도 내에서) 계속 반복하다가, Best Child Selection으로는 robust child, 즉 가장 많이 방문한 node를 선택한다.</li>
</ol>

<p>AlphaGo는 위에서 언급한 policy network를 supervised learning (SL), reinforcement learning (RL) 두 가지로 나눠서 학습한다. SL network는 그 동안 실제 프로기사가 둔 기보를 바탕으로 특정 기보에 대한 다음 수를 classification하는 방식으로 learning하고, RL network는 SL network로 initialize한 후, reinforcement learning 방식 (AlphaGo의 자가대국이라고 부르는 방식)으로 주어진 기보에 대한 다음 수의 distribution을 학습한다.</p>
<p>AlphaGo는 앞에서 설명한 Rollout Policy, SL network, RL network 그리고 마지막 value network를 한 번에 pipeline 방식으로 learning하는 architecture를 디자인했다.</p>

<h4 id="supervised-learning-of-policy-network-sl-policy-network">Supervised Learning of Policy Network (SL Policy Network)</h4>
<p>먼저 AlphaGo의 Supervised Learning (SL) Policy Network $p_\sigma (a | s)$에 대해 알아보자. 이 네트워크는 단순한 CNN으로, input은 시간 t일 때의 기보(s)이고, output은 시간 t+1 일 때의 기보(a)가 된다. 따라서 이 네트워크는 classification network가 된다. 다만 문제라면 output layer의 dimension이 너무 거대하다는 것. 개인적으로 이런 도대체 네트워크를 어떻게 learning시킨건지 상상조차 되지 않는다. 참고로 이 네트워크는 단순 classification task만 하기 때문에 sequencial할 필요는 없다. 때문에 그냥 모든 (s,a) pair에서 랜덤하게 데이터를 샘플해서 SGD로 learning하게 된다.</p>
<p>네트워크는 총 13 layer CNN을 사용했으며 KGS라는 곳에서 3천만건의 기보 데이터를 가져와서 학습했다고 한다. 네트워크 구조를 어떻게 만들었는지 궁금해서 살펴보니, inner product layer는 하나도 없이 처음부터 끝까지 convolution layer만 학습한 모양이다.</p>
<p>논문에 따르면, AlphaGo는 이 부분에서 기존 state-of-art였던 44.4%보다 훨씬 좋은 classification accuracy인 57%까지 성능개선을 보였다고 한다. 또한 이 accuarcy가 좋으면 좋을수록 AlphaGo의 최종 winning rate가 상승한다는 사실까지 다음 그림과 같이 실험적으로 보이고 있다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-6.png" width="400" /></p>

<h4 id="reinforcement-learning-of-policy-networks-rl-policy-network">Reinforcement Learning of Policy Networks (RL Policy Network)</h4>
<p>RL network는 SL network와 동일한 구조를 가지고 있으며, 초기 값 $\rho$ 역시 SL network의 parameter value $\sigma$로 초기화된다. RL network는 현재 RL network policy $p_\rho$와 이전 iteration에서 사용했던 policy network 중에서 랜덤하게 하나를 뽑은 다음 이 둘끼리 서로 대국을 하게 한 후, 둘 중에서 현재 네트워크가 최종적으로 이기면 reward를 +1, 지면 -1을 주도록 디자인되어있다. 그러나 당연히 그 reward는 대국이 끝난 시점의 T에서의 reward이지 현재 시점 t에서의 reward는 0이기 때문에, 대신 네트워크의 outcome을 $z_t = \pm r(s_T)$으로 정의한다. 즉, 이 네트워크의 outcome은 현재 player의 time t에서의 terminated reward가 된다. 이 네트워크 역시 Stochasic gradient method를 사용해 expected reward를 maximize하는 방식으로 학습이 된다. 여기에서 과거에 학습된 네트워크를 사용하는 이유는, 좀 더 generalize된 모델을 만들고, overfitting을 피하고 싶기 때문이라고 한다 (언론에서 말하는 &#8216;자기 자신이랑 계속 반복해서 대국을 진행하는 방식으로 더 똑똑해진다&#8217; 라는 표현은 여기에서 나오는 자가 대국을 의미하는 것 같다).</p>
<p>논문에 따르면 SL policy network와 RL policy network가 경쟁할 경우, 거의 80% 이상의 게임을 RL network가 승리했다고 한다. 또한 다른 state-of-art 프로그램들과 붙었을 때도 훨씬 좋은 성능을 발휘했다고 한다.</p>

<h4 id="reinforcement-learning-of-value-networks">Reinforcement Learning of Value Networks</h4>
<p>이제 AlphaGo의 deep learning architecture 중에서 마지막 단계인 value network $v_\theta (s)$만 남았다. Value network는 evaluation 단계에서 사용하는 네트워크로, position (현재 기보) s와 policy p가 주어졌을 때, value function $v^p(s)$를 predict하는 네트워크이다. 즉, 다음과 같은 식으로 표현할 수 있다.</p>
<p>$$v^p(s) = \mathbb E [z_t | s_t = s, a_{t\ldots T} \sim p].$$</p>
<p>문제는 그 누구도 바둑에서 최적의 수를 모르기 때문에 (다시 강조하지만 search space가 우주의 원자 개수보다 많다) optimal value function $v^*(s)$를 학습할 방법이 없다는 것이다. 그 대신, AlphaGo는 현재 시스템에서 가장 우수한 policy인 RL policy network $p_\rho$를 사용해 optimal value function을 approximation한다. Value network는 앞에서 설명한 policy network와 비슷한 구조를 띄고 있지만, 마지막 output layer으로 모든 기보가 아닌, single probability distribution을 사용한다. 따라서 이제 문제는 classification이 아니라 regression이 된다. Value network는 현재 가장 state-outcome pair인 (s,z)에 의해서 학습이 된다 (여기에서 z는 RL network에서 나왔던 최종 reward의 값으로 1 또는 -1이다).</p>
<p>따라서 Value network는 s에 대해 z가 나오도록 하는 regression network를 학습하게 되며, error는 $z - v_\theta(s)$가 된다. 그런데 문제는, state s는 한 개의 기보인데, reward target은 전체 game에 대해 정의되므로, succesive position들끼리 서로 강하게 correlation이 생겨서 결국 overfitting이 발생한다는 것이다. 이 문제를 해결하기 위해 AlphaGo는 3천만개의 데이터를 RL policy network들끼리의 자가대국을 통해 만들어낸 다음 그 결과를 다시 또 value network를 learning하는 데에 사용한다. 그 결과 원래 training error 0.19, test error 0.37로 overfitted되었던 네트워크가, training error 0.226, test error 0.234로 훨씬 더 generalized된 네트워크로 학습되었다는 것을 알 수 있다.</p>
<p>마지막으로, 아래 그림은 랜덤 policy, fast rollout policy, value network, SL network 그리고 RL network를 사용했을 때 각각의 value network의 expected loss가 plot되어있다. Loss는 실제 프로기사가 둔 수와, 각 policy로 둔 수와의 mean square loss이다. 결국, RL policy를 쓰는 것이 그렇지 않은 것보다 훨씬 우수한 결과를 낸다는 것을 알 수 있다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-7.png" width="400" /></p>

<p>이제 high level로 앞에서 살펴본 세 네트워크를 살펴보자.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-8.png" width="600" /></p>
<p>먼저 왼쪽 그림은 어떻게 AlphaGo에서 세 네트워크를 pipeline 형태로 묶었는지를 보여준다. 사람이 실제로 둔 기보를 바탕으로 rollout policy, SL policy를 learning하고, SL policy를 initialization 값으로 사용해 RL policy를 learning한다. 그 후 RL policy를 사용해 value network를 learning하는 것이다.</p>
<p>앞에서 깜빡하고 언급하지 않았는데, Fast Rollout Policy는 전체 바둑 상태가 아닌 local한 3 by 3 판에서 다음 수를 빠르게 예측해서 terminate state까지 게임을 play한 후 simulation하는 policy로, policy network를 사용한 방법보다 성능은 떨어질지 몰라도 약 1500배 정도 빠르다고 언급되고 있다. 그냥 빠른 naive approach라고 생각하면 될 것 같다.</p>
<p>오른쪽 그림에서는 policy network와 value network의 차이를 보여주고 있다. Policy network들은 전부 input, output이 기보로 나타나고 (input이 지금 기보, output이 다음 기보) value network는 board 전체에 대한 probability를 학습한다는 점이 다르다. 즉, policy network는 주어진 기보에서 가장 확률이 높은 action을 고르는 방식으로 MCTS의 selection을 하는 역할을 하고, value network는 simulation결과를 통해 실제로 둘 수 있는 점들 중에서 가장 이길 확률이 높은 (reward가 승리이므로) 곳을 찾아내는 역할을 하게 되는 것이다.</p>
<p>그리고 <a href="http://spri.kr/post/14725">[3]</a>에서 구체적인 CNN 구조를 설명한 그림이 있어서 인용하도록 하겠다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/97-9.png" width="600" /></p>

<h4 id="searching-with-policy-and-value-networks">Searching with Policy and Value Networks</h4>
<p>이제 policy와 value network를 설계하였으니 실제로 이 네트워크들을 어떻게 MCTS에서 사용하는지 살펴보자. MCTS의 각각의 edge (s,a)는 action value Q(s,a), visit count N(s,a), prior probability P(s,a)를 저장한다. Tree는 simulation을 사용해서 traversal을 root node에서부터 진행하게 된다. Simulation의 각 step t마다, action $a_t$는 state $s_t$에 대해 다음과 같이 정의된다.</p>
<p>$$a_t = \arg\max_a (Q(s_t,a) + u(s_t, a)), \mbox{ where } u(s,a) \propto \frac{P(s,a)}{1+N(s,a)}. $$</p>
<p>Traversal을 지속하다 leaf node L에 도달하게 되면, expand여부를 결정하게 된다 (방문횟수로 결정하는 듯 하다). 그 후 leaf node에서의 position $s_L$을 사용해서 SL policy $p_\sigma$를 prior P에 저장한다. 즉, $P(s,a) = p_\sigma(s,a)$가 된다. 이때 leaf node는 두 가지 방법으로 evaluate된다. 먼저 value network $v_\theta(s_L)$, 그리고 fast rollout policy $p_\pi$를 사용해 terminal step T까지 도달했을 때 random rollout play로 얻어진 outcome $z_L$. 이 둘은 parameter $\lambda$를 사용해 다음과 같이 combine된다.</p>
<p>$$ V(s_L) = (1-\lambda)v_\theta (s_L) + \lambda z_L. $$</p>
<p>앞에서 진행한 simulation이 끝나고나면, 이제 각 edge들이 가지고 있는 parameter들을 update할 차례다. 앞에서 언급했듯, AlphaGo의 MCTS는 각각의 edge (s,a)에 action value Q(s,a), visit count N(s,a), prior probability P(s,a)를 저장한다. 여기에서 P(s,a)는 SL network로 update가 되고, 남은건 Q와 N이다. 이 값들은 다음과 같은 과정으로 업데이트 된다.</p>
<p>$$N(s,a) = \sum_i \mathbf{1}(s,a,i)$$</p>
<p>$$Q(s,a) = \sum_i \frac{1}{N(s,a)} \mathbf{1}(s,a,i) V(s_L^i).$$</p>
<p>$s_L^i$는 i번째 simulation에서 leaf node를 의미하며, $\mathbf{1}(s,a,i)$는 i번째 simulation에서 edge (s,a)가 관측되었는지에 대한 indicator function이다. 이런 방식을 통해 서치가 다 끝나고나면 AlphaGo는 root에서 부터 가장많이 선택된 node를 선택하는 방식으로 한 수를 둔다.</p>
<p>재미있는 점은, MCTS의 policy function으로 SL policy를 쓰는 것이 RL policy보다 낫다고 논문에 report된 점이다. 이유는 (SL policy를 learning할 때 사용한) 사람이 두는 수는 뒤를 생각한 좀 더 global한 수를 두는 반면, RL policy는 그 순간의 가장 최고의 move를 하기 때문에, SL policy가 더 낫다는 것이다. 반면, value network는 SL network가 아닌 RL network를 사용하는 편이 훨씬 성능이 좋다고 한다.</p>

<h3 id="section-2">정리</h3>
<p>이 글에서는 자세한 네트워크의 구조나 코드 구현보다는 실제로 이 알고리즘이 어떻게 동작하는지, 그리고 모델은 어떻게 구성했는지에 대해 집중적으로 다뤘다. AlphaGo는 MCTS를 deep learning pipeline을 통해 훨씬 성능을 개선한 work이라 할 수 있으며, network는 SL, RL 두개의 policy network 그리고 value network 총 세 가지를 learning하게 된다. Policy network는 MCTS의 selection에서 쓰이게 되며, value network는 MCTS의 evaluation에서 쓰이게 된다.</p>
<p>각종 매체나 언론에서는 알파고가 인간이 1000년 동안 두어야 둘 수 있는 대국을 진행했고, 최적의 수를 항상 찾아내기 때문에 이세돌 사범에게 불리하다는 식으로 보도를 하고 있지만, 사실은 그 이전에도 AlphaGo가 사용하던 데이터와 동일한 데이터로 훨씬 못한 결과들을 내왔었다. 결국 알파고가 뛰어난 점은 기존 방법들보다 훨씬 smart한 architecture를 디자인하고, 그 architecture의 power를 최대한으로 끌어올리기 위해서 parallel computing 등의 각종 기법들을 사용해서 시스템을 엄청 정교하게 만들었다는 점이라고 할 수 있다. 그러나 아무리 대단한 시스템 엔지니어라고 하더라도, 근본이 되는 모델의 성능이 나쁘다면 그 시스템을 인간 수준으로 끌어올리지는 못했을 것이다. 결국 구글은, 그리고 딥마인드는, 정말 &#8216;인간답게&#8217; sequencial decision process를 학습하는 멋진 시스템을 디자인했다고 볼 수 있을 것 같다.</p>

<h3 id="references">References</h3>
<ol class="reference">
	<li><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[Nature] Silver, David, et al. &#8220;Mastering the game of Go with deep neural networks and tree search.&#8221;, 2016.</a>, <a href="http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf">[pdf링크]</a></li>
	<li><a href="http://googleresearch.blogspot.kr/2016/01/alphago-mastering-ancient-game-of-go.html">[Google Research Blog] &#8220;AlphaGo: Mastering the ancient game of Go with Machine Learning.&#8221;, 2016.</a></li>
	<li><a href="http://spri.kr/post/14725">[SPRI] 소프트웨어 정책연구소.&#8221;AlphaGo의 인공지능 알고리즘 분석.&#8221;, 2016.</a></li>
	<li><a href="http://www.cameronius.com/cv/mcts-survey-master.pdf">[Computational Intelligence and AI in Games] Browne, Cameron B., et al. &#8220;A survey of monte carlo tree search methods.&#8221;, 2012.</a></li>
	<li><a href="http://mcts.ai/about/index.html">MCTS.ai</a></li>
	<li><a href="https://www.aaai.org/ojs/index.php/aimagazine/article/viewFile/1208/1109">[AAAI] Schaeffer, Jonathan, et al. &#8220;CHINOOK the world man-machine checkers champion.&#8221;, 1996.</a></li>
	<li><a href="http://SanghyukChun.github.io/75/#75-cnn">Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN</a></li>
	<li><a href="http://SanghyukChun.github.io/76">Machine Learning 스터디 (20) Reinforcement Learning</a></li>
	<li><a href="http://SanghyukChun.github.io/90">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a></li>
</ol>

<h3 id="section-3">변경 이력</h3>
<ul>
  <li>2016년 3월 15일: 글 등록</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (20-1) Multi-armed Bandit]]></title>
    <link href="http://SanghyukChun.github.io/96/"/>
    <updated>2016-03-13T19:28:00+09:00</updated>
    <id>http://SanghyukChun.github.io/96</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p>이 글에서는 reinforcement learning의 한 갈래 중 하나인 Multi-armed Bandit에 대해 다룰 것이다. Multi-armed Bandit이 어떤 문제인지에 대해 간략히 설명한 다음, 좀 더 formal하게 문제를 정의하고, 이 문제를 푸는 여러 알고리즘들에 대해 다룰 것이다. 연구가 워낙 오래 진행된 분야라서 모델이나 알고리즘의 종류가 엄청나게 많지만, variation 중에서 몇 가지 간단한 것들 위주로 설명해보도록 하겠다.</p>

<h3>Motivation: Exploration and Exploitation Trade-off</h3>
<p>외팔이 강도가 (one-armed bandit이) 우연한 기회로 눈 앞에 여러 개의 슬롯머신을 공짜로 H시간 동안 플레이 할 수 있는 기회를 얻었다고 생각해보자. 이때 강도는 한 번에 한 개의 슬롯머신의 arm만 당길 수 있으며 (즉 총 H번 시도할 수 있다) 각각의 슬롯머신에서 얻을 수 있는 reward는 다르다고 가정한다. 또한 reward는 어떤 probabilistic distribution에 의해 draw되는 random variable이라고 했을 때, 이때 강도가 가장 수익을 최대화하기 위해서는 arm을 어떤 순서대로, 어떤 policy대로 당겨야할까?</p>
<div class="caption">
<img class="center" src="http://SanghyukChun.github.io/images/post/96-4.jpg" width="300" />
<p>그림 출처: <a href="http://research.microsoft.com/en-us/projects/bandits/">MS research</a></p>
</div>
<p>이 문제에서 가장 큰 난점은, 슬롯머신마다 보상이 다르고, 한 번에 한 슬롯머신의 reward만 관측할 수 있다는 점이다. 예를 들어 한 슬롯머신을 골라서 계속 그 슬롯머신만 당길 수도 있겠지만, 이 경우 다른 슬롯머신에서 더 좋은 reward를 얻었을 수도 있기 때문에 가장 최적의 전략은 아닐 것이다. 혹은 모든 슬롯머신을 동일한 횟수만큼 반복할 수도 있을 것이다. 그러나 슬롯머신 중에서 가장 좋은 reward를 보이는 머신은 오직 하나 뿐 일 것이므로, 이런 전략은 마찬가지로 최종 reward를 최적화하는 방법은 아닐 것이다. 혹은 일부 시간만 슬롯머신을 랜덤하게 당겨보고, 그 시간 동안 제일 좋았던 슬롯머신만 계속 당겨보는 수도 있을 것이다. 이때 기존 경험 혹은 관측값을 토대로 가장 좋은 arm을 선택하는 것을 exploitation이라 하며 더 많은 정보를 위하여 새로운 arm을 선택하는 것을 exploration이라고 한다. 결국 시간이 제한되어있기 때문에 이 둘 사이에는 trade-off관계가 성립하게 된다. 만약 exploration를 너무 하지 않게 될 경우, 잘못된 정보를 토대로 exploitation을 하게 되기 때문에 최종 결과가 좋을 거라는 보장을 하기가 힘들 것이다. 그렇다고 해서 너무 exploration을 많이 하게 되면, 충분히 정보를 가지고 있음에도 불구하고 더 정보를 얻기 위해 쓸데 없는 비용이 발생할 것이다. 따라서 이런 측면에서 exploration과 exploitation은 서로 trade-off 관계가 있다고 할 수 있고, 이런 상황에서 우리가 이 둘을 어떻게 조절하느냐가 Multi-armed Bandit problem의 핵심이 되는 것이다.</p>
<p>이런 유형의 문제의 가장 대표적인 예시는 여러 개의 새로운 치료법 중에서 실제로 환자들에게 trial을 해보고 가장 좋은 치료법을 찾는 clinical trial이라고 불리는 문제이다. 예를 들어 우리가 에이즈에 효과가 있어보이는 약물이 총 K개가 있다고 했을 때, 환자들에게 서로 다른 약물을 (혹은 치료법을) 시도해보면서 가장 효과가 좋은 약물이 어떤 것인지 찾아내는 문제이다. 이 경우 당연히 환자들의 건강상 문제라거나 고통 등의 문제를 최소화하는 방향으로 치료 순서를 정해야할 것이다. 이 문제를 푸는 가장 단순한 방법은 K개의 약물을 각각 n번 시도 해보고 각각의 expectation을 고르는 방법이 있다. 그러나 K와 n에 따라 너무 많은 시간이 필요할 뿐 아니라, 이 중에서 환자에게 치명적인 약물이 있으면 risk minimization이라는 측면에서 문제가 된다. 이런 문제점을 해결하기 위해 Multi-armed bandit을 사용하게 되며, Mutli-armed bandit을 사용하게 되면 이런 형태의 문제들을 굉장히 효율적으로, 그리고 practical하게 잘 동작하는 방식으로 풀 수 있다.</p>
<p>또 다른 예시로는 웹 사이트의 A/B 테스트를 들 수 있다. 만약 K개의 시안 중에서 가장 사람들이 좋아할만한 시안이 무엇인지 알고 싶어서 사람들에게 무작위로 K개의 시안을 보여준다고 생각해보자. 역시 한 사람에 한 번에 한 페이지만 보여줄 수 있기 때문에 이 문제도 위의 문제와 비슷하게 다룰 수 있고, 가장 최적화하고 싶은 값은 click rate라거나 광고 수익률 등이 될 것이다. 실제로 구글 analytics에서도 multi-armed bandit 실험을 제공하고 있다 (<a href="https://support.google.com/analytics/answer/2846882">[2]</a>). 그 밖에 네트워크 상에서 delay를 최소화하는 route를 구하고 싶을 때, MAB를 활용해 adaptive routing을 하거나, 여러 개의 schedule queue가 있을 때 MAB를 사용해 task를 효과적으로 scheduling하는 방법도 존재하는 등, 수 많은 application들이 존재한다.</p>

<h3>Multi-armed Bandit Problem</h3>
<p>그러면 이제 Multi-armed Bandit 문제를 좀 더 엄밀하게 정의해보자. Multi-armed bandit (혹은 단순히 bandit이나 MAB) 문제는 각기 다른 reward를 가지고 있는 여러 개의 슬롯머신에서 (Multi-armed) 한 번에 한 슬롯머신에서만 돈을 빼갈 수 있는 도둑(one-armed bandit)의 H 시간 후의 최종 보상을 maximize하는 문제이다. Bandit 문제에서 player는 매 시간 t마다 K개의 arm 중에 하나를 선택, 혹은 play할 수 있고, 그에 상응하는 reward distribution에서 draw된 보상 x를 받게 된다. Bandit에서 매 시간마다 arm을 고르는 방법을 strategy 혹은 policy라고 부르며, bandit 문제는 시간 H 후의 최종 reward를 maximize하는 (혹은 regret을 minimize하는) policy를 찾는 문제가 된다.</p>
<p>Bandit problem이 기존 general reinforcement learning과 가장 크게 다른 점이라면, reinforcement learning은 매 순간 reward를 전부 정확하게 알고 나서 행동하지만, bandit problem에서는 오직 내가 지금 선택한 arm에 대한 보상(payoff)만 알 수 있고, 나머지 arm들의 payoff에 대해서는 알 수 없다는 점이다. 이런 &#8216;partial information&#8217; 특성이 bandit problem의 가장 독특한 특징으로, 다른 arm들이 t 시간에 얼마만큼의 payoff를 주는지 알 수 없기 때문에 문제가 조금 더 까다로워지는 것이다.</p>
<p>Bandit problem에는 정말 많은 variants가 존재한다. 이 글에서는 가장 기본적인 (finite-aremd) stochastic bandit problem에 대해서만 다룰 것이다. Stochastic bandit problem에서 &#8216;stochastic&#8217;이라는 의미는 각각의 arm이 stochastic하게 특정 reward distribution에 의해 (모든 arm과 과거 play들에 대해 i.i.d.하게) draw된다고 가정한다. 또한 arm의 개수 K와 arm에서 나오는 payoff function x는 finite하고, stationary하다고 (즉, time-invariant하다고) 가정하게 된다. 마지막으로 우리가 arm을 play할 수 있는 시간 H 역시 finite하고 알려져 있다고 가정한다. 이런 문제를 finite-armed, stochastic multi-armed bandit problem이라 부른다. 참고로 보통 이론적인 분석을 할 때에는 각 arm의 reward distribution은 Bernoulli distribution을 많이 고른다. 따라서 많은 문제 세팅에서 각 시간마다 arm의 reward는 0 또는 1로 설정하게 된다.</p>
<p>실제로는 위의 조건이 상당히 strong하기 때문에, 여러 조건들이 relax될 수가 있다. 예를 들어 i.i.d. condition이라거나, finite, stationary arm condition이라거나 등의 조건들이 relax되는 variant도 존재한다. 우리가 아래에서 다룰 문제는 finite-armed, stochastic multi-armed bandit이 될 것이며, 당장은 contextual bandit이나 adversarial bandit 등의 variant들은 고려하지 않도록 하겠다. 실제로 bandit 문제는 앞서 정의한 statistical한 assumption에 의해서도 variant가 생길 수 있고, stochastic한 성질을 사용하고 하지 않느냐에 따라 또 달라지고, arm의 개수나 한 번에 관측할 수 있는 arm이 여러 개 있다거나, regret function을 어떻게 정의하느냐에 따라 엄청나게 많은 variant가 존재한다.</p>
<p>이제 bandit problem들의 variant에 대해서는 그만 이야기해보고 마지막으로 regret function에 대해 살펴보자. Regret이란, 개념적으로는 가장 optimal한 policy대로 arm을 play했을 때 얻어지는 reward에서 내 policy대로 play했을 때 얻어지는 reward의 차이이다. 개념적으로는 이렇지만, 실제로는 Regret function을 정의하는 방법에는 엄청나게 많은 종류가 있다. 물론 이 글에서는 그 모든 variant를 다루지 않고 대신 다음과 같이 생긴 가장 간단한 regret을 사용하도록 하겠다. 이때, $S_t$는 내 strategy로 time t 때 고른 arm의 index이다.</p>
<p>$$R = \left(\max_{i=1,\ldots,K} \mathbb E \sum_{t=1}^H x_{i,t}\right) - \mathbb E \sum_{t=1}^H x_{S_t,t}$$</p>
<p>혹은 다음과 같이 time t에서의 optimal policy로 얻은 reward $\mu_t^*$와 time t에서의 user의 policy로 얻은 reward $\mu_t$ 표현하기도 한다.</p>
<p>$$R = \mathbb E \left[ \sum_{t=1}^H&#92;left(\mu_t^* - \mu_t\right) \right]$$</p>
<p>다시 말해서 reward function은 처음부터 끝까지 가장 optimal한 policy를 취했을 때의 reward expectation에서 내 policy를 취했을 때의 reward expectation을 뺀 값이다. 앞에서 언급한 것처럼 reward는 (보통 Bernoulli distribution에서 draw되는) random variable이기 때문에 조금 더 정확한 분석을 위해서 expectation을 취하게 되는 것이다. 그러나 이 값은 모든 reward를 알고있는 절대자 (oracle)이 있어야 정확한 값을 구할 수 있기 때문에 실제로 많은 실제 문제에서 regret이 얼마나 되는지 계산할 수는 없다. 대신 이론적인 분석을 할 때에, 미리 각 arm들이 특정 distribution을 따른다고 가정하고 특정 distribution을 가지는 arm들에서 bandit algorithm이 얼마나 regret을 minimize할 수 있는지를 분석하는 데에 쓰인다고 생각하면 된다.</p>
<p>더 많은 bandit variant들이나 regret function의 종류에 대해 궁금하다면 reference로 참조한 survey paper <a href="http://arxiv.org/abs/1510.00757">[1]</a>를 읽어보길 권한다.</p>

<h3>Algorithm 0: Gittins index</h3>
<p>앞서 설명한 bandit problem을 풀기 위한 알고리즘으로 가장 먼저 설명할 알고리즘은 <a href="https://en.wikipedia.org/wiki/Gittins_index">Gittins index</a>이다. 이 알고리즘은 이론적으로 잘 증명되어있는 Bayes-optimal policy이지만, 실제로는 computation이 너무 많이 필요하기 때문에 practical하게 사용되는 대신 다른 알고리즘들이 많이 사용된다. 따라서 이 문단에서는 정말 짤막하게 언급만 하고 넘어가려고 한다. 매 time t마다 Gittins index 알고리즘은 다음과 같은 방식으로 arm을 고른다.</p>

<ol>
  <li>각 arm 별로 Gittins index를 계산한다.</li>
  <li>가장 index가 높은 arm을 고른다.</li>
</ol>

<p>Gittins index는 bandit problem을 풀기위한 초기 연구 중 하나로, 70 ~ 80년대에 연구된 결과이다. 이 방법론은 bandit 문제를 MDP로 취급하고 문제를 풀게 된다. 그냥 MDP만 사용하게 되면 문제를 풀기 위한 computation이 가능한 action의 모든 경우의 수와 bandit의 arm 개수의 exponential하게 증가하게 되기 때문에 Gittins는 이 문제를 해결하기 위하여 bandit problem이 n개의 1-D problem으로 reduce될 수 있음을 증명하고 각각의 1-D 문제의 계산 값을 Gittins index로 정의한 후, arm 중에서 가장 Gittins index가 높은 arm을 고르는 방법을 제안한다. 이 부분에서 Gittins index는 각 arm을 statistical distribution으로 생각하고 문제를 푸는 대신, 완전한 MDP문제로 해결하게 된다. 실제로는 별로 practical하지 않기에 쓰이지 않으며, UCB라고 하는 조금 더 practical한 approximation algorithm이 있기 때문에 보통 UCB를 사용하게 된다.</p>

<h3>Algorithm 1: $\varepsilon$-greedy</h3>
<p>Bandit problem을 푸는 가장 popular하면서도 간단한 알고리즘 중 하나로 $\varepsilon$-greedy라는 알고리즘이 있다. 이 알고리즘은 $1-\varepsilon$의 확률로 지금까지 관측한 arm 중에 가장 좋은 arm을 고르고 (exploitation), $\varepsilon$의 확률로 나머지 arm 중에서 random한 arm을 골라서 play하는 (explore) 알고리즘이다. 알고리즘은 다음과 같다.</p>

<ol>
	<li><p>$1-varepsilon$의 확률로 지금까지 empirical reward가 가장 좋은 arm을 고른다.</p></li>
	<li><p>$varepsilon$의 확률로 uniformly random하게 arm을 고른다.</p></li>
</ol>

<p>이 알고리즘은 뒤에서 설명할 다른 알고리즘들보다 이론적으로, 또 실험적으로 우수하지는 못하지만, 매우 직관적이다. 이 알고리즘의 parameter $\varepsilon$ 자체가 맨 처음 motivation으로 말했던 exploration and exploitation trade-off를 조절하는 term이 되기 때문이다. $\varepsilon$의 값이 크면 그만큼 exploration을 많이 하게 되고, 반대의 경우도 마찬가지로 생각할 수 있다. 이 알고리즘의 치명적인 단점을 몇 꼽자면, 먼저 시간이 많이 지나서 optimal한 arm이 무엇인지 알게 되었더라도 계속해서 $\varepsilon$만큼의 exploration을 해야하므로, optimal한 값과 멀어지는 결과를 낳게 된다는 점이다. 또 하나는 $\varepsilon$의 확률로 sub-optimal arm들을 뽑고, 그 마저도 uniformly random하게 뽑기 때문에, 전체 arm 중에서 관측하지 못하는, 혹은 관측을 많이 못해서 정보를 많이 얻게 되지 못하는 arm이 생기게 될 가능성이 크다는 것이다.</p>
<p>여러 문제들을 해결하기 위해서 $\varepsilon$을 constant로 사용하는 대신, adaptive하게 update하거나, 혹은 일정 비율로 감소시키는 방법론도 존재하며 $\varepsilon$-first 등의 variant algorithm 등도 역시 존재하지만 이 글에서는 다루지 않도록 하겠다.</p>

<h3>Algorithm 2: UCB</h3>
<p>앞서 설명한 $\varepsilon$-greedy는 항상 empirical mean이 좋은 arm만 고르고, 나머지를 $\varepsilon$의 확률로 고르지만, 실제로는 매 시간마다 arm $i$에서 얻는 보상은 constant가 아닌 특정 distribution에서 draw되는 random variable이기 때문에, 지금 empirical mean이 크다고 해서 정말로 그 arm이 늘 best일거라고 확신할 수 없다. 특히 관측 횟수가 적을 경우에는 empirical result와 실제 결과 간의 큰 차이가 발생할 확률이 높기 때문에 $\varepsilon$-greedy에는 심각한 결점이 있는 셈이다. 반대로 관측 횟수가 충분히 많다면 explore를 굳이 할 필요가 없음에도 $\varepsilon$ 만큼의 explore를 반드시 해야한다는 점 역시 문제가 된다.</p>
<p>UCB 알고리즘은 empirical mean이 가장 좋은 arm을 play하는 대신, 시간 t마다 과거의 관측결과(empirical mean과 관측 횟수)와 몇 가지 probabilistic한 계산들을 토대로구한 각각의 arm i의 upper confidence bound (UCB)를 구하고 이것이 가장 좋은 arm을 고르는 알고리즘이다. UCB를 간단히 설명하자면 그 동안의 관측 결과에서부터 time t에서 arm i의 expected reward의 confidence (확률이 높은) upper bound 정도로 설명할 수 있을 것이다. UCB 알고리즘은 매 시간 t에서 다음과 같은 rule로 arm i를 고른다.</p>

<ol>
	<li><p>다음 식을 만족하는 arm i를 고른다. $i = \arg\max_i \mu_i + P_i.$</p></li>
</ol>

<p>뒤에 붙는 $P_i$ term이나 UCB를 정의하는 방법에 따라서 UCB1, UCB2, UCB-Tuned, MOSS, KL-UCB, Bayes-UCB 등의 variant가 있지만, 기본적인 아이디어는 동일하다고 생각하면 된다. 이론적으로 더 우수한 UCB를 가지게 될 경우 더 적은 regret을 가지게 되는데, 각각의 UCB variant 들에 대해서 이런 confidence bound를 증명한 work이 상당히 많이 있기 때문에 가장 좋은 UCB를 사용하면 된다. 이 글에서는 가장 간단한 UCB1만 소개를 해보도록 하겠다. UCB1의 policy는 다음과 같다.</p>
<p>$$i = \arg\max_i \bar x_i + \sqrt{\frac{2\ln t}{n_i}}.$$</p>
<p>여기에서 $\bar x_i$는 i번째 arm의 지금까지 관측한 평균 값이고, t는 현재 시간, $n_i$는 현재 시간에서 arm i가 play된 횟수를 의미한다. 이 값은 arm i의 실제 보상에 대한 $1-\frac{1}{t}$의 confidence의 upper bound로, Chernoff-Hoeffding bound를 통해 얻어지는 값이다. 처음에는 관측 결과가 좋은 arm을 고르되, 관측 결과가 적은 arm들을 고를 확률이 더 높을 수 있지만, 시간이 충분히 지나고나면 (time은 log scale이지만 관측은 linear scale이므로) empirical result가 더 큰 가중치를 얻게 되고, 그 결과 시간이 많이 지나고 나면 empirical하게 가장 좋은 arm 위주로 arm을 뽑게 된다.</p>
<p>UCB는 이론적으로 우수한 결과를 가지고 있고 (앞서 설명한 Gittins index의 아주 효율적인 appoximation algorithm이라는 것이 알려져 있다), 또한 실험에서도 잘 동작하는 것이 이미 알려져있지만 UCB 알고리즘도 만능은 아니다. UCB를 계산하기 위해서는 empirical mean이 필수적이기 때문에 반드시 처음에 모든 arm들을 explore해야한다는 이슈가 있기 때문에 초기에는 한 번 이상 exploration이 필요하다는 문제점이 있다.</p>

<h3>Algorithm 3: Thompson Sampling</h3>
<p>마지막으로 Thompson sampling, 혹은 probability matching에 대해 알아보자. 이 알고리즘은 google analytics에서도 사용하고 있는 알고리즘으로 <a href="https://support.google.com/analytics/answer/2846882">[2]</a> 최근 이론적인 증명과 실험적인 결과에서 모두 두각을 보이고 있는 알고리즘이다. <a href="http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf">[3]</a>, <a href="http://arxiv.org/abs/1506.00779">[4]</a></p>
<p>Thompson sampling의 기본 아이디어는 간단하다. 각각의 시간 t마다 policy에 따라 action a를 선택하고, 그에 상응하는 reward r을 받는다고 가정해보자. Thompson sampling은 observation $(a_t, r_t)$과 parameter $\theta$를 사용해 likelihood function $Pr[r ~|~ a, \theta]$를 설계한 다음, prior를 가정해 MAP 문제를 푸는 것이다. Arm의 i.i.d condition 등의 적절한 몇 가지 가정을 더하면, MAP 문제는 다음과 같이 기술된다.</p>
<p>$$\max_\theta Pr[\theta ~|~ D] = \prod Pr[r_t ~|~ a_t, x_t, \theta] Pr[\theta].$$</p>
<p>일반적인 경우, reward는 action a와 true parameter $\theta^*$에 대한 stochastic random variable이기 때문에, expected reward인 $\mathbb E[r~|~a,x,\theta^*]$를 maximize하는 방식으로 학습을 하게 된다. 여기에서 $\theta^*$이 unknown이기 때문에 다음과 같은 식을 maximize하는 action을 찾는 것이 더 합리적이다.</p>
<p>$$\mathbb E [r ~|~ a] = \int \mathbb E [r~|~a,\theta] Pr[\theta~|~D] d\theta.$$</p>
<p>이 문제를 풀기 위해서 probability mathing은 다음과 같은 heuristic을 사용하게 된다.</p>
<p>$$\int \mathbb I \left( \mathbb E [r~|~a,\theta] = \max_{a^\prime} \mathbb E [r~|~a^\prime,\theta] \right) Pr[\theta~|~D] d\theta.$$</p>
<p>여기에서 $\mathbb I(\cdot)$은 indicator function이다. 즉, 매 순간마다 전체 parameter에 대해서 가장 reward의 expectation을 maximize하는 action을 뽑는 방법이 된다. 이 방법에 따라 Thomson sampling algorithm은 다음과 같다. (x는 context vector라는 것인데, 지금은 무시해도 된다)</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/96-1.png" width="300" /></p>
<p>이때, 각각의 arm이 Bernoulli distribution을 따른다고 가정했을 때, <a href="http://SanghyukChun.github.io/58">예전 글의</a> <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> 설명에서 다뤘던 것 처럼, prior를 Beta distribution으로 잡았을 때 계산 상의 이점이 생긴다. Beta distribution Beta(a,b)는 $x^a (1-x)^b$를 normalize하는 형태로 생겼는데, a가 커질수록 관측될 확률이 높아지고 b가 커질수록 그 확률이 낮아진다. Thompson sampling에서는 a에는 arm을 play해서 성공한 횟수, b에는 arm을 play해서 실패한 횟수와 관련된 term을 assign함으로써 Beta distribution을 다음과 같이 정의하고 있다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/96-2.png" width="400" /></p>
<p>기본 아이디어도 어렵지 않고, 알고리즘 또한 엄청나게 간단한 편임에도 Thompson sampling은 다음 그래프에서 볼 수 있듯 UCB 등의 기존 알고리즘보다도 더 좋은 performance를 내는 것을 알 수 있다. (<a href="http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf">[3]</a>에서 따옴)</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/96-3.png" width="600" /></p>
<p>Thompson sampling의 장점 중 하나는, 한 번에 하나의 arm만 play하는 single play 문제에서 여러 개의 N개의 arm을 play할 수 있는 multi play 문제로의 확장이 용이하다는 것이다. 이 방법으로는 두 가지 방법이 있는데, 하나는 action을 maximization 문제를 만족하는 N개의 action을 순서대로 고르는 방법이 하나가 있으며 (Multiplay Thompson sampling, 줄여서 MP-TS), 또 다른 방법으로는 m-1개의 arm은 empirical result가 제일 좋은 arm을 고르고, 마지막 m번째 arm만 Thompson sampling으로 푸는 방법도 있다 (Improved MP-TS, 줄여서 IMP-TS). 흥미롭게도, 두 번째 방법이 실제로는 asymptotic bound를 유지하면서, 첫 번째 방법보다는 조금 더 나은 성능을 보인다고 한다. (<a href="http://arxiv.org/abs/1510.00757">[1]</a>과 <a href="http://arxiv.org/abs/1506.00779">[4]</a>에서 언급됨)</p>

<h3>정리</h3>
<p>이 글에서는 Multi-armed bandit problem에 대해 설명하고, 그 중 finite-armed stochastic multi-armed bandit problem을 푸는 네 가지 알고리즘에 대해 다뤘다. 현재 empirical하게 <a href="http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf">[3]</a>, (그리고 최근에는 theoretical하게까지 <a href="http://arxiv.org/abs/1506.00779">[4]</a>) 가장 우수한 성능을 보이는 알고리즘은 Thompson sampling (arm의 prior를 Bernoulli distribution으로 가정했을 때)이다. 실제로는 위에서 설명한 bandit 보다 훨씬 더 복잡하고 어려운 bandit problem들이 많이 있으며 그것들을 해결하기 위한 알고리즘들 역시 많이 있지만, 이 글에서는 그런 variant들을 모두 다루기보다는, bandit을 이해하기 위해서 가장 필수적으로 이해하고 있어야할 요소들만 다루었다. 조금 더 advanced한 bandit들은 추후에 다른 글들을 통해 소개해볼 수 있도록 하겠다.</p>

<h3>References</h3>
<ol class="reference">
	<li><a href="http://arxiv.org/abs/1510.00757">Burtini, Giuseppe, Jason Loeppky, and Ramon Lawrence. &#8220;A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit.&#8221;, 2015.</a></li>
	<li><a href="https://support.google.com/analytics/answer/2846882">Google Anayltics Help - multi-armed bandit computational and theoretical details</a></li>
	<li><a href="http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf">Chapelle, Olivier, and Lihong Li. &#8220;An empirical evaluation of thompson sampling.&#8221;, 2011.</a></li>
	<li><a href="http://arxiv.org/abs/1506.00779">Komiyama, Junpei, Junya Honda, and Hiroshi Nakagawa. &#8220;Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays.&#8221;, 2015.</a></li>
</ol>

<h3>변경 이력</h3>
<ul>
  <li>2016년 3월 13일: 글 등록</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (17-1) Recommendation System with Implicit Feedback]]></title>
    <link href="http://SanghyukChun.github.io/95/"/>
    <updated>2016-03-06T01:30:00+09:00</updated>
    <id>http://SanghyukChun.github.io/95</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p>이전 글<a href="http://SanghyukChun.github.io/73">[1]</a>에서 다룬 recommendation system은 사용자가 점수를 정확하게 매긴 경우에 대해, 즉 explicit feedback에 대해서만 문제를 푸는 방식이다. 그러나 실제로는 사용자가 점수를 직접 매기는 대신에 단순히 클릭했거나, 조회, 구매한 간접적인 정보, 즉 implicit feedback에만 의존하게 되는 경우도 빈번하게 발생하게 된다. 이 글에서는 그런 implicit feedback만 존재하는 상황에서 어떻게 matrix completion 문제를 디자인하고 해결하는지에 대해 총 세 개의 논문을 들어 설명할 것이다.</p>
<p>이 글의 맨 앞은 implicit feedback이 정확히 무엇이고, 어떤 상황의 문제들이 있는지에 대해 설명할 것이다. 그리고 총 세 개의 논문에서 어떤 방법으로 문제를 접근하는가 설명하도록 할 것이다. 소개할 논문은 <a href="http://yifanhu.net/PUB/cf.pdf">Collaborative Filtering for Implicit Feedback Datasets [2]</a>, <a href="http://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf">Logistic Matrix Factorization for Implicit Feedback Data [3]</a>, <a href="http://www.ismll.uni-hildesheim.de/pub/pdfs/Gantner_et_al2011_KDDCup.pdf">Bayesian Personalized Ranking for Non-Uniformly Sampled Items [4]</a> 총 세가지이다.</p>

<h3>Explicit Feedback and Implicit Feedback</h3>
<p>본격적으로 논문들이 제안한 방법론을 살펴보기 전에, explicit feedback과 implicit feedback의 차이점에 대해 논해보도록 하겠다. 먼저 explicit feedback은 사용자가 정확하게 본인이 얼마나 이 item에 호감이 있는지를 수치로 feedback을 주는 것이다. 예를 들어서 Netflix problem의 별점 데이터는 정확하게 1점부터 5점 사이의 well-define된 범위를 가진다. 그러나 실제로는 사용자의 item에 대한 정확한 호감도를 요구하는 것이 어려울 때가 있다. 아마존의 상품 추천을 예로 들어보자. 아마존이 가질 수 있는 데이터는 사용자가 어떤 상품들을 조회하였는지와, 어떤 상품들을 구매하였는지 정도의 정보 밖에 가질 수 없다. 이 경우 사용자가 살펴본 물건들이 사용자가 정말 매력있게 느껴서 살펴본 것인지, 만약 그렇다면 얼마만큼의 호감도가 있는지 판별하는 것이 매우 어려울 것이다. 이런 종류의 데이터를 사용자가 직접적인 점수를 주는 대신 간접적인 정보만을 제공한다고 하여 implicit feedback이라 부른다. 이외에도 sound cloud나 youTube 등에서 사용자가 재생한 재생목록이나 반복하여 청취 혹은 시청한 횟수 등의 데이터도 마찬가지로 implicit feedback의 대표적인 예가 될 수 있다.</p>
<p>Implicit feedback에서 관측 값은 click이나 재생 횟수 (0 혹은 0보다 큰 정수) 일 수도 있고, 음악 등의 item을 재생한 총 시간 (0 이상의 실수) 일 수도 있다. 한 가지 주의할 점은, explicit feedback처럼 사용자가 구체적으로 item에 대한 preference를 제공하지 않기 떄문에 사용자가 선호하지 않아서 선택하지 않은 item과 아직 관측하지 않은, 그러나 잠재적으로 흥미가 있는 item 모두 값이 0일 것이라는 점이다. 보통 사용자들이 item을 굉장히 조금만 click하거나 (뉴스) 사용하거나 (음악, 동영상) 구매하기 때문에 (쇼핑) 실제로는 matrix의 거의 대부분이 비어있고 아주 일부분의 데이터만 관측되기 때문에, negative observation이 positive observation의 수를 압도한게 되고, 때문에 이런 점을 고려하지 않고 모델을 설계하게 되면 아주 일부분의 positive 데이터와 거의 대부분의 negative observation (값이 0인 observation)들에의해 model이 overfitting된다. 또한, implicit feedback은 굉장히 노이즈가 많기 때문에 주어진 데이터를 얼마나 믿을 수 있을지 알 수 없다는 것이다. 예를 들어서 사용자가 물건을 하나 구매하였더라도, 이 물건에 대해 반드시 긍정적으로 생각할 것이라 기대할 수는 없을 것이다. 가끔은 구매한 물건이 아주 불만족스럽고 다시는 비슷한 물건을 구매하고 싶지 않을 수도 있지 않은가? 이런 두 가지 이슈 (negative observation, confidence)는 recommendation model이 implicit feedback을 처리하기 위해 반드시 고려되어야 할 이슈가 된다.</p>

<h3>Recall: Matrix Factorization with Explicit Feedback</h3>
<p>이전 글 <a href="http://SanghyukChun.github.io/73">[1]</a>에서 다뤘던 objective function은 다음과 같다.</p>
<p>$$\min_{X,Y} \sum_{u,i \in \kappa} (r_{ui} - x_u^\top y_i)^2 + \lambda ( \| x_u \|_2^2 + \| y_i \|_2^2 ) .$$</p>
<p>이전 글에서는 x,y 대신 p,q noataion을 사용했으나 이 글에서는 전부 x, y notation으로 통일하도록 하겠다. 이 objective에 대해서는 이전 글을 참고하면서 읽으면 좋을 것 같다. 원래는 bias term까지 포함해야하지만, 이 글에서는 편의상 bias term은 생략하도록 하겠다. 이 문제는 SGD (Stochastic Gradient Descent), ALS (Alternating Least Square) 등의 solver를 사용해 풀 수 있으며 이 글에서는 자세한 설명을 생략하도록 하겠다.</p>

<h3><a href="http://yifanhu.net/PUB/cf.pdf">Collaborative Filtering for Implicit Feedback Datasets [2]</a></h3>
<p>Implicit feedback을 처리하는 가장 기본적인 접근법을 소개해보자. 이 논문은 user u가 item i를 선호하는지 하지 않는지 여부를 가르키는 preference vector $p_{ui}$를 정의한다. $p_{ui}$의 값은 $sign(r_{ui})$으로 정의할 수 있다. sign 함수는 input 값의 &#8216;sign&#8217;을 return하는 함수로, 즉 input이 negative value면 -1, positive value면 1을 return한다. 따라서 perference의 값은 rating r이 0보다 크다면 1이고, r이 0이라면 p도 0이 되는 것이다.</p>
<p>앞서 설명한 것 처럼, preference vector의 값을 항상 신뢰할 수 있는 것은 아니다. 때문에, 이 논문에서는 confience level $c_{ui}$라는 것을 정의하게 된다. 우리가 한 가지 가정할 수 있는 것은 만약 $r_{ui}$의 값이 크다면, 예를 들어 한 사용자가 한 항목을 엄청 많이 재구매했다거나 한다면, u는 i를 아주 높은 확률로 prefer한다는 사실을 가정할 수 있다. 따라서 confidence level은 r에 대한 increasing function으로 정의하는 것이 타당하다고 할 수 있다. 이 논문에서는 confidence level $c_{ui}$를 다양한 방식으로 정의할 수 있다고 언급하고 있으며, 실제 실험에서는 가장 직관적이고 단순한 increasing function은 linear function을 사용한다. 따라서 이 논문에서는 다음과 같은 confidence를 사용한다.</p>
<p>$$c_{ui} = 1 + \alpha r_{ui}.$$</p>
<p>혹은 $c_{ui} = 1 + \alpha \log (1 + r_{ui}/ \varepsilon)$ 등의 confidence도 대안으로 제안하기는 하지만, 기본적으로 위에서 설명한 linear confidence를 사용하는 듯하다. 한 가지 짚고 넘어가야할 점은, $c_{ui}$는 실제 데이터 $r_{ui}$와 hyper-parameter $\alpha$에 의해서만 정의되므로 optimize해야 할 parameter가 아니라 한 번 confidence를 정의하기만하면 고정되는 constant라는 점이다. 따라서 confidence의 값을 어떻게 정의하더라도 전체 알고리즘의 로직을 바꾸거나 하지는 않는다.</p>
<p>c를 정의하는 것에는 또 하나의 이점이 있다. Parameter $\alpha$가 positive observation과 negative observation의 중요도를 조절하는 역할을 하게 되면서, negative feedback에 대한 중요도를 조절할 수 있는 것이다. 예를 들어 $\alpha$의 크기가 작다면, positive와 negative observation의 confidence 차이가 큰 $\alpha$를 가질 때 보다 상대적으로 작을 것이라는 것을 기대할 수 있게 된다.</p>
<p>이제 objective를 정의할 차례이다. Explicit feedback에서는 복원한 rating $\hat r_{ui}$와 관측한 데이터 $r_{ui}$의 RMSE를 바로 계산하였으나, 앞서 말한대로 이 값을 그대로 계산하기에는 confidence의 문제가 있다. 이 논문에서는 앞서 정의한 confidence를 고려하여 objective function은 다음과 같이 정의한다.</p>
<p>$$\min_{X,Y} \sum_{u,i \in \kappa} c_{ui}(p_{ui} - x_u^\top y_i)^2 + \lambda ( \| x_u \|_2^2 + \| y_i \|_2^2 ) .$$</p>
<p>맨 처음 objective와 달라진 점은, rating vector r (0 이상의 real value) 이 preference vector p (0 또는 1) 로 바뀌었다는 점과, 각각의 u,i pair에 대해 confidence $c_{ui}$가 곱해진다는 점이다. 이때, $c_{ui}$는 optimization parameter와는 상관없이 맨 처음 정해지고 변하지 않는 값이므로, 이렇게 바뀐 objective function을 풀기 위해서 이전 문제와 마찬가지로 살짝 변형된 SGD나 ALS 등을 사용할 수 있다. 논문에서는 조금 더 scalability를 고려한 방법론을 제안하는데, matrix product를 조금 더 효율적으로 하도록 matrix들을 decompose하여 조금 더 order가 낮은 연산을 하는 방법을 사용한다. 자세한 알고리즘은 논문을 참고하면 좋을 것 같다.</p>
<p>정리하자면 이 논문은 rating vector r을 preference vector p로 변환하고, confidence level c를 정의한 후, p와 c를 사용해 RMSE objective function을 optimize하는 work인 것이다. 그리고 앞서 설명했던 두 가지 문제는 confidence level c를 정의하는 방법에 의해 해결할 수 있다.</p>

<h3><a href="http://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf">Logistic Matrix Factorization for Implicit Feedback Data [3]</a></h3>
<p>앞선 논문 <a href="http://yifanhu.net/PUB/cf.pdf">[2]</a>에서는 RMSE를 minimize하는 objective를 설계하였지만, RMSE가 아닌 다른 형태의 objective를 optimize하는 것도 가능하다. 앞선 논문에서 RMSE를 minimize함으로써 얻을 수 있는 효과는 관측한 preference $p_{ui}$와 복원한 preference $\hat p_{ui}$가 서로 (RMSE의 관점에서) 최대한 비슷한 값을 가지도록 optimization이 된다는 것이다. 이 논문은 perference의 RMSE를 minimize하는 문제 대신, 관측 값 $r$과 optimization parameter $\Theta = (x, y, b)$ 등의 posteriori를 maximization하는 방식을 취한다. 참고로 이 논문은 Spotify에서 발표한 논문으로, 실제 음악 추천에서 응용하고 있는 듯 하다.</p>
<p>계속 강조하듯, 여기에서 실제 유저가 선호하는 것과 유저의 implicit feedback 결과는 다를 수 있다. 그렇기 때문에 이 논문은 u가 i를 좋아할 확률을 logistic function으로 확률적으로 정의한 후, 관측한 데이터로부터 posteriori를 maximize하는 방향으로 학습을 하게 된다. 그러기위해 이 논문은 $\ell_{ui}$이라는 새로운 notation을 introduce하고 이를 사용자 u가 item i를 선호하는 &#8216;event&#8217;로 정의한다. 어렵게 설명했지만, 그냥 &#8216;user u가 음악 i를 좋아한다 좋아하지 않는다&#8217;에 대한 0, 1 값이다. 이 논문은 주어진 $\Theta (x, y, b)$에 대해 $\ell$이 1이 될 확률 $pr_{ui}$를 다음과 같이 logistic form으로 정의한다.</p>
<p>$$pr_{ui} := Pr[\ell_{ui} = 1 ~|~ \Theta] = \frac{\exp(x_u^\top y_i + b_u + b_i)}{1 + \exp(x_u^\top y_i + b_u + b_i)}. $$</p>
<p>직관적으로 생각해보았을 때, 앞선 논문 [2]은 r의 값이 0보다 크기만 하면 항상 u가 i를 좋아한다고 생각하지만, 이 논문에서는 그것이 r의 값에 대한 확률로 나타난다는 점을 알 수 있다. Reconstructed rating $\hat r$을 $\hat r_{ui} = x_u^\top y_i$이라고 했을 때, 위의 함수는 $\hat r_{ui}$에 대한 증가함수이므로 ($\hat r_{ui}$의 값이 $-\infty$가 되면 함수값이 0이고, $\hat r_{ui}$ 값이 $\infty$가 되면 함수값이 1이 된다), 위의 식은 rating 값이 더 크면 호감을 가질 확률이 더 높아지는 형태의 확률 함수가 된다.</p>
<p>따라서 이 모델의 likelihood는 positive observation u,i의 set을 $\mathcal S$라 하였을 때 다음과 같이 쓸 수 있다.</p>
<p>$$\mathcal L_{naive} (R ~|~ \Theta) = \prod_{u,i \in \mathcal S} pr_{ui} \prod_{u,i \notin \mathcal S} (1-pr_{ui})$$</p>
<p>그러나 앞선 논문 [2]에서도 언급되었듯, negative feedback은 &#8216;싫어한다&#8217;와 다른 의미를 가지고 있기 때문에, 이 논문에서도 confidence라는 것을 정의하게 된다. [2]와의 차이점이라면 RMSE 관점이 아니라 앞에서 살펴본 likelihood function의 관점에서 정의를 한다는 점이다. 여기에서 $c_{ui}$는 $\alpha r_{ui}$로 사용하는데, 만약 hyper-parameter $\alpha$의 값이 크면 positive observation에 더 큰 비중을 두고, $\alpha$의 값이 작다면 negative observation에 더 큰 비중을 두는 식으로 다음과 같이 정의를 하게 된다.</p>
<p>$$ \mathcal L (R ~|~ \Theta) = \prod_{u,i} Pr[ \ell_{ui} | \Theta]^{\alpha r_{ui}} (1 - Pr[ \ell_{ui} | \Theta]).$$</p>
<p>위의 식에서 negative feedback에 대한 (즉, 만약 관측값 $r_{ui}$가 0이라면) likelihood 값은 $\mathcal L (r_{ui} ~|~ \Theta ) = 1 - pr_{ui}$ 가 되므로 앞에서 계산한 naive한 likelihood function과 일치한다. 그러나 positive feedback에 대한 likelihood는 $pr_{ui}^{\alpha r_{ui}} (1-pr_{ui}) $가 되므로, $\alpha$의 값을 조절함에 따라 앞에서 계산한 값과 차이가 있다.</p>
<p>개인적인 생각으로는 여기에서 저자가 증명을 잘못한 것이 아닐까 생각된다. 만약 Positive feedback의 likelihood에 weight를 주기위해 exponent c를 추가한다고 했을 때, likelihood 식은 다음과 같이 된다.</p>
<p>$$ \mathcal L = \prod_{u,i \in \mathcal S} ( p_{ui}^{\ell_{ui}} (1-p_{ui})^{1-\ell_{ui}} )^{\alpha r_{ui}} \prod_{u,i \notin \mathcal S} p_{ui}^{\ell_{ui}} (1-p_{ui})^{1-\ell_{ui}}. $$</p>
<p>이때, $r_{ui} &gt; 0$ 일 때 $\ell = 1$이고, 혹은 둘 다 0이라는 특성을 잘 사용하면 이 식은 다음과 같이 표현이 된다.</p>
<p>$$\mathcal L = \prod_{u,i} p_{ui}^{\alpha r_{ui}} (1-p_{ui})^{1-\ell_{ui}}. $$</p>
<p>즉, 만약 저자가 의도한대로 positive feedback의 likelihood에 exponent로 weight를 주고 싶었다면, positive feedback의 likelihood function에서 1-p 부분이 없어야한다는 점이다. 이 부분은 저자가 실수를 했거나 혹은 내가 이해를 잘못했을 가능성이 있다. 혹시 몰라서 논문을 좀 찾아봤는데, 약 한 달 전쯤 나온 <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004760">논문</a>에서 증명한 결과가 내가 증명한 결과와 같은걸 보면, 저자가 틀린게 맞는 것 같다.</p>
<p>그리고 또 다른 문제는 $pr_{ui}$는 언제나 값이 0에서 1사이이기 때문에 $c_{ui} = \alpha r_{ui}$의 값이 크면 클수록 $pr_{ui}$의 값은 오히려 감소하게 된다는 것이다. 그래서 논문의 설명과는 반대로 $\alpha$의 값을 키우는 것이 오히려 positive observation의 weight를 낮추는 것이 아닌가하는 생각이 드는데, 논문을 여러 번 다시 읽어보고 계산을 해봐도 아직 아리송하다. 오히려 이렇게 하고 싶었다면, 최종 objective가 완전히 바뀌기는 하지만, 관측된 데이터 $\mathcal S$에 대해서 likelihood를 $ (1 + \alpha_{ui}) pr_{ui}$와 같은 형태로 정의하는 편이 훨씬 좋지 않을까? 왜 base가 1보다 작은데, weight term을 exponent으로 올렸는지 이해가 되질 않는다. 이 부분은 혹시 나중에 이해가 완전히 되면 내용을 추가하도록 하겠다.</p>
<p>다시 본문으로 돌아오자. Likelihood를 계산했으니, prior만 있다면 posteriori를 계산할 수 있게 된다. 이 논문은 $x, y$의 prior를 전부 0 mean Normal distribution으로 정의한다. 이렇게 정의할 경우, 나중에 log MAP 문제를 풀게 될 때, L2 regularization과 같은 형태의 식 $\frac{\lambda}{2} (\| x \|^2 + \| y \|^2) $ 을 얻을 수 있다. 자세한 증명은 생략한다. Prior를 정했으니 이제 posteriori를 구해서 다음과 같이 log MAP 문제를 정의할 수 있다. (논문에서 증명한 결과로, 내가 증명한 결과와는 차이가 있다)</p>
<p>$$ \log Pr[ \Theta | P ] = \sum_{u,i} c_{ui} \widehat r_{ui} - ( 1 + c_{ui} ) \log ( 1 + \exp \widehat r_{ui} ) - \frac{\lambda}{2} ( \| x_u\|^2 + \| y_i \|^2 ). $$</p>
<p>이 문제 역시 다른 문제들 처럼 한 번에 update하기가 어렵기 떄문에 alternative하게 update를 하게 된다. 정확히는 coordinate gradient method를 사용한다 (maximize문제이므로 ascent가 될 것이다). 여기에서 한 가지 문제가 발생하는데, 앞에 붙어있는 summation term이 <b>모.든.</b> (u,i) pair에 대한 summation이기 때문에 한 번 gradient를 계산하는 비용이 어마어마해진다는 것이다. 정확히는 아이템의 개수 I와 유저의 숫자 U에 대해 linear한 비용이 필요하다. 보통 그 둘의 값은 몇 백만, 몇 천만이 될 정도로 크기 때문에 scalability 이슈가 굉장히 중요해진다. 이 논문은 그런 문제를 해결하기 위하여 (속도가 느리다는 문제) AdaGradient를 사용하라거나, 전체에 대해 summation을 하는 대신, 전체 positive pair (u,i)와 일부 negative pair (u,i)만 사용해서 문제를 해결하라고 언급되어있다.</p>
<p>정리하자면 이 논문은 Matrix Factorization을 RMSE minimization 문제가 아닌 MAP 문제로 해결하려는 시도를 한 논문으로, MAP로 바꾸기 위하여 confidence가 포함된 likelihood function을 정의한다. (개인적으로는 이 likelihood function이 왜 이런 꼴을 하고 있는지 이해하지 못하였다) 알고리즘은 coordinate ascent를 사용하지만, 각각의 gradient 값이 아이템과 유저의 개수에 linear하기 때문에 실제 데이터에서 practical하지 못하다는 문제가 발생한다. 이런 문제를 해결하기 위하여 이 논문은 전체 matrix의 거의 대부분을 차지하는 negative observation을 전체 다 사용하는 대신, 일부만 sample하여 사용하는 방식을 제안하고 있다.</p>

<h3><a href="http://www.ismll.uni-hildesheim.de/pub/pdfs/Gantner_et_al2011_KDDCup.pdf">Bayesian Personalized Ranking for Non-Uniformly Sampled Items [4]</a></h3>
<p>앞의 두 논문은 같은 문제의 objective function만 RMSE와 MAP로 서로 다르게 잡은 경우이지만, 이 논문은 앞의 방법들과 다소 다른 접근 방식을 취하고 있다. 이 논문은 먼저 선행 연구<a href="http://arxiv.org/abs/1205.2618">[5]</a>를 조금 발전 시킨 논문인데, 선행 연구에서는 partially observed pair-wise competition 문제를 푸는 Baysian Personalized Ranking (BPR) optimization과 그것을 푸는 알고리즘을 제안하고, 그것을 MF로 확장하고있다. 그리고 그 다음 논문 [4]에서는 원래 논문이 가지는 단점을 negative observation을 adaptive하게 sample하는 방식으로 개선하고 있다.</p>
<p>먼저 핵심 notation들을 정의해보자. 관측된 (u,i) pair는 $\mathcal S$라는 set으로 정의된다. 여기에서 새로운 notation $I_u^+$와 $U_i^+$ 2개가 introduce된다. $I_u^+$는 user u가 관측한 적 있는 item의 set이고, $U_i^+$는 item i를 관측한 적 있는 user u의 set이다. 그러면 이 set들을 통해 $\mathcal D_S$라는 triplet을 다음과 같이 정의할 수 있다.</p>
<p>$$D_S := \{(u,i,j) ~|~ i \in I_u^+ \mbox{ and } j \notin I_u^+ \}.$$</p>
<p>즉, user u, user u가 관측한 item i와 관측하지 못한 item j 이렇게 셋의 triplet인 것이다.</p>
<p>이제 이 논문의 핵심아이디어인 pair-wise ranking에 대해 살펴보자. 이 논문은 먼저 각각의 user u에게 item i와 item j간의 pair-wise ranking이 존재한다고 가정한다. 이 논문에서는 user u가 item i를 j보다 높은 order를 가질 때 $i &gt;_u j$의 꼴로 표현한다. 이 pair-wise ranking은 (혹은 order는) totality, antisymmetry, transitivity를 만족하는 order로 정의된다. 자세한 수식은 논문에 있으니 생략한다.</p>
<p>이 논문은 user u가 item i를 item j보다 더 높게 평가할 확률을 다음과 같이 가정하고 있다.</p>
<p>$$Pr( i &gt;_u j | \Theta) = \sigma(\hat r_{uij} (\Theta)).$$</p>
<p>여기에서 $\hat r_{uij} := \hat r_{ui} - \hat r_{uj}$로 정의되는 값이고, $\sigma$는 sigmoid function으로, $\sigma(x) = \frac{1}{1+\exp(-x)}$의 꼴로 정의된다. 각각의 user에 대해 pair-wise ranking에 대한 확률을 정의했고, 또한 실제 관측값도 있기 때문에, 우리는 ranking에 대한 likelihood를 정의할 수 있고, prior를 가정하게 되면 posteriori역시 계산할 수 있다. 먼저 likelihood $\prod_u p(&gt;_u | \Theta)$부터 계산해보자. (이전과 마찬가지로 user들은 전부 independent하다고 가정했기 때문에 곱으로 표현된다)</p>
<p>$$\prod_u p(&gt;_u | \Theta) = \prod_{u,i,j} \prod_u p( i &gt;_u j | \Theta)^{I_{(u,i,j) \in \mathcal D_S}} (1 - p( i &gt;_u j | \Theta) )^{I_{(u,i,j) \notin \mathcal D_S}} $$</p>
<p>$I_x$는 x가 true면 1이고 false면 0인 indicator function이다. 여기에서 order를 정의할 때 totality와 antisymmetry를 가정하였기 때문에, 위 식을 잘 건개하면 아래와 같은 간단한 식으로 표현하는 것이 가능하다고 한다.</p>
<p>$$\prod_u p(&gt;_u | \Theta) = \prod_{(u,i,j) \in \mathcal D_S} p( i &gt;_u j | \Theta). $$</p>
<p>[3]과 같이 prior를 zero mean normal distribution으로 정의하게 되면, log posteriori는 다음과 같이 표현된다.</p>
<p>$$ \max \ln Pr(\Theta | &gt;_u) = \max \sum_{(u,i,j) \in \mathcal D_S} \ln \sigma(\hat x_{uij}) - \lambda \| \Theta \|^2. $$</p>
<p>이 문제는 stochastic gradient descent로 푸는 것이 가능하다. 자세한 미분 값은 논문에 있으니 생략한다. 참고로 이 논문은 이 문제를 optimization했을 때 얻을 수 있는 solution이 AUC (area under the ROC curve, ROC curve는 <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">wiki</a> 참고)를 maximization하는 것과 비슷한 문제라는 것을 따로 증명해두었으니 관심이 있다면 한 번쯤 읽어보면 좋을 것 같다.</p>
<p>[4]에서 제안한 BPR (baysian personalized ranking) problem을 풀기 위해서 SGD를 사용한다는 언급을 했는데, 문제가 하나 있다. 바로 triplet $\mathcal D_S$의 개수가 너무 많아서 전부 uniform하게 뽑기에는 데이터가 너무너무 많다는 것이다. 그래서 [5]에서는 adapted sampling 방식을 제안하고 있다.</p>
<p>$$ \max  \sum_{(u,i,j) \in \mathcal D_S} w_u w_i w_j \ln \sigma(\hat x_{uij}) - \lambda \| \Theta \|^2. $$</p>
<p>이때 $w_u = 1/| I_u^+ |$, $w_i = 1$로 정의가 된다. 즉, 관측한 아이템이 더 많은 유저는 적게 뽑고, 모든 positivie item은 uniform하게 뽑는다. 마지막으로 $w_j = \frac{1}{|U||I|} \sum_{u} I_{j \in I_u^+}$으로 취하게 되는데, 더 많이 사용자들이 관측한, 혹은 좋아한 데이터 위주로 sample을 뽑는 방식이다.</p>
<p>정리해보면, BPR은 다른 논문들처럼 reconstructed error를 바로 measure하는 것이 아니라, pair-wise ranking을 정의하고, 복원된 rating $\hat r_{ui}$에 대해 user가 item i보다 j를 좋아할 확률을 sigmoid로 정의한다. 이 확률을 사용해 MAP문제를 정의하는데, 이 문제는 ROC curve의 넓이를 구하는 것과 비슷한 문제가 된다. 이때, sigmoid function을 step function으로 바꾸면 완전히 ROC curve의 넓이를 구하는 것과 같은 문제가 된다. Sigmoid가 step function의 가장 popular한 differentiable approximation 중 하나이기 때문에 sigmoid로 정의하게 되는 것이다. Algorithm은 SGD를 사용하는데, 데이터 셋이 user u가 관측한 item i와 관측하지 않은 item j의 triplet이기 때문에 uniform sampling을 하게 되면 결과가 좋지 않을 수 있다. 때문에 adaptive하게 (u,i,j)에서 j 고를 때, popular한 j를 더 고르도록 sample을 하여 성능을 개선하고 있다.</p>

<h3>정리</h3>
<p>이 글에서는 Implicit feedback에 대해 recommendation을 어떻게 할 수 있을지 서로 다른 세 가지 접근방법을 소개했다. 첫 번째는 가장 기본적인 방법으로, confidence level $c_{ui}$를 정의하고, real value variable인 $r_{ui}$를 binary variable인 $p_{ui}$로 바꾼 다음 optimization을 푸는 방법에 대해 소개했다. 두 번째로는 RMSE를 optimization하는 대신, u가 i를 좋아할 확률을 모델링하고, 주어진 데이터에 대해 MAP를 푸는 방법에 대해 소개했다. 마지막으로는 각각의 user별로 item들끼리의 personalized pair-wise ranking을 정의하고, 역시 마찬가지로 u가 i보다 j를 좋아할 확률을 모델링하고 이것의 MAP를 구하는 방법에 대해 소개했다. 알고리즘은 SGD로 해결할 수 있지만, 조금 더 smart하게 item을 뽑는 adaptive sampling을 사용할 경우 성능이 더 올라간다고 한다.</p>

<h3>References</h3>
<ol class="reference">
	<li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a></li>
	<li><a href="http://yifanhu.net/PUB/cf.pdf">Hu, Yifan, Yehuda Koren, and Chris Volinsky. &#8220;Collaborative filtering for implicit feedback datasets.&#8221;, 2008</a></li>
	<li><a href="http://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf">Johnson, Christopher C. &#8220;Logistic matrix factorization for implicit feedback data.&#8221;, 2014</a></li>
	<li><a href="http://www.ismll.uni-hildesheim.de/pub/pdfs/Gantner_et_al2011_KDDCup.pdf">Gantner, Zeno, et al. &#8220;Bayesian personalized ranking for non-uniformly sampled items.&#8221;, 2012</a></li>
	<li><a href="http://arxiv.org/abs/1205.2618">Rendle, Steffen, et al. &#8220;BPR: Bayesian personalized ranking from implicit feedback.&#8221;, 2009</a></li>
</ol>

<h3>변경 이력</h3>
<ul>
  <li>2016년 3월 6일: 글 등록</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (17) Recommendation System (Matrix Completion)]]></title>
    <link href="http://SanghyukChun.github.io/73/"/>
    <updated>2016-03-01T17:58:00+09:00</updated>
    <id>http://SanghyukChun.github.io/73</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p>이 글에서는 recommendation 문제가 어떤 문제인지에 대해 간략하게 설명하고, 각각을 푸는 가장 대표적인 알고리즘인 matrix factorization에 대해서 설명할 것이다. 이 글의 많은 부분이 예전에 적었던 글 <a href="http://SanghyukChun.github.io/30">[1]</a>, <a href="http://SanghyukChun.github.io/31">[2]</a>을 기반으로 작성하였으니, 궁금하다면 추가로 읽어보면 좋을 것 같다.</p>
<h3>Recommendation Problem</h3>
<p>Recommendation problem은 여태까지 사용자가 item에 대해 evaluate한 history data를 기반으로 사용자가 아직 사용하지 않은 item에 대한 사용자의 평가를 예측하는 문제라고 할 수 있다. 추천과 랭킹 문제는 마케팅을 포함한 다양한 분야에서 오랜 세월 관심을 가져왔던 분야이다. 특히 광고를 제작하는 사람들 입장에서는 적은 비용으로 최대한의 효율을 낼 수 있는 타겟광고는 그야말로 금덩이나 다름없는 영역이라 할 수 있을 것이다. 추천은 그만큼 제한된 자원을 최대한 효율적으로 분배할 수 있는 방법이기도 하며, 사람들의 지갑을 더 열게 할 수 있는 중요한 문제인 것이다. Netflix와 왓챠는 내가 점수를 매긴 별점을 바탕으로 내가 좋아할만한 영화나 드라마를 추천해준다. 아마존, 이베이, Gmarket 등의 온라인 쇼핑몰들 역시 내가 클릭했던 상품들의 history를 기반으로 내가 좋아할만한, 사고싶어할만한 상품들을 추천해주거나, 이 상품들을 묶어서 하나의 작은 할인 패키지를 구성하기도 한다. 페이스북은 내가 좋아요를 누른 포스트들을 바탕으로 내가 좋아할만한 페이지를 추천하고, 타겟 광고를 내보낸다. 우리가 지금은 너무나 자연스럽게 받아들이는 이 사실들은 전부 머신러닝에 의해 가능해진 것들이다.</p>
<h3>Matrix Completion</h3>
<p>그러면 이제 추천 문제를 보다 엄밀하게 정의해보도록 하자. 먼저 데이터에 대해 살펴보도록 하자. 이 문제에서는 사용자와 상품이라는 두 가지 요소들이 존재한다. 사용자 $u$가 아이템 $i$를 얼마나 좋아할 것인지 나타내는 값을 rating $r_{ui}$라 하자. 이때, 이 rating은 Netflix처럼 1에서 5 사이의 real value일수도 있으며, 아마존이나 페이스북처럼 클릭했는지 하지 않았는지에 대한 데이터일수도 있다. 앞선 경우는 사용자가 자신이 얼마나 이 아이템을 좋아하는지 &#8216;명시적으로&#8217; 나타냈기 때문에 explicit feedback이라 부르며, 후자의 경우는 사용자가 해당 상품을 좋아했는지 싫어했는지 표현을 직접적으로 하지 않으므로 &#8216;implicit feedback&#8217;이라고 부른다. 이에 대해서는 나중에 다른 글을 통해 더 자세히 다루도록 하겠다. (<a href="http://SanghyukChun.github.io/95">Implicit feedback에 대한 글</a>을 새로 추가하였다) 지금은 $r_{ui}$의 정확한 값을 알고 있고, 이 값이 전혀 noise가 없는 값이라고 가정하고 문제를 계속 설명하도록 하겠다. 이런 경우 우리가 가지고 있는 데이터는 $r_{ui}$들의 값이 될 것이고, 대략 아래와 같은 방식으로 matrix로 표현할 수 있을 것이다.</p>
<p><a align="center" href="http://www.codecogs.com/eqnedit.php?latex=movie.&space;{\begin{matrix}&space;1&space;&amp;&space;2&space;&amp;&space;3&space;&amp;&space;4&space;&amp;&space;5&space;&amp;6&space;&amp;&space;7&space;&amp;&space;8\end{matrix}}&space;\\&space;\left\{\begin{matrix}&space;user&space;1&space;\\&space;user&space;2&space;\\&space;user3&space;\\&space;user4&space;\\&space;user5&space;\\&space;user6&space;\\&space;user7&space;\\user8\,&space;\end{matrix}\right.&space;\begin{bmatrix}&space;3&space;&amp;&space;5&space;&amp;&space;*&space;&amp;&space;4&space;&amp;&space;1&space;&amp;*&space;&amp;&space;*&space;&amp;&space;2&space;\\&space;*&space;&amp;&space;3&space;&amp;&space;5&space;&amp;&space;1&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;3&space;\\&space;4&space;&amp;&space;1&space;&amp;&space;*&space;&amp;&space;4&space;&amp;&space;1&space;&amp;*&space;&amp;&space;3&space;&amp;&space;2&space;\\&space;5&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;2&space;&amp;&space;3&space;&amp;&space;*&space;&amp;&space;*&space;\\&space;*&space;&amp;&space;2&space;&amp;&space;4&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;1&space;&amp;&space;2&space;\\&space;5&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;5&space;&amp;&space;4&space;&amp;*&space;&amp;&space;*&space;&amp;&space;4&space;\\&space;1&space;&amp;&space;*&space;&amp;&space;5&space;&amp;&space;2&space;&amp;&space;3&space;&amp;1&space;&amp;&space;5&space;&amp;&space;3&space;\\&space;*&space;&amp;&space;3&space;&amp;&space;2&space;&amp;&space;1&space;&amp;&space;4&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;*&space;\\&space;\end{bmatrix}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?movie.&space;{\begin{matrix}&space;1&space;&amp;&space;2&space;&amp;&space;3&space;&amp;&space;4&space;&amp;&space;5&space;&amp;6&space;&amp;&space;7&space;&amp;&space;8\end{matrix}}&space;\\&space;\left\{\begin{matrix}&space;user&space;1&space;\\&space;user&space;2&space;\\&space;user3&space;\\&space;user4&space;\\&space;user5&space;\\&space;user6&space;\\&space;user7&space;\\user8\,&space;\end{matrix}\right.&space;\begin{bmatrix}&space;3&space;&amp;&space;5&space;&amp;&space;*&space;&amp;&space;4&space;&amp;&space;1&space;&amp;*&space;&amp;&space;*&space;&amp;&space;2&space;\\&space;*&space;&amp;&space;3&space;&amp;&space;5&space;&amp;&space;1&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;3&space;\\&space;4&space;&amp;&space;1&space;&amp;&space;*&space;&amp;&space;4&space;&amp;&space;1&space;&amp;*&space;&amp;&space;3&space;&amp;&space;2&space;\\&space;5&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;2&space;&amp;&space;3&space;&amp;&space;*&space;&amp;&space;*&space;\\&space;*&space;&amp;&space;2&space;&amp;&space;4&space;&amp;&space;2&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;1&space;&amp;&space;2&space;\\&space;5&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;5&space;&amp;&space;4&space;&amp;*&space;&amp;&space;*&space;&amp;&space;4&space;\\&space;1&space;&amp;&space;*&space;&amp;&space;5&space;&amp;&space;2&space;&amp;&space;3&space;&amp;1&space;&amp;&space;5&space;&amp;&space;3&space;\\&space;*&space;&amp;&space;3&space;&amp;&space;2&space;&amp;&space;1&space;&amp;&space;4&space;&amp;&space;*&space;&amp;&space;*&space;&amp;&space;*&space;\\&space;\end{bmatrix}" title="movie. {\begin{matrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp;6 &amp; 7 &amp; 8\end{matrix}} \\ \left\{\begin{matrix} user 1 \\ user 2 \\ user3 \\ user4 \\ user5 \\ user6 \\ user7 \\user8\, \end{matrix}\right. \begin{bmatrix} 3 &amp; 5 &amp; * &amp; 4 &amp; 1 &amp;* &amp; * &amp; 2 \\ * &amp; 3 &amp; 5 &amp; 1 &amp; 2 &amp; * &amp; * &amp; 3 \\ 4 &amp; 1 &amp; * &amp; 4 &amp; 1 &amp;* &amp; 3 &amp; 2 \\ 5 &amp; 2 &amp; * &amp; * &amp; 2 &amp; 3 &amp; * &amp; * \\ * &amp; 2 &amp; 4 &amp; 2 &amp; * &amp; * &amp; 1 &amp; 2 \\ 5 &amp; * &amp; * &amp; 5 &amp; 4 &amp;* &amp; * &amp; 4 \\ 1 &amp; * &amp; 5 &amp; 2 &amp; 3 &amp;1 &amp; 5 &amp; 3 \\ * &amp; 3 &amp; 2 &amp; 1 &amp; 4 &amp; * &amp; * &amp; * \\ \end{bmatrix}" /></a></p>
<p>이때 *은 아직 사용자가 평가하지 않은 데이터를 의미한다. 이제 recommendation problem은 이 매트릭스의 비어있는 부분의 값을 예측하는 문제로 바꿔서 생각할 수 있다. 이를 수식으로 표현하면 아래와 같은 형태로 표현하는 것이 가능하다.</p>
<p>$$\min_{\hat R} \| \hat R - R \|_F^2 $$</p>
<p>이때 $R$는 비어있는 곳이 없는 원래 데이터를 의미한다. User의 숫자를 n, item의 개수를 m이라고 하면 R은 n by m matrix가 될 것이다. $\hat R$는 원래 데이터로부터 비어있는 곳을 복구한 데이터를 의미한다. 여기에서, 원래 데이터 matrix $R$에서 값이 없었던 부분은 제외하고 error를 (oot mean squared error라 하여 RMSE라 부른다) 계산하는 것이 이 문제의 objective function이 된다. 이런 문제를 일컬어, 비어있는 matrix를 완성시키는 문제다 해서 <a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix Completion</a>이라고 부른다. 따라서 앞으로 남은 글에서는 recommendation problem이라는 말 대신, matrix completion이라는 이름으로 바꿔서 부를 것이다.</p>

<h3>Matrix Factorization</h3>
<p>그러면 앞서 설명한 matrix completion 문제를 어떻게 해결할 수 있을까? 지금까지는 문제를 정의하는 법에 대해서만 설명했지만, 이제부터는 이 문제를 풀기 위한 가정과 그 가정을 사용하여 만든 모델, 그리고 그 모델을 풀기위한 알고리즘을 설명할 것이다. Matrix Completion 문제를 풀기 위한 방법은 여러 가지가 있다. 이전 글 <a href="http://SanghyukChun.github.io/31">[2]</a> 에서 다뤘던 baseline predictor와 neighborhood method 등의 방법도 그런 방법들 중 하나이지만, 이 글에서는 단일 모델로 가장 우수한 성능을 보이는 것으로 알려진 matrix factorization에 대해서만 다룰 것이다. Matrix factorization의 가정은 original data matrix $R$가 low rank matrix라는 것이다. 따라서 우리가 복원하는 $\hat R$ 역시 low rank 조건을 가지게 되므로 constrained optimization 문제로 바꿔서 쓸 수 있게 된다. 이 경우 optimal한 matrix completion의 objective function은 다음처럼 표현된다.</p>
<p>$$\min ~\mbox{rank}(\hat R) ~\mbox{ s.t. }~ \Omega(r_{ui} - \hat r_{ui}) = 0 ~\forall u,i$$</p>
<p>여기에서 $\Omega(A_{ij} - B_{ij})$는 matrix A와 B의 i,j 번째 element 중 하나라도 비어있으면 0, 둘 다 element가 존재하면 둘의 차이로 정의가 된다.</p>
<p>이 문제를 어떻게 풀지에 대해 논하기 전에 먼저 low rank assumption은 어떤 의미가 있는지에 대해 먼저 논해보자. 먼저 모든 matrix는 다른 두 matrix의 곱으로 표현이 가능하다. 이때 만약 matrix의 rank가 작다면 두 matrix 의 rank역시 더 작은 형태로 표현이 가능하게 된다. 즉, 원래 n by m matrix R이 n이나 m보다 작은 k만큼의 rank를 가졌을 때, R은 n by k matrix P와 m by k matrix Q의 곱으로 표현할 수 있다. 즉, $R = P Q^\top$ 으로 표현이 된다는 사실이 이미 알려져있다.</p>
<p>$p_u$와 $q_i$는 각각 P와 Q의 u, i번째 row vector라고 정의하면 (둘 다 k차원 벡터가 된다) 앞선 수식에서부터 우리는 다음과 같은 수식을 얻을 수 있다.</p>
<p>$$r_{ui} = p_u \cdot q_i$$</p>
<p>이 사실로부터 우리는 user u가 item i의 점수를 주는 방식은, user u의 item들에 대한 숨겨진(latent) interest $p_u$와 그에 대응하는 item들의 숨겨진 특성 $q_i$에 의해 결정된다는 사실을 알 수 있다. 이를 그림으로 표현하면 다음과 같다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/30-1.png" width="400" /></p>
<p>따라서 앞에서 설명했던 rank를 minimize하는 문제는, 최대한 적은 latent feature를 사용하여 user와 item을 표현하도록 하는 문제가 되는 것이다. 그러나 실제로는 rank condition이 convex optimization이 아니기 때문에 이 문제를 optimal하게 풀 수 없다는 문제점이 존재한다. 여기에서 두 가지 접근 방법이 가능하다. 하나는 rank condition을 convex 조건으로 바꿔서 푸는 convex relaxation이 있고, 또 하나는 rank의 값은 우리가 직접 넣어주는 hyper-parameter로 사용하고, 대신 RMSE를 minize하는 방법이 있다. 각각의 objective function은 다음과 같이 표현된다.</p>
<p>$$\min_{\hat R} \| \hat R \|_* ~\mbox{ s.t. }~ \Omega(r_{ui} - \hat r_{ui}) = 0 ~\forall u,i.$$</p>
<p>$$\min_{\hat R} \sum_{u,i \in \kappa} (r_{ui} - \hat r_{ui})^2 ~\mbox{ s.t. }~ \mbox{rank}(\hat R) = k.$$</p>
<p>여기에서 $u,i \in \kappa$는, 전체 데이터 중에서 오직 관측된 u,i pair만을 의미한다. 즉, $\Omega$ notation을 대체하는 기호라고 생각하면 된다.</p>
<p>전자는 항상 convex relaxed된 문제에서는 optimal한 solution을 찾을 수 있다는 것이 보장된다는 장점이 있지만, 원래 문제와 다른 문제를 풀기 때문에 원래 문제와 같은 optimal solution을 갖는 상황이 아니면 의미가 없을 수 있다는 문제점이 있다. 후자는 convex한 방법론을 사용할 수 없기 때문에 global optimal solution 대신 local optimal solution을 얻게 되지만, practical하게 잘 동작한다는 장점이 존재한다. 이 글에서는 둘 다 짤막하게 (그러나 너무 깊지 않게) 다뤄볼까 한다.</p>

<h3>Methodology 1: Convex Relaxation</h3>
<p>먼저 왜 rank condition이 non-convex condition인지부터 살펴보자. Matrix X의 rank는 X의 0이 아닌 singular value들의 숫자로, 다시 말해 이들의 l-0 norm의 합으로 표현이 가능하다. 즉, 원래 objective function은 다음처럼 표현할 수 있다.</p>
<p>$$\min \sum_\ell \| \sigma_\ell(\hat R) \|_0 ~\mbox{ s.t. }~ \Omega(r_{ui} - \hat r_{ui}) = 0 ~\forall u,i$$</p>
<p>이때, $\sigma_i(A)$는 A의 i번째 singular value를 의미한다. 만약 matrix의 rank k가 full rank가 아니라면, 정확하게 k개의 singular value만 값이 0이 아니고, 나머지 singular value들의 값은 0이 된다. 따라서 이 문제가 위에 적은 것 처럼 l-0 norm의 합으로 표현이 되는 것이다. 문제는 이 l-0 norm 자체가 non-convex norm이라는 것이다. 보통 l-0 norm을 relax하는 가장 tight한 방법 중 하나가, l-0 norm을 l-1 norm으로 바꾸는 것이다. 이때, matrix A의 singular value들의 l-1 norm합은, matrix A의 nuclear norm $\| A \|_* $이 된다. 따라서 위의 문제를 $ \min \sum_\ell \| \sigma_\ell(\hat R) \|_1 $으로 relax하게 될 경우 (편의상 constriant는 생략한다), objective function은 $\min \| \hat R\|_*$을 하는 것과 같다.</p>
<p>이 문제는 convex problem이기 때문에 아무 convex solver를 가져와서 문제를 풀면 된다. 그러나 조금 더 효율적인 풀이방법을 위하여 singular value thresholding이라는 alternating update 방식의 알고리즘도 제안이 되어있는 상태이다. 이 글에서는 최대한 개념 위주로 설명을 할 생각이기 때문에, 설명 대신 좋은 review paper <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.365.8055">[3]</a>와 원본 논문 <a href="http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf">[4]</a>을 reference로 추가한다.</p>
<p>이렇듯 convex relaxation에서 algorithm은 보통 큰 issue는 아니다. 물론 기존 solver보다 더 좋은 알고리즘을 제안하는 것도 중요한 일이지만, convex relaxation에서 가장 critical한 issue는, relaxed problem이 언제 원래 problem과 같은 해를 가지는지에 대한 조건이 무엇이냐 하는 것이다. 즉, 내가 그 어떤 조건에서도 relaxed problem을 사용해서 원래 문제를 풀 수 없다면, 그 convex relaxation은 가치가 없는 relaxation이 되는 것이다. 아마도 데이터가 많으면 많을수록 복원이 쉬울 것이고, 적으면 적을수록 복원이 어려워지다가, 어느 순간부터 복원이 불가능해지는 시점이 존재할 것이라는 것이다. 예를 들어 데이터가 하나 빼고 전부 있다면 MC가 어려운 문제가 아닐 수 있지만, 반대로 데이터가 하나만 있다면 복원할 수 있는 경우의 수가 거의 무한하게 많을 것이라는 것이다. 다행히도 리뷰 페이퍼 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.365.8055">[3]</a>의 Theorem 1에도 나와있듯, user와 item 숫자 중 더 큰 숫자를 $n_0$라고 했을 때, convex relaxed problem이 optimal한 solution을 얻기 위해서는 order of $n_0 \log^2 n_0$ 만큼의 observed data가 필요하다는 증명이 이미 되어있다. 이에 대한 자세한 설명은 해당 논문을 읽어보면 좋을 것 같고, 이 글에서는 생략하도록 하겠다.</p>

<h3>Methodology 2: Solve Non-convex Problem Directly</h3>
<p>두 번째 방법으로는 non-convex한 objective function을 바로 푸는 것이다. 혼동을 피하기 위하여 이 objective function을 다시 적어보자.</p>
<p>$$\min_{\hat R} \sum_{u,i \in \kappa} (r_{ui} - \hat r_{ui})^2 ~\mbox{ s.t. }~ \mbox{rank}(\hat R) = k.$$</p>
<p>그리고, 이미 앞에서 $\hat r_{ui} = p_u \cdot q_i$로 표현할 수 있다는 것 까지 확인했었으므로, 이를 기반으로 문제를 적으면 다음과 같은 문제가 된다.</p>
<p>$$\min_{P,Q} \sum_{u,i \in \kappa} (r_{ui} - p_u \cdot q_i)^2.$$</p>
<p>앞에서 설명한 것 처럼 P,Q는 각각 n by k, m by k matrix가 된다. 이때, overfitting을 피하고 보다 generalized된 문제로 바꿔주기 위하여 regularization term을 뒤에 붙여주면 문제는 다음과 같이 바뀐다.</p>
<p>$$\min_{P,Q} \sum_{u,i \in \kappa} (r_{ui} - p_u \cdot q_i)^2 + \lambda ( \| p_u \|_2^2 + \| q_i \|_2^2 ) .$$</p>
<p>이 pair-wise optimization 문제는 non-convex 문제이지만, gradient descent method로 local optimum에 수렴하는 결과를 얻을 수 있으며 실제로 꽤 효율적으로 좋은 결과를 얻을 수 있다.</p>
<p>또 다른 solver로는 Alternating Least Square (ALS) 라는 방법이 있다. 이 방법은 alternative하게 주어진 objective를 update하는 방법인데, 주어진 objective가 pairwise optimization으로 생각하면 non-convex이지만, p나 q 중 하나를 고정하고 나머지에 대해 optimization을 하게 되면 convex, 그것도 closed form으로 계산된다는 점을 이용한 방법이다. 따라서 이 방법을 사용해 예전에 설명했었던 k-means style의 알고리즘을 설계할 수 있는데, 이를 ALS라고 부르는 것이다. Gradient descent가 더 빠른 경우도 있지만, ALS를 사용하게 되면 각각의 element들이 다른 element에 independent하기 때문에 분산처리가 간편하기 때문에 실제로는 ALS 방법도 많이 사용된다고 한다.</p>
<p>지금까지 설명한 방법은 그야말로 가장 기본이 되는 모델이고, 이 모델을 조금 더 확장해보도록 하자. 가장 간단하게 확장할 수 있는 방법은 bias term을 추가하는 것이다. 예를 들어서 어떤 user는 항상 모든 영화 평점을 비교적 &#8216;짜게&#8217; 주는 user가 있을 수 있고, 반대로 모든 영화에 점수를 후하게 주는 user도 있을 수 있다. 어떤 영화는 개봉 전부터 평단이나 기자들에게서 호평을 받았거나 유명 배우가 나와 기본 점수가 높을 수도 있고, 그 반대도 가능하다. 따라서 이런 현상들을 반영할 수 있는 bias term이 추가가 되는 것은 지극히 자연스럽다고 할 수 있다. user의 bias를 $b_u$, item의 bias를 $b_i$라고 하면 (이 값들은 vector가 아니라 scalar value이다) user u의 item i에 대한 bias $b_{ui}$는 $b_{ui} = \mu + b_u + b_i$로 표현할 수 있을 것이다. 여기에서 $\mu$는 전체 모든 r_{ui}의 평균 값으로, bias라는 개념이 평균에서부터 얼마나 멀어지는 가에 대한 개념이므로 평균 값도 함께 고려하는 것이다. $\mu$는 데이터와 함께 주어지는 값이고, bias term들은 p,q처럼 optimization을 통해 찾아야하는 optimization parameter가 된다. 이를 사용하면 reconstructed rating $\hat r_{ui}$는 다음과 같이 표현된다.</p>
<p>$$\hat r_{ui} = \mu + b_u + b_i + p_u \cdot q_i.$$</p>
<p>이제 이 결과를 원래 objective에 대입하면 다음과 같은 식을 얻게 된다.</p>
<p>$$\min_{P,Q,B} \sum_{u,i \in \kappa} (r_{ui} - \mu - b_u - b_i - p_u \cdot q_i)^2 + \lambda ( \| p_u \|_2^2 + \| q_i \|_2^2 + b_u^2 + b_i^2 ) .$$</p>
<p>마찬가지로 이 결과 역시 gradient method나 ALS로 푸는 것이 가능하다.</p>

<h3>Matrix Factorization in Netflix Prize Competition</h3>
<p>위의 methodology 2는 실제로 Netflix problem에서 (<a href="http://SanghyukChun.github.io/30">[1]</a> 참고) 단일 성능이 가장 우수했던 알고리즘을 소개한 것이다. 조금 더 구체적으로는 다음과 같은 그래프로 표현할 수 있다. (출처: <a href="http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf">[5]</a>)</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/73-1.png" width="600" /></p>
<p>Plain은 진짜 가장 기본 term만 사용한 것이고, with bias는, 앞에서 설명한 bias를 사용한 방식이다. Implicit feedback은 생략하고, temporal dynamics는 rating이 시간에 따라 바뀐다고 가정하고 다음과 같이 모델을 설계한 것이다.</p>
<p>$$\hat r_{ui}(t) = \mu + b_u(t) + b_i(t) + p_u(t) \cdot q_i.$$</p>
<p>여기에서, item의 성질 $q_i$는 static하다고 가정하고, 대신 사람이 평가하는 방식인 $p_u$만 시간에 대해 변한다고 가정하는 것이다. 이 모델은 시간이 지남에 따라 사람들이 서로 상호작용하고, 그로 인해 별점을 매기는 방식이 바뀔 수 있다는 것을 가정으로 한 방법으로, 실제 최종 결과를 살펴보면 이 방식을 채용한 방법이 가장 우수한 결과를 얻는 것을 확인할 수 있다. 실제 Netflix prize에서는 다른 방법들까지 고려한 ensemble method가 더 좋은 성능을 내지만, 실제로 RMSE metric에서 단일 모델로 가장 좋은 성능을 내는 것은 여전히 matrix factorization 기반 접근법이다.</p>

<h3>정리</h3>
<p>이 글에서는 recommendation 문제가 어떤 문제인지 설명하고, 보다 수학적으로 정의된 matrix completion 문제로 recommendation을 설명한다. 그 후 이 문제를 푸는 가장 popular한 방법인 matrix factorization에 대해서 다룬다. 해당 문제가 non-convex 문제이기 때문에 convex relaxation을 통해 문제를 푸는 방법 (더 늦게 나온 방법이다), non-convex optimization을 바로 푸는 방법 (Netflix prize에서 실제 사용했던 방법) 두 가지를 소개하고 각각에 대해 간략하게 설명한다. 실제 recommendation은 matrix factorization 뿐 아니라 여러 다른 methodology들을 결합해서 문제를 풀게 되지만, 여전히 단일 model로 가장 좋은 performance를 보여주는 것은 matrix factorization을 기반으로 한 방법론들이기 때문에 matrix factorization을 제대로 아는 것이 recommendation 문제를 풀기 위한 첫 걸음이라 할 수 있을 것이다.</p>

<h3>변경 이력</h3>
<ul>
  <li>2016년 3월 1일: 글 등록</li>
</ul>

<h3>References</h3>
<ol class="reference">
	<li><a href="http://SanghyukChun.github.io/30">인터넷 속의 수학 - How Does Netflix Recommend Movies? (1/2)</a></li>
	<li><a href="http://SanghyukChun.github.io/31">인터넷 속의 수학 - How Does Netflix Recommend Movies? (2/2)</a></li>
	<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.365.8055">Kennedy, Ryan. &#8220;Low-rank matrix completion.&#8221;, 2013</a></li>
	<li><a href="http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf">Candès, Emmanuel J., and Benjamin Recht. &#8220;Exact matrix completion via convex optimization.&#8221;, 2009</a></li>
	<li><a href="http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf">Koren, Yehuda, Robert Bell, and Chris Volinsky. &#8220;Matrix factorization techniques for recommender systems.&#8221;, 2009</a></li>
</ol>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML2015)]]></title>
    <link href="http://SanghyukChun.github.io/93/"/>
    <updated>2015-10-19T18:20:00+09:00</updated>
    <id>http://SanghyukChun.github.io/93</id>
		<content type="html"><![CDATA[<p>주어진 이미지에 대한 설명을 하는 문장, 혹은 캡션을 생성하는 문제를 image caption 문제라고 한다. 이 문제는 여러 가지 문제들이 복합적으로 얽혀있는 문제라고 할 수 있는데, 먼저 이미지가 어떤 것에 대한 이미지인지 판별하기 위하여 object recognition을 정확하게 할 수 있어야한다. 그 다음에는 detect한 object들 사이의 관계를 추론하여 이미지가 나타내는 event가 무엇인지 알아내어야 하고, 마지막으로 event를 caption으로, 즉 natural language로 재생성해야한다. 얼핏 생각하면 간단한 문제같지만 자그마치 비전과 NLP 분야의 핵심 문제들이 복합적으로 얽혀있는 복잡한 문제인 것이다. <s>방금 너희들이 본 건 간단해 보이지만 자그마치 3개의 퀑 기술이 합쳐진 컴비네이션!</s> 예를 들어보자.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/93-1.jpg" width="400" /></p>

<p>이 사진을 보고 우리는 &#8216;오바마가 청소부와 인사를 하고 있다&#8217; 라고 바로 문장으로 풀어낼 수 있지만, 이런 문제를 푸는 알고리즘을 디자인하기 위해서는 먼저 이 사진에서 오바마와 청소부라는 핵심 object를 detect 해야한다. 그러나 아직까지는 어떤 object가 중요한지 알 수 없으므로, 먼저 복도에 있는 보좌관들, 기둥, 바닥, 뒤에 보이는 계단 난간, 전등, 복도 등등을 모두 detect 해야한다. 그러므로, caption을 만들기 위해서 가장 먼저 우리는 높은 수준의 object detection과 segmentation을 필요로 하다. 다음으로는 segmentation된 정보를 사용해서 여러 event를 알아내야 한다. &#8216;오바마와 청소부가 인사를 하고 있다&#8217; &#8216;보좌관이 복도를 걷고 있다&#8217; &#8216;한 남자가 책을 읽고 있다&#8217; &#8216;사람이 기둥 뒤에 서있다&#8217; <s>&#8216;기둥 뒤에 공간이 있다&#8217;</s> 등의 여러 event를 추정해야하고, 그 중에서 가장 가능성이 높은 event를 판별해야한다. 마지막으로 해당 event를 설명하는 문장을 generate해야한다.</p>
<p>현재 caption generation 문제를 해결하는 방법들 중에서 가장 널리 쓰이고 있고, 가장 잘 동작하는 (state-of-art) 방법들은 neural network를 사용한 접근 방식들이다. Neural network를 사용하지 않은 기존 접근 방법은 크게 두 가지가 있었다. 하나는 object detection과 attribute discovery를 먼저 진행한 후에, 그것들을 사용해 미리 만들어놓은 caption template을 채우는 것이고, 또 하나는 비슷한 이미지 들의 caption 데이터를 사용해 지금 image에 적합한 caption으로 수정하는 방식이었다고 한다. Reference들을 보면 약 2010년부터 2013년 정도까지 연구가 활발하게 진행되었던 모양이지만, 지금은 전부 neural network 기반의 work에게 밀려서 사용되지 않는다고 한다.</p>
<p>현재 state-of-art를 찍고 있는 Neural network 기반 image description generator 모델들은 주로 2014년쯤부터 활발하게 연구가 진행되고 있다. Image caption 문제를 해결하기 위해 기존 deep learning 연구 그룹들은 마치 image를 하나의 language처럼 취급하고 실제 언어로 &#8216;translate&#8217; 하는 concept을 도입해서 문제를 machine traslation의 연장선으로 바라보는 접근 방법을 취한다고 한다. 그래서 대부분의 neural network 기반의 work들은 machine translation에서 사용하는 encoder-decoder 아이디어를 활용하여 caption generation을 한다고 한다. 보통 encoder-decoder 과정에서 이미지 하나를 그대로 사용하는 대신, CNN을 사용하여 이미지 하나를 single feature vector로 표현하고, 그 feature vector를 model에서 사용하는 방식을 취하고 있다.</p>
<p>올해 초, 기존 state-of-art를 뛰어넘는 RNN visual attention 기반 caption generation model이 <a href="http://arxiv.org/abs/1502.03044">Xu, Kelvin, et al. &#8220;Show, attend and tell: Neural image caption generation with visual attention.&#8221; ICML 2015</a>이라는 work을 통해 제안되었다. 이 논문은 기본적으로 기존의 방법들처럼 encoder-decoder 개념을 사용하지만, 추가로 visual attention이라는 개념을 caption generator에 도입하여 image caption 문제를 해결한다. Attention이란 사람이 시각 정보를 처리할 때 일부 데이터에 &#8216;focus&#8217;하면서 계속 focus되는 대상이 움직이는 현상을 일컫는다. 최근 RNN을 사용하여 visual attention을 반영한 새로운 모델 들이 여기저기에서 등장하고 있는데, 이는 기존 CNN 기반 접근 방법들이 모든 이미지 픽셀을 그대로 사용하는 것과 대조적이라 할 수 있다. 참고로, 이미 앞선 <a class="tip" title="Recurrent Models of Visual Attention (NIPS 2014)" href="http://SanghyukChun.github.io/91">다른 work</a>에서도 visual attention이라는 개념을 사용해 classification 문제를 해결했었다.</p>
<p>이 논문의 가장 큰 contribution은 visual attention을 &#8220;hard&#8221; attention과 &#8220;soft&#8221; attention, 두 가지 attention machanism을 제안하고 새로운 방식의 two attention-based image caption generator 모델을 제안했다는 것이다.</p>

<ul>
  <li>“Soft” attention은 deterministic machanism으로, standard back-propagation 방법으로 train할 수 있기 때문에 전체 모델이 end-to-end로 learning된다. Soft attention model은 hard attention model의 approximation model이라고 생각하면 된다.</li>
  <li>“Hard” attention은 stochastic mechanism이며, reinforcement learning으로 train할 수 있다. Hard attention model은 매 iteration마다 데이터를 sampling을 해야하고, reinforcement learning과 neural network 부분이 분리되어있어 end-to-end learning이 아니라는 단점이 있다.</li>
</ul>

<p>(+ 이 논문을 처음 읽을 때는 두 가지 모델을 &#8216;섞어서&#8217; 한 모델에서 soft와 hard attention이 복합적으로 작용하는 모델을 만드는 것이라고 생각했었지만, 논문을 자세히 읽어보니, 먼저 hard attention을 제안한 후에, 이 모델의 approximation version으로 soft attention이라는 모델을 추가로 제안한 것이었다.)</p>
<p>그럼 이제 모델이 구체적으로 어떻게 구성이 되어있는지 자세하게 알아보도록 하자.</p>

<h3>Image Caption Generation with Attention Mechanism: Model details</h3>

<p>먼저 이 논문에서 제안하는 caption generation task를 정의하자. 이 논문은 caption의 길이를 $C$로, 사용할 수 있는 단어의 개수를 $K$로 고정시킨채 문제를 해결한다. 이 정의에 따라 caption을 vector $y$로 표현할 수 있다.</p>

<p>$$ y = \{y_1, \ldots, y_c \}, y_i \in \mathbb R^K. $$</p>

<p>이 식에서 각 $y_i$는 단어 하나를 의미한다. 즉, 이 논문의 목적은 &#8216;적절한&#8217; caption vector $y$를 생성하는 것이다. 이 논문은 &#8216;적절한&#8217; caption vector를 hard loss와 soft loss 두 가지 loss function을 사용해 정의하고 있으며, 여기에서 attention 개념이 사용된다. 자세한 설명은 아래에서 마저 설명하도록 하겠다.</p>

<p>이제 자세한 모델 설명을 해보자. 앞에서도 잠깐 언급했듯, 이 논문 역시 다른 기존 deep learning caption generator model들처럼 image에서 caption을 생성하는 과정을 image라는 언어에서 caption이라는 언어로 &#8216;translatation&#8217; 하는 개념을 사용한다. 따라서 이 논문은 machine translation의 encoder-decoder 개념을 사용하게 되는데, encoder는 우리가 잘 알고 있는 CNN을 사용하고, decoder로 RNN, 정확히는 LSTM을 사용하게 된다. 이 논문의 핵심이라고 할 수 있는 attention 개념은 LSTM에서 사용된다.</p>
<p>이 논문에서 제안하는 모델을 그림으로 표현하면 다음과 같다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/93-8.png" width="450" /></p>

<h3>Encoder: CNN</h3>

<p>Encoder CNN은 주어진 이미지를 input으로 받아, output으로 feature vector $a$를 내보낸다. 이 CNN의 마지막 layer는 총 $L$ 개의 filter로 이루어져있으며, 각각의 filter마다 $D$ 개의 neuron을 가지도록 설계하였다. 즉, 다음과 같이 쓸 수 있다</p>

<p>$$ a = \{ a_1, \ldots, a_L \}, a_i \in \mathbb R^D. $$</p>

<p>이 논문에서는 encoder를 위한 CNN으로 VGG network를 선택하였는데, 이 네트워크는 바로 <a href="http://SanghyukChun.github.io/92">전 글</a>에서 다뤘으니 자세한 설명은 생략하도록 하겠다. 19 layer짜리를 사용한 것 같고, VGG11 layer로 pre-training만 시키고 fine-tunning은 하지 않은 상태로 사용했다고 한다. 당연한 얘기지만, VGG 네트워크말고도 다른 네트워크도 사용가능하다.</p>

<h3>Decoder: LSTM</h3>

<p>이 논문은 decoder로 LSTM을 사용한다. 이 LSTM은 매 time stamp $t$ 마다 caption vector $y$의 한 element $y_t$를 생성한다. 즉, 전체 &#8216;unfold&#8217; 하게되는 시간은 caption의 길이 $C$와 같다. 즉 이 LSTM은 한 time stamp $t$ 마다 바로 전 hidden state $h_{t-1}$과 바로 전에 generate된 단어 $y_{t-1}$을 input으로 받아서 지금 time stamp에 해당하는 단어 $y_t$를 생성하는 것이다. 이 논문에서 사용하는 LSTM 모델은 다음과 같다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/93-2.PNG" width="450" /></p>

<p>LSTM에 대한 자세한 설명은 생략하도록 하겠다. 추후 다른 포스트를 통해 LSTM 자체에 대해 자세히 다뤄보도록하겠다. $T_{s,t}: \mathbb R^s \to \mathbb R^t$를  간단한 affine transformation이라고 정의해보자 ($T_{n,m} (x) = W x + b$라는 의미이다). 그러면 LSTM은 다음과 같이 간단하게 표현할 수 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/93-3.png" width="300" /></p>

<p>이때 1번 식의 $i_t,f_t ,c_t ,o_t ,h_t $는 각각 input, forget, memory, output, hidden state를 의미한다. 이 논문은 LSTM의 initial memory state와 hidden state를 $a$의 평균 $\bar a = \frac{1}{L}\sum_i^L a_i$을 input으로 하는 두 개의 MLP ($f_{init,c}$ $f_{init,h}$)로 estimate한다고 한다.</p>
<p>그럼 이제 LSTM cell 하나에 input으로 들어오는 $Ey_{t-1}, h_{t-1}, \hat z_t$에 대해 알아보자. $h_{t-1}$은 바로 전 hidden state이니 제외하고, $Ey_{t-1} $는 $t-1$ 시점에서 생성된 caption $y_{t-1}$을 embedding matrix $E \in \mathbb R^{m \times K}$로 embedding한 $m$ dimensional vector이다. $E$는 맨 처음에 randomly initialize를 한 이후 train 과정에서 update되는 parameter이다. 마지막으로 $\hat z \in \mathbb R^D$는 context vector라고 하는데, 이 context vector는 attention model들에 의해서 결정된다.</p>

<p>Context vector $\hat z_t$는 CNN encoder output $a$와 바로 전 hidden state $h_{t-1}$에 의해 다음과 같이 결정된다.</p>

<p>$$ \hat z_t = \phi (a, \alpha_t), \mbox{ where } \alpha_{ti} = \frac{\exp(f_{att}(a_i, h_{t-1}))}{\sum_{k=1}^L \exp(f_{att}(a_k, h_{t-1}) )}. $$</p>

<p>먼저 $\alpha_t$는 time $t$에서의 $a$의 weight vector를 의미하며, $\alpha_{ti}$는 time $t$에서의 $a$의 $i$번째 element $a_i$에 해당하는 weight value값이다. 이때 weight란, 우리가 주어진 annotation (CNN의 output) 중에서 어느 location에 focus를 맞출 것인지, 혹은 어떤 것이 중요하지 않은지를 결정하는 값으로, 모델에서 &#8216;attention&#8217; 개념이 적용된 부분이다. 위의 식에서 알 수 있듯, softmax로 정의가 되기 때문에, weight $\alpha_t$의 element-wise summation은 1이다. $f_{att}$는 attention model이라는 것으로, weight vector $\alpha$를 계산하기 위한 모델이며, 이 논문은 이 모델을 hard와 soft 두 가지로 정의하였다. $\phi$ function은 주어진 $a$와 그것의 weight vector $\alpha_t$를 사용해 $\hat z_t$를 계산하기 위한 function이다. 정리해보면 다음과 같다.</p>

<ul>
	<li><p>$\alpha_t$: $a$의 weight vector로, 어디에 &#8216;attend&#8217; 할지 결정하는 값. 모두 더하면 1.</p></li>
	<li><p>$f_{att}$: $a$와 $h_{t-1}$을 사용해 weight vector $\alpha$를 계산하기 위한 attention model.</p></li>
	<li><p>$\phi$: $a$와 $\alpha_t$를 받아 $\hat z$를 계산하는 mechanism (예: $\phi(a,\alpha_t) = \sum_i\alpha_{ti} a$).</p></li>
</ul>

<p>마지막으로, 모델이 주어졌을 때, time $t$에서의 단어 $y_t$는 다음과 같이 바로 전 context vector $\hat z_{t-1}$와 그 동안의 LSTM의 state를 저장하고 있는 hidden state $h_t$, 그리고 바로 직전 단어 $y_{t-1}$에 관련된 확률에 의해 결정된다.</p>

<p>$$p (y_t | a, y^{t-1}_1) \propto \exp(L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)). $$</p>

<p>여기에서 $L_o \in \mathbb R^{K \times m}, L_h \in \mathbb R^{m \times n} L_z \in \mathbb R^{m \times D}, E \in \mathbb R^{m \times K} $는 train과정에서 learning하는 parameter들이다.</p>

<p>이제 이 논문의 핵심인 attention model $f_{att}$들을 살펴보도록하자.</p>

<h3>Stochastic &#8220;Hard&#8221; Attention</h3>

<p>캡션 모델이 i번째 단어를 생성하기 위해 focus attention해야 하는 위치를 $s_t$라고 하는 location variable을 사용해 표현해보자. 우리가 원하는 벡터는 정확하게 focus해야하는 부분 한 부분만 값이 1이고 나머지는 전부 0인 벡터이다.</p>
<p>첫 번째 attention model인 stochastic &#8220;hard&#8221; attention은 attention location $s_t$를 latent variable로 취급한 다음, 이 값을 $\alpha_t$로 multinoulli distiribution parameterize시킨다. 다시 말하면, 주어진 시간 $t$에서, $s_t$의 $i$번째 element s_{ti}의 값이 1이 될 확률이 $\alpha_{ti}$이 되는 것이며, 주어진 시간 $t$에서 모든 $i$에 대해 $\alpha_{ti}$를 더하면 그 값은 1이 된다. 즉, $\sum_i \alpha_{ti} = 1$ 이다. $s_t$와 $\alpha_t$를 정의하게 되면 $\hat z_t$라는 새로운 random variable을 다음과 같이 정의할 수 있다.</p>

<p>$$p(s_{ti} = 1 | s_{j &#60; t, a}) = \alpha_{ti} \mbox{ and } \hat z_t = \sum_i s_{ti} a_i. $$</p>

<p>우리의 목표는 주어진 feature vector $a$에 대해 가장 확률이 높은 caption $y$를 고르는 것이다. 이 작업은 간단하게 maximum log likelihood $\max_y \log p(y|a)$를 계산하는 것으로 구할 수 있는데, 이 값을 직접 계산하는 대신, 앞에서 정의한 attention location $s_t$를 사용하게 된다면 log likelihood의 lower bound를 다음과 같이 계산할 수 있으며, 이 값을 새로운 objective function $L_s$로 정의한다.</p>

<p>$$ L_s = \sum_s p(s|a) \log p(y|s,a) \leq \log \sum_s p(s|a) p(y|s,a) = \log p(y|a).$$</p>

<p>$L_s$가 log likelihood의 lower bound이므로, 이 값을 증가시키게 되면 likelihood 역시 함께 증가할 것이다. 따라서 log-likelihood의 maximum값을 구하는 대신, $L_s$의 maximum 값을 구하는 것으로 문제를 대략적으로 풀 수 있다 (하지만 엄밀하게 증명해본 것은 아니지만, 아마도 $L_s$가 local optimum으로 converge한다고 해서 원래 log likelihood가 converge할 것 같지는 않기 때문에 정확한 문제의 solution을 찾게 되는 것은 아닌 것 같다. 그러나 이미 neural network 쪽 algorithm들이 그러하듯, 정확한 답보다는 그 답을 향해 진행하는 것이 훨씬 중요하기 때문에 이 work에서는 큰 문제가 될 것 같지는 않다). Parameter $W$에 대한 $L_s$의 미분값은 다음과 같이 주어지며, 이 값을 사용하면 gradient descent를 통해 $\max L_s$ 문제를 해결할 수 있다. </p>

<p>$$ \frac{\partial L}{\partial W} = \sum_s p(s|a) \left[ \frac{\partial p(y| s,a)}{\partial W} + \log p(y|s,a)\frac{\partial p(s|a)}{\partial W} \right]. $$</p>

<p>이 미분 값을 직접 구하기 위해서는 모든 attention location $s$에 대해 summation 기호 안에 있는 연산을 계산해야하기 때문에 computation이 간단하지 않다. 이미 많은 기존 deep learning approach들에서 &#8216;정확한&#8217; 값을 구하는 데에 시간이 오래걸린다면, 그냥 &#8216;적당히&#8217; 빠르게 근사하는 것이 더 낫다는 것이 알려져 있는 만큼, 이 논문에서는 정확한 값을 계산하는 대신 Monte Carlo based sampling을 사용해 이 값을 다음과 같이 근사하고 있다. 이때, $\tilde s_t \sim \mbox{Multinouli}_L (\alpha)$로 주어진 값이다.</p>

<p>$$ \frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y| \tilde s^n,a)}{\partial W} + \log p(y| \tilde s^n,a)\frac{\partial p(\tilde s^n |a)}{\partial W} \right]. $$</p>

<p>Monte Carlo based sampling을 사용해 gradient를 근사하게 되면, 굉장히 효율적이고 빠르게 gradient를 근사할 수 있지만, 그렇게 계산된 gradient는 variance가 크기 때문에 이를 handle하기 위한 추가적인 아이디어들이 도입되게 된다. 먼저 moving average를 사용한다. 이 논문에서는 moving average baseline을 다음과 같이 이전 log likelihood들의 exponential decay를 사용한 합으로 표현하였다.</p>

<p>$$ b_k = 0.9 \times b_{k-1} + 0.1 \times \log p (y | \tilde s_k, a). $$</p>

<p>여기에 또 varinace를 줄이기 위하여 entroy term $H[s]$를 더한다. 그뿐 아니라 주어진 이미지에 0.5의 확률로 sampled attention location $\tilde s$의 값을 $\tilde s$의 기대값인 $\alpha$로 설정한다. 이 방법들을 사용하게 되면 stochastic attention learning algorithm의 robustness를 증대시킬 수 있다고 한다. 이 방법들을 모두 섞으면, 기존의 gradient는 다음과 같이 바뀐다.</p>

<p>$$ \frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y| \tilde s^n,a)}{\partial W} + \lambda_r (\log( p(y|\tilde s^n, a) - b)\frac{\partial p(\tilde s^n |a)}{\partial W} + \lambda_e \frac{H[\partial \tilde s^n]}{\partial W} \right]. $$</p>

<p>이렇게 구해진 식은, reinforcement learning의 update rule과 같다. 그렇기 때문에 hard visual attention을 reinforcement learning을 사용해 learning할 수 있다고 이야기하는 것이다. Action은 attention의 위치를 고르는 것이 될 것이고, reward는 log-likelihood의 lower bound인 $L_s$가 된다.</p>
<p>Hard attention model은 매 순간마다 $\hat z$를 &#8216;hard choice&#8217;를 통해 계산하게 된다. 여기에서 hard choice란, 주어진 시간 $t$에서 $\alpha$로 parameterize된 multinouilli distribution에서 $a_i$를 sampling하여 얻게되는 choice를 의미한다.</p>

<h3>Deterministic &#8220;Soft&#8221; Attention</h3>
<p>Hard attention은 train phase의 매 timestamp마다 attention location $s_t$를 매 번 sampling해줘야하기 때문에, 이 논문에서는 hard attention 대신 soft attention이라는 개념을 추가로 도입한다. Soft attention은 stochastic하게 매 번 sampling을 하는 대신 deterministic하게 context vecot $\hat z_t$을 계산한다. Soft attention $\phi$는 다음과 같이 표현된다.</p>

<p>$$\phi (\{ a_i \}, \{ \alpha_i \}) = \sum_i^L \alpha_i a_i. $$</p>

<p>이렇게 표현되는 이유는 $\hat z_t$의 expectation이 $\alpha, a$로 다음과 같이 직접 계산할 수 있기 때문이라고 한다.</p>

<p>$$ \mathbb E_{p(s_t|a)}p[\hat z_t] = \sum_{i=1}^L \alpha_{ti} a_i. $$</p>

<p>따라서 이 방법을 취하게 되면 전체 모델이 좀 더 smooth해지고, differentiable해지기 때문에 back-propagation을 사용해서 end-to-end로 learning이 가능하다는 장점이 있다. Deterministic attention, 혹은 soft attention 역시 앞에서 구한 likelihood $p(y|a)$를 $s_t$를 사용하여 구한 approximation을 optimizing하는 것으로 구할 수 있다. 이 논문에 따르면 $h_t$가 stochatic context vector $\hat z_t$를 tanh를 사용한 linear projection이기 때문에, $\mathbb E_{p(s_t|a)[h_t]}$를 1st order Taylor approximation하게 되면 이 값은 $\hat z_t$의 expectation인 $\mathbb E_{p(s_t|a)}p[\hat z_t]$를 사용하여 forward propagation을 한 번 진행하여 $h_t$를 구한 값과 같아진다고 한다. $n_t = L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)$로 정의하고 ($p (y_t | a, y^{t-1}_1)$를 approximation했을 때 쓰인 값과 같다), $n_{ti}$를 $a_i$를 사용하여 계산한 random variable $\hat z_t$를 대입하여 구한 $n_{t}$값이라고 해보자. 이 값들을 사용하여 이 논문은 k번째 word prediction을 위한 NWGM (Normalized Weighted Geometric Mean)이라는 것을 다음과 같이 정의한다.</p>

<p>$$NWGM[p(y_t = k | a)] = \frac{\prod_i \exp(n_{tki})^{p(s_{ti}=1|a)}}{\sum_j \prod_i \exp(n_tji)^{p(s_{ti}=1|a)}} = \frac{\exp(\mathbb E_{p(s_t|a)}[n_{tk}])}{\sum_j\exp(\mathbb E_{p(s_t|a)}[n_{tj}])}.$$</p>

<p>정의에 따라 $\mathbb E [n_t] = L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)$이 되므로, 이 식을 통해 우리는 caption prediction을 위한 NWGM이 expected context vector를 사용하여 approximate된다는 것을 알 수 있다. 이전의 다른 work에 의하면, NWGM은 (softmax acivation일 때만) $\mathbb E [ p(y_t = k | a)]$로 근사가 된다고 한다. 이 말은 바꿔 말하면, 모든 가능한 attention location $s_t$에 대해 구한 output의 expectation이 expected context vector $\mathbb E [\hat z_t]$를 사용하여 간단한 feedforward propagation으로 계산된다는 의미가 된다.</p>

<p>또한 이 논문에서는 doubly stochastic attention이라는 개념도 같이 제안한다. $\alpha$의 정의에 따라 우리는 $\sum_i \alpha_{ti} = 1$ 이라는 관계식을 가진다. 이 논문이 주장하는 것은, $\sum_t \alpha_{ti} \approx 1$ 이라는 조건을 하나 더 추가하는 것이 실제 실험결과 더 좋은 성능을 낸다는 것이다. 그 이유는 모델이 모든 이미지의 모든 부분을 전체 기간 동안 보는 것을 방해하기 때문에 더 focus된 attention이 가능하기 때문이라고 한다. 그리고 추가로, soft attention model에 </p>

<p>결론적으로, 이 모델은 아래와 같은 negative log-likelihood를 minimize하는 방식으로 end-to-end learning을 할 수 있다.</p>
<p>$$ L_d = -\log (p(y|x)) + \lambda \sum_i^L \left(1 - \sum_t^C \alpha_{ti} \right)^2. $$</p>

<h3>Experiment</h3>
<p>다음 그림은 실제 hard attention과 soft attention이 어떻게 동작하는지 잘 보여주는 그림이다. 가장 왼쪽의 주어진 이미지에 대해 soft attnetion과 hard attention 모두 &#8216;a bird flying over a body of water.&#8217; 이라는 caption을 생성했는데, soft와 hard attention 각각 경우에 대해 caption generation 모델이 어느 곳을 attend하도록 동작했는지 표현되어있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/93-5.png" width="600" /></p>

<p>위의 그림이 deterministic하게 attention을 계산하는 soft attention이다. 정확한 &#8216;attention location&#8217;이 존재하는 것이 아니라, 하얗게 mapping된 부분을 전반적으로 attend한다고 생각하면 된다. 반면 아래에 있는 hard attention을 보면, 매 번 정확한 attention location을 고르는 것을 알 수 있다. 이 과정을 매 번 확률적으로, sampling based approximaiton을 취하기 때문에 hard attention model은 stochastic machanism이다.</p>
<p>다음 그림을 통해, 실제 각 word 별로 어느 곳을 attend하는지 대략적으로 확인할 수 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/93-6.png" width="600" /></p>

<p>Attend하는 위치가 비교적 굉장히 정확한 곳을 고르는 것을 알 수 있다. 좀 더 많은 예시가 gif로 <a href="http://kelvinxu.github.io/projects/capgen.html">이 링크</a>에 업로드되어있으니 참고하면 좋을 것 같다. 이 논문은 성공했을 때 뿐 아니라 실패했을 경우의 attention도 아래 그림과 같이 기재하여두었다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/93-7.png" width="600" /></p>

<p>이 사진들을 통해 저자들은, 이 모델이 attend하는 위치는 잘 골랐으나, 각 object를 조금 잘못 classification했기 때문에 실패하는 경우가 나온다고 주장하고 있다. 예를 들어 위 실패 경우를 보면 바이올린을 스케이트 보드라고 했거나, 돛을 서핑보드라고 분류하여 잘못된 caption 결과가 나왔음을 알 수 있다.</p>
<p>좀 더 많은 이미지 데이터셋에 대해 hard attention과 soft attention을 사용한 caption generator의 성능은 다음 표에 잘 나타나있다. 점수는 높을수록 좋다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/93-4.png" width="600" /></p>

<p>기존에 알려진 모델들에 비해 attention based model들의 성능이 훨씬 좋은 것을 알 수 있다.</p>

<h3>Summary of Show, Attend and Tell</h3>

<ul>
  <li>Image caption 문제는 상당히 어려운 문제이며, 기존 deep learning 기법들은 machine translation에서 사용하는 encoder-decoder concept를 사용해 문제를 해결한다.</li>
  <li>이 논문 역시 encoder-decoder 개념을 사용하지만, decoder에서 attention이라는 개념을 추가로 사용하여 새로운 모델을 제안하고 있다.</li>
  <li>Encoder는 CNN을 사용한다. 실제 실험에서는 CNN 모델로 VGG 네트워크를 선택하여 사용하고 있다.</li>
  <li>Decoder는 RNN, 정확히 말하면 LSTM을 사용하는데, 바로 전 state h, 바로 전 caption word y, 그리고 attention model을 통해 생성되는 context vector z가 LSTM cell의 input이 된다.</li>
  <li>Context vector z는 hard attention과 soft attention 두 가지 방법 중에 한 가지 방법을 선택하여 생성하게 된다. Hard attention은 stochastic machanism이고, soft attention은 deterministic machanism이다.</li>
  <li>Hard attention은 먼저 location variable s를 정의하고, 이것을 사용해 log-likelhood의 lower bound Ls를 계산한다. Ls를 optimization하기 위해 gradient를 구해야하는데, 이 값을 정확하게 구하는 것이 까다롭기 때문에 Monte Carlo based sampling approximation을 사용해 문제를 해결하게 된다. 이 update rule은 reinforcement learning의 update rule과 일치한다.</li>
  <li>Soft attention은 매 iteration마다 sampling을 하는 대신, s의 확률 alpha를 직접 사용하여 z를 계산한다.</li>
  <li>Attention based caption generation model은 기존 image caption generation 모델들에 비해 훨씬 좋은 성능을 보인다.</li>
</ul>

<h3>Reference</h3>
<ul>
	<li><p><a href="http://arxiv.org/abs/1406.6247">Mnih, Volodymyr, Nicolas Heess, and Alex Graves. &#8220;Recurrent models of visual attention.&#8221; Advances in Neural Information Processing Systems. 2014.</a></p></li>
	<li><a href="http://kelvinxu.github.io/projects/capgen.html">http://kelvinxu.github.io/projects/capgen.html</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[고흐의 그림을 따라그리는 Neural Network, A Neural Algorithm of Artistic Style (2015)]]></title>
    <link href="http://SanghyukChun.github.io/92/"/>
    <updated>2015-10-14T00:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/92</id>
		<content type="html"><![CDATA[<p>얼마 전, <a href="http://arxiv.org/abs/1508.06576 ">&#8216;A Neural Algorithm of Artistic Style &#8217;</a> 이라는 이름의 충격적인 논문이 arXiv에 업로드되었다. 기술 전문 잡지나 신문이 아닌 스브스뉴스 같은 일반적인 기사를 보도하는 매체에서도 보도가 되었을 정도로 요즘 꽤 이슈가 되고 있는 논문이다.</p>

<ul>
  <li><a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: “이건 ‘반 고흐’의 그림이 아닌 ‘컴퓨터’의 그림입니다.”</a></li>
  <li><a href="http://www.dailymail.co.uk/sciencetech/article-3214634/The-algorithm-learn-copy-artist-Neural-network-recreate-snaps-style-Van-Gogh-Picasso.html">DailyMail: “The algorithm that can learn to copy ANY artist: Neural network can recreate your snaps in the style of Van Gogh or Picasso”</a></li>
  <li><a href="http://www.theguardian.com/technology/2015/sep/02/computer-algorithm-recreates-van-gogh-painting-picasso">The Guardian: “Computer algorithm recreates Van Gogh painting in one hour”</a></li>
</ul>

<p>자고로 백문이 불여일견이라, 이게 도대체 무슨 contribution이 있길래 사람들의 이목이 쏠리고 있는지 논문에 첨부되어있는 그림을 먼저 보자.</p>

<p><img src="http://SanghyukChun.github.io/images/post/92-1.jpg" width="600" /></p>

<p>이 그림들은 모두 사람이 그린 것이 아니라 neural network를 사용하여 generate한 것이다. 이 논문은 제목 그대로, &#8216;artistic style&#8217;을 learning하는 neural network algorithm을 제안한다. 여기에서 artistic style이라는 것을 어떻게 정의하였는지는 나중에 조금 더 자세히 살펴보도록하자. 위 그림은 이 논문에서 제안한 알고리즘을 사용하여 report한 결과이다. 원본이 되는 A가 독일의 튀빙겐이라는 곳에서 찍은 &#8216;사진&#8217;이다. B부터 F는 유명한 거장들의 그림 &#8216;style&#8217;과 A의 &#8216;content&#8217;를 가지는 그림을 generate한 결과이다. 순서대로 B는 J.M.W. 터너의 &#60;미노타우르스 호의 난파&#62;, C는 그 유명한 빈센트 반 고흐의 &#60;별이 빛나는 밤&#62;을, D는 뭉크의 &#60;절규&#62;, E는 피카소의 &#60;앉아 있는 나체의 여성&#62;, F는 칸딘스키의 &#60;구성 VII&#62;이다. 놀랍게도 알고리즘을 통해 얻은 그림은 원본 사진의 content는 거의 그대로 보존하면서, 동시에 다른 그림의 style을 특징을 잘 살려서 가지고 있다.</p>
<p>이 짧은 논문이 사람들에게 얼마나 큰 충격을 주었는지는 길게 적지 않아도 알 수 있을 것이라고 생각한다. 논문이 나오고 얼마 지나지 않아 <a href="https://github.com/jcjohnson">jcjohson</a> 이라는 github user가 torch 기반으로 만든 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>이라는 프로젝트를 github에 공개하였다. 논문이 정말 좋은 결과를 낸 것인지 사람들이 이런 저런 사진과 그림들을 사용해 실험해본 결과, 논문에서 이야기하는 것 처럼 실제로 아무 사진을 적당히 골라서 적당한 그림을 넣어주면 사진의 내용은 보존한 채로 질감만 바꿔서 출력해주는 것을 알 수 있었다. 아래 그림은 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>을 사용해 금문교 사진과 여러 예술가들의 그림을 사용해 generate한 결과이다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/92-3.png" width="600" /></p>

<p>이 논문이 나온게 9월 말이었는데, 벌써 한국 개발팀에서 스마트폰 app까지 개발했을 정도로 관심이 뜨겁다. (<a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: 이건 &#8216;반 고흐&#8217;의 그림이 아닌 &#8216;컴퓨터&#8217;의 그림입니다.</a>)</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/92-4.jpg" width="300" /></p>

<p>꽤나 흥미로운 논문인 만큼, 어떤 아이디어를 사용했고, 어떤 방법론을 사용했는지까지 한 번 차근차근 살펴보도록 하자.</p>

<h3>Content &#38; Style Reconstruction using CNN</h3>

<p>Deep learning이 지금처럼 급부상하게 된 배경에는 (비전 분야를 중심으로 한) <a class="red tip" title="Convolutional Neural Network">CNN</a>의 엄청난 힘이 있었다. CNN이 비전에서 월등한 성능을 내는 이유를 여러가지로 설명할 수 있겠지만, 일반적으로는 CNN은 각각의 layer가 &#8216;feature&#8217;의 의미를 지니기 때문이라고 설명한다. 각각의 layer가 feature를 생성해내고, 이 feature들이 hierarchy하게 쌓이면서 더 높은 layer로 갈수록 더 좋은 feature를 만들어낸다는 것이다. CNN은 이 feature를 hard-coding하여 뽑아내는대신, 데이터에서부터 &#8216;가장 좋은&#8217; 최종 feature를 만들도록 학습시키기 때문에 아주 좋은 feature를 사용해 perceptron 등의 간단한 classifier로 높은 performance를 얻게 되는 것이다 (CNN에 익숙하지 않다면 <a href="http://SanghyukChun.github.io/75/#75-cnn">CNN에 대해 설명했었던 이전 글</a>을 참고하면 좋을 것 같다). 그렇기 때문에 각각의 convolution layer의 output은 흔히 feature map으로 표현이 된다.</p>
<p>주어진 이미지에서 feature를 뽑아내는 것은 CNN을 통하여 지금까지 항상 하던 일이었다. 그렇다면 반대로 할 수도 있지 않을까? 즉, CNN의 중간 feature map을 사용하여 원래 이미지를 복원하는 작업을 하는 것이다. 이렇게 feature map에서부터 이미지를 reconstruction 할 수만 있다면, deep CNN에서 layer를 지면서 어떤 재미있는 일들이 벌어지고 있는지 사람이 직접 눈으로 확인할 수 있을 것이다. 이런 visualization에 대한 motivation 때문에 그 동안 CNN의 convolution layer에서 원래 이미지를 reconstruction하는 작업들은 꾸준하게 제안되어 왔다. 그 중 가장 유명한 work으로 다음과 같은 work이 있다.</p>

<ul>
  <li><a href="http://arxiv.org/abs/1412.0035">Mahendran, Aravindh, and Andrea Vedaldi. “Understanding deep image representations by inverting them.” arXiv preprint arXiv:1412.0035 (2014).</a></li>
</ul>

<p>이 논문은 주어진 feature map에서 image를 복원하는 방법을 제안한다. 단순히 이미지를 복원하는 것이 아니라, 이미지를 특정 목적에 맞게 변형하는 work도 진행되어왔다. 대표적인 예가 아래 논문과 Google DeepMind의 <a href="http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html">Deep Dream</a>이다.</p>

<ul>
  <li><a href="http://arxiv.org/abs/1412.1897">Nguyen, Anh, Jason Yosinski, and Jeff Clune. “Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.” arXiv preprint arXiv:1412.1897 (2014).</a></li>
</ul>

<p>이 논문은 deep CNN이 거의 100% 확률로 오답을 발생시키도록 이미지를 조작한 work이다. 예를 들어 주어진 이미지가 &#8216;새&#8217; 라는 label을 가지고 있다고 판별했을 때, &#8216;문어&#8217;라는 label을 100% 로 가지도록 이미지를 조작하는 것이다. 사람이 봤을 때는 여전히 &#8216;새&#8217; 사진이지만, CNN은 &#8216;문어&#8217;라고 판별해버리는 것이다.</p>
<p>이렇듯, 다양한 목적으로 CNN이 이미 주어져 있을 때, 특정 목적에 따라 이미지를 update하는 방법론들은 이미 예전부터 연구가 계속 진행되어왔다. 위에 링크한 3개의 work은 꽤 흥미로운 주제들이기 때문에 나중에 또 따로 포스팅할 수 있도록 하겠다. 이 논문의 contribution은 각 convolution layer에서부터 style과 content를 reconstruct하는 방법론을 제안했다는 것이다. 이 방법은 앞에서 언급한 <a href="http://arxiv.org/abs/1412.0035">Understanding deep image representations by inverting them</a> 논문 처럼 현재 feature map에서 원래 이미지를 최대한 복원하는 content reconstruction과, 아래 논문 등에서 제안되어왔던 texture 분석과 생성 등을 복원하는 texture reconstruction을 결합한 것이다. 참고로 이 work들은 맨 처음 논문을 제외하면 neural network 기반 work은 아니다 (첫 번째 논문은 이 논문을 작성한 연구팀이 이 논문을 arXiv에 올리기 3개월 전에 arXiv에 올린 다른 논문이다). 자세한건 뒤에서 더 다루도록하자.</p>

<ul>
  <li><a href="http://arxiv.org/abs/1505.07376">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks.” arXiv preprint arXiv:1505.07376 (2015).</a></li>
  <li><a href="http://www.cns.nyu.edu/heegerlab/content/publications/Heeger-siggraph95.pdf">Heeger, David J., and James R. Bergen. “Pyramid-based texture analysis/synthesis.” Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. ACM, 1995.</a></li>
  <li><a href="http://dl.acm.org/citation.cfm?id=363108">Portilla, Javier, and Eero P. Simoncelli. “A parametric texture model based on joint statistics of complex wavelet coefficients.” International Journal of Computer Vision 40.1 (2000): 49-70.</a></li>
</ul>

<p>이 논문에서 제안하는 두 가지 reconstruction 방법을 CNN의 각 layer에 대해 적용해보면 다음과 같은 결과를 얻을 수 있다. 참고로 이 논문에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a>를 선택했다. 이 네트워크는 총 16개의 convolution layer와 3개의 fully connected layer로 이루어져있다. 이 네트워크에 대한 설명은 method 부분에서 더 자세히 다루도록 하겠다.</p>

<p><img id="92-reconst-img" src="http://SanghyukChun.github.io/images/post/92-5.png" width="600" /></p>

<p>위 그림은 CNN 하나에서 서로 다른 두 가지 방법으로 각각 style과 content를 layer 별로 reconstruction한 결과이다. 하나의 같은 CNN에 대해 두 가지 다른 reconstruction을 진행한 것인데, 위쪽 그림은 고흐의 &#60;별이 빛나는 밤&#62;의 style을 layer 별로 reconstruction한 것이고, 아래 그림은 튀빙겐에서 찍은 사진의 content를 layer 별로 reconstruction한 것이다.</p>
<p>먼저 style reconstruction에서 알 수 있는 것은 layer가 얕을수록 원래 content 정보는 거의 무시하고 &#8216;texture&#8217;를 복원한다는 것이다. 반면 깊은 layer로 가게 될수록 점점 원래 content 정보가 포함이 되는 것을 볼 수 있다. 이런 현상이 발생하는 이유는 이 논문에서는 style을 같은 layer에 있는 feature map들 간의 correlation으로 정의하기 때문이다. 이를 구체적으로 어떻게 수학적으로 정의하였는지는 뒤에서 좀 더 자세하게 살펴보도록 하자. Style을 correlation으로 생각하기 때문에, 가장 style이 복원이 잘되는 얕은 layer에서는 원본 content가 거의 무시되고 correlation을 가장 좋게하는 style만 나오는 것이고, 깊은 layer로 갈수록 style이 제대로 복원이 되지 않을 것이므로 원본 content의 정보가 증가해 correlation이 작아지는 결과를 얻게 되는 것이다.</p>
<p>다음으로 content reconstruction을 보자. 이 그림을 통해 낮은 level의 layer는 거의 완벽하게 원본 이미지를 보존하고 있고 layer가 깊어질수록 원본 이미지의 정보는 조금씩 소실되지만, 가장 중요한 high-level content는 거의 유지가 되는 것을 볼 수 있다.</p>
<p>이 논문은 같은 CNN이라고 할지라도 content와 style에 대한 representation이 분리가 되어있다는 것을 중요하게 언급하고 있다. 그렇기 때문에 같은 network을 사용하여 서로 다른 이미지에서 서로 다른 content와 style을 reconstruction해서 그 둘을 섞는 것이 가능한 것이다. 이것이 중요한 이유는 실제로 reconstruction을 하는 과정은 임의의 image를 input으로 삼고, image를 parameter로 하여 목표하는 style과 content에 대한 loss를 minimize하는 optimization 과정이기 때문이다. 이 두 가지 다른 optimization process를 오직 하나의 network만 사용하여 진행할 수 있기 때문에 $A$(혹은 튀빙겐에서 찍은 사진)이라는 input의 content를 가지면서 $B$(혹은 고흐의 &#60;별이 빛나는 밤&#62;)이라는 input의 style을 가지도록하는 방향으로 input 이미지의 gradient를 구할 수 있는 것이다. 수식으로 나타내보자. input image를 $x$라고 해보자. 우리 목표는 $x$와 $A$ 간의 content가 얼마나 다른지 표현하는 loss function $\mathcal L_{content} (x,A)$와 $x$와 $B$ 간의 style이 얼마나 다른지 표현하는 loss function $\mathcal L_{style (x,A)}$를 minimize하는 $x$를 찾는 것이다. 따라서 우리가 풀고 싶은 optimization problem은 다음과 같다.</p>
<p>$$x = \arg\max_x \alpha\mathcal L_{content} (x,A) + \beta\mathcal L_{style} (x,B)$$</p>
<p>이런 식으로 식을 쓸 수 있을 것이다 ($\alpha$와 $\beta$는 적당한 상수라고 하자). 이런 optimization을 푸는 가장 간단한 방법으로 $x$에 대한 gradient를 구하고 gradient descent optimization을 하는 것인데, 두 loss를 같은 network에 대해 design할 수 있기 때문에 gradient가 간단해지는 것이다. 그러면 이제 구체적으로 어떻게 각각의 loss가 정의되었는지 살펴보자.</p>

<h3>Methods</h3>
<p>이 paper에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a> 네트워크를 사용한다. 이 네트워크는 옥스포드의 VGG(Visual Geometry Group)에서 만든 네트워크로, <a href="http://SanghyukChun.github.io/88">Batch Normalization</a>이 적용되기 이전 inception network (혹은 GoogleNet) 등에 비해 꽤 우수한 성능을 보이는 네트워크이다. 아래 논문을 통해 발표하였다.</p>

<ul>
  <li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>

<p>자세한 method를 설명하기에 앞서 VGG 19 네트워크 자체에 대해 다뤄야하는데, 이 네트워크는 총 16개의 convolution layer, 5개의 pooling layer, 3개의 fully connected layer로 구성되어있다. 이 논문은 제공된 16개의 convolution layer에서 생성되는 feature map을 사용해 style loss와 content loss를 계산한다. 이 네트워크는 다음과 같은 형태로 구성되어있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/92-6.png" width="400" /></p>

<p>이 논문에서 사용한 VGG 19는 E에 해당하며, conv 2개 - pooling - conv 2개 - pooling - conv 4개 &#8230; 이런 식으로 구성되어있다. 각각의 conv layer들은 pooling layer를 기준으로, 순서대로 conv 1_1, conv 1_2, conv 2_1, conv 2_2, conv 3_1, conv 3_2, conv 3_3, &#8230; conv 5_4 라는 이름을 가지고 있다. 즉, conv 5_1 이면 4번쨰 pooling layer 바로 다음 conv layer를 말하는 것이다.</p>
<p>이 논문에서는 fully connected layer는 사용하지 않고, 16개의 conv layer와 5개의 pooling layer만 사용하는데, image reconstruction에 있어서는 max pooling보다는 average pooling을 고르는 것이 그림이 조금 더 자연스럽고 좋아보이는 결과로 나오기 때문에 max pooling 대신 average pooling을 사용하였다고 한다.</p>

<p>그럼 먼저 비교적 간단한 content loss 부터 살펴보도록하자. 이 논문은 feature map을 $F^l \in \mathcal R^{N_l \times M_l}$으로 정의하였다. 이때 $N_l$은 $l$ 번째 레이어의 filter 개수이고, $M_l$은 각각의 filter의 가로와 세로를 곱한 값이며, 즉 각 filter들의 output 개수이다. 또한 $F^l_{ij}$는 $i$ 번째 필터의 $j$ 번째 output을 의미하게 된다. 이제 우리가 비교하려는 두 가지 이미지를 각각 $p$와 $x$라 하고, 각각의 $l$ 번째 layer의 feature representation을 $P^l, F^l$로 정의하자. 이렇게 정의하였을 때, $l$ 번째 layer의 content loss는 다음과 같이 간단하게 정의된다.</p>
<p>$$ \mathcal L_{content} (p, x, l) = \frac{1}{2} \sum_{ij} \big( F^l_{ij} - P^l_{ij} \big)^2. $$</p>
<p>즉, $p$와 $x$에 대해 각각 feature map $P^l, F^l$을 계산하고, 이 둘의 차의 Frobenius norm ($\| P^l - F^l \|_F$)을 loss로 선택한 것이다. 이 error를 각각의 layer에 대해 따로 정의하게 된다. 이제 튀빙겐에서 찍은 사진 $p$의 $l$ 번째 layer의 representation을 사용해 image reconstruction을 한다고 가정해보자. 이때 $l$ 번째 layer에서 복원한 이미지를 $x^l$라고 하자. 앞서 정의한 loss를 minimize하는 $x^l$를 찾아야하므로, 우리는 다음과 같은 식을 얻는다.</p>
<p>$$x^l = \arg\max_x \mathcal L_{content} (p, x, l). $$</p>
<p>이 식을 풀기 위한 가장 간단한 방법은 $x^l$을 random image로 initialize하고 $\frac{\mathcal L_{content} (p, x, l)}{x}$를 게산해 gradient descent method를 사용하는 것이다. Loss를 layer의 각각의 activation으로 미분한 결과는 다음과 같다.</p>
<p>$$ \frac{\partial \mathcal L_{content} (p, x, l)}{\partial F^l_{ij}} = (F^l_{ij} - P^l_{ij})_{ij} \mbox{ if } F^l_{ij} &gt; 0, \mbox{ otherwise, } 0.$$</p>
<p>이 값을 사용하면 전체 gradient를 back-propagation 알고리즘을 사용해 간단하게 계산할 수 있게 된다. <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 아래 부분에서 복원한 5개의 이미지는 각각 conv 1_1, conv 2_2, conv 3_1, conv 4_1, conv 5_1에서 loss를 계산하여 복원한 것이다.</p>

<p>다음으로, style에 대한 loss를 정의해보자. 이 논문에서 style이라는 것은 같은 layer의 서로 다른 filter들끼리의 correlation으로 정의한다. 즉, filter가 $N_l$개 있으므로 이것들의 correlation은 $G^l \in \mathcal R^{N_l \times N_l}$이 될 것이다. 이때, correlation을 계산하기 위하여 각각의 filter의 expectation 값을 사용하여 correlation matrix를 계산한다고 한다. 즉, $l$번째 layer에서 필터가 100개 있고, 각 필터별로 output이 400개 있다면, 각각의 100개의 필터마다 400개의 output들을 평균내어 값을 100개 뽑아내고, 그 100개의 값들의 correlation을 계산했다는 것이다. 이렇게 계산한 matrix를 Gram matrix라고 하며 $G^l_{ij}$라고 적으며 다음과 같이 계산할 수 있다.</p>
<p>$$ G^l_{ij} = \sum_{k} F^l_{ik} F^l_{kj}.$$</p>
<p>두 개의 image $a$와 $x$ 간의 style이 얼마나 다른지를 나타내는 style loss $\mathcal L_{style}$은 $G^l_{ij}$를 사용하여 다음과 같이 정의된다.</p>
<p>$$ \mathcal L_{style} (a,x) = \sum_{l=0}^L w_l E_l $$</p>
<p>$L$은 loss에 영향을 주는 layer 개수, $w_l$은 전부 더해서 1이 되는 weight이고, $E_l$은 layer $l$의 style loss contribution이다. 이 값은 다음과 같이 정의된다.</p>
<p>$$ E_l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} \big( G^l_{ij} - A^l_{ij} \big)^2. $$</p>
<p>역시 마찬가지로, $p$, 혹은 고흐의 &#60;별이 빛나는 밤&#62;의 layer 별 style reconstruction 역시 이 $\mathcal L_{style} (a,x)$를 minimize하는 $x$를 찾는 것으로 풀 수 있으며 이 문제는 back-propagation algorithm으로 풀 수 있다.</p>
<p>$$ \frac{\partial \mathcal E_l}{\partial F^l_{ij}} = \frac{1}{N_l^2 M_l^2} \sum_{i,j} \big( \big(F^l)^\top \big( G^l_{ij} - A^l_{ij} \big)\big)_{ji} \mbox{ if } F^l_{ij} &gt; 0 \mbox{ otherwise } 0. $$</p>
<p>다시 한 번 <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 윗 부분에서 복원한 이미지를 살펴보면, 순서대로 loss 계산을 위해 conv 1_1만 사용하여 복원한 그림, conv 1_1, conv 2_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1, conv 4_1을 선택한 그림 &#8230; 이런 식으로 선택하여 복원을 한 그림이다. 이때 &#8216;선택&#8217; 한다는 것의 개념은 선택한 layer의 $w_l$의 값을 0이 아닌 같은 값으로 두고 나머지는 전부 0으로 설정하는 것이다. 예를 들어 c 그림은 4개만 영향을 주므로 conv 1_1, conv 2_1, conv 3_1, conv 4_1만 $w_l = 0.25$이고 나머지는 0이다.</p>
<p>이제 마지막으로 이 두 가지 loss를 한 번에 optimization하는 과정만 남았다. $\alpha$와 $\beta$는 content와 style 중 어느 쪽에 더 초점을 둘 것인지 조정하는 파라미터로, 보통 $\alpha/\beta$으로 $10^{-3}$이나 $10^{-4}$ 정도를 고른다고 한다.</p>
<p>$$\mathcal L_{total} (p,a,x) = \alpha \mathcal L_{content} (p, x) + \beta \mathcal L_{style} (a,x)$$</p>
<p>논문에서는 style에 얼마나 많은 layer를 고려하는지에 따라, 그리고 $\alpha/\beta$의 값을 조정함에 따라 다음과 같이 결과가 달라진다고 report하고 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/92-7.png" width="600" /></p>

<p>x 축이 $\alpha/\beta$, y축은 순서대로 앞에서처럼 conv 1_1, conv 2_1, conv 3_1, conv 4_1, conv 5_1을 선택한 것이다 (A: 1_1, B: 1_1, 2_1, C: 1_1, 2_1, 3_1, &#8230;) 이 값들을 어떻게 조정하느냐에 따라 style과 content의 적당한 trade-off를 조정할 수 있다. Layer를 더 많이 사용할수록, 그리고 $\alpha/\beta$ 값이 작아질수록 content보다는 style에 더 치중된 결과가 나오게 된다. 그리고 당연히 layer를 더 적게 사용하거나 $\alpha/\beta$의 값을 키울수록 그 반대의 결과가 나오게 된다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/92-2.png" width="600" /></p>
<p>위 그림은 앞에서 언급한 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>을 사용해 만든 그림이다. 원본 content 이미지로 브래드 피트의 사진을 넣고, style 이미지로 피카소의 &#60;자화상&#62;을 넣은 다음, $\alpha/\beta$ 값을 조정하면서 값이 변하는 것을 관측한 것이다.</p>
<p>맨 처음 글을 시작하며 보았던 그림에서는, content representation은 conv 4_2의 것만을 사용하고, style representation은 conv 1_1, 2_1, 3_1, 4_1, 5_1 에 각각 $w_l = 1/5$, 나머지는 $w_l=0$으로 하여 사용했다. 또한 B,C,D 그림은 $\alpha/\beta = 10^{-3}$, E,D 그림은 $\alpha/\beta = 10^{-4}$를 사용하였다고 한다.</p>

<h3>Comments</h3>

<ul>
  <li>개인적인 생각으로는, 이 논문의 결과는 고흐나 뭉크 등의 ‘스타일’이 분명한 인상주의, 표현주의, 야수파 화풍의 화가들의 그림을 더 잘 generate할 것으로 생각된다. 나중에 사람들이 실험해본 결과도 그렇고, 대체로 고흐 등의 경우 스타일이 특색이 뚜렷해서 그러한지 꽤 그럴싸한 결과가 나오는 반면, 피카소 등으로 대변되는 입체파 처럼 ‘스타일’을 넘어서는 그 무언가가 존재하는 경우 기대만큼 좋은 결과로 이어지는 것 같지는 않다. 원래 이 논문에서 제안하는 알고리즘의 목적 자체가 그림의 texture를 learning하여 content는 유지한 상태로 texture만 변경시키는 것이므로, 내용 자체가 변화하는 입체파 등의 독특한 그림을 제대로 따라하는 것은 불가능하기 떄문에 그런 것으로 보인다. (+ 글을 쓰면서 개인적으로 궁금해진게, 캐리커쳐는 어떻게 반응할지 궁금해졌다. 나중에 public하게 공개된 코드를 사용해서 실험해봐야겠다)</li>
  <li>왜 method에서 전체 conv layer를 사용하는 것이 아니라 일부만 사용하는 것인지 다소 아리송하다. 또한 왜 style reconstruction을 위해 conv 1<em>1, 2</em>1, … 5<em>1 의 정보만 사용했고, content는 왜 conv 4</em>2를 사용하였는지 역시 의아하다. 아마 제일 잘 되는 것을 골랐을텐데, 왜 그것들이 제일 잘되는 것일까 궁금해진다.</li>
  <li>CNN에 대한 이해가 충분히 있어야 쉽게 읽을 수 있는 논문이었다. 추가로 VGG network에 대한 이해도도 있으면 도움이 되는 것 같다. 맨 처음 논문을 읽을 때는 이런 것들에 대해 감이 좀 약해서 읽어도 이해하기가 어려웠는데, CNN 공부를 다시 끝내고 다른 선행 연구들을 적당히 이해한 채로 다시 읽어보니 쉽게 이해할 수 있었다.</li>
</ul>

<h3>Summary of A Neural Algorithm of Artistic Style</h3>

<ul>
  <li>CNN의 conv layer가 feature map이라는 것에서부터 착안하여, feature map에서 style과 content를 reconstruct하는 optimization problem을 제안하였다.</li>
  <li>하나의 CNN에서 content와 style representation이 separable하므로 style과 content를 한 번에 update하는 알고리즘을 만들 수 있다.</li>
  <li>Content loss는 두 이미지 각각의 feature matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 4_2 만 사용하였다.</li>
  <li>Style loss는 두 이미지 각각의 Gram matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 1<em>1, 2</em>1, 3<em>1, 4</em>1, 5_1 만 사용하였다.</li>
  <li>이때 style loss가 Gram matrix가 되는 이유는 style을 한 레이어 안에 있는 filter들의 correlation으로 정의했기 때문이다. 이때 correlation 계산은 각각의 filter들의 expectation 값들을 사용한다.</li>
  <li>VGG 19 네트워크를 사용했으며, FC layer는 제거하고 max pooling 대신 avg pooling을 사용하였다.</li>
  <li>Content loss와 Style loss의 비율을 조정하여 style과 content 중에서 어느 것에 집중할지 선택할 수 있다. 논문에서는 0.001 정도를 사용하였다.</li>
</ul>

<h3>Reference</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1508.06576">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “A Neural Algorithm of Artistic Style.” arXiv preprint arXiv:1508.06576 (2015).</a></li>
  <li><a href="https://github.com/jcjohnson/neural-style">‘neural style’</a></li>
  <li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Basic Principles in Deep Neural Networks]]></title>
    <link href="http://SanghyukChun.github.io/54/"/>
    <updated>2015-09-29T17:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/54</id>
		<content type="html"><![CDATA[<h3>들어가기 전에</h3>
<p>이 포스트는 2014년 6월 16일 카이스트에서 당시 Yoshua Bengio 교수님 연구실에서 포닥 과정을 밟고 계신 장민석 박사님의 The Basic Principles in Deep Neural Networks 라는 이름의 세미나를 요약한 내용이다. 내용은 주로 Deep learnining을 supervised learning, unsupervised learning의 관점에서 각각 바라보면서 어떤 컨셉들이고, 어떤 연구들이 진행이 되어있는지 훑어보는 정도의 간단한 내용이었다.</p>
<p>어쩌다보니 1년 넘게 포스팅을 못하다가 이제와서 포스트를 등록하게 되었는데, 이 글을 제대로 정리할 정도로 여유가 없기도 했고, 내가 이 내용을 이해할 수 있을 정도의 내공이 없기 때문이기도 했다. 지금은 어느 정도 여유가 생기기도 했고, 내가 내용을 대략이나마 이해하고 있기 때문에 일 년 전 내용이기는 하지만, 다시 한 번 내용을 정리해서 올려본다.</p>
<p>이 세미나는 크게 네 가지 파트로 나뉘어진다. 먼저 Deep learning이 무엇인지 간단한 introduction part, supervised deep learning, unsupervised deep learning, 마지막으로 아직 연구가 진행 중인 advanced topic이다. 이 글에서는 introduction과 supervised learning part에 대해서 주로 다루고, 나머지 부분에 대해서는 간단하게 훑고 지나가기만 하도록 하겠다.</p>

<h3>Part 1 - Introduction</h3>
<p>Deep Learning이 무엇인지 알기 전에 먼저 Machine Learning이 무엇인지 알 필요가 있다. Machine Learning에 대해 보다 깊게 알고 싶다면 내가 아직 계속 작성 중인 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning Study</a> 글들을 읽어보아도 좋고, 다른 좋은 글들을 참고해도 좋을 것 같다. 이 세미나에서는 Machine Learning이 주 주제가 아니기 때문에, 머신러닝이라는 것을 &#8216;데이터를 통해 모델을 learning하고 learning한 모델을 사용해 주어진 query에 대답하는 것&#8217;이라고 정의하였다. 결국 내가 <a href="http://SanghyukChun.github.io/57">예전 글</a>에서 아래 그림에서 정의했던 것과 크게 차이는 없다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/57-2.png" width="500" /></p>

<p>Deep Learning에 대해 설명하기 전에 먼저 perception이라는 문제에 대해 살펴보자. Perception이란 우리말로 하면 &#8216;인지&#8217; 정도로 해석할 수 있다. 이 문제는 주어진 정보에 대해 내가 필요한 어떤 특정 정보를 inference 하는 것이다. 예를 들어서 이 포스트의 글자가 무엇인지 읽어들이는 문제는 간단한 문제이지만, 이 문장들을 바탕으로 어떤 의미를 가지고 있는지 inference하는 것은 어려운 문제이다. 또 다른 예로는 vision 데이터를 하나 주고 주어진 vision data가 어떤 object인지 classification하는 문제도 perceptron이다. 이 문제는 사람에게는 아주 간단한 문제이지만 컴퓨터에게는 엄청나게 어려운 문제이다. Deep learning은 사람이 perception하는 방식을 모방하여, 사람만큼 perception을 해보자는 취지로 만들어진 model이라고 생각할 수 있다. 사람은 뇌의 neuron과 synapse 등으로 대표되는 일어나는 일렬의 화학적, 전기적 신호 전달 과정을 통해 perception을 하게 된다. 이것을 수학적 모델로 표현하고, 그것을 조금 deep하게 만든 것이 deep learning이다. (Neural Network와 이에 대한 intuition을 조금 더 자세히 알고 싶다면 내가 쓴 <a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a>과, <a href="http://SanghyukChun.github.io/75">Deep Learning 1</a>을 읽어보기를 권한다.)</p>
<p>Deep Learning이라는 분야는 최근 10년 동안 엄청나게 hot해진 분야이다. 그런데 사실 deep learning이라는 분야, 혹은 neural network는 사실 4-50년도 넘은 엄청 오래된 분야이다.</p>

<ul>
  <li>1958 Rosenblatt proposed perceptrons</li>
  <li>1980 Neocognitron (Fukushima, 1980)</li>
  <li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
  <li>1985 Boltzmann machines (Ackley et al., 1985)</li>
  <li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
  <li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
  <li>1993 Sparse coding (Field, 1993)</li>
</ul>

<p>즉 우리가 지금 쓰고 있는 Neural network의 기본적인 연구는 이미 90년대 이전에 다 끝나있었다. (심지어 1995년에 Machine Learning에 어마어마한 연구 결과를 남긴 Vapnik과 Jackel이 10년 뒤인 2005년에 아무도 Neural net을 쓰고 있지 않을 것이라고 내기를 했을 정도라고 한다) 근데 갑자기 Deep learning이 왜 이렇게 hot해졌을까? 이는 내가 <a href="http://SanghyukChun.github.io/75">Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN</a> 글에서 자세히 다뤘으므로 이 글에서는 간단하게 결론만 말하도록 하겠다. 그 이유는 Deep Learning이 ImageNet에서 기존 결과를 거의 박살을 냈기 때문이다. Deep Learning이 나오기 전에는 error가 0.27 ~ 0.30정도, 그러니까 27%에서 30% 정도였다고 한다. 기존 computer vision 연구자들은 이 정도 결과가 거의 한계치라고 여겨지고 있었는데 갑자기 2012년에 Deep convolutional neural network를 하는 팀이 0.153으로 거의 2배 가까운 성능 향상을 보여주었다고 한다. 그리고 그 다음 해에는 상위 20개 팀에서 2개 팀 빼고 전부 Deep learning을 써서 Classification을 했는데, 최고 성능은 또 0.117로 개선되었다고 한다. 그 결과 computer vision을 비롯한 기존 학계에서 엄청나게 주목을 받게 되었다고 한다.</p>
<p>Deep learning에 주목하는 것은 학계만이 아니다. Deep learning을 기본 기술로 사용하는 스타트업도 엄청나게 늘어나고 있고, (<a href="http://techcrunch.com/2014/01/26/google-deepmind/">Deep learning 기술 회사인 Deep mind M 이상에 인수</a>, <a href="http://techneedle.com/archives/15662">Captcha 퍼즐 암호 99.8% 성공률로 해석</a>, <a href="http://www.technologyreview.com/news/525586/facebook-creates-software-that-matches-faces-almost-as-well-as-you-do/">사람의 얼굴 인식 능력을 상회하는 소프트웨어 개발</a>) 심지어 현재 구글이나 애플 등에서 음성 인식에 쓰는 알고리듬도 deep learning이다. 이렇듯 deep learning은 학계에서만 관심을 가지는 분야가 아니라 실제 산업에서도 아주 빠르게 적용되고 사용되고 있는 분야이기 때문에 더더욱 주목할만하다. 보통 학계에서 연구한 결과가 실제 산업에서 적용되기까지의 시간이 분야마다 조금씩 다른데, deep learning은 오히려 산업에서 먼저 개발하고 학계에 발표할 정도로 학계와 산업이 함께 집중하고 있는 분야이다.</p>
<h3>What is &#8216;Deep&#8217; Learning?</h3>
<p>기존 machine learning 문제를 푸는 방법은 크게 3단계로 구분 할 수 있다.</p>

<ol>
  <li>Feature Engineering: 주어진 데이터를 사용해 machine learning tool에서 사용할 수 있는 feature를 뽑아내는 과정. 이 과정은 machine learning이 아니며, domain knowledge와 engineer의 knowhow가 강하게 drive하는 과정</li>
  <li>Learning: 1에서 주어진 feature 데이터를 사용해 machine learning model을 train하는 과정</li>
  <li>Inference: 2에서 학습한 model을 사용해 새로운 데이터를 inference하는 과정</li>
</ol>

<p>첫 번째 단계는 machine learning은 아니지만, 실제 최종 performance에 큰 영향을 미치는 과정이다. 예를 들어 우리가 SVM을 사용해 image를 학습한다고 가정해보자. 100만 화소짜리 이미지를 사용해 learning을 해야하는 경우, 1번 과정이 없으면 우리는 엄청나게 high dimensional data를 사용해 SVM을 풀어야하지만, 이렇게 높은 dimension의 데이터를 사용하게 되면 <a href="http://SanghyukChun.github.io/59#59-4-cd">curse of dimensionality</a> 등의 문제로 인해 나쁜 performance를 얻게 될 확률이 크다. 그 밖에도 특정 pattern의 noise가 계속 등장하고 그 noise로 인해 outlier가 많이 생기는 등의 상황도 생길 수 있다. 특히 여러 multi media 데이터를 처리하기 위해서는 이 feature engineering이 중요한 문제이며, 단순히 feature를 잘 고르는 것 만으로도 어려운 모델을 사용하지 않고 가장 간단한 linear 모델만으로도 문제가 해결되는 경우도 많이 있다 (예: 지문 인식). 때문에 이 과정은 절대 무시할 수 없는 과정이지만 domain knowledge에 너무 크게 dependency가 있고, general purpose machine learning과 분리가 된다는 문제가 존재한다. 그러나 여기에서 중요한 점은, domain knowledge를 반영하는 것이 그렇지 않은 것에 비해 훨씬 더 우수한 결과를 낸다는 점이다.</p>
<p>그렇기 때문에 우리는 feature 역시 machine learning technique를 사용해 learning해보자는 idea를 제안할 수 있다. 예를 들어 앞에서 제안한 image같은 경우, PCA 등의 dimensionality reduction technique들을 사용한다면 더 낮은 차원의 데이터로 문제를 해결할 수 있다.</p>

<p>1-a. Feature Engineering: domain knowledge를 반영하여 representation learning의 input으로 사용할 feature를 생성하는 과정
1-b. Feature/Representation Learning: 1-a의 결과를 사용해 PCA 등의 unsupervised feature learning을 뽑아내는 과정
2. Learning: 생략
3. Inference: 생략</p>

<p>이 경우 feature extraction에서도 general machine learning 방법론을 적용할 수 있다는 장점이 존재하지만, 여전히 representation learning은 domain knowledge를 반영하지 못하기 때문에 domain knowledge와 general machine learning 간의 간극이 발생한다. 다시 말해서, PCA는 Image data의 특성을 살릴 수 없는 learning 모델이기 때문에, 결국 domain knowledge를 반영하기 위한 새로운 feature engineering 과정이 필요하다는 의미이다.</p>
<p>Deep learning은 이런 문제를 해결하기 위하여 아래와 같은 모델을 제안한다.</p>

<ol>
  <li>Jointly learning everything: 한 번에 모델 하나로 feature engineering, representation learning, model learning까지 끝내는 과정</li>
  <li>Inference</li>
</ol>

<p>우리가 domain knowledge를 반영하여 모델을 하나 설계한 다음, 나머지 feature engineering이나 representation learning 등의 과정을 한 번에 짬뽕해서 해결하자는 것이다. 이것이 가능한 이유는 deep learning 모델의 특성 때문이라고 할 수 있다. Neural network 모델은 아래 그림처럼 layer가 쌓여있는 형태로 구성이 되어있는데, 마치 각각의 layer를 feature extraction 과정으로 바꿔서 생각할 수 있다. 위로 올라갈수록 점점 우수한 feature를 뽑아내게 되고, 맨 마지막 layer에서 linear classifier를 learning하는 과정처럼 생각할 수 있는 것이다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/74-3.png" width="600" /></p>

<p>또한 중요한 것은, PCA 등의 unsupervised feature extraction을 결합한 경우에는 feature extraction이 unsupervised learning이기 때문에 데이터나 최종 output loss function에 영향을 받지 못하는데 반해서, deep learning model은 모든 것들을 하나의 loss function으로 한 번에 handle하기 때문에 모든 것들을 jointly learning한다고 표현할 수 있는 것이다. 이를 가장 잘 표현하는 말이 Yoshua Bengio 교수의 &#8220;Let the data decides&#8221;라고 할 수 있다. 어떤 모델을 써야 좋은 feature를 뽑을 수 있을까에 신경쓰지말고, 처음부터 deep learning 모델처럼 좋은 모델을 사용해 데이터에서부터 좋은 결과를 낼 수 있도록 데이터가 알아서 하도록 하라는 취지의 말인데, 개인적으로 이 얘기는 특히 CNN 모델에 잘 맞는 얘기라고 생각한다. <a href="http://SanghyukChun.github.io/75#75-cnn">예전 글</a>에서도 다뤘듯이 CNN은 모델은 vision 데이터의 특성을 최대한 활용하여 feature map을 만들어내는 것이 목적이다보니, convolution과 polling layer는 최대한 feature를 만들어 내는 과정으로 쓰이고, fully connected layer에서 해당 feature를 사용하여 classification을 하는 형태가 된다. 그렇기 때문에 한 번에 preprocessing 혹은 feature engineering part와 learning하는 part가 합쳐진 형태가 되는 것이 아닐까 추측해본다.</p>
<p>그러나 단순히 deep learning을 사용하면 feature extraction과 결합된 형태로 모델을 learning할 수 있다는 이유로 아무도 쓰지 않던 deep learning을 많이 쓰기 시작한 것은 아니다. Deep learning을 많이 연구하게 된 원인으로 박사님은 두 가지 이유를 꼽았는데, 하나는 서로 다른 분야라고 생각하면서 연구되었던 PCA, Neural PCA, Probabilistic PCA, Autoencoder, Belief Network, Restricted Boltzmann Machine 등의 분야가 사실은 서로 각자의 특수한 케이스이거나 혹은 다른 표현형이라는 것이 알려지면서 결국 한 분야로 수렴하였다는 것과, 또 하나는 예전에는 알려지지 않았던 것들이 이제는 많이 알려져서 예전에는 어렵게 접근했던 것들을 이제는 쉽게 learning할 수 있다고 한다. 그 중에서 특히 non-convex optimization에 대해 많은 기술들이 연구되어서 non convex optimization이기 때문에 optimization이 불가능하다고 두려워 할 필요가 없다는 것이 가장 큰 이유라고 한다. 또한 inference와 training사이의 interaction에 대해서도 더 많이 이해하고 있다는 점도 꼽을 수 있으며, GPU 등의 하드웨어 발전으로 인해 예전보다 computation power가 exponential하게 증가한 것도 그 중의 한 원인이라고 한다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/54-4.png" width="600" /></p>

<p>위의 그림은 각 종과 현재 개발된 NN들의 뉴런 개수를 비교한 것이다 (y 축 뉴런 개수의 스케일은 log scale이다). 맨 처음 DBN이 나올 때만 해도 편충보다 뉴런이 조금 많고 거머리보다 10배 적었다. 2012년 ImageNet에서 우수한 성과를 거둔 AlexNet의 neuron 개수는 개미보다 조금 많고 벌에 비해서 한참 적다. 그리고 2014년 기준으로 가장 큰 AdamNet의 뉴런 개수는 아직도 개구리의 뉴런 개수보다 적다. 그렇기 때문에 앞으로 사람이 가지고 있는 뉴런의 개수만큼 뉴런을 가지는 NN 모델이 개발되려면 많은 시간이 남았다고 전망하고 있다. 뉴런의 개수가 많을수록 좋은 모델이 된다는 것은 이미 알고 있지만 뉴런의 개수를 단순하게 많이 늘릴 수 없는 이유는 learning time이 엄청나게 오래걸리기 때문이다. 개인적으로는 이 부분이 아직까지도 deep learning이 발전할 여지가 많이 있다는 것을 의미한다고 생각한다. 왜냐하면 뉴런의 개수를 늘리는 일은 computation cost가 엄청나게 많이 드는 일이고, parameter 역시 exponential하게 증가하기 때문에 overfitting issue를 handle하는 것이 점점 더 중요해지기 때문이다. 더 이상의 regularization은 없다고 생각할 수도 있지만, 최근에 나온 <a href="http://SanghyukChun.github.io/88">Batch Normalization</a> work이 dropout 등의 기존 성과를 뛰어넘는 좋은 performance를 내고 있는 것을 보면, 충분히 더 좋은 접근 방법이 나올 수 있을 것이라고 믿는다.</p>

<h3>Part 2 - Supervised Neural Network</h3>
<p>Deep learning도 machine learning의 일종이기 때문에 supervised/unsupervised/reinforcement learning의 세 가지 접근 방법으로 바라보는 것이 가능하다. 이 part에서는 neural network로 supervised learning, 특히 classification을 어떻게 푸는지에 대해서 주로 다뤘었다. 세미나에서는 multilayer perceptron과 그것의 learning, regularization, 그리고 기타 등등에 대해 다뤘었지만, 이 글에서는 learning algorithm인 back-propagation에 대한 설명은 <a href="http://SanghyukChun.github.io/74#backprop">예전에 쓴 글의 링크</a>로 대체하고, 주로 어떻게 MLP를 regularization할 수 있는지 등에 대해서 다룰 것이다.</p>
<p>예전에도 설명했듯 neural network는 back-propagation이라는 알고리즘을 사용해 model parameter를 찾는다. 이 알고리즘은 any cost function이 주어졌을 때, 그 문제를 풀기 위한 gradient descent method를 chain rule을 사용해 간단하게 바꾼 알고리즘이라고 할 수 있다. 이 알고리즘을 사용하게 되면 모든 노드의 derivative를 전부 계산할 필요가 없고, 대신 매우 적은 양의 계산으로 마치 전체의 gradient를 계산한 것과 같은 효과를 얻을 수 있기 때문에 NN update는 거의 이 방법을 사용한다. 또한 모든 data에 대한 gradient를 계산하여 완벽한 gradient를 찾는 대신, batch라는 개념을 도입해 stochastic gradient descent method를 사용해 문제를 해결한다. 원래 문제가 Convex가 아니기 때문에 제대로 된 gradient descent와 SGD가 서로 다른 곳으로 수렴하기는 하지만, 이 점은 크게 중요한 이슈는 아니라고 한다.</p>
<p>여기에서 질문이 하나 나왔었다. 이렇게 할 수 있는 이유는 neural network에서 chain rule을 적용할 수 있기 때문인데 혹시 그렇다뎐 Hessian을 계산할 수는 없을까라는 질문이 나왔다. 왜냐하면 second derivative method가 gradient method보다 훨씬 좋다는 것은 이미 잘 알려진 사실이기에, 만약 Hessian을 efficient하게 계산할 수 있다면 learning 속도를 크게 향상시킬 수 있을 것으로 기대할 수 있기 때문이다. 그러나 아직까지는 실제 NN에서 Hessian 등의 2nd derivative를 계산하는 것은 Hessian Matrix를 계산해야할 뿐 아니라, 그것의 inverse까지 계산해야하므로 매우 expensive하다고 하고, 구현도 복잡하다고 한다. 그렇기 때문에 대신 Hessian matrix를 직접 구하지 않고 그것의 inverse를 estimate하는 방법들이 있지만, 그러나 여전히 큰 NN에는 부적합하기 때문에 보통 Gradient를 사용한다고 한다 (여기에서 언급된 Hessian matrix의 inverse를 estimate하는 방법을 Hessian-Free optimization이라고 부른다).</p>

<h3>Regularization</h3>
<p>Regularization을 Bayesian 관점에서 바라본다면 좋은 prior를 제안하는 것과 같다. 예를 들어 &#8216;모델이 이렇게 복잡할리 없으니 모델의 complexity를 penalty term으로 추가해야겠다&#8217; 라는 regularization method도 model complexity에 대한 prior를 반영한 것이라 할 수 있다. Neural Network에서도 마찬가지로 여러 prior를 바탕으로 다양한 regularization 방법들이 존재한다.</p>
<h6>Weight Decay</h6>
<ul>
	<li>Prior: Weight의 값이 너무 크지 않을 것이다 (이를 model이 sharp하지 않고 smooth하다 라고 표현한다).</li>
	<li>Approach: 다음과 같은 형태로 optimization objective를 바꾼다.</li>
	<p>$$\min_w E(w) + \lambda w^\top w.$$</p>
	<li>Gradient update rule은 다음과 같이 바뀐다</li>
	<p>$$w(t+1) = w(t) (1 - 2 \eta \lambda) - \eta \nabla E (w(t)). $$</p>
</ul>

<h6>Smoothness and Noise Injection</h6>
<ul>
	<li><p>Prior: Smoothness, $f(x)$와 $f(x+\varepsilon)$은 거의 비슷할 것이다.</p></li>
	<li><p>Approach: noise에 대한 change를 줄이는 것은 곧, $\min \sum_i | \frac{\partial f(x_i)}{\partial x} |^2$ 과 같다.</p></li>
	<li>따라서 위 식을 optimization function에 추가하여 문제를 풀게 되는데, Bishop 책에 따르면, 이 regularization term을 넣고 optimization하는 것은, random Gaussian noise를 input에 추가하여 learning하는 것과 equivalent하다는 것이 알려져 있으므로, input data에 random Gaussian noise를 섞는 것으로 대체할 수 있다.</li>
</ul>

<h6>Dropout (Hinton 2012)</h6>
<ul>
	<li>Prior: 하나의 classifier를 learning하는 것 보다, 여러 개의 classifier를 learning하고 이를 ensemble하여 classification하는 것이 더 좋다.</li>
	<li>Approach: 하나의 neural network 모델에서 여러 개의 모델을 learning할 수 있도록, NN의 node를 random하게 지운다. 이 경우 lower bound optimization과 같은 효과를 내기 때문에 조금 더 general한 model을 학습하는 것이 가능하며, dropout을 선택하는 것과 그렇지 않은 것은 약 10~20%의 성능 차이를 보인다. Drop하는 node는 보통 50%를 선택한다 (이 값도 제일 좋은 값을 learning해보려고 시도해봤는데 모두 0.5로 converge하였다고 한다).</li>
</ul>

<h3 id="54-common-recipe">Common Recipe for DNN.</h3>
<p>세미나에서 박사님은 앞에서 살펴본 regularization과 optimization method들을 바탕으로 아래와 같은 common recipe를 제안하였다.</p>

<ol>
  <li>Rectifier나 Maxout을 사용해라 (이 방법은 dropout처럼 값이 음수인 뉴런은 drop하고, 양수인 뉴런만 존재한다고 생각하면 계속 다른 NN을 사용하는 것처럼 생각할 수 있다. 이런 접근 방식을 사용하게 되면 differentiable하지는 않지만 sub-gradient를 사용하여 문제를 풀 수 있다고 한다).</li>
  <li>Preprocess data and choose features carefully (각 데이터 domain에 맞는 preprocessing을 취하자)
    <ul>
      <li>Image: Whitening, Raw, SIFT, HoG?</li>
      <li>Speech: Raw? Spectrum?</li>
      <li>Text: Characters? words? tree?
 *General: z-Normalization?</li>
    </ul>
  </li>
  <li>Dropout이나 다른 regularization method들을 사용하자.</li>
  <li>데이터가 적은 경우라면 Unsuperviesd Pretraining (Hinton et al 2006) 을 사용해보자. 그러나 데이터가 많으면 오히려 나쁜 결과를 내게 되므로 쓰지 말자.</li>
  <li>Carefully search for hyperparameters (Random search, Bayesian optimization + Greedy search는 hyperparameter가 exponential하게 많으므로 불가능하다.)</li>
  <li>Often, deeper the better (이미지 - 레이어 7개 이상, 스피치 - 12개, 14개,…, 그 이외에는 hyper parameter라고 한다. 박사님은 2개부터 시작한다고 한다. + 참고로 지금 이미지에서는 VGG등의 최신 논문들은 19개까지 쌓기도 하고, Google의 Inception의 경우 어마어마하게 깊다.)</li>
  <li>Build an ensemble of neural networks</li>
  <li>Use GPU. (좋은 파워와 메인보드가 필요하고, 쿨링과 전기세를 조심하라고 조언해주셨다)</li>
</ol>

<p>그러나 중요한 점은, 아무도 vanilla MLP (모든 layer가 fully connected layer인 NN)는 사용하지 않는다는 사실이다. 그 대신 domain knowledge가 많이 반영된 CNN이나 RNN등의 모델을 선택하여 사용한다. 최근 work들을 보면 Image등의 static한 데이터는 보통 CNN을 사용하고, sequencial data는 거의 RNN을 사용한다. CNN과 RNN에 대한 설명은 생략하도록 하겠다.</p>
<p>박사님은 deep learning의 좋은 점으로, 모델을 만들 때 부터 domain knowledge를 적용해서 CNN이나 RNN 등의 새로운 모델을 만들 수 있다는 점을 꼽았다. 만약 SVM 등의 기존 방법들을 사용한다면 모델 자체에서 그런 아이디어를 적용하지 어렵지만, deep learning은 그러기에 용이하다는 것이다. 예를 들어 Convolutional Neural Network의 Translation, Rotation, Temporal, Frequency invariance 등을 꼽을 수 있을 것이다. 이렇게 만든 CNN은, convolution layer와 pooling layer를 엄청나게 deep하게 쌓아 좋은 feature를 뽑아내고 마지막에 그것에 fully connected layer를 붙여서 linear classifier를 learning하는 방식으로 &#8216;deep&#8217; CNN을 구성할 수 있다. 반면 RNN은 time을 길게 가져가는 방식으로 &#8216;deep&#8217; RNN을 구성할 수 있는데, 이렇듯 &#8216;deep&#8217;하게 만드는 방식도 모델 특성을 따라가기 때문에 domain knowledge가 잘 반영된 모델이 deep해졌을 때도 잘 동작할 수 있는 것 같다고 생각한다.</p>

<h3>나머지 part들</h3>
<p>Unsupervised Learning 파트는 RBM, DNN 그리고 NADE에 대해 설명하고 마지막에 manifold leraning과 denoising autoencoder를 다루는 것으로 끝났다.</p>
<p>Advanced Topics 파트는 Deep Reinforcement Learning에 대해 다루고 (<a href="http://SanghyukChun.github.io/90">Atari 논문</a>을 다뤘다), 당시에 막 연구가 되고 있던 NLP 연구에 대해 잠시 언급했다 (1년이 지난 지금은 NLP 쪽으로 많은 연구가 진행되었다). 그 밖에 optimization 관점에서, local minima라고 여겼던 부분이 사실 local minima가 아니라 flat한 부분이었다는 관측결과가 많이 나오고 있다는 말과, 적당하게 2nd order method를 섞으면 성능이 크게 향상된다는 말까지 언급하였다 (그러나 아직까지 2nd order optimization을 많이 사용하는 것 같지는 않다). 마지막으로 deep learning의 최대 약점으로 꼽히던 이론적인 분석 결과에 대해 많은 연구가 진행되고 있다는 얘기까지 잠깐 언급하고 세미나가 마무리 되었다.</p>

<h3>정리</h3>
<p>이 글은 내가 거의 1년 3개월 전에 들었던 세미나를 바탕으로 쓰여진 글이다. 박사님이 해주신 얘기도 많이 섞여있고 내 개인적인 의견도 많이 섞여있지만, 맨 처음 deep learning을 접할 때 큰 도움이 되었던 세미나인 만큼, 공유할 수 있으면 좋을 것 같아 정리해서 올려본다.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle competition - Poker rule induction]]></title>
    <link href="http://SanghyukChun.github.io/87/"/>
    <updated>2015-09-26T15:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/87</id>
		<content type="html"><![CDATA[<p>이 글은 2015년 봄 내가 조교를 했었던 KAIST 빅데이터 분석개론 수업에서 중간 프로젝트로 나왔었던 Kaggle Competition을 내가 개인적으로 진행해보면서 느꼈던 점들을 시간 순서에 맞춰 적은 일종의 개발기이다. <a href="https://github.com/SanghyukChun/kaggle-poker_rule">깃허브 레포지토리</a>도 있으니 코드가 궁금하다면 이 레포지토리를 확인하면 될 것 같다. 깃허브 기준으로 이 프로젝트는 2015년 3월 20일부터 4월 19일까지 대략 한 달간 진행하였다.</p>
<p>이 competition을 위하여 4가지 정도의 아이디어를 냈었다. 먼저 normal KNN을 최대한 튜닝해보는 것, 문제가 &#8216;rule&#8217;이라는 것에 초점을 맞추어 rule을 정의하고 그것을 learning하는 방법, 세 번째로 card set에 대한 확률모델을 정의하여 likelihood를 maximization하는 방법, 마지막으로 기존 KNN의 input을 적당하게 바꾸어 KNN으로 처리하는 방법이었다. 이 중 실제로 submission까지 이어진 아이디어는 처음과 마지막 아이디어인데, 첫 번째는 0.67575, 두 번째에는 0.96908을 달성하였다. 이 글에서는 각각의 아이디어를 생각하게 된 계기와 왜 실패했고, 왜 성공했는지에 대해 정리해보도록하겠다.</p>
<p>이 competition을 시작하게 된 계기는, 내가 조교를 맡고 있던 수업에서 프로젝트로 <a href="https://www.kaggle.com">Kaggle competition</a>의 poker rule induction 문제를 풀기로 했기 때문이었다. (<a href="https://www.kaggle.com/c/poker-rule-induction">competition 링크</a>). 문제 자체가 간단하고, 마음만 먹으면 99% 이상을 찍는 단순한 알고리즘을 만드는 것이 어렵지 않은 문제였기 때문에, 나름의 조건으로 poker rule이 아니라 그 어떤 카드 게임에도 general하게 learning 결과를 적용할 수 있는 framework을 만들자는 것과, 또 하나는 deep learning을 사용하지 않고, 학생들이 이미 알고있는 기초적인 framework에 재미있을 법한 아이디어들을 섞어보기로 하였다. 그렇기 때문에 시작은 가장 간단하고 직관적인 KNN부터 적용해보는 것으로 시작해보았다. training set을 8:2로 나눠서 validation 실험을 진행해본 결과, 그냥 raw data를 사용하고 그냥 KNN을 사용하게 되면, 결과가 50% 정도 밖에 나오지 않았다. 여기에서 내가 해볼 수 있는 가장 간단한 개선은 k의 개수를 조절하는 것과 metric을 바꾸는 것, 그리고 input data를 바꾸는 것이다.</p>
<h3>첫 번째 아이디어 KNN</h3>
<p>K를 이리저리 조절해보아도 크게 차이가 나지는 않았다. 잘 선택하니 55% 언저리도 나오기는 했지만, K와는 다른 문제가 있어보였다. 그 문제를 해결하기 위해 먼저 input data를 5차원 데이터로 바꿔보았다. 즉, 지금은 5개의 카드 각각에 대해 rank와 suit를 따로 표현하여 10차원 데이터로 표현이 되지만, rank와 suit를 합쳐보는 것이다. 그리고 knn을 해보니 결과는 50% 미만. 왜 이럴까 생각을 해보니 당연히 1~52까지 카드가 배치가 될텐데, 이 숫자는 인덱스를 의미하지 진짜 &#8216;거리&#8217;를 의미하지 않기 때문에 문제가 발생한다는 것을 알 수 있었다. 따라서 input data는 real number여서는 안되고, 52차원짜리 binary로 개선해야겠다는 생각을 하게 되었다. 즉, 원래 10차원 real value data가 이제 52차원 binary data로 바뀌게 되었다. 같은 카드가 두 번 나오지 않기 때문에 반드시 binary임이 보장된다. 그러나 52차원은 너무 크기 때문에 PCA를 사용하여 차원을 조금 더 낮은 차원으로 보내보기도 하였다. 정리하자면 인풋을 binary로 정의하고, distance는 cosine으로 바꿔보고 PCA에 사용할 low dimension을 잘 선택하고 K를 잘 조절해본 결과 8:2 세팅에서 68.233%까지 성능이 향상되는 것을 볼 수 있었다. 2015년 3월 20일 새벽 2시 반, 리소스 문제로 인해 아직 test data를 서버에 제출하지는 못하였다. 아무래도 코드를 개선해야할 것 같다.</p>
<h3>두 번째 아이디어 &#8216;rule&#8217; learning</h3>
<p>그러나 이런 식의 방법을 사용해 KNN의 성능을 개선시킬 수는 있지만, 이는 근본적이 해결책이 되지 못한다. 문제를 해결하기 위해서는 조금 더 문제를 잘 정의하고, 좋은 알고리즘을 찾아야만 한다. 즉, 문제를 어떻게 모델링하느냐에 대한 이슈가 아직 해결되지 않은 것이다. 우리는 무엇을 찾아야하는가? 나는 여기에서 한 가지 생각을 했다. 결국 우리가 찾고 싶은 것은 Poker rule이다. 즉, 만약 우리가 rule에 대한 전체 domain을 정의할 수 있고, 각각의 rule에 대한 performance measure를 정의할 수 있다면, 가장 좋은 rule을 찾는 알고리즘을 디자인 할 수 있지 않을까? 생각이 여기까지 진행되니 바로 자연스럽게 다음 질문이 나오게 되었다. &#8216;rule&#8217;은 어떻게 정의해야할까? 처음에는 이렇게 생각했다. 결국 카드게임은 카드들을 비교하여 좋은 &#8216;조합&#8217;을 가진 사람이 이기는 게임이다. 따라서 나는 5장의 카드들로 이루어진 모든 pair 조합만큼의 dimension을 가지고 (즉, $5 \choose 2$ = 10개) 두 카드를 비교하는 rule의 개수가 $n$개라고 했을 때 각각의 piar들에게 n개 중의 하나에 대응시키게 되면, 우리는 10차원 real value vector로 rule을 표시할 수 있지 않을까? 그런데 문제가 있었다. 앞에서 본 것 처럼, rule의 개수를 $n$개 라고 한다고 해서, 1번째 rule과 100번째 rule이 어떤 우열관계가 있는게 아니다. 따라서 결국 binary로 만들어야할 것 같다. 그래서 어떻게 두 카드 사이의 rule을 binary로 만들 수 있을지 가만 생각해보니, 결국 rule이라는 것은 현재 이 카드가 다른 카드 어떤 것과 대응되는지 되지 않는지가 아닌가라는 생각이 들었다. 예를 들어 우리가 52개의 카드를 가지고 있을 때, 같은 숫자를 가진다는 rule은, 총 52 * 52개의 모든 카드 조합 중에서 정확하게 4*13 = 52개의 조합들을 의미하는 것이 아닐까. 다시 말해서, (1, 1), (1, 14), (1, 27), (1,40), (2,2), &#8230; 이런 식으로 정의가 된다고 생각할 수 있는 것이다. 그렇게 생각하게 되면 총 52*52 = 2704개의 rule이 필요하게 되더라.</p>
<p>이렇게 문제를 단순화시키고나니 문제의 목적은 &#8216;rule&#8217;을 찾는 것이며, rule은 다음과 같은 binary operation으로 정의할 수 있었다. 먼저 우리는 임의의 두 개의 카드 pair에 대해 총 52 * 52 = 2704개의 rule을 가진다. 왜냐하면 1부터 52까지 범위를 가지는 숫자 두 개를 연결하는 방법이 52*52개 만큼 있기 때문이다. 그리고 그 중 k개의 rule을 뽑아내고, k-1개의 binary operation을 사용해 각 점수에 대한 rule을 구해낸다. 예를 들어 카드의 값을 1부터 52까지 대응시켰을 때, 1 pair rule을 이 operation으로 나타내보면, (A 카드의 값이 1이고, B 카드의 값이 1인 rule) or (A 카드의 값이 2이고, B 카드의 값이 2인 rule) or &#8230;. 이 될 것이다. 이 경우 k는 13이고, k-1개의 operation들은 모두 or이다. 마찬가지로 2에 대해서 rule을 구하고, 계속해서 9까지 rule을 구한다.</p>
<p>우리가 모든 card set을 가지고 있다면 위의 문제를 direct하게 풀면 문제를 완벽하게 풀 수 있지만, 그렇지 않기 때문에 overfitting이 일어나게 된다. 따라서 나는 이 문제를 어떻게 더 generalize시킬 수 있느냐를 고민해보았다. 이 문제는 Rule의 총 개수를 2704개 보다 훨씬 더 적은 양으로 mapping할 수 있다면 비교적 쉽게 풀 수 있다. 나머지는 전부 training data에서 optimization으로 해결할 수 있는 문제들이니까.</p>
<p>그러나 이윽고 나는 다른 문제에 부딪히게 되었다. Rule이 이것보다 훨씬 많다는 것을 깨달았기 때문이다. Rule은 and operation으로만 이어지는 것이 아니라 or operation으로도 이어질 뿐 아니라 연산 순서 역시 중요했었다. 예를 들어 Rule = (Rule 1 and Rule 2 and Rule 3) or Rule 4 or (Rule 5 and Rule 6) 같은 지저분한 rule도 있을 수 있었기 때문이다. 즉, 내가 문제를 너무 단순하게 봤었다. 이렇게 문제를 생각하게 되면 rule에 대한 강력한 assumption이 없는 이상 더 이상의 generalization은 불가능하고, 직접적으로 rule을 learning하는 것이 어렵다는 결론에 도달하였다.</p>
<h3>세 번째 아이디어 grapical probability model</h3>
<p>다음으로 내가 생각했던 것은, 확률 모델로 문제를 디자인해보자는 것이었다. 주어진 데이터를 $X$라고 한다면 $p(y|X)$가 제일 큰 $y$를 고르면 되도록 말이다. 여러가지 확률 모델들이 있지만 (예를 들어 naive baysian도 확률모델이다) 내가 생각했던 아이디어는 각각의 $y$마다 확률 모델을 만들고, $X$가 $y$인지 아닌지 binary로 판단하게 하는 방식이었다. 이렇게 생각한 이유는, 실제로 rule이 중복해서 나올 수 있기 때문이다. 예를 들어 Triple은 two pair이기도 하고, four card는 triple이면서 two pair이기도 하다. 이렇게 모델을 정의하고 모든 $y$에 대해 지금 $X$를 대입한 후, 특정 threshold를 넘는 $y$ 중에서 가장 큰 숫자를 고르게 하는 것이다.</p>
<p>이 아이디어를 실행하기 위해 가장 중요한 것은 어떤 probability model을 선택하느냐는 것이었다. 내가 처음 생각한 아이디어는 RBM이었지만, RBM은 주어진 데이터에 대한 joint probability만 표현할 수 있지, 어떤 데이터가 그것에 속하지 않는지 learning하는 것이 어려웠다. 이 문제가 근본적으로 기존 방법들로 접근하기 어려운 이유는 sample bias가 너무 심하기 때문이다. 즉, 0점 짜리가 거의 대부분에 속하고 그 다음으로 1점, 2점&#8230; 순서로 가기 때문에 각 sample들이 uniform하게 분포하지 않고, 그 때문에 일반적인 방법으로 접근하게 되었을 때 과도하게 0에 치중된 모델이 나오게된다는 점이었다. 나는 이 때문에 각각의 점수별로 모델을 따로 가져가고, 대신 binary로 모델을 풀고 bias를 최대한 해결할 수 있는 방법을 생각해보려 했었다.</p>
<p>그러나 conditional probabilty를 이런 방식으로 leanring할 수 있는 모델이 (학생들이 알 수 있거나 생각할 수 있을법한) 모델 중에서는 도저히 떠오르지 않아서 이 방법도 선택하지 않게 되었다.</p>
<h3>마지막 아이디어 결국 다시 KNN</h3>
<p>그래서 결국 다시 KNN으로 돌아가게 되었다. 학생 중 하나가 나에게 sorting을 하는 방법에 대해서 물어봤었고, suit를 차라리 없애는 것이 훨씬 결과가 잘 나온다는 얘기를 들었기에 그렇게 한 번 진행해보았다. 그런데 결과가 너무 잘 나오는게 아닌가 (0.96908) 심지어 1-NN을 선택했고, 내가 한 것이라고는 suit를 무시하고 rank만 5개 고르고 5개를 sorting한 것을 넣은 것에 불과했는데. 도저히 이해가 안되서 하루쯤 생각해보니 이게 왜 잘 동작하는지 알 수 있었다. KNN 세팅에서 binary로 바꾸고 하는 과정을 거치지 않았을 때 50%의 성공을 거뒀던 것에 비해 너무 말도 안되는 성능 향상이었기 때문이다. 게다가 1-NN이 아니라 2-NN이나 3-NN의 퍼포먼스가 형편없다는 것도 이해할 수 없었다.</p>
<p>실제 poker rule에서 suit와 카드 순서에 영향을 받는 룰은 Flush 와 straight, 그리고 Royal flush 뿐이지만, 이 녀석들은 거의 나타나지 않는 녀석들이었다. 이렇게 input 데이터를 sorting하게 되면 전체 경우수는 오직 ( 13 choose 5 - 13 ) = 1274 개 뿐이다. 13을 뺸 이유는 같은 rank가 같은 경우는 오직 4개 뿐이기 때문이다. 그런데 training 데이터의 개수는 25,000개 넘기 때문에 엄청 높은 확률로 rank만 고려했을 때 training 데이터와 정확하게 같은 training 데이터가 존재하게 되는 것이다. 이 말은 다시 말하면 왜 K=1 일때만 제대로 동작하는지를 증명하는 말이기도 하다. 즉, 아주 높은 확률로 항상 &#8216;정확하게 같은&#8217; 데이터 셋을 training데이터에서 고를 수 있으니 당연히 K=1을 선택해야만 제대로 결과가 동작하게 되는 것이다.</p>
<p>나는 여기까지만 시도하고 그만두었다. 이때부터 엄청 바빠지기도 했고, 내가 처음 걸었던 조건처럼 poker에 대한 가정을 최소화한 상태로 학생들도 알 수 있는 간단한 아이디어로 이 문제를 해결하는 것이 어렵다고 느꼈었기 때문이다. 그래도 최종 결과를 96% 정도 달성하였으니 이 정도면 나름 나쁘지 않은 결과가 아닐까 생각한다.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (20) Reinforcement Learning]]></title>
    <link href="http://SanghyukChun.github.io/76/"/>
    <updated>2015-09-21T15:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/76</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p><a href="http://SanghyukChun.github.io/57">첫 글</a>에서 Machine Learning은 크게 세 가지로 구분된다는 얘기를 했었지만, 지금까지 다뤘던 주제들은 모두 supervised learning이거나 unsupervised learning이었다. Reinforcement learning은 그 둘과는 구분되는 명백히 다른 task이지만, machine learning에서 그만큼 대중적인 분야는 아니다. 아직까지 reinforcement learning을 사용한 적절한 application이 많이 제안된 것도 아니라서 practical하게 많이 사용지도 않는다. 그러나 reinforcement learning을 사용하면 supervised, unsupervised learning와는 전혀 다른 방법으로 문제를 접근하는 것이 가능하다. 최근 deep learning과 reinforcement learning을 결합한 재미있는 연구주제들도 나오고 있는 만큼 (<a href="http://SanghyukChun.github.io/90">Atari 리뷰</a>, <a href="http://SanghyukChun.github.io/91">Visual Attention 리뷰</a>), 앞으로 더 재미있는 방향으로 연구될 수 있는 주제가 아닐까 생각한다.</p>
<p>이 글은 Andrew Ng. 교수가 Stanford에서 강의하는 CS229 Machine Learning 수업의 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">lecture note</a>를 바탕으로 쓰여졌다. 이 글이 조금 부족하다고 느끼는 경우에는 해당 reference를 읽어보면 큰 도움이 될 것으로 생각된다.</p>

<h3>Reinforcement Learning: Problem Definition</h3>
<p>Supervised learning은 주어진 데이터의 label을 mapping하는 function을 찾는 문제이다. 이 경우 알고리즘은 얼마나 label을 정확하게 분류하느냐 혹은 정해진 loss function을 minimize시킬 수 있느냐에만 초점을 맞추어 모델을 learning하게 된다. 분명 supervised learning은 상당히 많은 application들에 응용될 수 있는 방법이다. 하지만 모든 문제들이 이런 방식으로 해결할 수 있는 것은 아니다. 예를 들어 2족 보행을 하는 알고리즘을 디자인한다고 생각해보자. 우리가 알고 싶은 것은 어떻게 로봇의 관절들을 움직여야 로봇이 넘어지지 않고 잘 걸을 수 있을까이다. 이 경우 우리는 관절의 움직임을 control하는 function을 learning해야한다. 이 문제를 머신러닝으로 풀기 위해서는 어떻게 문제를 정의해야할까? <a href="http://SanghyukChun.github.io/57">첫 글</a>에서 머신러닝 문제는 (1) 데이터 (2) output (3) target function (4) loss를 minimize하는 algorithm이 필요하다고 언급했었다. 먼저 데이터는 현재 관절들의 상태(각도, 위치 등)와 주변 환경(흙 위인지 아스팔트 위인지 앞에 벽이 있는지 등등)을 데이터라고 정의하자. 우리는 지금 보행을 learning하는 알고리즘을 찾고 있으므로 원하는 output은 지금 상태 다음의 관절 상태가 될 것이다. 즉, [f: 현재 관절 상태, 환경 -&gt; 다음 관절 상태]라는 target function까지 정의할 수 있다. Loss는 특정 시간 이후 얼마나 많이 걸었는지 등으로 판단할 수 있을 것이다.</p>
<p>그렇다면 이 문제는 supervised learning이나 unsupervised learning으로 풀 수 있을까? 데이터만 무한하게 있다면 가능할지도 모르지만 현실적으로는 그럴 수 없다는 것을 알 수 있다. 왜냐하면 (data, output)의 조합이 너무 많기 때문에 의미있는 learning을 할 수 있을 정도로 많은 데이터를 모을 수 없기 때문이다. 즉, 어떤 action이 &#8216;correct&#8217; action인가 판단하는 것이 불가능하다. 대신에 이렇게 생각해보면 어떨까? 우리가 알고 싶은 것은 관절 상황과 환경이 주어졌을 때 로봇이 어떻게 action을 취해야하는가라는 policy이다. 매 action을 주어진 policy를 통해 시행하고 나면 다음 state가 정의된다. 만약 성공적으로 걸었다면 +1 점을 주고 넘어졌다면 -1점을 주는 방식을 통해 매 action의 reward를 정의할 수 있을 것이다. 그렇다면 static한 데이터 셋에서 거의 무한하게 많이 필요한 (data, output)를 사용해 learning하는 방법 대신에, 직접 매 순간 action을 실행해 reward를 받으면서 최종적으로 맨 마지막에 모든 reward의 합이 가장 좋게 만드는 policy를 learning하는 것이다.</p>
<p>이런 식으로 reinforcement learning을 high level로 설명할 수 있다. 그렇다면 RL을 조금 더 formal하게 정의해보자.</p>

<h3 id="76-mdp">Markov Decision Process (MDP): Problem definition</h3>
<p>앞에서 설명한 방식대로 RL을 정의하면 RL problem은 정말 여러가지 형태로 정의할 수 있지만, 보통 RL문제를 푼다고 하면 Markov Decision Process (MDP)를 의미한다. MDP는 $ (S, A, \{P_{sa}\}, \gamma, R ) $ 이라는 것들의 튜플이다. 각각에 대해 알아보도록 하자.</p>

<ul>
	<li><p>$S$ - state들의 set을 의미한다. 앞에서 예를 든 2족 보행 로봇의 경우 모든 가능한 관절의 상태와 환경 등이 state가 된다. 참고로 state와 다음에 기술한 action의 개수가 유한하다면 $|S| &lt; \inf, |A| &lt; \inf$, 주어진 MDP를 finite MDP라고 부른다.</p></li>
	<li><p>$A$ - action들의 set을 의미한다. 2족 보행 로봇의 경우 어떻게 관절을 control할 것인가를 의미한다.</p></li>
	<li><p>$P_{sa}: (s_t, a_t) \to s_{a_t}$ - State의 transition probability로, 특정 state에서 특정 action을 취했을 떄 다음 state는 어떤 state가 될지에 대한 확률 값이다.</p></li>
	<li><p>$R: S \times A \to \mathbb R$ - 주어진 state에 action을 수행했을 때 얻게 되는 reward를 function으로 표현한 것이다. Reinforcement learning의 목표는 시간이 $T$만큼 흘렀을 때 최종적으로 얻게 되는 모든 reward들의 합을 (정확하게는 그것의 expectation을) maximization하는 policy를 learning하는 것이다.</p></li>
	<li><p>$\gamma \in [0,1) $ - 앞에서 설명한 reward의 discount factor로, 시간이 지날수록 reward의 가치를 떨어뜨리고, 처음 받은 reward의 가치를 더 키워주는 역할을 한다. 즉, time $t$에서 얻은 reward를 $r_t$라고 했을 때, 전체 reward $R_{tot}$는 $\sum_{t=0}^T \gamma^t r_t$가 된다.</p></li>
</ul>

<p>MDP의 dynamics는 다음과 같은 식이다. 먼저 initial state $s_0$에서 어떤 초기 action $a_0$를 수행하게 된다. 이 행동으로 인하여 주어진 probability $P_{s_0 a_0}$에 따라 다음 state $s_1$이 확률적으로 결정된다. 그리고 그 결과로 reward $R(s_0, a_0)$를 얻게 된다. 이를 다시 $s_1$에 대해 반복하면서 state가 terminal state에 도달할 때 까지 이 과정을 반복하게 된다. 이때, MDP의 Markov property 때문에 다음 step의 reward와 transition probability는 오직 지금 state와 지금 action에 의해서만 결정된다.</p>
<p>$$s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} \ldots.$$</p>
<p>이때, 앞에서 설명한 바와 같이 매 action을 취할 때 마다 reward가 결정된다. 이때 최대한 빠르게 좋은 reward를 받을수록 좋기 때문에 나중에 얻은 reward보다 일찍 얻은 reward의 값이 같더라도, 일찍 얻은 reward가 더 valuable하다고 가정한다. 이것을 우리는 discount factor를 통해 조절하게 되며, 그 결과 우리가 maximization하고 싶은 최종 reward는 discount factor $\gamma$에 의해 다음과 같이 결정된다. (참고로 이 값을 sum of discounted reward라고 부른다.)</p>
<p>$$R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots.$$</p>
<p>하지만 아무리 같은 state와 action으로 시작했다고 하더라도 이 과정은 전부 $P_{sa}$에 의해 확률적으로 결정되는 값이기 때문에, 실제로 maximization하기 위한 target은 그 값의 expectation으로 주어진다.</p>
<p>$$\mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots ].$$</p>
<p>우리가 위 expectation을 maximization하기 위해 learning해야하는 것이 바로 policy $\pi: S \to A$ 이다. Policy는 state에서 action으로 mapping되는 function이다 (즉, $a_t = \pi (s_t)$.  앞에서 설명했던 transition probability는 현재 state와 action을 다음 state와 mapping해주는 function이었고, policy는 지금 state에서 내가 어떤 action을 취해야하는지 mapping해주는 function인 것이다. 즉, policy가 어떻게 변화하느냐에 따라 최종 reward가 크게 바뀌게 된다.</p>

<h3 id="76-bellman">Value function and Bellman Equation</h3>
<p>그럼 어떻게 reward를 maximize하는 policy를 learning할 수 있을까? 그것을 설명하기에 앞서, 먼저 policy $\pi$의 value function $V^\pi (s)$이라는 것을 정의해보자. 이때 앞으로 reward $R(s,a)$는 state에 의해서만 결정된다고 가정하고, notation을 $R(s)$로 바꾸도록 하겠다. 만약 action과 state 둘 다에 의해 reward가 결정되는 경우는 앞으로 설명하게 될 value function $V$가 아니라 <a href="76-qfunction">action-value function $Q$</a>라는 것을 정의하고 그것에 대한 Bellman Equation을 구해 아래와 같은 방식을 그대로 적용하는 것이 가능하다.</p>
<p>olicy $\pi$의 value function $V^\pi (s)$은 다음과 같이 정의된다.</p>
<p>$$ V^\pi (s) = \mathbb E[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \ldots | s_0 = s, \pi ]. $$</p>
<p>이 값은 즉, 간단하게 이야기 하여 주어진 state $s$를 initial state로 두고 action을 policy $\pi$를 사용하여 고르게 되었을 때 우리가 얻게되는 reward의 expectation 값이 된다. 이렇게 정의했을 경우 fixed policy $\pi$에 대해 value function은 다음과 같은 관계식을 가진다. 증명은 크게 어렵지 않으므로 생략하겠다.</p>
<p>$$ V^\pi (s) = R(s) + \gamma \sum_{s^\prime \in S} P_{s \pi(s)} (s^\prime) V^\pi (s^\prime). $$</p>
<p>이 관계식을 Bellman Equation이라고 부르며, 이 관계식을 통해 우리는 $V^\pi (s)$과 다음과 같은 두 가지 성분으로 표현된다는 것을 알 수 있다. 첫째로 immediate reward $R(s, \pi(s))$이다. 이 값은 우리가 처음 state $s$에서 바로 얻을 수 있는 reward를 의미한다. 다음으로 future reward의 expectation이다. 이 값에 discount factor를 곱하고 immediate reward를 더하게 되면 우리가 원하는 $V^\pi (s)$를 구하는 것이 가능하다. 이때, future reward term은 사실 $ \mathbb E_{s^\prime \sim P_{s \pi(s)}} [V^\pi (s^\prime)] $ 으로 표현할 수 있는데, 다시 말해 future reward term은 처음 state $s$에서 policy $\pi$로 정해진 다음 state $s^\prime$의 distribution에 대한 sum of discounted reward의 expectation 값과 같다는 것을 알 수 있다. 그러므로 두 번째 term은 MDP의 한 step이 지나고 난 이후에 발생하는 모든 sum of discounted reward들의 expectation이라는 것을 알 수 있는 것이다.</p>
<p>Bellman Equation을 사용하면 finite MDP에 대해 value function $V^\pi (s)$를 효율적으로 계산할 수 있다. 만약 finite MDP에 대해 문제를 풀고 있다고 가정해보자. 그렇다면 주어진 모든 state $s$에 대해 $V^\pi (s)$의 Bellman Equation을 적는 것이 가능한데, 이렇게 되면 우리는 $|S|$개의 linear equation과 $|S|$개의 variable들 (이 경우는 각 state에 대한 $V^\pi (s)$들)이 존재하기 때문에 간단한 연립방적식으로 value function의 값을 찾는 것이 가능하다.</p>
<p>하지만 우리가 처음부터 원했던 것은 optimal policy $\pi^*$이지, 주어진 $\pi$에 대한 expectation of sum of discounted reward가 아니다. 하지만 이 optimal policy 역시 Bellman Equation을 사용하면 계산할 수 있다. 이것을 어떻게 하는지 설명하기 전에 앞서, 먼저 optimal value function $V^* (s)$를 다음과 같이 정의해보자.</p>
<p>$$ V^*(s) = \max_\pi V^\pi (s). $$</p>
<p>즉, optimal value function은 모든 policy $\pi$에 대한 value function $V^\pi (s)$ 중에서 가장 reward를 maximize시키는 policy를 통해 얻게 된 reward가 된다. Optimal value function 역시 Bellman Equation을 증명할 수 있는데, 그 식은 다음과 같다.</p>
<p>$$ V^* (s) = R(s) + \max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). $$</p>
<p>앞의 term은 위와 동일하고, 두 번째 term은 모든 expected future sum of discounted reward를 action $a$에 대해 maximize한 결과이다. 즉, 모든 action 중에서 reward를 가장 maximize하는 action을 선택하였을 때 얻게되는 reward의 값이다. 그런데 그런 action을 고르는 방법이 바로 optimal policy $\pi^*$이므로, optimal policy는 다음과 같이 구할 수 있다.</p>
<p>$$ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). $$</p>
<p>모든 state $s$와 모든 policy $\pi$에 대해 우리는 다음과 같은 관계식을 얻을 수 있다.</p>
<p>$$V^*(s) = V^{\pi^*}(s) \geq V^\pi (s).$$</p>
<p>첫번째 관계식은 optimal policy $\pi$에 대한 value function $V^{\pi^*}(s)$와 optimal value function $V^*(s)$가 모든 state $s$에 대해 equivalent하다는 것을 보여준다. 이 내용이 중요한 이유는, 초기 state가 무엇인지와 관계없이 항상 같은 optimal policy $\pi^*$를 사용해 optimal value function을 구할 수 있다는 의미가 되기 때문이다. 즉, 만약 optimal policy를 구할 수 있는 algorithm이 있다면 그 알고리즘의 initial state로 어느 state를 고르더라도 우리는 항상 같은 policy를 얻게된다는 사실을 암시한다. 그리고 두 번째 equation은 모든 policy $\pi$에 대한 value function보다 $V^{\pi^*}(s)$의 값이 더 크거나 같다는 것을 의미한다. 만약 optimal policy를 구하는 algorithm이 value function을 monotonically increase 시키는 방향으로 learning한다고 했을때, update되는 값의 upper bound가 존재하므로 항상 converge하게 된다는 것을 알 수 있다.</p>
<p>Finite MDP의 optimal policy를 구하는 대표적인 두 알고리즘으로는 value iteration과 policy iteration이라는 알고리즘이 존재한다. 두 알고리즘은 이름에서 알 수 있듯 모두 iterative algorithm이며, 위에서 언급한 intuition이 그대로 적용되는 알고리즘들이다. 즉, initial state에 invariant하며 iteration 동안 value function이 monotonically increase한다. 그리고 그 값이 converge하게 되면 우리가 원하는 optimal policy를 구할 수 있다.</p>

<h3>Value Iteration</h3>
<p>Value iteration 알고리즘은 다음과 같다.</p>
<ol>
	<li><p>Initialize $V(s) = 0, ~\mbox{ for all } s$.</p></li>
	<li>Repeat until converge</li>
	<p>$$V(s) = R(s) + \max_{a\in A} \gamma \sum_{a^\prime} P_{sa} (s^\prime) V(s^\prime), ~\mbox{ for all } s. $$</p>
</ol>
<p>이 알고리즘은 앞서 설명했던 Bellman Equation에서 optimal value function의 관계식을 iterative하게 반복하면서 찾아나가는 알고리즘이다. 이 알고리즘의 두 번째 state는 synchronous update와 asynchronous update 총 두 가지 방법으로 접근이 가능하다. 먼저 synchronous update는 모든 $s$에 대해 value function $V(s)$ 값 을 계산하고 그 값들을 한 번에 update하는 방법이고, asynchronous update는 한 state $s$에 대해 $V(s)$를 구하고 바로 update를 하는 방법이다. 쉽게 생각하면 asynchronous update는 stochastic gradient descent같은 방법이라 생각하면 된다. 이 두 가지 방법 모두 finite하고 polynomial time 안에 $V$가 optimal value function $V^*$으로 수렴한다는 것을 증명할 수 있다. 이렇게 구해진 $V^*$를 사용하면 앞에서 구했던 다음 관계식을 통해 optimal policy를 구할 수 있다.</p>
<p>$$ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). $$</p>

<h3>Policy Iteration</h3>
<p>이번에는 policy iteration 알고리즘에 대해 살펴보자. 알고리즘은 다음과 같다.</p>
<ol>
	<li><p>Initialize $\pi$ randomly</p></li>
	<li>Repeat until converge</li>
	<ul>
		<li><p>Let $V = V^\pi$.</p></li>
		<li><p>For each state $s$, let $\pi(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). $</p></li>
	</ul>
</ol>
<p>Value iteration이 value function의 값을 update하는 알고리즘이라면 policy iteration은 policy를 udpate하는 iterative algorithm이다. 두 알고리즘 모두 Bellman equation을 통해 얻어지는 알고리즘이다. Policy iteration에서 $\pi(s)$를 업데이트하는 방식을 주어진 value function $V$에 대해 greedy한 policy update rule이라고 부른다. Policy iteration 역시 polynomial time 안에 optimal policy로 수렴하게 된다.</p>
<p>Value iteration과 policy iteration은 모두 MDP를 해결하는 알고리즘이며, 둘 중 무엇이 더 좋냐를 비교할 수는 없다. 그러나 일반적으로 small MDP에 대해서는 policy iteration이 빠른 시간 안에 효과적으로 수렴하고, large MDP의 경우에는 policy iteration에서 greedy policy rule update가 비효율적일 수 있기 때문에 value iteration으로 문제를 푸는 것이 computationally 좀 더 efficient하다고 한다.</p>
<p>다만 value iteration과 policy iteration은 이론적으로 optimal value function을 계산할 수 있도록 보장하는 알고리즘이기는 하지만, 실제 세상의 large MDP에 적용하기에는 모든 state와 action에 대한 경우 수를 계산하는 이런 알고리즘들은 다소 비효율적이다. 대신 다른 방법으로 value function을 update할 수 있는 알고리즘을 제안하기도 하는데, 예를 들면 지난 번에 리뷰했던 <a href="http://SanghyukChun.github.io/90">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a> 논문을 예로 들 수 있을 것 같다.</p>

<h3 id="76-qfunction">Action Value Function</h3>
<p>Reward가 state, action 둘 다에 의해 결정될 경우, value function $V^\pi (s)$가 아니라 action value function $Q^\pi (s,a)$를 사용하여야한다. Q function은 다음과 같이 정의할 수 있다.</p>
<p>$$ Q^\pi (s,a) = \mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots | s_0 = s, a_0 = a, \pi ]. $$</p>
<p>이 값을 사용하게 되면 initial state와 action에 대해 앞에서 value function에 대해 계산했던 것들을 그대로 반복할 수 있다. 먼저 $Q^* (s,a) = \max_\pi Q^\pi (s,a)$이고, optimal action value function의 Bellman Equation은 다음과 같이 주어진다.</p>
<p>$$ Q^* (s,a) = R(s,a) + \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) \max_{a\in A} Q^* (s^\prime, a). $$</p>
<p>남은 부분은 value function으로 진행했던 내용과 동일하게 진행하면 된다.</p>

<h3>Learning Model for MDP</h3>
<p>지금까지 앞에서 살펴봤던 내용은 모두 MDP의 state transition probability와 reward function이 전부 주어진 상태라고 가정하고 문제를 푸는 방법이었다. 하지만 실제로는 transition probability와 reward가 직접적으로 알려져있지않고, 실제 action을 수행하기 전까지 알 수 없는 것들이 훨씬 많다. 이 경우 data를 통해 transition probability와 reward function을 estimate해야한다. 이 경우 각각의 state에 대해 모든 action을 반복적으로 수행하면서 probability의 approximation 값을 구하고, reward 역시 같은 방법으로 계산해야한다.</p>

<h3>정리</h3>
<p>이 글에서는 reinforcement learning의 가장 기본적인 모델인 MDP에 대해 다루었다. MDP는 state, action, reward function, transition matrix와 discount factor로 구성된 튜플이며, optimal policy를 구하기 위해서 value function이라는 개념을 도입하고, 이 optimal value function을 계산할 수 있다면 optimal policy를 구할 수 있다. Optimal value function을 update하기 위해서 Bellman Equation이라는 관계식을 사용해 value iteration과 policy iteration이라는 알고리즘까지 살펴보았다. 이 경우 모든 reward와 transition matrix는 이미 알려져있다고 가정하였고, 만약 모르는 경우 finite MDP에서는 실제 estimation을 통해 model을 leanring해야한다는 얘기까지 하였다. 실제로 MDP 문제를 다루게 될 경우 이것보다 훨씬 복잡한 문제를 다뤄야할 일이 많지만, 근본적으로는 value iteration 등을 사용하여 optimal value function 혹은 optimal action value function의 값을 구해 optimal policy를 구한다는 기본적인 아이디어는 같다.</p>

<h3>Reference</h3>
<ul>
  <li><a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">Reinforcement Learning Lecture Note - Stanford CS229 Lecture Note by Andrew Ng</a></li>
</ul>

<h3>변경 이력</h3>
<ul>
  <li>2015년 9월 21일: 글 등록</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (19) Deep Learning - RBM, DBN, CNN]]></title>
    <link href="http://SanghyukChun.github.io/75/"/>
    <updated>2015-09-21T00:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/75</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p><a href="http://SanghyukChun.github.io/74">이전 글</a>에서 기본적인 neural network에 대한 introduction과, feed-forward network를 푸는 <a href="http://SanghyukChun.github.io/74#backprop">backpropagtion 알고리즘</a>과 optimization을 하기 위해 기본적으로 사용되는 <a href="http://SanghyukChun.github.io/74#sgd">stochastic gradient descent</a>에 대해 다루었다. 이 글에서는 deep learning이란 것은 정확히 무엇이며, 왜 deep learning이 최근 크게 급부상하게 되었는지에 대해 시간 순으로 다룰 것이다. 이 글에서는 unsupervised learning의 한 방법인 Restricted Boltzmann Machine (RBM)과 그것을 사용한 Deep Belief Network (DBN)에 대해 다룰 것이다. 또 다른 unsupervised learning 방법 중 하나인 (denoising) auto-encoder 역시 다루고자하였으나, 이 내용까지 전부 다루기에는 내용이 너무 길어져서 이 글에서는 생략하였다. 주의할 점은, 2007년 2008년에 막 deep learning이라는 이름으로 나왔던 연구들인 unsupervised pretraining 방법들은 현재 거의 쓰이지 않는 연구방법들이라는 것이다. 때문에 지금까지도 사용되는 조금 더 practical한 방법들인 neural network regularization (예를 들어서 ReLU, Dropout 등), optimization method (momentum, adagrad 등)에 대해서는 이 이후 한 번의 posting을 더 하여 다루도록 하겠다. 추가로, 오래된 work임에도 불구하고 아직도 computer vision 쪽에서 엄청나게 많이 사용하는 Convolutional Neural Network (CNN) 에 대해서도 다루도록 하겠다.</p>

<h3>Deep Neural Network</h3>
<p>Deep learning이라는 것은 사실 deep neural network를 의미하는 것이다. Neural network에 대해서는 <a href="http://SanghyukChun.github.io/74">이전 글</a>에서 설명하였고, 그럼 deep이란 무엇인가하면, 다른게 아니라 feed-forward network에서 가운데 hidden layer가 1개 보다 많으면 &#8216;deep&#8217;하다고 말하는 것이다. 요즘은 layer를 무조건 1개보다는 많이 쌓기 때문에 요즘 나오는 neural network 연구는 모두 deep learning 연구라고 생각하면 된다.</p>
<p>그런데 사실 deep learning은 전혀 새로운 연구분야가 아니고 이미 몇 십년 전에 기본적인 연구가 끝난 분야이다.</p>

<ul>
  <li>1958 Rosenblatt proposed perceptrons</li>
  <li>1980 Neocognitron (Fukushima, 1980)</li>
  <li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
  <li>1985 Boltzmann machines (Ackley et al., 1985)</li>
  <li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
  <li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
  <li>1993 Sparse coding (Field, 1993)</li>
</ul>

<p>이렇듯 이미 가장 중요한 기초적인 연구는 예전에 다 끝났다. 지난 글에서 설명한 backpropagation 알고리즘은 이미 1986년 나온 알고리즘이고, 1989년에 나온 convolutional network가 요즘도 vision 분야에서 늘 사용하는 그 CNN이다. 그런데 정작 deep learning은 2000년도 중반이 지나고나서야 주목을 받기 시작했다. 왜 그랬을까?</p>

<h3>Why Deep Learning?</h3>
<p>Deep learning이 예전에 &#8216;사기꾼&#8217; 취급을 받았던 이유는 크게 세 가지 이유가 있었다. 먼저 &#8216;deep&#8217; learning에 대한 이론적인 결과가 전무했다는 점 (network가 deep 해지면 문제가 더 이상 convex해지지 않는데, 이 상태에 대해 좋은 convergence는 어디이며 어떤게 좋은 initialization인가 등에 대한 연구가 전무하다. 즉, learning하면 overfitting이 너무 심하게 일어난다), 둘째로 이론적으로 연구가 많이 진행되어있는 &#8216;deep&#8217; 하지 않은 network (perceptron이라고 한다) 는 xor도 learning할 수 없는 한계가 존재한다는 점 (linear classifier라 그렇다). 마지막으로 computation cost가 무시무시해서 그 당시 컴퓨터로는 도저히 처리할 엄두조차 낼 수 없었다는 점이다.</p>
<p>그렇다면 지금은 무엇이 바뀌었길래 deep learning이 핫해진걸까? 가장 크게 차이 나는 점은 예전과는 다르게 overfitting을 handle할 수 있는 좋은 연구가 많이 나오게 되었다. 처음 2007, 2008년에 등장했던 unsupervised pre-training method들 (이 글에서 다룰 내용들), 2010년도쯤 들어서서 나오기 시작한 수많은 regularization method들 (dropout, ReLU 등). 그리고 과거보다 하드웨어 스펙이 압도적으로 뛰어난데다가, GPU parallelization에 대한 이해도가 높아지게 되면서 에전과는 비교도 할 수 없을정도로 많은 computation power를 사용할 수 있게 된 것이다. 현재까지 알려진바로는 network가 deep할 수록 그 최종적인 성능이 좋아지며, optimization을 많이 하면 할 수록 그 성능이 좋아지기 때문에, computation power를 더 많이 사용할 수 있다면 그만큼 더 좋은 learning을 할 수 있다는 것을 의미하기 때문에 하드웨어의 발전 역시 중요한 요소이다.</p>
<p>그리고 무엇보다 무시할 수 없는 것은, deep learning 기반의 approach들이 다른 방법론들을 압도하는 분야들이 있다는 것이다. 대표적인 분야가 바로 computer vison이다. 우리가 잘 알고 있는 <a href="http://yann.lecun.com/exdb/mnist/">MNSIT</a> 데이터셋은 물론이고, <a href="http://www.image-net.org/">ImageNet</a>과 그것 중에서 10개의 class만 떼어내서 만들어낸 데이터셋인 <a href="http://www.cs.toronto.edu/~kriz/cifar.html">Cifar-10</a> 대해서도 가장 잘 하고 있는건 역시 neural network이다. 아래 표들을 살펴보자 (<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">출처</a>)</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-1.png" width="600" />
<img src="http://SanghyukChun.github.io/images/post/75-2.png" width="600" /></p>

<p>여기서 잠시 MNIST가 28 by 28 짜리 handwrite digit을 모아놓은 데이터셋이라는 것은 많이 언급했던 내용이니 넘어가고, ImageNet competition에 대해 잠깐 언급하고 넘어가보도록 하자. <a href="http://www.image-net.org/">ImageNet</a>이라는 것은 사실 데이터셋의 이름이 아니라 매년 새로운 task가 주어지는 competition이다. 보통 실험에 사용하는건 <a href="http://image-net.org/challenges/LSVRC/2012/index">2012년 데이터셋 (ILSVRC 2012)</a>인데, training 데이터가 1000개 class에 데이터 개수는 거의 128만개 가까이 되는 엄청나게 큰 데이터 셋이다. Test data는 공개되지 않았고, 대신 validation set으로 공개된 데이터는 총 5만개짜리 데이터이다. 전체 데이터 사이즈는 거의 150GB가까이 된다. 이때 데이터를 많이 사용하는 이유는, 이때 task가 iamge classification이고, 많은 논문들이 이 때의 데이터를 기준으로 실험하기 때문인듯 하다. 예를 들어서 <a href="http://image-net.org/challenges/LSVRC/2015/">ILSVRC 2015</a>에는 더 이상 classification task가 존재하지 않고, object detection이나 object localization등의 task만 주어져있는 상태이다.</p>
<p>현재 ImageNet dataset (혹은 ILSVRC 2012)에서 state-of-art classification performance를 보이는 work은 <a href="http://SanghyukChun.github.io/88">지난 번에 review</a>했던 <a href="http://arxiv.org/abs/1502.03167">Batch Normalization</a> 논문인데, classification error는 20.1%이고, 1000개의 class 중에서 확률이 가장 높다고 판단한 top 5개 중에서 우리가 원하는 정답이 있을 확률인 top-5 error는 4.9%에 달한다. ImageNet dataset을 보면 1000개의 데이터가 전부 독립적인 것이 아니라 어느 정도 비슷한 데이터도 섞여있는 만큼, top-5 error가 5% 이하라는 것은 진짜 어마어마한 수준이라고 할 수 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/88-8.png" width="600" /></p>

<p>이렇듯 deep learning은 computer vision 쪽에서 압도적인 성능을 보이고 있을 뿐 아니라, 최근에는 language model, NLP, machine translation 등의 다양한 분야에서도 좋은 결과를 내고 있다. 무엇보다 deep learning 쪽 분야는 Google, MS, Yahoo 심지어는 Apple과 삼성 등에서도 투자를 많이 하고 있고 실제로 엄청나게 많은 연구들이 행해지고 그 연구들이 나오자마자 거의 바로 산업에 적용될 정도로 practical하게 많이 쓰이고 있는 분야가 되었다. 그렇기 때문에 아마도 당분간은 머신러닝 분야에서 deep learning의 강세는 이어질 것으로 보인다.</p>
<p>이 글의 남은 부분의 첫 번째 부분에서는 NIPS 2006에 발표된 Bengio 교수 연구팀의 <a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Greedy layer-wise training of deep networks</a> 연구와 NIPS 2007에 발표된 Hinton 교수 연구팀의 <a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">A fast learning algorithm for deep belief nets</a> 두 논문을 통해 제안되었던 unsupervised pretraining method 들에 대해서 다룰 것이다. 이 부분은 더 이상 practical usage로 사용되지는 않지만, deep learning의 거의 첫 번째 연구결과라고 해도 좋을 정도로 의미있는 연구결과들이므로 한 번쯤 알아둘 필요가 있다고 생각한다.</p>
<p>그리고 나머지 부분에서는 정말 오래된 연구결과이지만 아직까지도 쓰이고 있는 Convolutional Neural Network (CNN)에 대해 다룰 것이다. 이 결과는 앞서 ImageNet에서 가장 좋은 결과를 내고 있다는 Batch Normalization 에서도 기본 골격으로 사용하고 있는 vision 쪽에서는 가장 기초가 되는 엄청나게 중요한 개념이므로 마찬가지로 이 글에서 다루도록 하겠다. 만약 practical한 목적으로 이 글을 읽고 있다면 아래 unsupervised pretraining 섹션은 건너뛰고 바로 <a href="#75-cnn">CNN</a> 섹션부터 읽더라도 크게 상관없다.</p>

<h3>Problems to solve for deep learning</h3>
<p>Deep learning이 흥하기까지 수 많은 연구결과들이 있었지만, 지금처럼 deep learning이 hot하게 되기까지는 앞에서 말했던 것처럼 regularization method들이나 initialization method들, 그리고 overfitting을 최대한 피할 수 있는 optimization mehtod 등이 많이 제안되면서부터라고 할 수 있다. 이 연구들이 공통적으로 고민하고 있는 것은 <a href="http://SanghyukChun.github.io/59">overfitting</a>이다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/59-1.png" width="500" /></p>

<p>Overfitting은 주어진 데이터의 양에 비해 모델의 complexity가 높으면 발생하게 된다. 안타깝게도 neural network가 깊어질수록 model의 complexity는 exponential하게 증가하게 된다. 그렇기 때문에 거의 무한한 표현형을 learning할 수 있는 deep network가 좋다는 것을 다들 알고 있음에도 불구하고, 너무나 overfitting이 심하게 발생하기 때문에 neural network 연구가 멈추게 된 것이다. 하지만 2007~8년 즈음하여 overfitting을 막기 위하여 새로운 initialziation을 제안하는 work이 나오게 되는데 그 work이 바로 앞에서 설명 했던 NIPS에 발표되었던 두 work이다.</p>
<p>먼저 Restricted Boltzmann Machine (RBM) 에 대해 설명해보자.</p>

<h3>Restricted Boltzmann Machine (RBM): Introduction</h3>
<p>이 섹션은 상당히 수식이 많으며, 너무 복잡한 수식은 생략한 채 넘어가기 때문에 다소 설명이 모자랄 수 있다. 조금 더 관심이 있는 사람들을 위하여 아래의 참고자료들을 추천한다. 난이도 순서대로 당장 필요한 정도에 따라 읽으면 좋을 것 같은 순서대로 배치하였다. 내가 생각했을 때 알고리즘에 대한 심층적인 이론적 설명이 많은 순서대로 나열하였으니 처음부터 천천히 읽어보면 좋을 것 같다. 특히 마지막 참고자료는 상당히 이론적인 내용들을 굉장히 차근차근 어렵지 않게 담고 있으므로, RBM을 제대로 공부하고 싶다면 꼭 읽어보면 좋을 것 같다.</p>

<ul>
  <li><a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Hinton, Geoffrey. “A practical guide to training restricted Boltzmann machines.” Momentum 9.1 (2010): 926.</a></li>
  <li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">Bengio, Yoshua. “Learning deep architectures for AI.” Foundations and trends® in Machine Learning 2.1 (2009): 1-127.</a> (20쪽 부터)</li>
  <li><a href="http://image.diku.dk/igel/paper/AItRBM-proof.pdf">Fischer, Asja, and Christian Igel. “An introduction to restricted Boltzmann machines.” Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Springer Berlin Heidelberg, 2012. 14-36.</a></li>
</ul>

<p>RBM은 graphical probabilistic model의 일종으로, undirected graph로 표현되는 모델이다. Probability는 energy function의 형태로 표현이 되는데, 원래 RBM이라는 모델 자체가 Ising model이라는 물리 분야에서 많이 사용되는 모델의 일종이기 때문에 그 형식을 그대로 본 따온 것으로 보인다. RBM의 기본적인 형태는 다음과 같다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-3.png" width="200" /></p>
<p>이 모델은 complete undirected bipartite graph을 띄고 있다. 이때 각각의 biparition은 visual unit들과 hidden unit들로 이루어져있으며 이 경우는 모든 unit들이 binary인 경우에 대해서만 다룬다. 따라서 visual layer와 hidden layer는 서로 internal edge가 존재하지 않고, layer들끼리 undirected fully connected된 형태를 띄고 있다. 이 모델은 graphical probabilistic model이기 때문에 각각의 visual node $v$들과 hidden node $h$들은 random variable을 의미하게 되며, 이 모델은 $v,h$의 joint probability를 표현하는 모델이 된다. 이 모델은 joint probability를 아래와 같은 energy function form으로 표현한다.</p>
<p>$$p(v,h) = \frac{e^{-E(v,h)}}{Z}, \mbox{ where } E(v,h) := -\sum_i a_i v_i - \sum_j b_j h_j - \sum_i \sum_j v_i w_{ij} h_j \mbox{ and } Z = \sum_{v,h} e^{-E(x,h)} $$</p>
<p>RBM의 parameter는 bais term $a_i, b_j$와 weight term $w_{ij}$로, 이 값들이 변화함에 따라 joint probability가 변하게 된다. RBM은 주어진 데이터들을 가장 잘 설명하는, 즉 $p(v)$의 값이 가장 커지도록 하는 parameter를 learning하게된다. 보통 이 값은 log likelihood로 다음과 같이 표현된다.</p>
<p>$$\theta = \arg\max_\theta \log \mathcal L (v) = \arg\max_\theta \sum_{v \in V} \log P(v). $$</p>
<p>이때 likelihood $P(v)$는 $P(v) = \sum_h P(v,h) = \frac{1}{Z} \sum_h e^{-E(v,h)}$로 계산할 수 있다. 이 log-likelihood 값을 maximize하는 문제는 non-convex문제이기 때문에 global optimum을 찾는 것은 불가능하고, 대신 stochastic gradient descent를 사용하여 local optimum을 계산하게 된다. SGD를 사용해 update를 하기로 하였으니, 각각의 sample $v$에 대한 gradient $\frac{\partial\log p(v)}{\partial\theta}$를 계산해보자.</p>
<p>$$ \log p(v) = -\log \frac{1}{Z}\sum_h e^{-E(v,h)} = \ln \sum_h e^{-E(v,h)} - \ln \sum_{v,h} e^{-E(v,h)}$$</p>
<p>$$ \frac{\partial\log p(v)}{\partial\theta} = \frac{\partial\ln \sum_h e^{-E(v,h)}}{\partial\theta} - \frac{\partial\ln \sum_{v,h} e^{-E(v,h)}}{\partial\theta}$$</p>
<p>$$ = -\frac{1}{\sum_h e^{-E(v,h)} } \sum_h e^{-E(v,h)} \frac{\partial E(v,h)}{\partial \theta} + \frac{1}{\sum_{v,h} e^{-E(v,h)} } \sum_{v,h} e^{-E(v,h)} \frac{\partial E(v,h)}{\partial \theta}$$</p>
<p>$$ = -\sum_h p(h|v) \frac{\partial E(v,h)}{\partial \theta} + \sum_{v,h} p(v,h) \frac{\partial E(v,h)}{\partial \theta}.$$</p>
<p>이때, $p(h|v)$는 $p(h|v) = \frac{p(v,h)}{p(v)} = \frac{ e^{-E(v,h)} }{\sum_h e^{-E(v,h)}}$ 로부터 유도되는 값이다. 즉, 우리가 optimization하고 싶은 gradient는 $\frac{\partial E(v,h)}{\partial \theta}$의 값의 $p(h|v)$와 $p(v,h)$에 대한 expectation 값이 된다. 예를 들어 $w_{ij}$의 경우 $\frac{\partial E(v,h)}{\partial w_{ij}} = v_i h_j$이므로 $v_i h_j$의 $p(h|v)$와 $p(v,h)$에 대한 expectation을 구하게 된다면 우리가 목표하는 gradient를 얻는 것이 가능하다.</p>
<p>$$ \frac{\partial\log p(v)}{\partial\theta} = \sum_h p(h|v) v_i h_j - \sum_{v,h} p(v,h) v_i h_j$$</p>
<p>$$ = \sum_h p(h|v) v_i h_j -  \sum_{v,h} p(v,h) v_i h_j = p(h_j=1|v) v_j - \sum_v p(v) p(h_j=1|v)v_j$$</p>
<p>라는 결과를 얻을 수 있다. (지금은 i와 j가 고정된 상황이므로 $\sum_h$를 하게 되면 $h_j$의 값이 0이거나 1인 경우 둘 밖에 없고, 0인 경우는 $v_i h_j$가 0이므로 위와 같은 식을 얻을 수 있다). 이때, $p(h_j = 1|v)$는 아래와 같이 간단하게 계산할 수 있다.</p>
<p>$$p(h_j = c|v) = \frac{1}{Z} exp(-\sum_i a_i v_i - \sum_{\ell\neq j} b_\ell h_\ell - b_j * c - \sum_i \sum_{\ell\neq j} v_i w_{i\ell} h_\ell - \sum_i v_i w_{ij} * c)$$</p>
<p>$$p(h_j = 1|v) = \frac{p(h_j = 1|v)}{p(h_j = 1|v) + p(h_j = 0|v)} = \frac{1}{1 + exp(-b_j-\sum_i v_i w_{ij})} = \sigma(b_j+\sum_i v_i w_{ij}).$$</p>
<p>즉, conditional probability는 sigmoid function이 된다. 마찬가지로 $p(v_i = 1 | h) = \sigma(a_i+\sum_j h_j w_{ij})$로 계산할 수 있다. 그렇기 때문에 우리가 주어진 데이터 $v_i$도 알고 있고, $p(h_j=1|v)$ 역시 sigmoid로 계산할 수 있기 때문에, log likelihood의 weight에 대한 gradient값인 $\sum_h p(h|v) v_i h_j -  \sum_{v,h} p(v,h) v_i h_j$의 앞부분은 간단하게 계산할 수 있다.</p>
<p>그러나 문제가 되는 부분은 뒷 부분이다. 안타깝게도 이 경우는 모든 $v,h$의 조합에 대해 값을 모두 계산해야하기 때문에 이 값을 정확하게 계산하기 위해 필요한 computational complexity는 exponential이 된다. 그런데 이 값이 정확하게 우리가 구하고 싶은 마지막 final 값도 아니고, 겨우 중간 단계의 한 번의 gradient를 계산하기 위해 필요한 step에 불과한데 iteration 안에 exponential complexity를 가지는 step이 있는건 큰 문제가 된다. 그렇기 때문에 이 RBM문제를 해결하기 위해 도입되는 알고리즘이 Contrastive Divergence라는 gradient approximation 알고리즘이다.</p>

<h3>Restricted Boltzmann Machine (RBM): Contrastive Divergence</h3>
<p>Contrastive Divergence 알고리즘을 한 마디로 요약하면: $p(v,h)$를 계산하는 MCMC (Gibbs Sampling)의 step을 converge할 때 까지 돌리는 것이 아니라, 한 번만 돌려서 $p(v,h)$를 approximate하고, 그 값을 사용하여 $\sum_{v,h} p(v,h) v_i h_j$을 계산해 gradient의 approximation 값을 구한다.</p>
<p>MCMC는 원하는 stationary distribution을 가지는 MC를 design하여 목표로하는 distribution을 만들어내는 알고리즘 family를 일컫는다. 이 내용도 꽤나 방대한 내용이므로, 필요하다면 나중에 추가로 포스팅을 하도록 하겠다. Gibbs Sampling은 MCMC 알고리즘 family 중 하나로, 여러 random variable들의 joint probability를 계산하기 위한 알고리즘이다. 사실 내용은 엄청 간단한데, 한 variable을 제외한 나머지 r.v.를 fix하고 나머지 fixed된 r.v.가 주어졌다고 가정하고 conditional probability를 구해 현재 r.v.를 update하는 것을 모든 variable들에 대해 distribution이 converge할 때까지 반복하는 것이다. 이 과정을 엄청나게 많이 반복해서 stationary distribution에 converge했을 정도로 많이 iteration을 돌리게 되면, 우리는 iteration을 돌리면서 얻어내는 sequence들로부터 r.v.들의 joint probability로부터 sample하는 것과 같은 확률로 sample들을 얻을 수 있다.</p>
<p>따라서 이 알고리즘을 사용하면 앞에서 exponential complexity가 문제가 되었던 $p(v,h)$를 계산하는 것이 가능하다. 그런데 문제는 보통 MCMC가 converge할 때 까지 걸리는 시간이 결코 적지 않다는 것이다. 이론적으로 polynomial complexity를 보장할 수는 있지만, 실제 leanring time이 너무 길어져서 practical하게 쓰기 어렵다. 앞에서 설명한 것 처럼 이 distribution이 한 번의 gradient update만을 위해 사용되는 RBM에서는 그 시간을 모두 사용하기에는 너무 비효율적이다.</p>
<p>그래서 RBM은 Gibbs sampleing을 끝까지 돌리는 대신 이런 생각을 하게 된다. &#8216;어차피 정확하게 converge한 distribution이나, 중간에 멈춘 distribution이나 대략의 방향성은 공유할 것이다. 그렇기 때문에 완벽한 gradient 대신 Gibbs sampling을 중간에 멈추고 그 approximation 값을 update에 사용하자.&#8217; 이 아이디어가 바로 Contrastive Divergence의 전부라고 할 수 있다. Contrastive Divergence는 전체 RBM update 과정 중에서 이 Gibbs sampling을 한 번만 돌리는 부분을 일컫는 말이며, Hinton이 처음 제안한 이후 나중에 이 알고리즘이 충분한 시간이 흐른 후에 전체 log likelihood의 local optimum으로 converge한다는 이론적 결과까지 증명된다.</p>
<p>Contrastive Divergence를 도입한 RBM update 알고리즘은 다음과 같다. (notation이 조금 다를 수 있다)</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-4.png" width="600" /></p>

<p>이 과정을 계속 반복하면 우리가 원래 원했던 hidden node와 visible node들의 joint probability를 표현하는 RBM을 learning할 수 있게 된다. RBM이 이렇게 간단하게 learning되는 이유는 restricted라는 조건이 있기 때문이다. 즉, 같은 layer들끼리는 connection이 없기 때문에 $p(h|v) = \prod_j p(h_j|v)$로 간단하게 표현되기 때문에 leanring이 간단해지는 것이다. 그렇기 때문에 restricted 되지 않은 general boltzmann machine은 RBM 처럼 마냥 간단하게 update되지 않는다.</p>
<h3>Deep Beilf Network (DBN)</h3>
<p>DBN은 $\ell$ 개의 layer를 가진 joint distribution을 표현하는 graphical model이다. 참고로 앞에서 RBM은 1-layer 모델이었다. DBN의 확률 모델은 다음과 같은 식으로 표현된다. 이때 $h^k$는 k번째 layer의 hidden variable들을 표현하는 notation이다.</p>
<p>$$P(x, h^1, \ldots, h^\ell) = \bigg( \prod_{k=1}^{\ell-2} P(h^k|h^{k-1}) \bigg) P(h^{\ell-1},h^{\ell})$$</p>
<p>이때 data $x$는 $h^0$이고, 각각의 $P(h^k|h^{k-1})$는 RBM에서 visible unit이 given된 conditional probability로 표현되고, joint probability $P(h^{\ell-1},h^{\ell})$는 RBM의 joint probability로 given된다. 이 모델은 아래와 같은 알고리즘으로 learning할 수 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-5.png" width="600" /></p>

<p>즉, 이 모델은 RBM을 맨 아래 data layer부터 차근차근 stack으로 쌓아가면서 전체 parameter를 update하는 모델이다. 이 모델을 그림으로 표현하면 아래와 같은 그림이 된다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-6.png" width="200" /></p>

<p>마지막 layer는 joint probability를 의미하고, 나머지 layer들은 모두 conditional probability로 표현된다. 참고로 전체를 jointly하게 표현하는 모델을 Deep Boltzmann Machine (DBM) 이라고 하는데, 이 모델의 경우 RBM update를 하는 알고리즘과 비슷한 알고리즘으로 전체 모델을 update하게 된다. 그러나 이 논문이 발표될 당시에는 DBN이 훨씬 간단하고 computational cost가 적기 때문에 DBN이라는 모델을 제안한 것으로 보인다.</p>
<p>이 모델이 의미있는 이유는 joint probability를 잘 표현하는 좋은 graphical model이어서가 아니라, 이 모델로 deep network를 pre-training하고 backpropagation 알고리즘을 돌렸더니 overfitting 문제가 크게 일어나지 않고 MNIST 등에서 좋은 성과를 거뒀기 때문이다. 즉, parameter initialization을 DBN의 joint probability를 maximize하는 (layer-wise로 $\ell$개의 RBM을 learning하는) 방식으로 하고 나서, 그렇게 구해진 parameter들로 deep network를 initialization하고 fine-tuning (backpropation) 을 했을 때, 항상 그 정도 size의 deep network에서 발생하던 overfitting issue가 사라지고 성능이 우수한 classifier를 얻을 수 있었기 때문이다.</p>
<p>DBN으로 unsupervised pre-training한 deep network 모델을 사용했을 때 MNIST 데이터 셋에서 그 동안 다른 모델들로 거뒀던 성능들보다 훨씬 우수한 결과를 얻을 수 있었고, 그때부터 deep learning이라는 것이 큰 주목을 받기 시작했다. 그러나 지금은 데이터가 충분히 많을 경우 이런 방식으로 weight를 initialization하는 것 보다 random initialization의 성능이 훨씬 우수하다는 것이 알려져있기 때문에 practical한 목적으로는 거의 사용하지 않는다.</p>

<h3 id="75-cnn">Convolutional Neural Network (CNN): Introduction</h3>
<p>DBN이 지금은 practical한 목적으로 거의 사용되지 않는 것과는 대조적으로, 1989년에 제안된 이 모델은 아직까지도 많이 쓰이는 deep network 모델이다. 특히 computer vision에 특화된 이 네트워크는 인간의 시신경 구조를 모방하여 인간이 vision 정보를 처리하는 것을 흉내낸 모형이다.</p>
<p>DBN은 overfitting issue를 initialization으로 해결하였지만, CNN은 overfitting issue를 모델 complexity를 줄이는 것으로 해결한다. CNN은 convolution layer와 pooling layer라는 두 개의 핵심 구조를 가지고 있는데, 이 구조들이 model parameter 개수를 효율적으로 줄여주어 결론적으로 전체 model complexity가 감소하는 효과를 얻게 된다.</p>

<h3>Convolutional Neural Network (CNN): Convolution Layer</h3>
<p>먼저 convolution layer에 대해 설명해보자. Convolution layer를 설명하기 전에 먼저 convolution operation에 대해 알아보자. Convolution이란 signal processing 분야에서 아주 많이 사용하는 operation으로, 다음과 같이 표현된다.</p>
<p>$$s(t) = (x * w)(t) = \int x(a)w(t-a) da.$$</p>
<p>예를 들어 이 operation은 주어진 데이터 $x$에 filter $w$를 사용해 데이터를 처리할 때 사용된다. 이 operation을 적용한 간단한 예를 보자. (<a href="http://www.sfu.ca/~truax/conv.html">출처</a>)</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-7.jpg" width="400" /></p>

<p>이렇듯 convolution은 어떤 filter를 사용하여 주어진 image의 적절한 feature를 뽑아내기 위해 사용했던 operation이다. 이때 $s(t)$를 데이터 $x$의 feature map이라고 부른다. Deep learning이 널리 사용되기 이전에는 다른 머신러닝 framework에 이미지를 input으로 넣고 처리하기 위해서는 먼저 filter를 고르고 그 filter로 image를 convolution하는 preprocessing을 거쳐서 적절한 feature map을 얻어낸 이후에 그것을 machine learning framework의 input으로 넣어 돌리는 방식을 사용했었다. 그렇기 때문에 이런 feature engineering이 전체 performance에 큰 영향을 미치는 경우가 많았다. 어떤 filter를 선택할 것이며, 얼마나 많은 filter를 고를 것인지 등의 영역은 feature engineering의 영역이고, 이론적인 영역이 아니기 때문에 machine learning 분야에서는 큰 관심을 두는 분야는 아니었다. 데이터는 잘 처리되었다고 가정하고 그 데이터를 사용해 어떤 좋은 알고리즘을 개발하느냐가 그 동안 머신러닝 framework들의 아이디어였다면, CNN의 핵심 아이디어는 preprocessing이 실제 performance에 크게 영향을 미치니까, 아예 이 preprocessing을 가장 잘해주는, 가장 좋은 feature map을 뽑아주는 convolution filter를 learning하는 모델을 만들어버리자는 것이다.</p>
<p>최대한 작은 complexity를 가지면서 우수한 filter를 표현하기 위한 CNN의 핵심 아이디어는 다음 세 가지이다: sparse interactions (혹은 sparse weight라고도 한다), parameter sharing (혹은 tied wieght라고도 한다), equivariant representations. 즉, CNN은 layer와 layer간에 모든 connection을 연결하는 대신 일부만 연결하고 (sparse weight), 그리고 그 weight들을 각각 다른 random variable로 취급하여 따로 update하는 대신 특정 weight group들은 weight 값이 항상 같도록 parameter를 share한다 (parameter sharing). 그리고 앞의 아이디어를 잘 활용하여 shift 등의 transform에 대해서 equivariant한 (자세한 내용은 밑에서 설명한다) representation을 learning하도록 모델을 구성한다.</p>
<p>Sparse weight를 사용하게 되면 모든 가능한 connection을 사용하는 것 보다 훨씬 적은 표현형을 learning하게 된다는 단점이 있지만, 반대로 model의 complexity가 낮아진다는 장점이 존재한다. CNN은 vision과 관련된 task를 수행하도록 design된 network라는 것은 이미 언급한바 있다. 이런 vision 데이터를 처리하는 task를 하게 될 경우에는 주어진 input의 dimension에 비해 실제 필요한 feature의 dimension은 극히 적다는 domain knowledge를 우리는 이미 가지고 있다. 즉 input인 이미지의 경우 픽셀 값이 적으면 몇 백에서 많으면 몇 백만에 이를 정도로 dimension이 엄청나게 높지만, 우리가 필요한 &#8216;feature&#8217;는 그 중에서도 극히 일부 영역, 이를테면 edge detection 등의 그에 비해 훨씬 적은 dimension으로 표현 가능하기 때문에 최대한 parameter를 줄여서 더 효율적인 feature map을 뽑아내기 위하여 weight를 sparse하게 사용한다. 이를 그림으로 표현하면 아래와 같다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-9.png" width="600" /></p>

<p>이 그림은 같은 output $s_3$에 영향을 주는 edge들과 input node를 표현한 그림이다. 왼쪽은 가능한 connection이 전부 있는 것이 아니라 그 일부만 존재하고, $s_3$에 영향을 주는 input이 $x_2, x_3, x_4$ 뿐이지만 오른쪽은 모든 가능한 connection이 있어서 model parameter의 개수가 크게 차이나고 모든 input이 $s_3$에 영향을 주는 것을 알 수 있다. 실제 image 데이터를 처리하기에는 왼쪽 모델이 조금 더 나은데, 그 이유는 한 feature를 결정하기 위해서 모든 image 정보가 필요한 것이 아니라, image의 일부분만 필요하기 때문이다. 예를 들어 내가 face segmentation, 즉 얼굴 사진에서 눈 코 입 등을 찾아내는 task를 수행한다고 하면, 주어진 사진에서 &#8216;눈&#8217;이 어디인지 표현하기 위해서 모든 이미지가 다 필요한 것이 아니라 눈 주변의 local한 데이터만 필요할 것이라고 유추할 수 있다. 오른쪽 그림은 필요하지 않은 배경까지 모두 고려하여 눈에 대한 정보를 찾는 셈이고, 왼쪽 그림은 local한 정보만을 주고 눈에 대한 정보를 처리하게 하는 것이다. 따라서 vision task를 처리하기에는 적절한 sparse weight가 더 효율적인 모델이라는 것을 알 수 있다. 때문에 CNN의 convolution layer는 hidden node 하나가 image의 local한 patch와 연결되어있는 형태로 되어있다. 예를 들어 한 hidden node 마다 image의 3 by 3 patch만을 연결하는 방식이다. 그림으로 표현하면 아래와 같은 식이다. (출처: <a href="http://www.codeproject.com/Articles/523074/Online-handwriting-recognition-using-multi-convolu">Code project - Online handwriting recognition using multi convolution neural networks</a>)</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-11.png" width="600" /></p>

<p>여기에서 subsampling은 일단 나중에 설명하도록 하고 (subsampling part가 pooling layer에 해당한다) 가장 왼쪽의 image data의 일부분에 해당하는 patch만 다음 hidden layer의 한 unit에 연결하는 것이다. 이런 식으로 네트워크를 만들게 되면, patch size에 따라 다음 feature map의 size가 결정될 것이다. 예를 들어 100 by 100 이미지에서 5 by 5 patch를 사용해 convolution layer를 구축할 경우, 이 layer의 feature map은 96 by 96이 될 것이다.</p>
<p>CNN은 이런 sparse weight에 parameter sharing을 또 더하여 vision task에 최적화된 network를 learning하게 된다. Parameter를 share하게 되면 그러지 않는 것과 비교하여 보다 적은 parameter만을 가지게 되므로 model의 complexity가 줄어드는 효과가 있을 뿐 아니라, 각각의 patch마다 따로 필터를 learning하는 대신, 모든 patch에 동일한 필터를 적용하도록 강제하는 효과가 있다. CNN은 아래 그림과 같이 각각의 hidden node들이 같은 location에 대해 같은 weight를 가지도록 설정하여 모든 hidden node들이 각각 다른 patch에 대해 같은 filter를 처리하는 것과 같은 형태로 모델을 디자인한다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-8.png" width="200" /></p>

<p>위 그림에서 같은 색으로 칠해진 edge는 서로 같은 weight를 가진다. 위 그림에서 볼 수 있듯, CNN은 fully connected layer를 가지지 않고, 그 sparse한 weight들에서도 서로 weight를 공유하도록 설정되어있다. 그렇지 않은 네트워크와 비교해보면 다음과 같다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-10.png" width="600" /></p>

<p>각각의 그림에서 검은색으로 연결된 edge들은 서로 같은 parameter를 가진다. 즉, 왼쪽은 한 번에 5개의 edge가 같은 weight를 가지지만, 오른쪽은 하나의 parameter로 한 개의 edge만 표현할 수 있다. 이렇게 표현하게 되면 convolution layer operation이 간단한 matrix multiplication으로 주어지게 되어 gradient를 계산하기 한 층 더 수월해진다는 장점도 존재한다. 자세한 내용은 algorithm 쪽에서 다루도록 하자.</p>

<p>마지막으로 equivalent representations는 위와 같은 sparse weight와 tied weight를 어떤 특정한 형태로 효율적으로 배치하게 되었을 때, 주어진 input의 변화에 대해 output이 변화하는 방식이 equivariant해지는 현상을 의미한다 (equivalent가 아니다). 예를 들어 function $f$가 function $g$와 equivariant하다는 의미는, $f(g(x)) = g(f(x))$인 경우를 말한다. 이미지 처리를 예로 들면 $g$는 임의의 linear transform이라고 할 수 있다. Shift, rotate, scale등의 image에 대한 transform들이 그것인데, 우리는 같은 이미지가 돌아가거나 움직이거나 살짝 scale되더라도 그 이미지가 어떤 이미지인지 잘 판별할 수 있지만, 컴퓨터에게는 그런 transform이 픽셀 값이 완전히 바뀌는 결과를 낳기 때문에 어떤 정보인지 판별하기 어려운 것이다. 그런데 만약 우리가 어떤 transform $g$에 대해 equivariant representation을 만들어내는 network $f$를 만들 수 있다면, input이 shift되거나 rotate되더라도 항상 적절한 representation을 가지도록 할 수 있을 것이다. (실제 CNN은 shift에만 equivariant하다.)</p>
<p>즉, 앞에서 shared parameter가 각각의 patch에 대해 같은 filter를 처리하는 것 처럼 설정하였기 때문에, 만약 image가 shift되더라도 feature map의 형태가 크게 뒤틀리는 것이 아니라, feature map도 image와 함께 shift되는 형태를 보이게 될 것이다.</p>
<p>그런데 실제로는 한 image에 한 개의 filter가 아니라 여러 개의 filter가 필요할 수도 있다. 앞에서 설명한 convolution layer는 한 개의 convolution filter를 표현할 뿐이지만, 실제로는 이런 convolution filter가 한 개가 아니라 여러 개 만든 다음 그 값들을 concate하여 feature map을 표현해야할 수도 있다. 그렇기 때문에 실제로 CNN model은 한 개의 convolution layer가 아니라 아래와 같이 여러 개의 convolution layer가 결합된 꼴을 하고 있다. 참고로 공식적으로는 각각의 layer 혹은 filter를 kernel이라 하고, 그 kernel들이 모여있는 것을 한 layer로 부른다. (<a href="http://masters.donntu.org/2012/fknt/umiarov/diss/indexe.htm#p4">출처</a>)</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-12.png" width="600" /></p>

<h3>Convolutional Neural Network (CNN): Pooling Layer</h3>
<p>Convolution Layer만 여러 개 연결하여 deep network를 구성하는 것도 가능하지만, 실제로는 더 dimension이 낮은 feature map을 얻기 위하여 subsampling이라는 것을 하게 된다. 앞에서 예로 들었던 것처럼 100 by 100 이미지에 5 by 5 convoltion patch size를 가지는 convolution layer를 연결할 경우 feature map의 size는 96 by 96이 되는데, 사실 이 96 by 96 feature map은 서로 매우 highly correlated 되어있는 값이 것이다. 특히 서로 이웃해있을수록 겹치는 영역이 많기 때문에 거의 비슷한 값을 가질 것이라고 예상할 수 있다. 아래 그림을 보자.</p>

<p><img src="http://SanghyukChun.github.io/images/post/75-11.png" width="600" /></p>

<p>이미 앞에서 나왔던 그림이지만 설명을 위하여 다시 가져왔다. Pooling layer는 convolution layer의 feature map을 조금 더 줄여주는 역할을 한다. 전체 feature map을 그대로 들고가는 대신, 예를 들어 96 by 96 image feature map을 2 by 2 patch들로 쪼개는 것이다. 이렇게 할 경우 총 48 by 48 개의 output이 생기게 될텐데, subsampling이라는 것은 각각의 2 by 2 patch는 max, average 등의 operation을 행하는 것을 의미한다. 보통 max operation을 사용하고, 이 경우 간단하게 max pooling을 사용한다 라고 이야기 한다. 가끔 average pooling을 사용하는 경우도 있지만 보통 classification을 위한 모델들은 max pooling을 사용하니 참고하면 좋을 것 같다.</p>
<p>CNN은 이렇게 convolution layer와 pooling layer가 결합된 형태로 deep 하게 구성이 된다. 개인적으로 아래 그림이 CNN의 convolution layer와 max pooling layer를 잘 표현하는 그림이라고 생각한다. (<a href="http://inspirehep.net/record/1252539/plots">출처</a>)</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-13.png" width="300" /></p>

<h3>Convolutional Neural Network (CNN): Backpropagation</h3>
<p>CNN의 기본 model은 알았으니 이제 이 network의 parameter를 어떻게 learning해야할지 알아보자. 기본적인 update algorithm은 <a href="http://SanghyukChun.github.io/74">이전 글</a>에서 설명했던 <a href="http://SanghyukChun.github.io/74#backprop">backpropagation algorithm</a>을 사용한다.</p>
<p>먼저 간단한 max pooling layer 부터 살펴보자. Pooling layer는 아래 p by q size의 patch 중에서 max 값을 선택하는 layer이다. 때문에 이를 수식으로 표현해보면 다음과 같이 쓸 수 있다. $(x,y)$는 pooling layer feature map의 x,y좌표를 나타내고, ($h_l$은 l번째 layer의 hidden variable들)</p>
<p>$$h_{l+1} (x, y) = max_{a-p\leq a\leq a+p, b-q\leq b\leq b+q}(h_l (x+a, y+b))$$</p>
<p>Parameter는 없으므로 $\frac{\partial h_{l+1} }{\partial h_{l}}$만 계산하면 된다. 이 경우 주어진 $(x,y)$가 만약 max pooling을 통해 선택된 값이라면 값을 그대로 passing하고, 만약 선택되지 않은 값이라면 0을 할당하면 된다.</p>
<p>Convolution layer는 operation이 꽤 복잡한데, 먼저 아래 그림을 보자.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-14.png" width="400" /></p>

<p>이때 l+1 번째 layer 중에서 i 번째 convolution filter의 x,y 좌표 값은 아래와 같이 표현된다. 아래 conv layer의 kernel은 m 개, 위 conv layer의 kernel은 n개 라고 해보자.</p>
<p>$$h_{l+1}(i, x,y) = \sum_{j=1}^m \sum_{a=1}^p \sum_{b=1}^q h_l (j, x+a, y+b) * w(i,n;a,b)$$</p>
<p>값이 좀 많이 복잡하긴 한데, 미분 값을 계산해보면, parameter의 gradient는 $\frac{E}{\partial w} = \sum_x \sum_y \frac{E}{h_{l+1} } (x,y) h_l (x,y)$와 같이 바로 전 layer의 pixel값에 대해 gradient 값을 곱한 것을 전부 더한 형태로 구할 수 있고, $\frac{E}{\partial h_l} $은 weight w로 이전 layer의 gradient를 convolution한 것들을 전부 더한 것과 같은 결과를 얻게 된다.</p>
<p>CNN의 모든 operation들은 단순 연산이 많고 branch가 없기 때문에 core가 많고, 모든 core가 하나의 operation pointer를 공유하는 GPU를 사용해 효율적으로 parallization하기 좋다. 보통 CNN은 <a href="caffe.berkeleyvision.org">caffe</a>라는 C++ library를 사용해 learning하기 때문에 위에서 언급한 알고리즘을 실제로 구현할 일은 많지 않을 것 같다.</p>

<h3>정리</h3>
<p>Deep learning은 neural network의 layer를 deep 하게 쌓은 것에 지나지 않지만, 아무것도 하지 않고 layer를 깊게 쌓기만하면 overfitting이 너무 강하게 발생하여 제대로 된 결과를 얻을 수 없다. 이 글에서는 두 가지 overfitting을 피하는 방법을 설명하였다. 첫 번째 DBN은 주어진 network를 DBN이라는 RBM이 stack으로 쌓여있는 graphical probabilistic model로 표현한다. 그리고 주어진 데이터에 대해 likelihood를 maximize하는 parameter를 찾아서 그 값을 initial point로 사용해 gradient descent를 실행한다. 이때 RBM의 gradient 값을 정확히 구하는 것이 힘들기 때문에 Gibbs sampling의 iteration을 converge할때까지 돌리는 대신 한 번만 돌리는 Contrastive Divergence 알고리즘이 제안된다. DBN은 RBM을 layer wise greedy update rule을 통해 parameter를 update하게 된다.</p>
<p>두 번째로 설명한 CNN은 sparse weight, tied weight, equivariant representation이라는 세 가지 아이디어를 기반으로 모델의 complexity는 최소화하면서 vision에 최적화되어있는 형태의 모델이다. Parameter update는 backpropagation으로 하게 되는데, 보통 구현되어있는 툴을 사용하게 되므로 세부 update rule을 직접 구현할 일은 많지 않을 것 같다.</p>
<p>이 밖에 regularization이나 optimization method들과 같이 deep learning과 관련된 중요한 개념들 역시 추후 다른 포스트를 통해 소개할 수 있도록 하겠다.</p>

<h3>Reference</h3>
<ul>
  <li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
  <li><a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Bengio, Yoshua, et al. “Greedy layer-wise training of deep networks.” Advances in neural information processing systems 19 (2007): 153.</a></li>
  <li><a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. “A fast learning algorithm for deep belief nets.” Neural computation 18.7 (2006): 1527-1554.</a></li>
  <li><a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">Classification datasets results</a></li>
  <li><a href="http://deeplearning.net/tutorial/rbm.html">DeepLearning.net - Restricted Boltzmann Machines (RBM) Tutorial</a></li>
  <li><a href="http://deeplearning.net/tutorial/lenet.html">DeepLearning.net - Convolutional Neural Network (LeNet) Tutorial</a></li>
  <li><a href="http://image.diku.dk/igel/paper/AItRBM-proof.pdf">Fischer, Asja, and Christian Igel. “An introduction to restricted Boltzmann machines.” Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Springer Berlin Heidelberg, 2012. 14-36.</a></li>
  <li><a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Hinton, Geoffrey. “A practical guide to training restricted Boltzmann machines.” Momentum 9.1 (2010): 926.</a></li>
  <li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">Bengio, Yoshua. “Learning deep architectures for AI.” Foundations and trends® in Machine Learning 2.1 (2009): 1-127.</a></li>
</ul>

<h3>변경 이력</h3>
<ul>
  <li>2015년 9월 21일: 글 등록</li>
  <li>2015년 10월 31일: 오타 수정</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Models of Visual Attention (NIPS 2014)]]></title>
    <link href="http://SanghyukChun.github.io/91/"/>
    <updated>2015-09-19T13:14:00+09:00</updated>
    <id>http://SanghyukChun.github.io/91</id>
		<content type="html"><![CDATA[<p>이번에 리뷰할 논문은 Google DeepMind가 NIPS 2014에 발표한 Recurrent Neural Networ와 Reinforcement Learning을 결합한 <a href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a> 이라는 논문이다. <a href="http://SanghyukChun.github.io/90">지난 글</a>에서 리뷰했던 <a href="http://arxiv.org/abs/1312.5602">Playing Atari With Deep Reinforcement Learning</a>과 같은 연구팀에서 진행한 연구인듯하다. Atari 논문에서는 전통적인 RL 문제인 &#8216;게임&#8217;을 풀기 위하여 CNN으로 action-value function을 모델링하고 value iteration을 대체하는 새로운 action-value function learning 모델과 알고리즘을 제안했다면, 이 논문은 기존 RL 문제라기보다는 오히려 좀 더 클래식한 classification 문제라고 할 수 있는 image recognition 문제에 RNN 구조와 RL 구조를 결합하여 reward maximization optimization problem을 푸는 모델과 알고리즘을 제안한다.</p>

<h3>Motivation</h3>
<p>CNN 기반의 image classification은 이미 인간이 할 수 있는 수준에 거의 근접하였다. 그러나 CNN을 사용한 기존 접근 방법은 input size가 fix되어있어야하고, pixel size가 엄청나게 크면 그만큼 computation cost가 그대로 늘어난다는 단점이 존재한다. 하지만 실제 사람이 물체를 인식하거나 할 때를 생각해보면, <a class="red tip" title="주의, 집중 등의 뜻이 있다.">&#8216;attention&#8217;</a>이 존재한다는 것을 알 수 있다. 즉, 배경을 포함한 모든 정보를 사용하여 물체를 인식하는 것이 아니라 자신이 focus하고 있는 일부분과 그 주변 부의 정보들을 &#8216;훑어보면서&#8217; 훑어본 sequence들을 복합적으로 종합하여 결론을 내린다는 것을 알 수 있다. 만약 이런 방식으로 &#8216;focusing&#8217;을 하는 모델을 만들 수 있다면 지금 보고 있는 화면의 일부 만을 사용하므로 더 적은 &#8216;bandwidth&#8217;의 데이터를 저장해도 되고, 정보를 처리하기 위해 좀 더 적은 양의 pixel이 필요할 것이다. 그렇기 때문에 단순 pixel map을 파악하는 것 보다 이런 &#8216;atenttion&#8217;을 고려한 훨씬 더 human-like한 모델을 설계한다면 기존 CNN의 단점을 해결하는 데에 도움이 될 수 있을 것이라는 것이 이 논문의 motivation이다. 이 논문은 visual scence의 attention-based processing을 attention을 어떻게 취할 것인지를 action으로 생각하여 일종의 control problem으로 모델링하여 문제를 해결한다.</p>
<p>이 논문은 기존 CNN기반 approach들처럼 각 time stamp에 대해 전체 이미지를 한 번에 처리하거나 혹은 이미지 박싱을 하는 대신에 모델이 attend해야할 다음 location을 과거 정보와 현재 reward를 기반으로 선택하는 모델을 제안한다. 이 모델은 기존 CNN 모델과는 다르게 image의 크기가 바뀌더라도 computation이나 memory가 그 크기에 linear하게 증가하지 않고 모델에 의해 조절 가능하다는 특징이 있다. </p>

<h3>The Recurrent Attention Model (RAM)</h3>
<p>구체적인 모델을 정의하기 위하여 먼저 attention problem을 정의해보자. 이 논문은 attention problem을 visual 환경과 interact하는 목표지향적인 agent가 행하는 sequential decision process로 정의한다. 각 time stamp하다 agent는 bandwidth-limited sensor만을 사용해 environment를 observe하게 된다. 즉, agent는 한 번에 전체 environmnet를 감지하지 않고, 매 time stamp마다 local한 정보 만을 감지한다. 대신 agent는 sensor를 어떻게 사용할 것인지, 다시 말해서 sensor의 다음 location을 선택하는 action을 취할 수 있다. 마치 사람이 시선을 쭉 움직이면서 visual scence을 훑어보는 것처럼 말이다. 만약 reward를 image classification과 관련되도록 정의한다면 이런 attention 문제는 한 번에 센서가 볼 수 있는 정보가 한정되어있고, action을 어떻게 취하느냐에 따라 결과가 (reward가) 크게 달라지기 때문에 state별로 reward를 maximize하는 action을 취하는 policy를 learning하는 reinforcement learning 문제로 생각할 수 있다.</p>
<p>이제 모델을 조금 더 구체적으로 정의해보자.</p>

<ul>
	<li><p>$x_t$: agent가 time $t$에 관측한 environment (전체 image의 일부분)</p></li>
	<li><p>$\ell_t$: agent가 time $t$에 focus하고 있는 region의 좌표 값, 실제 agent는 $\ell_t$의 주변을 관측한다. 이 값은 논문에서 sensor control의 action으로 사용된다.</p></li>
	<li><p>$a_t$: agent의 time $t$에서의 environment action. Classification의 경우는 $a_t$가 classification을 하는 decision을 내리는 용도로 사용된다. 즉, MNIST data로 실험하는 경우 가능한 $a_t$의 경우 수는 [0-9]이며, 각각 0부터 9까지의 숫자를 나타내게 된다.</p></li>
	<li><p>$r_t$: agent가 maximize하고자하는 목표 값이다. Image classification은 time $t$에서 정확한 classification을 했으면 reward가 1, 아니라면 reward가 0이 되도록 설정하였다고 한다.</p></li>
	<li><p>$h_t$: time $t$에서 agent의 state를 &#8216;hidden&#8217; state로 표현한 것으로, 원래 state는 $s_{1:t} = x_1, \ell_1, a_1, \ldots, x_{t-1}, \ell_{t-1}, a_{t-1}, x_t$로 표현되지만, 만약 $h_t$를 이 모든 state들을 &#8216;summarize&#8217;하는 것과 같이 모델링 할 수 있다면, 전체 state를 보는 대신, summerized internal state인 $h_t$로 state 표현을 대신할 수 있다.</p></li>
</ul>

<p>위와 같은 모델을 설계하기 위하여 이 논문에서는 다음과 같은 RNN 형태의 neural netork model은 제안하고 있다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/91-1.png" width="200" /></p>

<p>Agent에게는 매 시간마다 전체 image의 일부분 정보인 $x_t$와 바로 전 state를 표현하는 $h_{t-1}$이 input으로 들어온다. 이 정보들을 사용하여 agent는 sensor를 어떻게 움직일 것인지 결정하는 (다음으로 살펴볼 위치 정보를 결정하는) $\ell_t$와 주어진 task를 수행하는 action (이 경우는 image classification이므로 $a_t$ 그 자체가 label 정보를 담은 action이 된다) $a_t$라는 action을 취하게 된다. 이 모델을 시간에 대해 unfold한 것이 논문에 나와있는 Figure 1.c이다. </p>

<p><img src="http://SanghyukChun.github.io/images/post/91-2.PNG" width="600" /></p>

<p>이때 $f_g, f_\ell, f_a$는 각각 input data에 대한 정보를 처리하는 네트워크 (glimpse network $f_g$), 위치 정보를 결정하는 네트워크 (location network $f_\ell$), 그리고 action의 값을 결정하는 네트워크를 (action network $f_a$) 의미한다. 각각의 네트워크에 대해 하나하나 살펴보도록 하자.</p>
<p>먼저 gimpse network $f_g$는 주어진 input image $x_t$와, 그 중 일부의 위치정보 $\ell_t$ 만을 받아서 원래 image의 일부분만 &#8216;attention&#8217; 하여 적절한 feature를 뽑아내는 네트워크이다. Glimpse라는 말은 한국어로 &#8216;언뜻 보다&#8217; 라는 의미를 가지고 있는데, 다시 말해 주어진 이미지를 살짝 훑어보고 그 정보를 잘 정리하여 주어진 RNN core network가 정보를 잘 처리할 수 있도록 만들어주는 역할을 한다. 이 네트워크는 아래 그림과 같이 구성되어있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/91-4.PNG" width="600" /></p>

<p>이 네트워크는 glimpse sensor라는 것의 output과 $\ell_{t-1}$의 정보를 concate하는 역할을 한다. 여기에서 중요한 것은 glimpse sensor라는 것인데, 이 센서는 마치 사람의 &#8216;망막처럼&#8217; (retina-like) 정보를 처리하는 역할을 한다. 즉, 이 센서를 사용해 전체 이미지에서 좁은 영역에 해당하는 정보를 뽑아내는 역할을 하는 것이다. 이 센서는 아래와 같은 구조를 띄고 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/91-3.PNG" width="600" /></p>

<p>Glimpse sensor는 주어진 이미지 $x_t$의 한 위치 $\ell_{t-1}$을 받아서 해당 위치에서 특정 거리 $d_1$만큼 떨어진 이미지를 추출한다. 그리고 나서 그것보다 더 넓은 범위인 $d_2$만큼 떨어진 이미지를 추출하고, 다시 그것보다 큰 $d_3$만큼 떨어진 이미지를 추출하는 과정을 $k$번 반복하여 $k$개의 patch를 만든다. 이렇게 하는 이유는 사람의 망막이 중심부에 가까울수록 데이터의 해상도를 높게 받아들이고 중심부에서 멀어질수록 이미지가 흐려지도록 처리하기 때문이다. Sensor에서 이 값들을 생성하고 나면 $\rho(x_t, \ell_{t-1})$ 이라는 transform을 처리하게 되는데, image classification 실험을 위해서 사용한 transform은 모든 사진을 같은 크기로 resize한 다음 concate하는 transform이라고 한다. 이렇게 될 경우 중심부에 가까울수록 정보량이 많아지고 정확해지지만 멀어질수록 해상도가 낮은 정보를 받게 될 것이다.</p>
<p>모든 glimpse network의 lyaer들은 기본적인 inner product layer를 사용한다 ($Linear(x) = Wx + b$). 그리고 neuron으로는 ReLU unit ($ReLU(x) = \max(x,0))$을 사용한다. 즉, </p>
<p>$$h_g = ReLU(Linear(\rho(x,\ell))), h_\ell = ReLU(Linear(\ell)) $$</p>
<p>그리고 glimpse network의 output $g$는 $g = ReLU(Linear(h_g) + Linear(h_\ell)$로 정의한다. Glimpse network 말고도 location network와 core network도 거의 같은 방식으로 정의하게 되는데, 각각 $f_\ell (h) = Linear(h)$, $h_t = f_h(h_{t-1}) = ReLU(Linear(h_{t-1}) + Linear(g_t) )$로 정의한다. 이때, core network는 state vecotr $h$의 dimension이 256인 LSTM을 사용한다. 마지막으로 action network $f_a (h) = exp(Linear(h))/Z$, 즉 linear softmax classifier로 정의한다. 그 이외 설정은 모두 앞에서 설명한 것과 같다.</p>

<h3>Training</h3>
<p>실험에 대해 알아보기 전에, 이 network를 어떻게 learning할 수 있는지 잠시 살펴보도록하자. 이 네트워크에서 우리가 learning해야할 parameter는 glimpse network, core network 그리 action network의 parameter인 $\theta_g, \theta_h, \theta_a$이다. Optimization을 하기 위한 target function은 total reward를 maximize하는 함수로 설정할 것이다. 조금 더 formal한 설명을 위하여 interaction sequences $s_{1:N}$과, 그것의 모든 가능한 state들의 distribution $p(s_{1:T}; \theta)$을 introduce해보자. 이렇게 정의할 경우 우리는 아래와 같은 target function의 $p(s_{1:T}; \theta)$에 대한 expectation을 maximize하는 문제로 reward maximization problem을 formal하게 정의할 수 있다.</p>
<p>$$J(\theta) = \mathbb E_{p(s_{1:T};\theta)} \bigg[ \sum_{t=1}^T r_t \bigg] = \mathbb E_{p(s_{1:T};\theta)} \big[R\big]. $$</p>
<p>그러나 이 함수 $J(\theta)$를 maximize하는 것은 trivial한 일이 아닌데, 다행스럽게도 이미 예전에 다른 work에서 이 $J(\theta)$의 gradient의 sample approximation이 아래와 같이 유도된다는 것을 보였다고 한다.</p>
<p>$$\nabla J(\theta) = \sum_{t=1}^T \mathbb E_{p(s_{1:T};\theta)} \big[ \nabla_\theta \log \phi (u_t ~|~ s_{1:t};\theta) R \big] \simeq \frac{1}{M} \sum_{i=1}^M \sum_{t=1}^T \nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta) R^i .  $$</p>
<p>위의 관계식에서의 $\nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta)$은 RNN의 gradient를 계산해야하는 것으로 간단하게 구할 수 있다. 다만 이 관계식이 unbiased estimation of gradient를 제공하기는 하지만, variance가 너무 높다는 단점이 있다고 한다. 그래서 이 논문에서는 아래와 같은 form으로 gradient를 estimation하여 variance의 값을 줄이도록 하였다고 한다.</p>
<p>$$ \frac{1}{M} \sum_{i=1}^M \sum_{t=1}^T \nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta) (R_t^i - b_t), \mbox{ where } R_t^i = \sum_{t^prime=1}^T r_{t^\prime}^i. $$</p>

<h3>Experiments</h3>
<p>이 논문에서는 MNIST에 대해 실험을 진행했다. 실험은 우리가 보통 사용하는 centered digit, non-centered digit은 물론이고, <a title="흐트러트리다, 어지럽히다라는 뜻이 있다" class="red tip">cluttered</a> non-centered digit에 대한 실험도 진행했다. 마지막 실험은 MNIST digit에 random하게 8 by 8 subpatch를 더하여 데이터를 조금 더 &#8216;지저분하게&#8217; 만들어서 실험을 진행했다. 비교군은 MNIST의 state-of-art인 모델들이 아니라, 가장 간단한 2 layer fully connect neural network를 사용하였다. 아마 state-of-art 모델들은 워낙 성능이 뛰어나서 아직 극복이 안되는 모양이다. 실험 결과는 아래와 같다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/91-5.png" width="600" />
<img src="http://SanghyukChun.github.io/images/post/91-6.png" width="600" /></p>

<p>결과가 outperform하다고 할 수는 없지만, 간단한 2-layer fully connected neural network보다 특수한 경우들에 대해 훨씬 잘 동작함을 볼 수 있고, 무엇보다 올바른 classification을 하기위한 policy rule이 (초록색 선으로 표현된 것들) 상당히 human-likely 한 결과를 보인다는 것이 고무적이다. 물론, 이 결과가 GoogleNet이나 AlexNet에 비해 엄청 우수한 결과를 보이느냐하면 그것은 아니지만, 새로운 형태의 접근을 할 수 있다는 가능성을 제시하는 것 만으로도 의미가 있다고 본다. 보다 자세한 실험에 대한 설명은 논문을 참고하면 좋을 것 같다.</p>

<h3>Summary of Visual Attention</h3>

<ul>
  <li>기존 CNN 기반 접근 방식의 문제점들 - 이미지 사이즈에 linear한 computation cost, human-like 하지 않은 처리 방법 등 - 을 처리하기 위한 목적으로 디자인되었음</li>
  <li>사람이 정보를 한 번에 처리하는 것이 아니라 배경을 무시하고 이미지의 일부만 인식하듯, ‘attention’을 모델에 대입하는 아이디어를 제안함</li>
  <li>Attention을 neural network에 도입하기 위하여 RNN과 Reinforcement Learning을 결합한 형태의 모델을 사용함</li>
  <li>RNN의 input으로는 이미지 정보, 위치 정보가 있으며, 그것들을 조금 더 retina-like하게 처리하기 위한 glimpse network라는 것을 추가로 붙여서 input으로 사용함</li>
  <li>output으로는 action network, location network가 있는데, action network는 classification을 위한 linear classifier이고, location network는 다음 state에 영향을 미치는 recurrent하게 다음 input과 함께 glimpse network의 input으로 쓰이는 값임</li>
  <li>reward는 time t에 올바른 classification을 하였는지 아닌지를 판단하여 0-1 으로 reward를 return함</li>
  <li>train을 하기 위하여 reward maximization을 하는데, 직접 gradient를 구하는 것이 non-trivial하여 estimation값을 사용함. 이때 unbaised estimator는 variance가 높아서 low variance estimator를 사용하여 update를 함</li>
  <li>MNIST에 대해 실험을 하였으며, centered digit은 기존 state-of-art에 비해 턱없이 모자라지만, 사람은 구분할 수 있지만 머신은 제대로 판단하지 못하는 cluttered non-centered digit을 기존 fully connected network보다 훨씬 잘 판별하는 것을 알 수 있었음</li>
</ul>

<h3>Reference</h3>
<ul><li><p><a href="http://arxiv.org/abs/1406.6247">Mnih, Volodymyr, Nicolas Heess, and Alex Graves. &#8220;Recurrent models of visual attention.&#8221; Advances in Neural Information Processing Systems. 2014.</a></p></li></ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Playing Atari with Deep Reinforcement Learning (NIPS 2013)]]></title>
    <link href="http://SanghyukChun.github.io/90/"/>
    <updated>2015-09-15T19:56:00+09:00</updated>
    <id>http://SanghyukChun.github.io/90</id>
		<content type="html"><![CDATA[<p>이번에 리뷰할 논문은 Google DeepMind가 NIPS 2013에 발표한 Deep Learning과 Reinforcement Learning을 결합한 <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> 이라는 논문이다. 그 전에도 DNN과 RL을 결합하려는 시도는 있었지만, 거의 사람이 플레이하는 수준으로 의미있는 수준까지 도달한 work은 이 work이 처음인듯 하다. 글을 시작하기 전에 먼저 이 논문에서 한 결과부터 살펴보자. Atari라는 게임 콘솔을 가지고 이 논문의 method를 적용한 결과이다. 2분 5초부터 시작되는 터널링 전략이 진짜 걸작이다.</p>

<div style="text-align: center;"><iframe width="420" height="315" align="middle" src="https://www.youtube.com/embed/iqXKQf2BOSE" frameborder="0" allowfullscreen=""></iframe></div>

<p>충격적인 사실은 다른 hand-coding feature나 parameter 튜닝 없이 오직! vision data만 사용해서 이런 결과를 냈다는 사실이다. 빨간 공이 object고  빨간 판이 내가 움직이는거라는 기본적인 hand-craft feature 조차 없이 이런 결과를 냈다는 것이다. Deep leanring이 RL에서 뛰어난 성과를 보이지 못한 이유가 주로 데이터에 관련된 것이었음을 생각해보면 대단한 결과라고 할 수 있다. 최근에는 이 work을 기반으로 <a href="http://arxiv.org/abs/1406.6247">Image Attention (NIPS 2015)</a> 이라고 부르는 work도 나온 것 같다. Image Attention 논문은 다음에 정리해보도록하겠다.</p>

<h3>Background</h3>
<p>이 논문은 RL 중에서도 MDP에 초점을 맞추고 있으며 (사실 MDP가 아닌 RL은 거의 없다고 봐도 무방하지만) 그 중에서도 model-free technique 중 하나인 <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning algorithm</a>을 neural network를 통해 해결하고 있다. 이 글은 RL에 대해 어느 정도 기초적인 지식이 있다고 가정하고 쓸 것이기 때문에 조금 더 자세한 내용은 <s>추후 작성할 RL 관련 포스트나</s> <a href="http://SanghyukChun.github.io/76">이 포스트</a>나 다른 reference들을 참고하면서 읽으면 좋을 것 같다.</p>
<p>Reinforcement Learning을 위해서는 먼저 환경 $\mathcal E$을 정의해야한다. 이 논문에서는 Atari 에뮬레이터가 환경이 될 것이다. Atari 에뮬레이터 환경을 구성하는 요소는 action의 sequence들, observe하는 화면과 최종 reward (점수)가 될 것이다. 그림으로 표현하면 다음과 같은 식이다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/90-6.png" width="600" /></p>

<p>매 시간마다 agent는 legal game action $\mathcal A = \{1, \ldots, K\}$ 중에서 action $a_t$를 하나 선택한다. 예를 들어 아래 그림과 같은 컨트롤러의 버튼 중 어떤 버튼을 누를 거인지를 결정하는 것이다. 옆으로 움직이는 버튼, 기타 다른 버튼들 하나하나가 $\mathcal A$의 element이며, 그 중 하나가 action $a_t$가 되는 것이다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/90-1.png" width="200" /></p>

<p>게임을 하는 동안 컨트롤러의 버튼을 누르는 action들이 모이게 되면 현재 점수에 어떻게든 영향을 주게 되고, 그 결과로 최종 점수가 결정된다. 즉, Atari 게임 환경에서 reward $r_t$는 게임 score이며, 현재 내가 선택한 action은 바로 reward에 반영되는 것이 아니라 엄청나게 나중에 반영될 수도 있는 것이다.</p>
<p>또한 게임 조작을 통해 변화하는 것 중 우리가 관측할 수 있는 것은 실제 게임 화면의 pixel 값들 뿐이다 (이 논문에서는 time stamp $t$에서의 픽셀 값을 $x_t$라고 정의하였다). 때문에 이 논문에서는 vision 데이터를 사용해서 state를 정의하는데, state를 정의하는 방식이 상당히 재미있다. 간단하게 $x_t$를 state로 삼으면 될 것 같지만, 실제로는 화면 하나만 보고 알 수 있는 정보가 제한적이고 현재 상태를 정확하게 판단하기 위해서는 vision정보와 내가 행한 action을 포함한 과거 history들까지 모두 있지 않으면 안되기 때문에 이 논문은 state $s_t$를 action과 image의 sequence로 정의한다. 예를 들어 아래 스크린샷에서 공은 왼쪽으로 움직일까 오른쪽으로 움직일까? 만약 관성을 implement한 게임이라면 (움직이는 버튼에서 손을 떼도 조금 움직이는 게임이라면) 과연 play agent는 위로 올라가고 있을까 아니면 아래로 내려가고 있을까 그것도 아니면 멈춰있을까? 이렇듯 한 time stamp에 대한 vision 데이터로는 파악할 수 있는 데이터가 너무 제한적이기 때문에 모든 history가 반드시 필요하다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/90-2.png" width="300" /></p>
<p>다시 말해서, $s_t  = x_1, a_1, \ldots, a_{t-1}, x_t$ 가 된다. 이런 방식으로 modeling을 하게 되면 policy 혹은 strategy를 learning할 때 모든 과거 sequence를 고려해서 strategy가 결정되게 된다. 게임은 언젠가 끝나게 되어있기 때문에 모든 $s_t$는 finite한 길이를 가지고 있으며 (비록 어마어마하게 크지만) $s_t$의 domain 역시 유한하다. 따라서 이 모델은 엄청나게 large하지만 어쨌거나 finite한 Markov dicision process(MDP)가 된다.</p>
<p>우리의 목표는 에뮬레이터에 agent가 어떤 strategy를 통해 게임을 조작하여, 최종적으로 게임이 끝났을 때 게임에서 가장 높은 점수를 획득하는 것이다. 즉, reward에 대한 수학적 정의만 있다면 이 문제는 간단한 optimization 문제가 된다. 게임은 보통 시간 마다 reward를 받는다. 하지만 일반적으로 시간이 오래 지날수록 해당 reward의 가치는 점점 내려가는데 이를 고려하기 위하여 discount factor $\gamma$가 정의된다. 시간 $t$의 reward를 t라고 한다면 time $T$에서의 discount factor를 고려한 future reward는 다음과 같이 정의한다.</p>
<p>$$R_t = \sum_{t^\prime=t}^T \gamma^{t^\prime-t} r_{t^\prime}.$$</p>
<p>Reward function을 정의했으니 Q-function (action-value function) 역시 정의할 수 있다. $\pi$를 $s_t$에서 $a_t$를 mapping하는 policy function이라하면, optimal Q-function $Q^*$는 다음과 같이 정의할 수 있다.</p>
<p>$$Q^*(s,a) = \max_\pi \mathbb E \big[ R_t \big| s_t = s, a_t = a, \pi \big]. $$</p>
<p>MDP에서는 이 optimal action value function 혹은 optimal Q-function 하나만 제대로 알고 있다면 언제나 주어진 state에 대해 가장 $Q^*$의 값을 크게 만드는 action을 고르는 간단한 policy만으로도 반드시 항상 optimal한 action을 고를 수 있다는 이론적 결과가 있기 때문에 Q-function은 매우매우 중요하다. 이 논문에서도 $Q^*$를 찾는 neural network를 만들어서 문제를 해결하고 있으니, Q-function에 대한 이해가 필수적이다.</p>
<p>다시 본문으로 돌아와서, Optimal Q-function은 <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>이라는 중요한 특성을 따르게 된다. 이 equation을 intuition은 대충 이런식이다: seqeunce $s^\prime$의 다음 time stamp에서의 optimal Q-function $Q^* (s^\prime, a^\prime)$ 값이 모든 action $a^\prime$에 대해서 알려져 있다면, optimal strategy는 $r + \gamma Q^* (s^\prime, a^\prime)$의 expected value를 maximize하는 것이라는 것이다. 이를 수식으로 표현하면 다음과 같다.</p>
<p>$$Q^*(s,a) = \mathbb E_{s^\prime \sim \mathcal E} \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].$$</p>
<p>많은 RL algorithm 들에서는 Q-function을 estimate하기 위하여 이 Bellman equation을 value iteration algorithm이라는 것을 통해 iterative update하여 구하게 된다. Value iteration algorithm에서는 매 $i$ 번쨰 iteration마다 다음과 같은 procedure를 수행하게 된다.</p>
<p>$$Q_{i+1} (s,a) = \mathbb E \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].$$</p>
<p>이런 value iteration algorithm은 MDP에서 $Q_i \to Q^* \mbox{ as } i\to\infty$라는 것이 알려져 있다. 그러나 이런 방식은 이론적으로는 의미가 있을지 몰라도 실제로는 완전히 impractical한데, 이 과정을 모든 seqeunce에 대해 독립적으로 시행해야하기 때문이다. 따라서 보통은 action-value function을 다음과 같은 식으로 적절한 approximator를 사용하여 approximation한다.</p>
<p>$$Q(s,a;\theta) \simeq Q^* (s,a).$$</p>
<p>보통 linear function으로 approximation하지만, 간혹 non-linear function으로 모델링하는 경우도 있다. 예를 들어 deep network를 쓰는 방법이 있는데, 이 논문은 기존 방법들에 비해 RL에 적합한 Deep Network 모델을 제안하여 뛰어난 non-linear function approximator를 제안한다.</p>
<p>일반적으로 RL에 deep learning approach를 바로 적용했을 때 몇 가지 이슈가 생기게 되는데, 먼저 deep learning을 하기 위해서는 엄청나게 많은 hand labelled training data가 필요하지만, RL에서는 모든 state와 action에 대한 labelled data가 없기 때문에 이를 어떻게 handle해야할지를 모델에서 고려해야만한다. 또한 현재까지 연구된 많은 deep learning structure들은 data가 i.i.d.하다고 가정하지만, 실제 RL 환경에서는 state들이 엄청나게 correlated되어있기 때문에 제대로 된 learning이 어렵다. 예를 들어 위 핑퐁 게임 화면에서 한 프레임 더 지난 화면과 지금 화면은 정말 엄청나게 높은 correlation을 가지지만, standard feed-forward deep learning은 그것을 처리할 수 있는 모델이 아니기 때문에 문제가 발생한다. 이 논문은 그 문제를 experience replay라는 것으로 해결하는데, 자세한 내용은 다음 section에서 다루도록 하겠다.</p>

<h3>Deep Q-Learning</h3>
<p>앞에서 정의한 Q-function을 modeling한 network를 ($Q(s,a;\theta) \simeq Q^* (s,a)$, 이때 $\theta$는 weight나 bias 등 neural network의 model parameter들) 이 논문에서는 Q-network라고 부르고 있다. 만약 parameter가 $\theta$가 정해진다면, 우리는 state $s$와 action $a$를 Q-network에 넣어서 forward pass를 돌리게 되면 해당하는 Q-value를 얻을 수 있을 것이다. 따라서 parameter가 정해진다면 주어진 $s$에 대해 모든 $a$에서의 Q-value를 얻을 수 있고, 이를 통해서 $Q^*$의 값과 그것을 achieve하는 action $a^*$를 구하는 것도 가능해진다. 다시 말해서 이 Q-network를 올바른 방향으로 update하는 알고리즘을 design하기만 한다면 주어진 문제를 해결할 수 있는 것이다. 이 논문에서는 Q-network가 우리가 원하는 목적대로 train되도록 하기 위하여 i 번째 iteration에서 아래와 같은 loss function을 가지도록 design한다. 이 논문은 현재 Q-network가 항상 target Q-value에 가까워지도록 loss를 설정함으로써 마치 value iteration이 converge하듯 Q-network의 update가 converge할 때 까지 iterative algorithm을 돌리도록 하는 것이다. 이때 $y_i$는 iteration i의 target value이고, $\rho(s,a)$는 sequence $s$와 action $a$의 probability distribution이며, 이를 이 논문에서는 behaviour distribution이라고 정의한다.</p>
<p>$$L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot)} \bigg[ \big(y_i - Q(s,a;w_i) \big)^2 \bigg], $$</p>
<p>$$\mbox{where, }y_i = \mathbb E_{s^\prime \sim \mathcal E} \bigg[r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime;w_{i-1}) ~\big|~ s, a \bigg]. $$</p>
<p>주의할 점은, optimization 과정에서 parameter $\theta$가 update되는 동안 loss function $L_i(\theta_i)$ 의 이전 iteration paramter $\theta_{i-1}$은 고정된다는 것이다. 이를 &#8216;freeze target Q-network&#8217; 라고 부르는데, 이렇게하는 이유는 supervised learning과는 다르게, target의 값이 $\theta$의 값에 (민감하게) 영향을 받기 때문에 stable한 learning을 위하여 $\theta$값을 고정하는 것이다. 이건 아래에서 조금 더 자세하게 설명하도록 하겠다. 이제 loss function을 정의되었으므로 gradient값만 있다면 그 값을 사용해서 backpropagation을 돌리면 쉽게 update할 수 있다. 이 network의 loss의 gradient는 다음과 같이 구할 수 있다.</p>
<p>$$\nabla_{\theta_i} L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot); s^\prime \sim \mathcal E} \bigg[ \big( r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime; \theta_{i-1}) - Q(s,a;\theta_i) \big) \nabla_{\theta_i} Q(s,a;\theta_i)\bigg]$$</p>
<p>하지만 이렇게 build한 Deep RL을 바로 사용할 수는 없고, 아래와 같은 몇 가지 이슈들을 처리해야한다.</p>

<ul>
  <li>Deep Learning은 데이터가 i.i.d.하다고 가정하지만 실제 RL input의 데이터는 sequential하고 highly correlated 되어있다.</li>
  <li>Policy 변화에 따른 (이 경우는 w의 변화에 따른) Q-value의 변화량이 너무 크기 때문에 policy가 oscillate하기 쉽다.</li>
  <li>위와 같은 세팅에서는 reward와 Q-value의 값이 엄청나게 커질 수 있기 때문에 stable한 SGD 업데이트가 어려워진다.</li>
</ul>

<p>첫 번째 이슈는 이미 전 문단에서 간단하게 언급했었으니 생략한다. 두 번째 문제도 크게 어렵지 않게 생각할 수 있는데, 게임을 하는 방식을 아주 조금만 바꾸더라도 게임의 결과가 완전히 크게 바뀌기 때문에 이런 현상이 발생한다. 핑퐁 게임에서 움직이는 속도를 조금 늦춘다거나 했다가는 바로 한 점을 잃게 될 것이다. 또한 앞에서 설명한 것 처럼 supervised learning과는 다르게 target의 값이 parameter에 영향을 아주 민감하게 받기 때문에, 이 값을 고정해주는 과정이 필요하다. 마지막 조건은 좀 practical한 이슈인데, Q-value의 값이 얼마나 커질지 모르기 때문에 stable update가 힘들 수도 있다. 이 논문에서는 다음과 같은 세 가지 방법으로 각각의 issue를 handling한다</p>

<ul>
  <li>Experience replay</li>
  <li>Freeze target Q-network</li>
  <li>Clip reward or normalize network adaptively to sensible range</li>
</ul>

<p>이 중에서 두 번째 idea는 이미 설명했고 (update하는 동안 target을 계산하기 위해 사용하는 paramter를 고정), 세 번째 idea는 reward의 값을 [-1,0,1] 중에서 하나만 선택하도록 강제하는 아이디어이다. 즉, 내가 100점을 얻거나 10000점을 얻거나 항상 reward는 +1 이다. &#8216;highest&#8217; score를 얻는 것은 불가능하지만, 이렇게 설정함으로써 조금 더 stable한 update가 가능해진다. 그리고 그와는 별개로 모든 게임에 적용가능한 DQL을 learning할 수 있다는 장점도 있다. 실제로 실험에서는 모든 게임을 단 하나의 네트워크로만 learning해서 기존의 모든 방법을 beating한다.</p>
<p>그럼 이제 마지막으로 이 논문의 핵심 아이디어라고 할 수 있는 experience replay에 대해 살펴보자. Experience replay는 agent의 experine를 각 time stamp마다 다음과 같은 튜플 형태로 메모리 $\mathcal D = e_1, \ldots, e_N$ 에 저장한 후 이를 다시 사용하는 것이다.</p>
<p>$$e_t = (s_t, a_t, r_t, s_{t+1}).$$</p>
<p>Experience replay는 이렇게 experience $e_t$를 메모리 $\mathcal D$에 저장해두었다가, 일부를 uniformly random하게 sample하여 mini-batch를 구성한 다음 parameter $\theta$를 mini-batch에 대해 backpropagation으로 update하는 과정을 의미한다.</p>
<p>Experience replay를 사용함으로써 data의 correlation을 깰 수 있고, 조금 더 i.i.d.한 세팅으로 network를 train할 수 있게 된다. 또한 방대한 과거 데이터가 한 번만 update되고 버려지는 비효율적 접근이 대신에, 지속적으로 추후 update에도 영향을 줄 수 있도록 접근하기 때문에 데이터 사용도 훨씬 효율적이라는 장점이 있다. 실제로 실험에서는 메모리 용량의 한계 때문에 bucket을 $N$으로 고정하고, FIFO 형태로 저장을 한 모양이다.</p>
<p>Experience replay가 끝난 후 agent는 action을 $\epsilon$-greedy policy라는 것을 사용해 선택하고 실행한다. 이 방법은 action을 $\epsilon$의 확률로 random하게 고르거나 $1-\epsilon$의 확률로 MDP의 optimal action selection criteria인 $a_t = \arg\max_a Q^*(s_t, a;\theta) $로 고르는 policy를 의미한다.</p>
<p>참고로, arbitrary length의 input을 다루는 것이 general feed-forward network에서는 어렵기 때문에, 이 논문에서는 function $\phi$라는 것을 정의해서 모든 $s_t$의 length를 fix한다. 이 알고리즘을 수식적으로 기술하면 다음과 같이 기술할 수 있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/90-3.png" width="600" /></p>

<p>Equation 3은 위에서 증명한 gradient 값이고, 함수 $\phi$는 다음과 같다: 주어진 history 중에서 가장 마지막 4개의 frame을 stack으로 쌓는 것. 이때 각각의 frame은 원래 128색 210 by 160 픽셀으로 구성되어있지만, gray-scale로 만들고 110 by 84로 down sampling한 후 84 by 84로 크롭한다. 이때 크롭을 하는 이유는 주어진 툴에서 정사각형 사진만 GPU 연산이 되기 때문이라고..</p>
<p>이렇게 experience replay라는 아이디어를 사용한 DQL은 몇 가지 이점이 있다.</p>

<ul>
  <li>각각의 experience가 potentially 많은 weight update에 reuse되기 때문에 experience를 weight update 한 번에만 사용하는 기존 방법보다 훨씬 data efficiency하다.</li>
  <li>두 번째로, mini-batch를 만드는 sampling 과정을 통해 데이터들 간의 high correlation을 효율적으로 관리하고, 이를 통해 보다 효율적인 update를 할 수 있다. 이 방법은 random하게 sample을 뽑아서 mini-batch로 구성하기 때문에 이런 high correlation을 break해서 update의 효율성을 높이기 때문이다.</li>
  <li>마지막으로 이 방법을 통해 parameter를 update하게 되면 다음 training을 위한 data sample을 어느 정도 determine할 수 있다. 예를 들어서 내가 지금 오른쪽으로 움직이는 쪽으로 action을 고른다면 다음 sample들은 내가 오른쪽에 있는 상태의 sample들이 dominate하게 나올 것이라고 예측할 수 있다. 따라서 이 방법을 통해 training을 위한 다음 데이터를 무작정 뽑는 것이 아니라 현재 action을 고려하여 효율적으로 뽑을 수 있다.</li>
</ul>

<p>몇 가지 주의점이라면 freeze target Q-network, 혹은 off-policy가 반드시 필요하다는 점 정도가 있겠다. 한계점으로는 메모리의 한계 때문에 앞에서 말한 것처럼 모든 history를 저장하지 못한다는 점과, uniform sampling을 사용하기 때문에 모든 과거 experience가 동일한 weight를 가진다는 점이다. 이 논문에서는 조금 더 wise한 sampling을 하게 되면 성능 향상이 있을 수도 있다고 언급하고 있다.</p>

<h3>Model Architecture of DQL and Experiment</h3>
<p>이제 구체적으로 어떤 neural network 모델을 사용해 learning을 하게 될지 알아보자. 항상 관심있게 살펴봐야 할 내용은 (1) input data는 무엇인가 (2) output data는 무엇인가 (3) 구체적인 network 구조는 어떻게 되는가 정도가 있겠다. 이 논문에서는 input으로 $\phi(s_t)$를 받고, output으로 가능한 모든 action에 대한 Q-value를 출력한다. 즉, 버튼이 4개 있다면 output은 4개이고, 12개 있다면 output은 12개이다. 실제 논문에서는 게임 종류에 따라 action을 4개에서 18개 사이에서 고른 것 같다. 이렇게 모델을 고르게 되면 $Q^*$를 단 한번의 forward pass 만으로 구할 수 있다는 장점이 있기 때문에 이 논문에서는 이러한 방법을 선택하였다. 구체적인 네트워크는 CNN을 사용한다. 인풋 데이터는 $\phi(s_t)$를 넣는다고 했으므로 84 by 84로 크롭한 후에 4개의 history를 stack으로 쌓은 데이터가 들어오게 된다. 그러니까 대충 다음과 같은 식이다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/90-5.png" width="600" /></p>
<p>이 네트워크를 사용한 자세한 실험 결과는 다음 표에 나와있다. 각각의 숫자는 모두 reward를 의미하기 때문에 값이 클 수록 좋은 결과이다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/90-4.png" width="600" /></p>
<p>Breakout, Enduro, Pong은 심지어 사람보다도 좋은 것을 알 수 있고, Space Invaders를 제외하면 기존 모델들보다 best 뿐 아니라 avg 까지 뛰어난 것을 볼 수 있다. 논문에서는 Q*bert, Seaquest, Spae Invaders 등의 게임에서 사람의 performance에 한참 미치지 못하는 이유로 이 게임들은 전략이 엄청나게 긴 time scale로 필요하기 때문에 조금 더 challenge한 문제라고 주장하고 있다. 아마 하드웨어의 발달과 모델의 발달로 언젠가는 극복할 수 있을 것으로 보인다.</p>

<h3>Summary of DQL</h3>

<ul>
  <li>문제 정의 자체가 흥미롭다. 특히 state space를 sequence of iamges and action으로 구성했다는 점이 흥미롭다.</li>
  <li>State에 대한 hand-craft feature가 전혀 없다. 오직 이미지 sequence만을 사용해서 CNN으로 feature를 자동으로 만들어내는 방법으로 이를 해결하고 있다는 점이 흥미롭다.</li>
  <li>Q-function을 learning하는 neural network를 구성하였는데 몇 가지 stable update를 위하여 ‘off-policy’를 사용하고, ‘experience replay’ 기법을 사용한다.</li>
  <li>Experience replay는 매 시간마다 experience tuple $e_t$를 메모리에 저장하고, 메모리에서 $e_t$를 uniformly sample하여 뽑아 mini-batch를 구성하고 이를 (off-policy를 적용한 채로 혹은 target network를 freeze하고) 사용해 parameter를 update하는 아이디어이다.</li>
  <li>서로 다른 Atari 게임 7개에 대한 policy를 learning하기 위해 단 하나의 neural network만을 사용했고, 그 결과가 기존 결과를 outperform한다.</li>
</ul>

<h3>Reference</h3>

<ul>
  <li><a href="http://arxiv.org/abs/1312.5602">Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” NIPS (2013).</a></li>
  <li><a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf">Deep Reinforcement Learning - ICLR 2015 tutorial</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Neural Network Regularization]]></title>
    <link href="http://SanghyukChun.github.io/89/"/>
    <updated>2015-09-14T19:26:00+09:00</updated>
    <id>http://SanghyukChun.github.io/89</id>
		<content type="html"><![CDATA[<p>이번에 review할 논문은 <a href="http://arxiv.org/abs/1409.2329">Recurrent neural network regularization</a>이라는 논문이다. 아직 학회나 저널에 publish된 논문은 아니지만 ICLR 2015 review를 기다리는 모양. 이 논문은 최근 머신러닝 분야에서 가장 주목받고 있는 분야는 Deep learning, 그 중에서도 요즘 가장 활발하게 연구 중인 Recurrent Neural Network, 혹은 RNN에 관한 논문이다. RNN은 sequencial data를 처리하는데에 적합한 형태로 디자인 되어있으며, 현재 language model, speech recognition, machine translation 등에서 우수한 결과를 성취하고 있는 neural network model 중 하나이다. 이 논문은 popular한 RNN 중 하나인 LSTM 모델을 regularization 시켜서 보다 기존 결과들보다 더 잘 동작하는 결과를 제안한다.</p>

<h3>Motivation: Regularization of Recurrent Neural Network</h3>
<p>RNN이 sequencial data에 대해 꽤 좋은 성능을 보이고 있는 것은 사실이지만, RNN을 regularization하는 방법은 많이 제안되어있지 않은 상황이다. 기존 MLP (Multi-Layer Perceptron, 혹은 feed-forward network) 쪽에서는 Dropout, ReLU 등의 컨셉들이 연구되면서 상당한 발전이 있었지만, 아직 RNN에는 Dropout조차 제대로 적용되지 않고 있다고 한다. Dropout을 붙이면 오히려 성능이 떨어지기 때문에 아직은 LSTM (Long-Short-Term-Memory) 모델에 dropout없이 연구가 진행되고 있는 모양이다. 이 논문에서는 LSTM에 dropout을 RNN의 특성을 잘 살린 형태로 적용하여 dropout이 잘 동작하도록 하는 방법을 제안한다.</p>

<h3 id="RNN-intro">RNN Introduction</h3>
<p>본 논문을 소개하기 전에 먼저 RNN에 대해 간단하게 설명을 하고 넘어가도록 하겠다. 이름에서도 알 수 있듯 일반적인 Feed-forward network와 RNN의 차이는 recurrent한 loop의 존재 유무이다. 이는 자기 자신을 향한 self-loop일 수도 있고, 아니면 cycle 형태이거나 아니면 undirected edge의 형태일 수도 있다. 보통 일반적으로 RNN이라 하면 아래 그림과 같이 hidden unit에 self-loop이 있는 형태를 일컫는 듯하다. (출처: Bengio Deep Learning book)</p>

<p><img src="http://SanghyukChun.github.io/images/post/89-1.PNG" width="600" /></p>

<p>이 그림에서도 알 수 있듯 self-loop의 존재는 RNN으로 하여금 자연스럽게 historical data를 현재 decision에 반영하도록 만들어준다. 즉, RNN 모델은 마치 HMM 등의 sequencial data를 처리하는 모델들처럼 동작하는 것이다. 실제 learning을 할 때는 시간에 대해 self-loop를 &#8216;unfold&#8217; 하여 마치 weight를 공유하는 deep layer를 연산하듯 update한다. RNN에 대한 더 자세한 설명은 추후 다른 포스팅을 통해 다룰 수 있도록 하겠다.</p>

<h3>Long-Short-Term-Memory (LSTM) Architecture</h3>
<p>기존 vanilla RNN은 long-term dependency를 가지도록 learnig을 하게되면 gradient vanishing이나 exploding 문제에 직면하기 쉬워진다. 이유는 dependency를 더 long-term으로 가져갈수록 gradient 값이 시간에 따른 곱하기 형태가 되어 gradient growth가 exponential해지기 때문이다 (역시 위와 마찬가지로 나중에 더 자세하게 다루도록 하겠다). 때문에 이를 해결하기 위한 아이디어 중 하나로 LSTM이라는 것이 존재한다.</p>
<p>LSTM은 historical information을 저장하기 위한 다소 복잡한 dynamics를 가지고 있다. &#8220;long term&#8221; memory라는 것은 memory cell ($c_t$)에 저장되며, 시간에 따라, 그리고 주어진 input data에 따라 저장해둔 information을 얼마나 간직하고 있을지 forget gate ($f_t$) 라는 것을 통해 결정하게 된다. LSTM을 그림으로 표현하면 아래 그림과 같다. (출처: 논문)</p>

<p><img src="http://SanghyukChun.github.io/images/post/89-2.PNG" width="600" /></p>

<p>이를 수식으로 한 번 나타내어보자. 먼저 몇 가지 notation을 정의해보자. 먼저 모든 state는 n-dimension이라고 가정하자. $h_t^l \in \mathbb R^n $은 layer $l$의 timestamp $t$일 때의 hidden state라고 하자. 그리고 $T_{n,m}: \mathbb R^n \to \mathbb R^m$ 을 n차원에서 m차원으로 가는 affine transform이라고 해보자. 예를 들어 parameter $W$와 $b$로 나타내어지는 $Wx + b$도 $T_{n,m}$에 포함된다 (즉, 이 논문에서는 복잡한 weight와 bias에 대한 식을 T라는 notation으로 간단하게 치환했다고 생각하면 된다). 마지막으로 $\odot$을 두 벡터의 element-wise multiplication이라고 정의해보자. 이렇게 notation을 정의하고 나면 일반적인 vanilla RNN을 다음과 같이 과거의 hidden state와 현재 hidden state의 이전 layer의 state로부터 현재 hidden state를 표현하는 function으로 표현할 수 있다.</p>
<p>$$\mbox{RNN: } h_t^{l-1}, h_{t-1}^l \to h_t^l, \mbox{ where } h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l). $$</p>
<p>이때, function $f$는 RNN의 경우 sigmoid나 tanh 함수 중 하나로 선택하는 것이 일반적이다. 그럼 이번에는 LSTM을 수식으로 표현해보자.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/89-3.PNG" width="250" /></p>

<p>앞에서 언급했던 LSTM의 graphical representation을 살펴보면서 수식을 읽어보면 어렵지 않게 이해할 수 있을 것이다.</p>

<h3>Dropout Regularization for LSTM</h3>
<p>저자들은 RNN에 Dropout을 붙였을 때 잘 동작하지 않는 이유가 dropout이 지워버리면 안되는 과거 information까지 전부 지워버리기 때문이라고 주장한다. 때문에 RNN에 Dropout을 적용하기 위해서는 recurrent connection이 아닌 connection 들에 대해서만 Dropout을 적용해야한다고 논문에서는 주장하고 있다. 아래 식에 조금 더 자세하게 적혀있다. 이때 $\mathbf D$ 는 dropout operator라는 것으로, 주어진 argument의 random subset을 0으로 만들어버리는 operator이다. 즉, $\mathbf D (h)$ 라고 한다면 vector $h$ 중 random하게 고른 일부를 (보통 50%) 0으로 설정하라는 뜻이다.</p>

<p><img class="center" src="http://SanghyukChun.github.io/images/post/89-4.PNG" width="250" /></p>

<p>이를 그림으로 표현하면 아래와 같다. 이때 실선은 일반적인 connection이고, 점선이 dropout으로 연결된 connection을 의미한다. (출처: 논문)</p>

<p><img src="http://SanghyukChun.github.io/images/post/89-5.PNG" width="600" /></p>

<p>위 그림에서도 알 수 있듯, 과거에서부터 propagation되는 information은 언제나 100% 보존되지만, 아래 layer에서 위 layer로 전달되는 information은 특정 확률로 dropout에 의해 corruption되어 진행된다. 이때, 맨 아래 data layer로부터 맨 위 $L$번째 layer까지 information이 전달되는 동안 점선으로 그려진 connection은 정확하게 $L+1$ 번 만큼만 지나게 된다. 만약 recurrent connection까지 dropout을 적용했다면, 이 횟수는 $L+1$ 보다 항상 같거나 클 것이며, 더 long-term dependency를 가질수록 그 효과가 더 강해져서 우리가 원하는 과거 정보는 거의 다 희석되고, 결과적으로 안좋은 결과를 얻게 될 확률이 높아질 것이다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/89-6.png" width="600" /></p>

<p>위 그림은 어떻게 information이 time t-2로부터 t+2까지 전달되는지 그 flow를 표현한 것이다. 굵은 선이 information path를 나타내는데, 앞서 설명한 것 처럼 이런 information flow는 data layer로부터 decision layer까지 정확하게 $L+1$ 번만 dropout의 영향을 받게 된다. 반면 standard dropout을 적용했더라면 information이 더 많은 dropout들에 의해 영향을 받아서 LSTM이 정보를 더 긴 시간 동안 저장할 수 없도록 만들게 되는 것이다. 때문에 recurrent connection에 dropout을 적용하지 않는 것 만으로도 LSTM에서 좋은 regularization 효과를 얻을 수 있는 것이다.</p>

<h3>Experiments</h3>
<p>논문에서는 총 4개의 실험을 진행한다. Language Modeling (Penn Tree Bank - PTB dataset), Speech Recognition, Machine Translation 그리고 마지막으로 Image Caption Generation이 그것이다. 결과는 순서대로 아래 Table 1,2,3,4에 나열되어있다.</p>

<p><img src="http://SanghyukChun.github.io/images/post/89-7.png" width="600" /></p>

<h3>Summary of Recurrent Neural Network Regularization</h3>

<ul>
  <li>Motivation: Dropout을 RNN에 그냥 적용하면 성능이 좋지 않다</li>
  <li>Idea: Dropout을 모든 connection에 적용하는 대신 non recurrent connection에만 dropout을 적용하자</li>
</ul>

<h3>Reference</h3>

<ul>
  <li><a href="http://arxiv.org/abs/1409.2329">Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. “Recurrent neural network regularization.” arXiv preprint arXiv:1409.2329 (2014).</a></li>
  <li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (18) Neural Network Introduction]]></title>
    <link href="http://SanghyukChun.github.io/74/"/>
    <updated>2015-09-13T23:13:00+09:00</updated>
    <id>http://SanghyukChun.github.io/74</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p>최근 Machine Learning 분야에서 가장 뜨거운 분야는 누가 뭐래도 Deep Learning이다. 엄청나게 많은 사람들이 관심을 가지고 있고, 공부하고 응용하고 있지만, 체계적으로 공부할 수 있는 자료가 많이 없다는 것이 개인적으로 조금 안타깝다. 이제 막 각광받기 시작한지 10년 정도 지났고, 매년 새로운 자료들이 쏟아져나오기 때문에 책이나 정리된 글을 찾기가 쉽지가 않다. 그러나 Deep Learning은 결국 artificial neural network를 조금 더 복잡하게 만들어놓은 모델이고, 기본적인 neural network에 대한 이해만 뒷받침된다면 자세한 내용들은 천천히 탑을 쌓는 것이 가능하다고 생각한다. 이 글에서는 neural network의 가장 기본적인 model에 대해 다루고, model paramter를 update하는 algorithm인 backpropagation에 대해서 다룰 것이다. 조금 더 advanced한 topic들은 이 다음 글에서 다룰 예정이다. 이 글의 일부 문단은 <a href="http://SanghyukChun.github.io/blog/categories/neural-network/">이전 글들</a>을 참고하였다.</p>
<h3>Motivation of Neural Network</h3>
<p>이름에서부터 알 수 있듯 neural network는 사람의 뇌를 본 따서 만든 머신러닝 모델이다 (참고: 원래 neural network의 full name은 artificial neural network이지만, 일반적으로 neural network라고 줄여서 부른다). 본격적으로 neural network에 대해 설명을 시작하기 전에 먼저 인간보다 컴퓨터가 훨씬 잘 할 수 있는 일들이 무엇이 있을지 생각해보자.</p>

<ul>
  <li>1부터 10000000까지 숫자 더하기</li>
  <li>19312812931이 소수인지 아닌지 판별하기</li>
  <li>주어진 10000 by 10000 matrix 의 determinant값 계산하기</li>
  <li>800 페이지 짜리 책에서 ‘컴퓨터’ 라는 단어가 몇 번 나오는지 세기</li>
</ul>

<p>반면 인간이 컴퓨터보다 훨씬 잘 할 수 있는 일들에 대해 생각해보자</p>

<ul>
  <li>다른 사람과 상대방이 말하고자하는 바를 완벽하게 이해하면서 내가 하고 싶은 말을 상대도 이해할 수 있도록 전달하기</li>
  <li>주어진 사진이 고양이 사진인지 강아지 사진인지 판별하기</li>
  <li>사진으로 찍어보낸 문서 읽고 이해하기</li>
  <li>주어진 사진에서 얼마나 많은 물체가 있는지 세고, 사진에 직접 표시하기</li>
</ul>

<p>컴퓨터가 잘 할 수 있는 0과 1로 이루어진 사칙연산이다. 기술의 발달로 인해 지금은 컴퓨터가 예전보다도 더 빠른 시간에, 그리고 더 적은 전력으로 훨씬 더 많은 사칙연산을 처리할 수 있다. 반면 사람은 사칙연산을 컴퓨터만큼 빠르게 할 수 없다. 인간의 뇌는 오직 빠른 사칙연산만을 처리하기 위해 만들어진 것이 아니기 때문이다. 그러나 인지, 자연어처리 등의 그 이상의 무언가를 처리하기 위해서는 사칙연산 그 너머의 것들을 할 수 있어야하지만 현재 컴퓨터로는 인간의 뇌가 할 수 있는 수준으로 그런 것들을 처리할 수 없다.</p>
<p>예를 들어 아래와 같이 주어진 사진에서 각각의 물체를 찾아내는 문제를 생각해보자 (출처: <a href="http://www.engadget.com/2014/09/08/google-details-object-recognition-tech/">링크</a>). 사람에게는 너무나 간단한 일이지만, 컴퓨터가 처리하기에는 너무나 어려운 일이다. 어떻게 어디부터 어디까지가 &#8216;tv or monitor&#8217;라고 판단할 수 있을까? 컴퓨터에게 사진은 단순한 0과 1로 이루어진 픽셀 데이터에 지나지 않기 때문에 이는 아주 어려운 일이다.</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/74-2.jpg" width="400" /></p>
<p>그렇기 때문에 자연언어처리, 컴퓨터 비전 등의 영역에서는 인간과 비슷한 성능을 내는 시스템을 만들 수만 있다면 엄청난 기술적 진보가 일어날 수 있을 것이다. 그렇기 때문에 인간의 능력을 쫓아가는 것 이전에, 먼저 인간의 뇌를 모방해보자라는 아이디어를 낼 수 있을 것이다. Neural Network는 이런 모티베이션으로 만들어진 간단한 수학적 모델이다. 우리는 이미 인간의 뇌가 엄청나게 많은 뉴런들과 그것들을 연결하는 시냅스로 구성되어있다는 사실을 알고 있다. 또한 각각의 뉴런들이 activate되는 방식에 따라서 다른 뉴런들도 activate 되거나 activate되지 않거나 하는 등의 action을 취하게 될 것이다. 그렇다면 이 사실들을 기반으로 다음과 같은 간단한 수학적 모델을 정의하는 것이 가능하다.</p>
<h3>Model of Neural Network: neuron, synapse, activation function</h3>
<p>먼저 뉴런들이 node이고, 그 뉴런들을 연결하는 시냅스가 edge인 네트워크를 만드는 것이 가능하다. 각각의 시냅스의 중요도가 다를 수 있으므로 edge마다 weight를 따로 정의하게 되면 아래 그림과 같은 형태로 네트워크를 만들 수 있다. (출처: <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>)</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-1.png" width="600" /></p>
<p>보통 neural network는 directed graph이다. 즉, information propagation이 한 방향으로 고정된다는 뜻이다. 만약 undirected edge를 가지게 되면, 혹은 동일한 directed edge가 양방향으로 주어질 경우, information propagation이 recursive하게 일어나서 결과가 조금 복잡해진다. 이런 경우를 recurrent neural network (RNN)이라고 하는데, 과거 데이터를 저장하는 효과가 있기 때문에 최근 음성인식 등의 sequencial data를 처리할 때 많이 사용되고 있다. 이번 ICML 2015에서도 RNN 논문이 많이 발표되고 있고, 최근들어 연구가 활발한 분야이다. 이 글에서는 일단 가장 간단한 &#8216;multi layer perceptron (MLP)&#8217;라는 구조만 다룰 것인데, 이 구조는 directed simple graph이고, 같은 layer들 안에서는 서로 connection이 없다. 즉, self-loop와 parallel edge가 없고, layer와 layer 사이에만 edge가 존재하며, 서로 인접한 layer끼리만 edge를 가진다. 즉, 첫번째 layer와 네번째 layer를 직접 연결하는 edge가 없는 것이다. 앞으로 layer에 대한 특별한 언급이 없다면 이런 MLP라고 생각하면 된다. 참고로 이 경우 information progation이 &#8216;forward&#8217;로만 일어나기 때문에 이런 네트워크를 feed-forward network라고 부르기도 한다.</p>
<p>다시 일반적인 neural network에 대해 생각해보자. 실제 뇌에서는 각기 다른 뉴런들이 activate되고, 그 결과가 다음 뉴런으로 전달되고 또 그 결과가 전달되면서 최종 결정을 내리는 뉴런이 activate되는 방식에 따라 정보를 처리하게 된다. 이 방식을 수학적 모델로 바꿔서 생각해보면, input 데이터들에 대한 activation 조건을 function으로 표현하는 것이 가능할 것이다. 이것을 activate function이라고 정의한다. 가장 간단한 activation function의 예시는 들어오는 모든 input 값을 더한 다음, threshold를 설정하여 이 값이 특정 값을 넘으면 activate, 그 값을 넘지 못하면 deactivate되도록 하는 함수일 것이다. 일반적으로 많이 사용되는 여러 종류의 activate function이 존재하는데, 몇 가지를 소개해보도록 하겠다. 편의상 $t = \sum_i w_i * x_i$ 라고 정의하겠다. (참고로, 일반적으로는 weight 뿐 아니라 bais도 고려해야한다. 이 경우 $t = \sum_i (w_i * x_i + b_i) $로 표현이 되지만, 이 글에서는 bais는 weight와 거의 동일하기 때문에 무시하고 진행하도록 하겠다. - 예를 들어 항상 값이 1인 $x_0$를 추가한다면 $w_0$가 bais가 되므로, 가상의 input을 가정하고 weight와 bais를 동일하게 취급하여도 무방하다.)</p>

<ul>
	<li><p>sigmoid function: $f(t) = \frac{1}{1+ e^{-t}}$</p></li>
	<li><p>tanh function: $f(t) = \frac{e^t - e^{-t}}{e^t + e^{-t} }$</p></li>
	<li><p>absolute function: $f(t) = \|t\|$</p></li>
	<li><p>ReLU function: $f(t) = max(0, t)$</p></li>
</ul>

<p>보통 가장 많이 예시로 드는 activation function으로 sigmoid function이 있다. (출처는 위의 <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>와 같음)</p>
<p><img class="center" src="http://SanghyukChun.github.io/images/post/74-4.png" width="300" /></p>
<p>이 함수는 미분이 간단하다거나, 실제 뉴런들이 동작하는 것과 비슷하게 생겼다는 등의 이유로 과거에는 많이 사용되었지만, 별로 practical한 activation function은 아니고, 실제로는 ReLU를 가장 많이 사용한다 (2012년 ImageNet competition에서 우승했던 <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> publication을 보면, ReLU와 dropout을 쓰는 것이 그렇지 않은 것보다 훨씬 더 우수한 결과를 얻는다고 주장하고 있다. 이에 대한 자세한 내용은 다른 포스트를 통해 보충하도록 하겠다). 참고로 neuron을 non-linearity라고 부르기도 하는데, 그 이유는 activation function으로 linear function을 사용하게 되면 아무리 여러 neuron layer를 쌓는다고 하더라도 그것이 결국 하나의 layer로 표현이 되기 때문에 non-linear한 activation function을 사용하기 때문이다.</p>
<p>따라서 이 모델은 처음에 node와 edge로 이루어진 네트워크의 모양을 정의하고, 각 node 별 activation function을 정의한다. 이렇게 정해진 모델을 조절하는 parameter의 역할은 edge의 weight가 맡게되며, 가장 적절한 weight를 찾는 것이 이 수학적 모델을 train할 때의 목표가 될 것이다.</p>

<h3>Inference via Neural Network</h3>
<p>먼저 모든 paramter가 결정되었다고 가정하고 neural network가 어떻게 결과를 inference하는지 살펴보도록하자. Neural network는 먼저 주어진 input에 대해 다음 layer의 activation을 결정하고, 그것을 사용해 그 다음 layer의 activation을 결정한다. 이런 식으로 맨 마지막까지 결정을 하고 나서, 맨 마지막 decision layer의 결과를 보고 inference를 결정하는 것이다 (아래 그림 참고, 빨간 색이 activate된 뉴런이다).</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-3.png" width="600" /></p>
<p> 이때, classification이라고 한다면 마지막 layer에 내가 classification하고 싶은 class 개수만큼 decision node를 만든 다음 그 중 하나 activate되는 값을 선택하는 것이다. 예를 들어 0부터 9까지 손글씨 데이터를 (MNIST라는 유명한 dataset이 있다) classification해야한다고 생각해보자. 그 경우는 0부터 9까지 decision이 총 10개이므로 마지막 decision layer에는 10개의 neuron이 존재하게 되고 주어진 데이터에 대해 가장 activation된 크기가 큰 decision을 선택하는 것이다.</p>

<h3 id="backprop">Backpropagation Algorithm</h3>
<p>마지막으로 이제 weight를 어떻게 찾을 수 있는지 weight paramter를 찾는 알고리즘에 대해 알아보자. 먼저 한 가지 알아두어야 할 점은 activation function들이 non-linear하고, 이것들이 서로 layer를 이루면서 복잡하게 얽혀있기 때문에 neural network의 weight optimization이 non-convex optimization이라는 것이다. 따라서 일반적인 경우에 neural network의 paramter들의 global optimum을 찾는 것은 불가능하다. 그렇기 때문에 보통 gradient descent 방법을 사용하여 적당한 값까지 수렴시키는 방법을 사용하게 된다.</p>
<p>Neural network (이 글에서는 multi-layer feed-forward network)의 parameter를 update하기 위해서는 backpropagation algorithm이라는 것을 주로 사용하는데, 이는 단순히 neural network에서 gradient descent를 chain rule을 사용하여 단순화시킨 것에 지나지 않는다 (Gradient descent에 대해서는 이전에 쓴 <a href="http://SanghyukChun.github.io/63">Convex Optimization글</a>에서 자세히 다루고 있으니 참고하면 좋을 것 같다). 모든 optimization 문제는 target function이 정의되어야 풀 수 있다. Neural network에서는 마지막 decision layer에서 우리가 실제로 원하는 target output과 현재 network가 produce한 estimated output끼리의 loss function을 계산하여 그 값을 minimize하는 방식을 취한다. 일반적으로 많이 선택하는 loss에는 다음과 같은 함수들이 있다. 이때 우리가 원하는 <a class="red tip" title="만약 MNIST라면 d=10">d-dimensional</a> target output을 $t=[t_1, \ldots, t_d]$로, estimated output을 $x=[x_1, \ldots, x_d]$ 로 정의해보자.</p>

<ul>
	<li><p>sum of squares (Euclidean) loss: $\sum_{i=1}^d (x_i - t_i)^2 $</p></li>
	<li><p>softmax loss: $-\sum_{i=1}^d \bigg[ t_i \log \big(\frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) + (1 - t_i) \log \big(1 - \frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) \bigg] $</p></li>
	<li><p>cross entropy loss: $\sum_{i=1}^d [ -t_i \log x_i - (1-t_i) \log (1-x_i) ]$</p></li>
	<li><p>hinge loss: $\max(0,1-t \cdot x)$, 이때 $\cdot$은 내적을 의미한다.</p></li>
</ul>

<p>상황에 따라 조금씩 다른 loss function을 사용하지만, classification에 대해서는 보통 softmax loss가 gradient의 값이 numerically stable하기 때문에 softmax loss를 많이 사용한다. 이렇게 loss function이 주어진다면, 이 값을 주어진 paramter들에 대해 gradient를 구한 다음 그 값들을 사용해 parameter를 update하기만 하면 된다. 문제는, 일반적인 경우에 대해 이 paramter 계산이 엄청 쉬운 것만은 아니라는 것이다.</p>
<p>Backpropagtaion algorithm은 chain rule을 사용해 gradient 계산을 엄청 간단하게 만들어주는 알고리즘으로, 각각의 paramter의 grdient를 계산할 때 parallelization도 용이하고, 알고리즘 디자인만 조금 잘하면 memory도 많이 아낄 수 있기 때문에 실제 neural network update는 이 backpropagtaion 알고리즘을 사용하게 된다.</p>
<p>Gradient descent method를 사용하기 위해서는 현재 parameter에 대한 gradient를 계산해야하지만, 네트워크가 복잡해지면 그 값을 바로 계산하는 것이 엄청나게 어려워진다. 그 대신 backpropataion algorithm에서는 먼저 현재 paramter를 사용하여 loss를 계산하고, 각각의 parameter들이 해당 loss에 대해 얼마만큼의 영향을 미쳤는지 chain rule을 사용하여 계산하고, 그 값으로 update를 하는 방법이다. 따라서 backpropagation algorithm은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>
<h6>Phase 1: Propagation</h6>
<ol>
	<li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -&gt; hidden -&gt; output 으로 정보가 흘러가므로 &#8216;forward&#8217; propagation이라 한다.)</li>
	<li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -&gt; hidden 으로 정보가 흘러가므로 &#8216;back&#8217; propagation이라 한다.)</li>
</ol>
<h6>Phase 2: Weight update</h6>
<ol>
	<li>Chain rule을 사용해 paramter들의 gradient를 계산한다.</li>
</ol>
<p>이때, chain rule을 사용한다는 의미는 아래 그림에서 나타내는 것처럼, 앞에서 계산된 gradient를 사용해 지금 gradient 값을 update한다는 의미이다. (그림은 bengio의 <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">deep learning book</a> <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/mlp.html">Ch6</a> 에서 가져왔다.)</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-6.png" width="400" /></p>
<p>두 그림 모두 $\frac{\partial z}{\partial x}$를 구하는 것이 목적인데, 직접 그 값을 계산하는 대신, $y$ layer에서 이미 계산한 derivative인 $\frac{\partial z}{\partial y}$와 $y$ layer와 $x$에만 관계있는 $\frac{\partial y}{\partial x}$를 사용하여 원하는 값을 계산하고 있다. 만약 $x$ 아래에 $x^\prime$이라는 parameter가 또 있다면, $\frac{\partial z}{\partial x}$와 $\frac{\partial x}{\partial x^\prime}$을 사용하여 $\frac{\partial z}{\partial x^\prime}$을 계산할 수 있는 것이다. 때문에 우리가 backpropagation algorithm에서 필요한 것은 내가 지금 update하려는 paramter의 바로 전 variable의 derivative와, 지금 paramter로 바로 전 variable을 미분한 값 두 개 뿐이다. 이 과정을 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -&gt; hidden k, hidden k -&gt; hidden k-1, &#8230; hidden 2 -&gt; hidden 1, hidden 1 -&gt; input의 과정을 거치면서 계속 weight가 update되는 것이다. 예를 들어서 decision layer와 가장 가까운 weight는 직접 derivative를 계산하여 구할 수 있고, 그보다 더 아래에 있는 layer의 weight는 그 바로 전 layer의 weight와 해당 layer의 activation function의 미분 값을 곱하여 계산할 수 있다. 이해가 조금 어렵다면 아래의 <a href="74#example">예제</a>를 천천히 읽어보기를 권한다.</p>
<p>이 과정을 맨 위에서 아래까지 반복하면 전체 gradient를 구할 수 있고, 이 gradient를 사용해 parameter들을 update할 수 있다. 이렇게 한 번의 iteration이 진행되고, 충분히 converge했다고 판단할 때 까지 이런 iteration을 계속 반복하는 것이 feed-forward network의 parameter를 update하는 방법이다.</p>
<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network">링크</a>)</p>
<p><img src="http://SanghyukChun.github.io/images/post/42-1.png" width="600" /></p>
<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 &#8216;역으로&#8217; 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>

<h3 id="sgd">Stochastic Gradient Descent</h3>
<p>Gradient를 계산했으니 이제 직접 Gradient Descent를 써서 parameter만 update하면 된다. 그러나 문제가 하나 있는데, 일반적으로 neural network의 input data의 개수가 엄청나게 많다는 것이다. 때문에 정확한 gradient를 계산하기 위해서는 모든 training data에 대해 gradient를 전부 계산하고, 그 값을 평균 내어 정확한 gradient를 구한 다음 &#8216;한 번&#8217; update해야한다. 그러나 이런 방법은 너무나도 비효율적이기 때문에 Stochastic Gradient Descent (SGD) 라는 방법을 사용해야한다.</p>
<p>SGD는 모든 데이터의 gradient를 평균내어 gradient update를 하는 대신 (이를 &#8216;full batch&#8217;라고 한다), 일부의 데이터로 &#8216;mini batch&#8217;를 형성하여 한 batch에 대한 gradient만을 계산하여 전체 parameter를 update한다. Convex optimization의 경우, 특정 조건이 충족되면 SGD와 GD가 같은 global optimum으로 수렴하는 것이 증명되어있지만, neural network는 convex가 아니기 때문에 batch를 설정하는 방법에 따라 수렴하는 조건이 바뀌게 된다. Batch size는 일반적으로 메모리가 감당할 수 있을 정도까지 최대한 크게 잡는 것 같다.</p>

<h3 id="example">Backpropagation Algorithm: example</h3>
<p>이전에 chain rule로 gradient를 계산한다고 언급했었는데, 실제 이 chain rule이 어떻게 적용되는지 아래의 간단한 예를 통해 살펴보도록하자. 이때 계산의 편의를 위해 각각의 neuron은 sigmoid loss를 가지고 있다고 가정하도록 하겠다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-5.png" width="600" /></p>
<p>이때 각각의 neuron의 input으로 들어가는 값을 $in_{o_5}$, output으로 나가는 값을 $out_{h_3}$와 같은 식으로 정의해보자 (이렇게 된다면 in과 out은 $out_{h_3} = \sigma(in_{h_3})$ 으로 표현 가능하다. - 이때 $\sigma$는 sigmoid function). 먼저 error를 정의하자. error는 가장 간단한 sum of square loss를 취하도록 하겠다. 우리가 원하는 target을 $t$라고 정의하면 loss는 $E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2$가 될 것이다 (1/2는 미분한 값을 깔끔하게 쓰기 위해 붙인 상관없는 값이므로 무시해도 좋다). 그리고 우리가 원하는 값들은 $\frac{\partial E}{\partial w_{13}}, \frac{\partial E}{\partial w_{14}}, \ldots, \frac{\partial E}{\partial w_{46}}$이 될 것이다. 이제 가장 먼저 $\frac{\partial E}{\partial w_{35}}$ 부터 계산해보자.</p>
<p>$$\frac{\partial E}{\partial w_{35}} = \frac{\partial E}{\partial out_{o_5}} * \frac{\partial out_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial w_{35}}. $$</p>
<p>즉, 우리가 원하는 derivative를 계산하기 위해서는 세 개의 다른 derivative ($\frac{\partial E}{\partial out_{o_5}}, \frac{\partial out_{o_5}}{\partial in_{o_5}}, \frac{\partial in_{o_5}}{\partial w_{35}}$)를 계산해야한다. 각각을 구하는 방법은 다음과 같다.</p>
<ul>
	<li><p>$\frac{\partial E}{\partial out_{o_5}}$: error를 $E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2$라고 정의했으므로, $\frac{\partial E}{\partial out_{o_5}} = out_{o_5} - t_5$이다. - 이때 $out_{o_5}$와 $t_5$는 weight update이전 propagation step에서 계산된 값이다.</p></li>
	<li><p>$\frac{\partial out_{o_5}}{\partial in_{o_5}}$: $o_5$는 sigmoid activation function을 사용하므로 $out_{o_5} = \sigma(in_{o_5})$이다. 또한 sigmoid function의 미분 값은 $\frac{\partial \sigma(x)}{\partial x} = \sigma(x) (1 - \sigma(x))$으로 주어지므로, 이 값을 대입하면 $\frac{\partial out_{o_5}}{\partial in_{o_5}} = out_{o_5} (1 - out_{o_5})$가 된다. - 역시 여기에서도 미리 계산한 $out_{o_5}$를 사용한다.</p></li>
	<li>$\frac{\partial in_{o_5}}{\partial w_{35}}$: $o_5$로 들어온 값의 총 합은 앞선 layer의 output과 $o_5$로 들어오는 weight를 곱하면 되므로 $in_{o_5} = w_{35} out_{h_3} + w_{45} out_{h_4}$이고, 이것을 통해 $\frac{\partial in_{o_5}}{\partial w_{35}} = out_{h_3}$가 됨을 알 수 있다. - $out_{h_3}$ 역시 이전 propagation에서 계산된 값이다.</li>
</ul>
<p>따라서 $\frac{\partial E}{\partial w_{35}}$의 derivative 값은 위의 세 값을 모두 곱한 것으로 계산 할 수 있다. 그림으로 표현하면 아래와 같은 그림이 될 것이다. 즉, &#8216;backward&#8217; 방향으로 derivative에 대한 정보를 &#8216;propagation&#8217;하면서 parameter의 derivative를 계산하는 것이다. 마찬가지 방법으로 $w_{36}, w_{45}, w_{46}$에 대한 derivative도 계산할 수 있다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-7.png" width="300" /></p>
<p>그럼 이번에는 그 전 layer의 paramter들 중 하나인 $w_{13}$의 derivative를 계산해보자. 이번에 계산할 과정도 위와 비슷한 그림으로 표현해보면 아래와 같다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/74-8.png" width="300" /></p>
<p>그러면 이제 $\frac{\partial E}{\partial w_{13}}$을 구해보자.</p>
<p>$$\frac{\partial E}{\partial w_{13}} = \frac{\partial E}{\partial out_{h_3}} * \frac{\partial out_{h_3}}{\partial in_{h_3}} * \frac{\partial in_{h_3}}{\partial w_{13}}.$$</p>
<p>마찬가지로 각각을 구하는 방법에 대해 적어보자.</p>
<ul>
	<li><p>$\frac{\partial E}{\partial out_{h_3}}$: $E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2$를 $E = E_{o_5} + E_{o_6}$로 decompose 하면 이 미분 식은 $\frac{\partial E_{o_5}}{\partial out_{h_3}} + \frac{\partial E_{o_6}}{\partial out_{h_3}}$로 쓸 수 있다. 각각의 계산은 다음과 같다.</p></li>
	<ul>
		<li><p>$\frac{\partial E_{o_5}}{\partial out_{h_3}} = \frac{\partial E_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial out_{h_3}}$으로 쓸 수 있다. 이 중 앞의 값인 $\frac{\partial E_{o_5}}{\partial in_{o_5}}$은 이미 전 과정에서 계산했던 $\frac{\partial E}{\partial out_{o_5}}$과 $\frac{\partial out_{o_5}}{\partial in_{o_5}}$의 곱으로 계산가능하다. 뒤의 값은 $\frac{\partial in_{o_5}}{\partial out_{h_3}} = w_{35}$이므로 간단하게 계산할 수 있다.</p></li>
		<li><p>$\frac{\partial E_{o_6}}{\partial out_{h_3}}$도 위와 같은 방법으로 연산이 가능하다.</p></li>
	</ul>
	<li><p>$\frac{\partial out_{h_3}}{\partial in_{h_3}}$: $\frac{\partial out_{o_5}}{\partial in_{o_5}}$와 같다. 따라서 $out_{h_3} (1-out_{h_3})$이다.</p></li>
	<li><p>$\frac{\partial in_{h_3}}{\partial w_{13}}$: $\frac{\partial in_{o_5}}{\partial w_{35}}$와 같다. 따라서 $out_{i_1}$이다.</p></li>
</ul>
<p>이렇게 $\frac{\partial E}{\partial out_{h_3}}$에서는 앞에서 계산했던 값들을 재활용하고, 아래의 값들은 activation function과 network의 topological property에 맞는 derivative를 곱하는 방식으로 $\frac{\partial E}{\partial w_{13}}$을 구할 수 있다.</p>
<p>이렇듯 backpropagation algorithm은 forward propagation을 통해 필요한 값들을 미리 저장해두고, backward propagation이 진행되면서 위에서부터 loss에 대한 derivative를 하나하나 계산해나가면서 다음 layer에서 바로 전 layer에서 계산한 값들과 각 neuron 별로 추가적으로 필요한 derivative들을 곱해나가면서 weight의 derivative를 계산하는 알고리즘이다.</p>
<p>이렇게 한 번 전체 gradient를 계산한 다음에는 learning rate를 곱하여 전체 parameter의 값을 update한 다음, 다시 처음부터 이 과정을 반복한다. 보통 에러가 감소하는 속도를 관측하면서 &#8216;이 정도면 converge한 것 같다&#8217; 하는 수준까지 돌린다.</p>
<p>익숙해지려면 다소 시간이 걸리지만, 개념적으로 먼저 &#8216;error를 먼저 계산하고, 그 값을 아래로 전달해나가면서 바로 전 layer에서 계산한 미분값들을 사용해 현재 layer의 미분값을 계산한 다음, 그 값을 사용해 다음 layer의 미분값을 계산한다.&#8217; 라고 개념만 이해해두고 다시 차근차근 chain rule을 계산해나가면서 계산하면 조금 편하게 익숙해 질 수 있을 것이다.</p>

<h3>Backpropagation Algorithm: In Practice</h3>
<p>실제 backpropagtion을 계산해야한다고 가정해보자. 편의상 $l$번째 hidden layer를 $y_l$ 이라고 해보자. 이 경우 각 layer에 대해 backpropagation algorithm을 위해 계산해야할 것은 총 두 가지 이다. Loss를 $E$라고 적었을 때 먼저 layer $l$의 parameter $\theta_l$의 gradient인 $\frac{\partial E}{\partial w_l }$을 구해야한다. 이 값은 $\frac{\partial E}{\partial w_l } = \frac{\partial E}{\partial y_{l} } \frac{\partial y_{l} }{\partial w_{l} } $을 통해 계산한다. 이때, $\frac{\partial E}{\partial y_{l} } = \frac{\partial E}{\partial y_{l+1} } \frac{\partial y_{l+1} }{\partial y_{l} } $이므로 $\frac{\partial E}{\partial y_{l} }$은 바로 전 layer에서 넘겨준 $\frac{\partial E}{\partial y_{l+1} }$의 값을 사용하여 계산하게 된다. 정리하면 실제 계산해야하는 값은 $\frac{\partial y_{l+1} }{\partial y_{l}},\frac{\partial y_{l} }{\partial w_{l} } $ 두 가지이고, 이 값들을 사용해 $\frac{E}{\partial y_{l}}, \frac{E}{\partial w_{l}}$을 return하게 된다. 앞의 값은 다음 layer에 넘겨줘서 다음 input으로 사용하고, 두 번째 값은 저장해두었다가 gradient descent update할 때 사용한다.</p>
<p>두 가지 예를 들어보자. 먼저 Inner Product layer 혹은 fully connected layer이다. 이 layer가 inner product layer라고 불리는 이유는 input $y_l$에 대해 output $y_{l+1}$이 간단한 inner product 들이 모여있는 형태로 표현되기 때문이다. 예를 들어 $y_{l+1, i}$를 l+1 번째 layer의 i 번째 node라고 한다면, $y_{l+1, i} = \sum_{j} w_{ij} y_{l, j}$으로 표현할 수 있음을 알 수 있다. 그런데 이 값은 사실 vector $w$와 $y_{l}$의 inner product로 표현됨을 알 수 있다. 그렇기 때문에 fully connected layer를 inner product라고 부른다. 다시 본론으로 돌아와서 inner product의 output은 input과 weight의 matrix-vector multiplication인 $y_{l+1} = W_l * y_l$으로 표현할 수 있다.</p>
<p>따라서 $\frac{\partial y_{l+1} }{\partial y_{l}} = W_l^\top$이고, $\frac{\partial y_{l} }{\partial W_{l} } = y_l$이다. 이 값을 통해 실제 return하는 값은 $\frac{\partial E }{\partial y_{l} } = \frac{\partial E }{\partial y_{l+1} } * W_l^\top $와 $\frac{\partial E }{\partial w_{l} } = \frac{\partial E }{\partial y_{l+1} } * y_{l} $이 된다.</p>
<p>두 번째로 많이 사용하는 ReLU non-linearity의 gradient를 계산해보자. 이때 activation function은 마치 하나의 layer가 더 있는 것처럼 생각할 수 있다. 즉 $y_{l+1} = max(0, y_{l})$로 표현할 수 있을 것이다. Parameter는 없으니까 생략하면 만약 $y_{l} \geq 0$라면 $\frac{\partial y_{l+1} }{\partial y_{l} = 1} $이고, 아니라면 0이 될 것이다. 따라서 $y_{l} \geq 0$라면 $\frac{\partial E }{\partial y_{l} } = \frac{\partial E }{\partial y_{l+1} } $이 되고, 0보다 작다면 0이 될 것이다.</p>

<h3>정리</h3>
<p>Deep learning을 다루기 위해서는 가장 먼저 aritifitial neural network의 model에 대한 이해와 gradient descent라는 update rule에 대한 이해가 필수적이다. 이 글에서는 가장 기초적이라고 생각하는 feed-forward network의 model을 먼저 설명하고, paramter를 update하는 gradient descent algorithm의 일종인 backpropagation에 대한 개념적인 설명을 다루었다. 조금 어려울 수 있는 내용이니 다른 글들을 계속 참고하면서 보면 좋을 것 같다.</p>

<h3>Reference</h3>
<ul>
  <li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
  <li>wiki (<a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks">링크</a>)</li>
  <li><a href="http://courses.cs.tau.ac.il/Caffe_workshop/Bootcamp/pdf_lectures/Lecture%203%20CNN%20-%20backpropagation.pdf">Caffe workshop - CNN backpropagation</a></li>
</ul>

<h3>변경 이력</h3>
<ul>
  <li>2015년 9월 13일: 글 등록</li>
  <li>2015년 9월 14일: 오타수정, SGD 내용 추가 등</li>
  <li>2015년 9월 20일: BP in practice 추가</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Batch Normalization (ICML 2015)]]></title>
    <link href="http://SanghyukChun.github.io/88/"/>
    <updated>2015-08-25T21:25:00+09:00</updated>
    <id>http://SanghyukChun.github.io/88</id>
		<content type="html"><![CDATA[<p>Batch Normalization은 현재 <a href="http://image-net.org/challenges/LSVRC/2015/">ImageNet competition</a>에서 state-of-art (Top-5 error: 4.9%)를 기록하고 있는 Neural Network model의 기본 아이디어이다. 이 글에서는 arXiv에 제출된 (그리고 ICML 2015에 publish된) <a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 논문을 리뷰하고, batch normalization이 어떤 기술이고, 어떤 원리에 의해 작동하는지 등에 대해 다룰 것이다.</p>
<h3>Motivation: Deep learning의 속도를 어떻게 더 빠르게 만들 수 있을까?</h3>
<p>Deep learning이 잘 동작하고, 뛰어난 성능을 보인다는 것은 이제 누구나 알고 있다. 그러나 여전히 deep learning은 굉장히 시간이 오래 걸리는 작업이고, 그만큼 computation power도 많이 필요로 한다. 그 동안의 연구 결과를 보면, converge한 것 처럼 보이더라도 더 많이 돌리게 된다면 더 좋은 결과로 수렴한다는 것을 알 수 있는 만큼, deep neural network의 train 속도를 높이는 것은 전체적인 성능 향상에 도움이 될 것이다.</p>
<p>보통 Deep learning을 train할 때에는 stochastic gradient descent (SGD) method를 사용한다. SGD의 속도를 높이는 가장 naive한 방법은 learning rate를 높이는 것이지만, 높은 learning rate는 보통 gradient vanishing 혹은 gradient exploding problem을 야기한다는 문제가 있다.</p>
<p>Gradient vanishing은 backpropagation algorithm에서 아래 layer로 내려갈수록, 현재 parameter의 gradient를 계산했을 때 앞에서 받은 미분 값들이 곱해지면서 그 값이 거의 없어지는 (vanish하는) 현상을 의미한다. Gradient exploding은 learning rate가 너무 높아 diverge하는 현상을 말한다. Learning rate의 값이 크면 이 두 가지 현상이 발생할 확률이 높기 때문에 우리는 보통 작은 learning rate를 고르게 된다. 그러나 우리는 이미 일반적으로 learning rate의 값이 diverge하지 않을 정도로 크면 gradient method의 converge 속도가 향상된다는 것을 알고 있다. 따라서 이 논문이 던지는 질문은 다음과 같다. 자연스럽게 나오는 궁금증은 Gradient vanishing/exploding problem이 발생하지 않도록 하면서 learning rate 값을 크게 설정할 수 있는 neural network model을 design할 수 있는가?</p>
<h3>Internal Covariate Shift: learning rate의 값이 작아지는 이유</h3>
<p>Gradient vanishing problem이 발생하는 이유에 대해서는 여러가지 설명이 가능하지만 (exploding은 그냥 우리가 값을 작게 설정하여 해결할 수 있다) 이 논문에서는 internal covariate shift라는 개념을 제안한다. Covariate shift는 machine learning problem에서 아래 그림과 같이 train data와 test data의 data distribution이 다른 현상을 의미한다. 아래 그림 참고 (<a href="http://blog.bigml.com/2014/01/03/simple-machine-learning-to-detect-covariate-shift/">출처</a>)</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-1.jpg" width="500" /></p>
<p>이 논문에서는 단순히 train/test input data의 distribution이 변하는 것 뿐 아니라, 각각의 layer들의 input distribution이 training 과정에서 일정하지 않기 때문에 문제가 발생한다고 주장하며, 이렇게 각각의 layer들의 input distribution이 consistent하지 않은 현상을 internal convariate shift라고 정의한다. 이 논문에서 이것이 문제가 된다고 주장하는 이유는, 각각의 layer parameter들은 현재 layer에 들어오는 input data 뿐만 아니라 다른 model parameter들에도 영향을 받기 때문이라고한다. 즉, gradient vanishing problem이 발생하는 이유를 backpropagation 과정에서 아래로 내려갈수록 이전 gradient들의 영향이 더 커져서 지금 parameter가 거의 update되지 않는다고 설명하는 것과 같은 맥락이다.</p>
<p>기존에는 이런 현상을 방지하기 위하여 ReLU neuron을 사용하거나 (Nair &amp; Hinton, 2010), cafeful initialization을 사용하거나 (Bengio &amp; Glorot, 2010; Saxe et al., 2013), leanring rate를 작게 취하는 등의 전략을 사용했지만, 그런 방법이 아닌 다른 방법을 통해 internal covariate shift 문제가 해결이 된다면 더 높은 learning rate를 선택하여 learning 속도를 빠르게하는 것이 가능할 것이다.</p>
<h3>Navie approach: Whitening</h3>
<p>따라서 이 논문의 목표는 internal covariate shift를 줄이는 것이다. 그렇다면 internal covariate shift는 어떻게 줄일 수 있을까? 이 논문에서는 엄청 간단하게 input distribution을 zero mean, unit variance를 가지는 normal distribution으로 normalize 시키는 것으로 문제를 해결하며, 이를 whitening이라한다 (LeCun 1998, Wiesler &amp; Ney 2011). 주어진 column data $X\in R^{d\times n}$에 대해 whitening transform은 다음과 같다.</p>
<p>$$\hat{X} = Cov(X)^{-1/2} X, Cov(X) = E[( X - E[X] ) ( X - E[X] )^\top ].$$</p>
<p>그러나 이런 naive한 approach에서는 크게 두 가지 문제점들이 발생하게 된다.</p>
<ol>
  <li>multi variate normal distribution으로 normalize를 하려면 inverse의 square root를 계산해야 하기 때문에 필요한 계산량이 많다.</li>
  <li>mean과 variance 세팅은 어떻게 할 것인가? 전체 데이터를 기준으로 mean/variance를 training마다 계산하면 계산량이 많이 필요하다.</li>
</ol>
<p>따라서 이 논문에서는 이런 문제점들을 해결할 수 있으면서, 동시에 everywhere differentiable하여 backpropagation algorithm을 적용하는 데에 큰 문제가 없는 간단한 simplification을 제안한다.</p>
<h3>Batch Noramlization Transform</h3>
<p>앞서 제시된 문제점들을 해결하기 위하여 이 논문에서는 두 가지 approach를 제안한다.</p>
<ol>
  <li>각 차원들이 서로 independent하다고 가정하고 각 차원 별로 따로 estimate를 하고 그 대신 표현형을 더 풍성하게 해 줄 linear transform도 함께 learning한다</li>
  <li>전체 데이터에 대해 mean/variance를 계산하는 대신 지금 계산하고 있는 batch에 대해서만 mean/variance를 구한 다음 inference를 할 때에만 real mean/variance를 계산한다</li>
</ol>
<p>먼저 naive approach에서 covariance matrix의 inverse square root를 계산해야했던 이유는 모든 feature들이 서로 correlated되었다고 가정했기 때문이지만, 각각이 independent하다고 가정함으로써, 단순 scalar 계산만으로 normalization이 가능해진다. 이를 수식으로 표현하면 다음과 같다.</p>
<p>d dimensional data $x = (x^{(1)}, x^{(2)}, \ldots, x^{(d)})$에 대해 각각의 차원 $k$ 마다 다음과 같은 식을 계산하여 $\hat x$를 계산한다</p>
<p>$$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}.$$</p>
<p>그러나 이렇게 correlation을 무시하고 learning하는 경우 각각의 관계가 중요한 경우 제대로 되지 못한 training을 하게 될 수도 있으므로 이를 방지하기 위한 linear transform을 각각의 dimension $k$마다 learning해준다. 이 transform은 scaling과 shifting을 포함한다.</p>
<p>$$y^{(k)} = \gamma \hat{x}^{(k)} + \beta.$$</p>
<p>이때 parameter $\gamma, \beta$는 neural network를 train하면서 마치 weight를 update하듯 같이 update하는 model parameter이다.</p>
<p>두 번째로, 전체 데이터의 expectation을 계산하는 대신 주어진 mini-batch의 sample mean/variance를 계산하여 대입한다.</p>
<p>이제 앞서 설명한 두 가지 simplification을 적용하여 다음과 같은 batch normalization transform이라는 것을 정의할 수 있다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-2.png" width="500" /></p>
<p>이때, backpropagation에 사용되는 $\gamma, \beta$ 그리고 layer를 위한 chain rule은 다음과 같이 계산된다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-4.png" width="500" /></p>
<h3>Train/Inference with BN network</h3>
<p>앞에서 batch normalization transform을 각각의 layer input을 normalization하는데에 사용할 것이라는 설명을 했었다. 다시말해서 BN network는 기존 network에서 각각의 layer input 앞에 batch normalization layer라는 layer를 추가한 것과 구조가 동일하다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-5.png" width="500" /></p>
<p>이때 자세한 알고리즘은 다음과 같다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-3.png" width="500" /></p>
<p>주의해야할 점 하나는 train 과정에서는 mini-batch의 sample mean/variance를 사용하여 BN transform을 계산하였지만, inference를 할 때에도 같은 규칙을 적용하게 되면 mini-batch 세팅에 따라 inference가 변할 수도 있기 때문에 각각의 test example마다 deterministic한 결과를 얻기 위하여 sample mean/variance 대신 그 동안 저장해둔 sample mean/variance들을 사용하여 unbiased mean/variance estimator를 계산하여 이를 BN transform에 이용한다.</p>
<h3>BN network의 장점</h3>
<p>저자들이 주장하는 BN network의 장점은 크게 두 가지이다.</p>
<ol>
  <li>더 큰 learning rate를 쓸 수 있다. internal covariate shift를 감소시키고, parameter scaling에도 영향을 받지 않고, 더 큰 weight가 더 작은 gradient를 유도하기 때문에 parameter growth가 안정화되는 효과가 있다.</li>
  <li>Training 과정에서 mini-batch를 어떻게 설정하느냐에 따라 같은 sample에 대해 다른 결과가 나온다. 따라서 더 general한 model을 learning하는 효과가 있고, drop out, l2 regularization 등에 대한 의존도가 떨어진다.</li>
</ol>
<p>논문을 살펴보면 BN transform이 scale invariant하고, 큰 weight에 대해 작은 gradient가 유도되기 때문에 paramter growth를 안정화시키는 효과가 있다는 언급이 있다. 또한 regularization효과를 더 강화하기 위하여 매 mini-batch마다 training data를 shuffling하여 input으로 넣는데, 이때 한 mini-batch 안에서는 같은 데이터가 중복으로 나오지 않도록 shuffling하여 대입한다.</p>
<h3>실험</h3>
<p>먼저 BN network가 주장하는대로 잘 동작하는지 보여주는 실험이다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-6.png" width="500" /></p>
<p>가장 왼쪽은 MNIST data에 대해 BN을 쓴 것과 쓰지 않은 것의 convergence speed를 비교한 것이며, 다음 그림들은 BN을 사용했을 때와 사용하지 않았을 때, internal covariate shift가 어떻게 변화하는지를 보여주는 것이다. 한 뉴런의 training 동안 activation value의 변화를 plot한 것으로, 가운데 있는 선이 평균 값이고, 위 아래가 variance를 의미한다고 생각하면 된다. BN을 사용하면 처음부터 끝까지 거의 비슷한 distribution을 가진다는 것을 알 수 있다.</p>
<p>다음으로 single network에 대해 inception network와의 성능을 비교한 실험이다</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-7.png" width="500" /></p>
<p>세팅은 전부 같고 inception network와 비교하여 BN이 추가되었는지 여부와 learning rate가 몇 배인지 (x5는 5배의 leanring rate를 취한 것이다) 여부만 다르게 설정하였음에도 불구하고 convergence speed와 심지어 최종 max acc까지 차이가 나는 것을 볼 수 있다.</p>
<p>이런 결과들을 기반으로 약간의 paramter tunning을 거쳐 아래와 같은 ImageNet state-of-art를 기록했다고 한다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/88-8.png" width="500" /></p>
<h3>BN 네트워크 성능 accelerating하기</h3>
<p>BN을 추가하는 것 만으로 성능 개선이 엄청나게 일어나는 것은 아니며 다음과 같은 parameter tunning이 추가로 필요하다고 한다.
1. learning rate 값을 키운다 (0.0075 -&gt; 0.045, 5)
2. drop out을 제거한다 (BN이 regularization 효과가 있기 때문이라고 한다)
3. l2 weight regularization을 줄인다 (BN이 regularization 효과가 있기 때문이라고 한다)
4. learning rate decay를 accelerate한다 (6배 더 빠르게 가속한다)
5. local response normalization을 제거한다 (BN에는 적합하지 않다고 한다)
6. training example의 per-batch shuffling을 추가한다 (BN이 regularization 효과를 증폭시키기 위함이다)
7. photometric distortion을 줄인다 (BN이 속도가 더 빠르고 더 적은 train example을 보게 되기 때문에 실제 데이터에 더 집중한다고 한다)</p>
<p>이런 parameter tunning이 추가로 이루어지고 나면, 기존 neural network보다 ImageNet에서 훨씬 좋은 성능을 내는 neural network를 구성할 수 있다고 한다.</p>
<h3>Summary of BN network</h3>
<ul>
  <li>아이디어: 각 nonlinearity의 input으로 BN transform을 추가한다</li>
  <li>BN transform은 다음과 같은 두 가지 simplification으로 구성된다
    <ul>
      <li>각 feature들의 correlation을 무시하고 각각 따로 normalize 하고 각각에 대한 linear transform을 같이 learning한다</li>
      <li>Mini-batch의 sample mean/variance로 normalize 한다</li>
    </ul>
  </li>
  <li>BN network의 train/inference에서는 다음과 같은 특이점이 있다
    <ul>
      <li>Train/inference의 forward rule이 다르다 (각각이 사용하는 mean/variance가 다르다)</li>
      <li>Train 과정에서 mini-batch를 (중복없이) shuffling하여 train시킨다</li>
    </ul>
  </li>
  <li>BN network를 사용함으로써 다음과 같은 효과를 볼 수 있다
    <ul>
      <li>Learning rate를 큰 값으로 설정할 수 있어 converge가 빠르다</li>
      <li>Generalized model을 learning하는 효과가 있다</li>
    </ul>
  </li>
</ul>
<h3>Reference</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, Sergey Ioffe, Christian Szegedy, ICML 2015</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (16) Dimensionality Reduction (PCA, LDA)]]></title>
    <link href="http://SanghyukChun.github.io/72/"/>
    <updated>2015-06-17T05:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/72</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p>Machine Learning problem을 풀다보면, 종종 high dimensional 데이터를 다뤄야할 일이 생긴다. 그런데 dimension 높은 데이터를 다루다보면 여러 문제가 발생하는데, 높은 dimension으로 인해 생기는 대표적인 문제가 <a href="http://SanghyukChun.github.io/59#59-4-cd">예전 글</a>에서 다뤘던 Curse of dimensionality이다. 또한 많은 algorithm들에서 dimension이 complexity에 영향을 주는 경우가 많으므로 높은 dimension은 알고리즘의 성능에 악영향을 미치는 경우가 많다. 그렇기 때문에 많은 경우 데이터의 dimension이 높다면 다양한 방식의 dimenionality reduction 기술을 적용해 데이터의 차원을 낮추는 작업을 한다. 일반적으로 많이 사용하는 Dimensionality Reduction으로는 LDA와 PCA가 있으며, ICA, CCA 등의 방법도 종종 사용되며, 그 이외에도 RBM, Auto-encoder 등의 Neural network와 관련된 모델들도 존재한다. 이 글에서는 가장 많이 사용되는 방법들인 LDA와 PCA에 대해서만 다룰 것이다.</p>
<h3>Recall: Curse of Dimensonality</h3>
<p>Curse of Dimensionality는 데이터의 차원이 높아질수록 발생하는 여러 문제들을 통틀어 일컫는 말이다. 이런 문제가 발생하는 이유는 차원이 높아질수록 우리가 일반적으로 사용하는 Euclidean distance가 예상치 못한 방식으로 동작하기 때문이다. 예를 들어 d-차원 공간에서 임의의 점으로부터 거리가 1인 점들을 모아놓은 공간을 생각해봤을 때, d가 점점 커지면 커질수록 그 구의 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 shell에 존재한다는 것을 이미 예전 글에서 증명한바 있다. 다시 말해서 아주 높은 차원의 데이터는 우리가 원하지 않는 방향으로 움직일 가능성이 크다.</p>
<h3>Feature Extraction</h3>
<p>많은 상황에서 차원의 크기는 feature의 개수를 의미한다. 예를 들어 키, 몸무게, 나이, 성별이라는 네 가지 정보를 가지고 클러스터링을 한다고 생각해보자. 이 경우 데이터의 차원은 4이다. 그런데 아마도 이 네 가지 정보 이외에도 소득, 학력, 자산크기 등의 정보 등을 추가로 사용해 클러스터링을 한다면 더 좋은 클러스터링이 가능할지도 모른다. 문제는, 모든 feature가 전부 의미있는 feature는 아닐 수 있다는 것이다. 몸무게 정보를 사용하여 클러스터링한 것과 사용하지 않고 클러스터링한 것 중에서 몸무게 정보를 사용하지 않고 클러스터링 한 것이 더 좋을 수도 있다는 것이다. 이렇게 주어진 정보들 중에서 정말 의미 있는 feature를 뽑아내는 과정을 feature extration이라고 한다. 가장 간단한 feature extraction은 모든 feature를 사용해보기도 하고 사용해보지 않기도 하면서 $2^d$ 개의 조합을 모두 확인해보는 것이다. 그러나 이 방법은 차원의 크기에 exponential할 뿐 아니라, 만약 기존 feature가 highly correlate되어있고, 여러 개의 feature를 묶어서 한 feature로 만들어야 성능이 좋아지는 경우 등에 대해 좋은 성능을 내기 어렵다. 따라서 이런 상황에서도 dimensionality reduction 방법을 사용해 feature를 뽑아낼 수 있다. 만약 우리가 100개의 feature를 가지고 있을 때, &#8216;가장 좋은&#8217; 30개의 feature만 뽑기 위해서 30차원으로 dimensionality reduction을 하는 것이다.</p>
<h3>Dimensionality Reduction</h3>
<p>데이터의 차원을 낮춘다는 것의 의미는, 현재 데이터가 존재하는 차원에서 그보다 낮은 다른 차원으로 데이터들을 mapping시키는 map을 찾는다는 것과 같다고 할 수 있다. 이때 어떤 임의의 차원에서 그보다 낮은 임의의 낮은 차원으로 가는 mapping은 셀 수 없이 많다. 그렇다면 우리는 어떤 mapping을 선택해야할까? 예를 들어 가장 간단한 방법으로, d 차원 데이터를 d&#8217; 차원으로 보내고 싶을 때, 앞에서 설명했던 간단한 방법처럼 임의의 d&#8217; 개의 축을 골라서 그 축만 사용하는 방법도 있을 수 있고, <a class="red tip" title="projection이라고도 한다.">d에서 d&#8217;으로 가는 linear map</a>을 임의로 하나 고르는 것도 가능하다. 물론 non linear map도 가능하지만, 이 글에서 다룰 LDA와 PCA 두 가지 방법은 모두 linear mapping을 찾는 알고리즘이다. LDA는 supervised learning이며, PCA는 unsupervised learning에 해당하게 된다. 모든 문제에서 데이터는 matrix $X$로 표기되며, 데이터의 차원은 d이고, 개수는 n개이다. 따라서 $X$는 d by n matrix가 된다.</p>
<h3>LDA</h3>
<p><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis (LDA)</a>는 Dimensionality reduction만을 위한 방법은 아니다. LDA로 약어가 표시되는 것들이 꽤 많아서 (예: Latent Dirichelt Allocation) 이 모델을 처음 제안한 사람의 이름을 따서 Fisher&#8217;s LDA 라고 부르기도 한다. LDA는 여러 클래스가 존재할 때 그 클래스들을 최대한 잘 분리시키는 projection을 찾는다. 철학은 굉장히 단순한데, projection 시킨 데이터들에서 같은 클래스에 속하는 데이터들의 variance는 최대한 줄이고 ($\sigma_{within}$), 각 데이터들의 평균 값들의 variance는 최대한 키워서 ($\sigma_{between}$) 클래스들끼리 최대한 멀리 떨어지게 만드는 것이다. 이를 수식으로 표현하면 다음과 같은 수식을 얻을 수 있다.</p>
<p>$$S = \frac{\sigma_{between}^2}{\sigma_{within}^2}$$</p>
<p>일단 가장 간단한 상황인 클래스가 2개일 때의 상황만 고려해보도록하자. 참고로 LDA의 결과는 항상 클래스 개수 - 1 개까지의 벡터 밖에 찾을 수 없기 때문에, 이 상황에서 우리가 찾게 될 projection은 1차원 projection을 찾는 것이므로 간단하게 vector $w$로 기술하도록 하겠다. 클래스가 2개 뿐이라면, 위의 식은 엄청 간단한 수식으로 바뀌게 된다. 먼저 $\sigma_{between}$은 데이터가 단 두 개 뿐이기 때문에 간단하게 $ (w \cdot \mu_1 - w \cdot \mu_2 )^2 $으로 표현이 된다. 이때, $\mu_1, \mu_2$는 각각 1번째 클래스와 2번째 클래스에 속한 데이터들의 평균 값이다. 다음으로 $\sigma_{within}$ 역시 어렵지 않게 계산할 수가 있다. Projection을 하게 되면 데이터의 variance는 $w^\top \Sigma w$로 표현이 되기 때문에, 1번 클래스와 2번 클래스에 대해 이 값을 계산하고 더해주기만 하면 된다. 식을 정리해보면</p>
<p>$$w = \arg\min_w \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_w\frac{(w \cdot \mu_1 - w \cdot \mu_2 )^2}{w^\top \Sigma_1 w + w^\top \Sigma_2 w} $$</p>
<p>그리고 위 식을 w에 대해 미분하고 좀 정리해보면 $w \propto	(\Sigma_1 + \Sigma_2)^{-1}(\mu_1 - \mu_2) $ 라는 식을 얻을 수 있다.</p>
<h3>Multiclass LDA</h3>
<p>그러면 클래스가 2개보다 많을 때, 벡터 $w$가 아닌 subspace $U$를 찾는 과정은 어떻게 되는지 살펴보도록하자. 다시 $\sigma_{within}$과 $\sigma_{between}$을 살펴보자. 먼저 $\sigma_{between}$은 위와 비슷한 형태로 표현할 수 없다. 그러나 우리가 각각의 클래스의 평균을 $\mu_i$라고 정의한다면, 그냥 이 값들의 variance를 계산하기만 하면 된다. 이 variance는 $\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top$로 표현이 된다. 이때 $\mu$는 모든 평균들의 평균이다. 이 variance가 있으므로, projection하여 얻는 variance도 앞 뒤에 proejction matrix를 곱해 쉽게 계산할 수 있다. $\sigma_{within}$은 위와 비슷한 방식으로 $\sigma_{within} = U^\top \left(\sum_i \Sigma_i \right) U $로 표현할 수 있다. 이때 $\Sigma_i$는 i번째 클래스의 variance이다. 이 식을 정리해보면 다음과 같은 식을 얻는다.</p>
<p>$$U = \arg\min_U \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_U \frac{U^\top \left(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\right) U}{U^\top \left(\sum_i \Sigma_i \right) U} = \arg\max_U \frac{U^\top A U}{U^\top B} $$</p>
<p>$A$와 $B$는 각각 괄호 안에 있는 값을 의미한다. 위 식에서 보게 되면, 분자에 해당하는 부분이 rank가 클래스 개수 - 1 이기 때문에, 우리가 이 식을 풀었을 때도 $U$의 rank가 클래스 개수 - 1이 되므로 LDA가 구할 수 있는 subspace의 축 개수가 클래스 개수 - 1로 제한되는 것이다. 이 식을 풀기 위해서는 eigenvalue를 계산하는 것으로 간단하게 식을 풀 수 있다. 지금부터 왜 이 식이 eigenvalue를 푸는 것으로 해결이 가능한지를 살펴보자.</p>
<h3>General Eigenvector problem</h3>
<p>문제를 간단하게 하기 위해 전체 subspace가 아닌 vector $w$만 고려해보도록 하자. 따라서 식은</p>
<p>$$\min_w \frac{w^\top A w}{w^\top B w}$$</p>
<p>로 표현 가능하다. 이제 이 식을 w에 대해서 미분해보면 다음과 같은 식을 얻게 된다</p>
<p>$$\left(w^\top B w\right)^2 \big[ 2A w \left( w^\top B w \right) - 2B w \left( w^\top A w \right) \big] = 0$$</p>
<p>이를 잘 정리하면</p>
<p>$$Aw = \frac{w^\top A w} {w^\top B w} B w$$</p>
<p>로 표현이 되고, 우리가 원래 풀려고 했던 $\frac{w^\top A w} {w^\top B w}$를 $\lambda$라고 정의하면 식이 $A w = \lambda B w$라는 엄청 간단한 식이 나오게 된다. 이런 형태를 만족하는 $\lambda$를 찾는 문제를 general eigenvalue problem이라 부른다. 만약 $B$가 Identity matrix라면 우리가 아는 일반 eigenvalue problem이 된다. 따라서 원래 문제가 $\lambda$의 minimum을 찾는 것이었으니 이제 가장 작은 general eigenvalue를 찾기만 하면 우리가 풀고 싶었던 문제를 풀 수 있는 것이다.</p>
<h3>PCA</h3>
<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>는 Dimensionality reduction의 가장 대표적인 방법 중 하나이다. PCA는 projection된 데이터의 variance가 최대화되는 projection matrix를 찾는 문제이다. 그런데 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Low-rank_matrix_approximation">Eckart–Young theorem</a>에 의해서, 이 문제의 답이 데이터 $X$에 대한 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition(SVD)</a>으로 계산할 수 있다는 것이 알려져 있다. 그래서 PCA를 variance를 maximize하는 원래 정의대로 설명이 하는 경우도 많고, low rank matrix 문제로 설명하는 경우도 있다. 이 글에서는 low rank matrix estimation 문제로 설명하도록 하겠다. 이 문제의 objective는 아래 식과 같다.</p>
<p>$$\min_{rank(Z)=k} \| X - Z \|_F^2$$</p>
<p>이때 $\|A\|_F$는 <a href="http://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a>을 의미한다. 이 norm은 matrix의 모든 element들의 제곱을 더한 값이며, $\| A \|_F^2 = {\tt tr} (A A^\top) = {\tt tr} (A^\top A) $ 라는 특성이 알려져있다. 이제 위의 식에서 $Z$를 UV decomposition한 후 대입해보면 다음과 같은 식을 얻는다.</p>
<p>$$\min_{U\in R^{d \times k}, V\in R^{n \times k}, U^\top U = I} \|X - UV^\top\|_F^2$$</p>
<p>이 식을 $V$에 대해 미분하면 optimal한 V는 $V=X^\top U$로 표현이 됨을 알 수 있으며, 이를 위의 식에 대입한 후, Frobenius norm의 성질을 잘 활용하면 아래와 같은 식이 나온다.</p>
<p>$$\max_{U\in R^{d \times k}, U^\top U = I} {\tt tr} (U^\top X X^\top U)$$</p>
<p>가 되며, 이 식의 답은 SVD를 통해 계산할 수 있다는 것을 알 수 있다. 그러나 만약 $X$의 mean이 0가 아니게 되면 UV decomposition을 하는 과정에서 문제가 생기게 되서 같은 방식으로 깔끔하게 구할 수가 없다. 이 과정은 다소 복잡하므로 생략하고 결과만 얘기하면, 그냥 $\hat X = X - \frac{1}{n} X \mathbf 1$을 SVD하면 된다. 뒤에 있는 term은 그냥 X의 평균값이다.</p>
<p>정리하면, 임의의 데이터 X에 대한 PCA는 다음과 같이 구할 수 있다.</p>
<ol>
	<li>데이터 $X$의 empirical mean을 계산한 후 모든 데이터에서 평균을 빼준다.</li>
	<li>새로 만들어진 데이터 $\hat X$의 가장 큰 singular value부터 k번째 큰 singular value까지에 대응하는 singular vector들을 구한다. (SVD를 통해)</li>
	<li>뽑아낸 k개의 singular vector로 U를 구성하고 return한다.</li>
</ol>
<p>쉽지만 그만큼 강력하다. 하지만 PCA의 경우 Frobenius norm의 제곱값을 사용하므로 각 element들이 norm을 계산할 때 한 번 제곱되고, 다시 전체에 제곱을 취할 때 또 제곱이 취해지므로 조금이라도 원래 데이터와 다른 outlier가 존재하게 된다면 그 효과가 굉장히 극적으로 증폭되기 때문에 noise에 취약하다. 이를 방지하기 위해 objective의 norm을 1-norm으로 바꾸거나 하는 등의 robust PCA 연구도 활발하게 진행되고 있다.</p>
<h3>정리</h3>
<p>Dimensionality Reduction은 매우 유용하고 많이 쓰이는 툴이다. 특히 PCA는 굉장히 빈번하게 사용되고 알고리즘도 매우 간단하기 때문에 알아두면 쓰임새가 많다. 이 글에서는 LDA와 PCA만 다뤘지만, ICA, CCA, RBM 등 굉장히 많은 dimensionality reduction 기술들이 존재한다. 이 중 RBM은 추후에 다시 한 번 자세하게 설명하도록 하겠다.</p>

<h3>변경 이력</h3>
<ul>
  <li>2015년 6월 17일: 글 등록</li>
</ul>

<h3>Reference</h3>
<ul>
  <li>Nie, Feiping, Jianjun Yuan, and Heng Huang. “Optimal mean robust principal component analysis.” Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (14) EM algorithm]]></title>
    <link href="http://SanghyukChun.github.io/70/"/>
    <updated>2015-06-14T03:50:00+09:00</updated>
    <id>http://SanghyukChun.github.io/70</id>
		<content type="html"><![CDATA[<h3>들어가며</h3>
<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM 알고리즘</a>은 <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 하나이다. 굉장히 많은 probabilistic 모델을 풀기 위해 널리 사용되는 알고리즘 중 하나이며, iterative한 알고리즘 중 하나이다. Clustering에서 다뤘던 GMM은 물론이고, HMM, RBM 등의 문제를 해결하는데 있어서도 사용되는 알고리즘이다. 이 글에서는 EM 알고리즘이 무엇인지, latent variable이 존재하는 probabilistic model은 무엇이며 어떤 장점이 있는지를 다룰 것이며, EM 알고리즘의 의미와 더 나아가 이 알고리즘이 어떻게 MLE나 MAP문제를 해결하는지에 대해 다룰 것이다.</p>
<h3>Probabilistic model having latent variable</h3>
<p>EM 알고리즘에 대해 다루기 전에 먼저 latent variable을 가지고 있는 probabilistic model에 대해 설명하도록 하겠다. Latent variable은 우리가 본래 가지고 있는 random variable이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다. 예를 들어, 아래 그림과 같은 Grapical model을 고려해보자. 이때 우리가 관측할 수 있는 random variable은 paramter $\theta$로 parameterized 되어있는 $\mathbf X$ 하나이고, $\mathbf Z$은 우리가 관측할 수 없는 hidden variable이라고 해보자.</p>
<p><img src="http://SanghyukChun.github.io/images/post/70-1.png" width="200" /></p>
<p>만약 위의 grapical model에서 $\mathbf X$의 maximum likelihood를 계산하고 싶다면 어떻게 해야할까? 먼저 $\mathbf X$의 maximum likelihood는 다음과 같이 표현된다.</p>
<p>$$\max_{\theta} p(\mathbf X | \theta) = \sum_{\mathbf Z} p(\mathbf X,Z | \theta). $$</p>
<p>문제를 조금 더 간단하게 하기 위하여 위의 식에서 $\mathbf Z$는 discrete variable이라고 정의하였다. 이 문제에서 우리가 가정할 것이 하나있다. 바로 marginal distribution $p(\mathbf X | \theta)$를 직접 계산하는 것이 매우 까다롭다는 것이다. 이때, $\mathbf Z$는 우리 마음대로 정할 수 있는 latent variable이기 때문에, joint distribution $p(\mathbf X,Z | \theta)$가 marginal distribution보다 쉬운 $\mathbf Z$를 잡는 것이 가능하다.</p>
<h3>Decomposition of log-likelihood</h3>
<p>만약 우리가 latent variable $\mathbf Z$의 marginal distribution을 $q(\mathbf Z)$라고 정의한다면, 앞에서 설명한 log-likelihood를 다음과 같이 decompose할 수 있다.</p>
<p>$$\ln p(\mathbf X | \theta) = \mathcal L(q,\theta) + ~\mbox{KL}(q\|p),$$
이때, $\mathcal L(q,\theta)$와 $\mbox{KL}(q\|p)$는 다음과 같이 정의된다. $$\mathcal L(q,\theta) = \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf X, \mathbf Z | \theta)}{q(\mathbf Z)} ~\mbox{and}~ \mbox{KL}(q\|p) = - \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf Z | \mathbf X, \theta)}{q(\mathbf Z)}. $$</p>
<p>위의 식에서 $\mathcal L(q,\theta)$는 hidden variable $\mathbf Z$의 marginal distribution $q(\mathbf Z)$의 functional이고, $\mbox{KL}(q\|p)$는 $q,p$의 KL divergence를 의미한다. 이렇게 log-likelihood를 decompose하게 되면, 한 쪽에는 random variable $\mathbf X, \mathbf Z$의 joint distribution, 그리고 또 한 쪽은 conditional distribution으로 표현이 된다는 것을 알 수 있다. 또한 KL divergence의 특성에서부터 재미있는 사실을 하나 더 유추할 수 있는데, 바로 KL divergence가 반드시 0보다 크거나 같기 때문에 $\mathcal L(q,\theta)$이 곧 log-likelihood의 lower bound가 된다는 사실이다. 이를 그림으로 나타내면 아래 그림의 오른쪽 그림과 같다. (일단 $theta^{\mbox{old}}, theta^{\mbox{new}}$는 무시하자)</p>
<p><img src="http://SanghyukChun.github.io/images/post/70-2.png" width="500" /></p>
<h3>EM algorithm</h3>
<p>위와 같은 사실로부터 lower bound가 maximum이 되도록하는 $\theta$와 $q(\mathbf Z)$의 값을 찾고, 그에 해당하는 log-likelihood의 값을 찾는 알고리즘을 설계하는 것이 가능할 것이다. 만약 $\theta$와 $q(\mathbf Z)$를 jointly optimize하는 문제가 어려운 문제라면 이 문제를 해결하는 가장 간단한 방법은 둘 중 한 variable을 고정해두고 나머지를 update한 다음, 나머지 variable을 같은 방식으로 update하는 alternating method일 것이다. EM 알고리즘은 이런 아이디어에서부터 시작하게 된다. EM 알고리즘은 E-step과 M-step 두 가지 단계로 구성된다. 각각의 step에서는 앞서 설명한 방법처럼 $\theta$와 $q(\mathbf Z)$를 번갈아가면서 한 쪽은 고정한채 나머지를 update한다. 이런 alternating update method는 한 번에 수렴하지 않기 때문에, EM 알고리즘은 E-step과 M-step을 알고리즘이 수렴할 때 까지 반복하는 iterative 알고리즘이 된다.</p>
<p>현재 우리가 가지고 있는 parater $\theta$의 값을 $\theta^{\mbox{old}}$라고 정의해보자. EM 알고리즘의 E-step은 먼저 $\theta^{\mbox{old}}$ 값을 고정해두고 $\mathcal L(q,\theta)$의 값을 최대로 만드는 $q(\mathbf Z)$의 값을 찾는 과정이다. 이 과정은 매우 간단하게 계산 수 있는데, 그 이유는 log-likelihood $\ln p(\mathbf X | \theta^{\mbox{old}})$의 값이 $q(\mathbf Z)$ 값과 전혀 관계가 없기 때문에, 항상 $\mathcal L(q,\theta)$를 최대로 만드는 조건은 KL divergence가 0이 되는 상황이기 때문이다. KL divergence는 $q(\mathbf Z) = p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})$ 인 상황에서 0이 되기 때문에, $q(\mathbf Z)$에 posterior distribution $p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})$을 대입하는 것으로 해결할 수 있다. 따라서 E-step은 언제나 KL-divergence를 0으로 만들고, lower bound와 likelihood의 값을 일치시키는 과정이 된다.</p>
<p>E-step에서 $\theta^{\mbox{old}}$을 고정하고 $q(\mathbf Z)$에 대한 optimization 문제를 풀었으므로 M-step에서는 그 반대로, $q(\mathbf Z)$를 고정하고 log-likelihood를 가장 크게 만드는 새 paramter $\theta^{\mbox{new}}$을 찾는 optimization 문제를 푸는 단계가 된다. E-step에서는 update하는 variable과 log-likelihood가 서로 무관했기 때문에 log-likelihood가 증가하지 않았지만, M-step에서는 $\theta$가 log-likelihood에 직접 영향을 미치기 때문에 log-likelihood 자체가 증가하게 된다. 또한 M-step에서 $\theta^{\mbox{old}}$가 $\theta^{\mbox{new}}$로 바뀌었기 때문에 E-step에서 구했던 $p(\mathbf Z)$로는 더 이상 KL-divergence가 0이 되지 않는다. 따라서 다시 E-step을 진행시켜 KL-divergence를 0으로 만들고, log-likelihood의 값을 M-step을 통해 키우는 과정을 계속 반복해야만한다.</p>
<p>위에 나왔던 그림에서 왼쪽이 E-step을 의미하고, 오른쪽 그림이 M-step을 의미한다. E-step을 의미하는 왼쪽 그림에서 KL divergence는 0이 되고, lower bound인 functional과 log-likelihood의 값이 같아진다. 오른쪽 그림은 M-step을 표현하고 있으며, $\theta$가 update되면서 log-likelihood의 값이 증가하게 되지만, 더 이상 KL divergence의 값이 0이 아니게 된다. 이 과정을 더 이상 값이 변화하지 않을 때 까지 충분히 많이 돌리게 되면 이 값은 log-likelihood의 어떤 값으로 수렴하게 될 것이다. 그리고 매 step마다 항상 optimal한 값으로 진행하기 때문에 이 값은 log-likelihood의 local optimum으로 수렴하게 된다는 사실까지 알 수 있다. EM algorithm은 아래와 같은 그림으로 표현할 수 있다.</p>
<p><img src="http://SanghyukChun.github.io/images/post/70-3.png" width="500" /></p>
<p>각 curve는 $\theta$ 값이 고정이 되어있을 때 $q(\mathbf Z)$에 대한 lower bound $\mathcal L(q,\theta)$의 값을 의미한다. 매 E-step마다 고정된 $\theta$에 대해 $p(\mathbf Z)$를 풀게 되는데, 이는 곧 log-likelihood와 curve의 접점을 찾는 과정과 같다. 또한 M-step에서는 $\theta$ 값 자체를 현재 값보다 더 좋은 지점으로 update시켜서 curve 자체를 이동시키는 것이다. 이런 과정을 계속 반복하면 알고리즘은 언젠가 local optimum으로 수렴하게 될 것이다. Local optimum에 수렴한다는 성질은 얼핏보면 나빠보일 수도 있지만, 이 글의 도입부에서 latent variable이 introduce되는 이유 자체가 원래 log-likelihood를 계산하는 것이 불가능에 가깝기 때문이었다는 사실을 돌이켜본다면, latent variable을 잘 잡기만 한다면 반드시 local optimum으로 수렴하는 EM 알고리즘은 매우 훌륭한 알고리즘이라는 사실을 알 수 있다. 즉, 아예 문제를 풀지 못하는 것 보다는 local optimum으로 수렴하는 것이 훨씬 좋다.</p>
<h3>Pratical issues</h3>
<p>대부분의 probabilistic model의 MLE 혹은 MAP는 EM 알고리즘을 사용하면 구할 수 있다. 그러나 EM 알고리즘이 항상 잘 동작하는 것은 아닌데, E-step 혹은 M-step의 optimization 문제를 푸는 것이 어려운 상황이 그러하다. E-step은 posterior를 계산하는 과정이므로 크게 문제가 되는 경우는 많지 않지만, M-step은 $\theta$에 대한 optimization 문제를 풀어야하는 과정인데, 이 과정에서 문제가 발생하는 경우가 많다. 예를 들어 모든 $\theta$를 한 번에 jointly optimize하는 것이 어려워 또 다른 alternative method를 사용해야할 수 도 있다. 그렇게 되면 iterative 알고리즘 안에 nested iterative 알고리즘이 발생하게 되어 전체 알고리즘의 수렴 속도가 매우 느려지게 된다. 가장 단순하게 이 문제를 해결하는 방법으로는 nested iterative 알고리즘을 완전히 푸는 것이 아니라, 수렴여부와 관계없이 iteration을 조금만 돌리고 다시 E-step을 구하고, 다시 M-step을 정확히 푸는 대신 iteration을 몇 번만 돌리는 등의 방식이 있을 것이다. 일반적인 경우에는 이런 방식이 수렴하지 않지만, 몇몇 경우에는 이런 방식이 local optimum에 수렴한다는 것이 증명되어있다. 가장 대표적인 예가 RBM을 푸는 Contrastive Divergence이다. 이에 대한 더 자세한 설명은 추후에 Deep learning에 대해 다루는 글에서 더 자세히 다루도록 하겠다.</p>
<h3>정리</h3>
<p>EM 알고리즘은 latent variable이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 가장 대표적인 알고리즘이다. 본문에서는 MLE를 계산하는 과정만 다뤘지만, MAP도 비슷한 방식으로 구할 수 있다. Latent variable을 쉽게 잡기만한다면, 아무리 풀기 어려운 문제일지라도 구하고자 하는 문제의 local optimum에 수렴한다는 좋은 성질을 가지고 있기 때문에 매우 다양한 모델에서 이 알고리즘을 사용해 문제를 해결하고는 한다. 그러나 간혹 특히 M-step의 optimization을 푸는 과정에서 시간이 너무 오래 걸리는 문제가 발생하는 경우가 생길 수 있는데, 이런 경우 몇 가지 휴리스틱을 사용해 문제를 해결하기도 한다.</p>

<h3>Reference</h3>
<ul>
  <li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
</ul>

<h3>변경 이력</h3>
<ul>
  <li>2015년 6월 14일: 글 등록</li>
</ul>

<hr />

<h3 id="machine-learning---">Machine Learning 스터디의 다른 글들</h3>

<ul>
  <li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
  <li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
  <li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
  <li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
  <li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
  <li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
  <li>Regression and Logistic Regression</li>
  <li>PAC Learning &amp; Statistical Learning Theory</li>
  <li>Support Vector Machine</li>
  <li>Ensemble Learning (Random Forest, Ada Boost)</li>
  <li>Graphical Model</li>
  <li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
  <li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
  <li>Hidden Markov Model</li>
  <li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
  <li><a href="http://SanghyukChun.github.io/73">Recommendation System (Matrix Completion)</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/95">Recommendation System with Implicit Feedback</a></li>
    </ul>
  </li>
  <li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
  <li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 - RBM, DNN, CNN</a></li>
  <li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a>
    <ul>
      <li><a href="http://SanghyukChun.github.io/96">Multi-armed Bandit</a></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
</feed>
