<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-Learning | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2014-08-10T21:16:43+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (4) Algorithm]]></title>
    <link href="http://SanghyukChun.github.io/60/"/>
    <updated>2014-08-04T20:26:00+09:00</updated>
    <id>http://SanghyukChun.github.io/60</id>
    <content type="html"><![CDATA[<p>내가 많은 책이나 lecture를 들어본 것은 아니지만, 보통 일반적인 Machine Learning과 관련된 책이나 강의 등에서 Algorithm을 따로 다루거나 하는 경우는 본 적이 없기는 하다. 그럼에도 불구하고 알고리듬을 중요하게 다루고자 하는 이유는, 결국 Machine Learning에 대해 다루기 위해서는 알고리듬이라는 것에 대해 이해가 필요하고, 어떤 알고리듬이 좋은 것이고 어떤 알고리듬이 나쁜 것인지에 대한 구분이 이뤄져야지만 향후 논의하게 될 많은 주제들에 대해 얘기하기 쉬워질 것이라고 생각해서이다.</p>


<p><a href="57" target="new">이전 글</a>에서 머신러닝에서 알고리듬이란 어떤 의미가 있는지를 얘기했었다. 쉽게 생각하면, 우리가 원하는 형태로 모델을 정의한 이후에 그 모델을 어떻게 learning할 것인가, 즉, 어떤 알고리듬을 사용하여 model을 learning할 것인가에 대한 얘기를 하기 위해서는 알고리듬에 대해 반드시 짚고 넘어가야만 한다. 사실 내 생각에 대부분의 머신러닝 렉쳐나 교재에서 알고리듬에 대해 깊게 다루지 않는 이유는 알고리듬이 전산학에서 매우 기초적인 학문이기 때문이기 때문에 당연히 알고 있을 것이라고 가정하기 때문이 아닐까하지만.. 그래도 아무튼 알고리듬 부분에서 반드시 짚고 넘어가야할 부분은 (1) Big O notation (2) P and NP (3) Reduction (4) NP Complete (5) Approximation Algorithm 정도가 아닐까한다.</p>


<h5 id="60-1-bigO">Algorithm, Big O notation</h5>


<p>먼저 algorithm이란 무엇인지에 대해 생각해보자. 알고리듬이란 것의 정확한 정의는 <a href="http://en.wikipedia.org/wiki/Algorithm" target="new">위키</a>를 참고하면 되고, 알고리듬에 있어서 중요한 몇 가지를 꼽자면, 먼저 input이 정의가 되어야하며 output을 가져야한다. 즉, 알고리듬은 특정 데이터에 대해 동작해야하며, 해당 데이터에 대한 알고리듬의 결과를 출력해야만한다. 그리고 알고리듬은 반드시 어떤 '목적'을 가지고 있다. 즉, 내가 만약 특정 지점부터 다른 특정 지점으로 이동하는 가장 짧은 path를 찾는 알고리듬을 작성해야만한다면 해당 알고리듬의 목적은 shortest path를 찾는 것이고, input은 임의의 graph와 시작점, 그리고 끝점이 될 것이다. 마지막으로 출력값은 shortest path가 될 것이다. 이런 것을 행할 수 있는 일종의 procedure가 알고리듬이라고 할 수 있다. 하지만 우리는 그냥 임의의 알고리듬이 필요한 것이 아니라 '좋은' 알고리듬이 필요하다. 예를 들어서 Algorithm A는 shortest path를 찾는데 1시간이 걸리고, Algorithm B는 4초가 걸린다면 당연히 B를 사용해야할 것이다. 그렇다면 알고리듬의 좋다 혹은 나쁘다는 무엇으로 구분하느냐, <a href="http://en.wikipedia.org/wiki/Big_O_notation" target="new">Big O notation</a>의 역할이 바로 그것을 구분하는 역할을 하는 것인데, 이 notation은 해당하는 알고리듬이 '주어진 input의 크기에 대해' 계산량이 얼마나 필요하느냐를 indicate하는 notation이다. 표기는 O(n) 와 같은 꼴로 표시하게 된다. n은 input의 크기이다. 예를 들어 shortest path면 전체 graph의 node의 개수가 될 것이다. O notation은 일종의 upper limit로, 아무리 최악의 상황에서도 계산량이 O 안에 있는 양보다 적게 걸린다는 의미이다. 또한 만약 소요 시간이 2n 이거나 n+1 이거나 10000000000000000n 이어도 이 알고리듬들은 모두 O(n)이 된다. 이 정도 얘기는 조금만 구글링해도 많이 나오는 얘기니 여기까지만 적고, 진짜 중요한건 'polynomial time'일 것이다. 무슨 얘기이냐하면, 알고리듬의 실행시간이 input의 크기가 늘어나는 것에 대해 polynomial scale로 증가하는 알고리듬이 좋은 알고리듬이라는 뜻이다. 당연히 input에 대해 최대한 적게 증가하는 알고리듬이 좋기는 하지만, \( O(e^n) \) 보다는 \( O(n^4) \) 이 훨씬 더 좋다는 얘기이다. Exponential time이 소요되는 알고리듬은 사실상 거의 무한대의 시간이 걸린다고 봐도 될 정도로 절망적인 computation time을 의미하며, 제대로 활용 가능한 알고리듬이 되려면 그 알고리듬의 computation time은 반드시 polynomial time이어야한다.</p>


<p>Expotentially increase라는 말에는 정말 어마어마한 파괴력이 있다. 이것이 왜 절망적이냐하면, 우리가 linear한 10n 알고리듬을 가지고 있을 때 input의 size가 1, 2, 3 의 순으로 증가하더라도 여전히 computation time은 10, 20, 30이지만, exponential time이 필요한 경우에는 예를 들어 10^n이라고 한다면 10, 100, 1000만큼의 시간이 필요하다. 즉 input이 3배 증가했을 뿐인데 두 알고리듬은 1000/30 = 33.33 배 만큼의 성능차이가 나는 것이다. 만약 input size가 100이라면? \(10^{97}\) 만큼의 차이가 난다. Exponential이라는 것에는 이만큼의 파괴력이 있다. 그러나 polynomial time안에 풀 수 있다면, 100배가 증가했을 때, n과 \(n^2\)의 차이는 100에 불과하다. 이 때문에 polynomial time algorithm은 풀 수 있는 문제로 취급되고, exponential algorithm은 실제로 쓸 수 없는 알고리듬으로 취급받는 것이다. <a href="http://SanghyukChun.github.io/59#59-3-ms">이전 글의 model selection part</a>에서 '그리고 데이터가 많아지면 그런 validation set이 엄청나게 많아진다. 정확히는 exponential로 늘어나기 때문에 마냥 모든 데이터에 대해 cross-validation을 하는 것은 불가능하다.' 라는 표현을 했을 때 exponential 로 증가하는 validation이 불가능하다는 표현을 했던 것이다.</p>


<h5 id="60-2-np">P and NP</h5>


<p>P problem이란 해당하는 문제를 polynomial time안에 풀 수 있는 알고리듬을 제시할 수 있는 문제를 의미한다. 예를 들어 우리에게 주어진 데이터의 개수를 sorting하는 알고리듬은 \(n log n\) 의 시간이 필요하므로 P problem이라 할 수 있다. NP problem은 올바르지는 않지만 진짜 진짜 간단하게, 표현하면, 'polynomial time안에 풀 수 없는 엄청나게 어려운 문제' 라고 할 수 있다. 하지만 이것은 올바르지 않은 정의이며, 심지어 정말 polynomial time안에 풀 수 없는지 조차 아직 확실하지 않다. NP problem의 정확한 정의는 'problem이 주어지고 해당 problem에 대해 어떤 suggested solution이 주어졌을 때 polynomial time안에 그 solution이 맞는 solution인지 아닌지 구분할 수 있는 문제'라고 할 수 있다. 당연히 정확한 답을 polynomial time안에 풀 수 있는 P는 NP의 subset이다. 즉, NP는 P를 포함하는 set이라고 할 수 있다. 그렇다면 NP는 반드시 P보다 크다고 할 수 있을까? 이 문제를 얘기하려면 먼저 Reduction에 대해 다뤄야한다.</p>


<h5 id="60-3-red">Reduction</h5>


<p>Problem X에서 Y로의 Reduction이란 만약 우리가 Problem Y를 풀 수 있는 Algorithm을 가지고 있을 때, 이 algorithm을 사용해 problem X를 풀 수 있는 algorithm을 찾을 수 있다는 것을 의미한다. 즉, 우리가 problem X를 풀기위해서는 problem Y를 풀기만 하면 된다. 즉, 간단히 생각하면 문제 Y가 X보다는 더 어려운 문제라고 생각하면 된다. 만약 우리가 problem Y 를 풀기위한 algorithm 중에서 P인 algorithm을 가지고 있다면, 그리고 reduction을 polynomial time안에 할 수 있다면 우리는 반드시 problem X를 polynomial time안에 풀 수 있을 것이다.</p>


<h5 id="60-4-npc">NP Complete</h5>


<p>NP Complete problem은 모든 NP problem이 해당 problem으로 reduction될 수 있는 문제를 의미한다. 즉, 내가 그 어떤 NP problem을 제시하더라도 반드시 어떤 NP complete problem으로 reduction시키는 것이 가능하다. 그리고 <a href="http://en.wikipedia.org/wiki/Cook%E2%80%93Levin_theorem" target="new">Cook-Levin Theorem</a>에서 Boolean SAT problem이 NP problem이라는 것을 증명한다. 그렇다면 당연히 SAT problem을 통해서 다른 NP complete problem들을 찾을 수 있다. 우리가 많이 사용하는 NP complete problem들은 <a href="http://en.wikipedia.org/wiki/Independent_set_problem" target="new">Independent set problem</a>, <a href="http://en.wikipedia.org/wiki/Clique_problem" target="new">Clique problem</a>, <a href="http://en.wikipedia.org/wiki/Vertex_cover_problem" target="new">Vertex Cover problem</a>, <a href="http://en.wikipedia.org/wiki/Set_cover" target="new">Set Cover problem</a>, <a href="http://en.wikipedia.org/wiki/Subset_sum_problem" target="new">Subset Sum problem</a> <a href="http://en.wikipedia.org/wiki/Hamiltonian_path_problem" target="new">Hamiltonian path problem</a>, <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem" target="new">Travelling salesman problem</a>, <a href="http://en.wikipedia.org/wiki/Graph_coloring_problem" target="new">Graph Coloring problem</a> 등이 있다. 해당 문제들의 전체 목록은 <a href="http://en.wikipedia.org/wiki/NP-complete#NP-complete_problems" target="new">위키</a>에서도 볼 수 있다.</p>


<p>그렇다면 당연히 필연적으로 할 수 있는 질문은, 'NP complete problem을 polynomial time안에 풀 수 있는가?' 라는 질문이 되겠다. 당연히 위에 서술한 그 어떤 문제 중에서 단 하나라도 polynomial solution을 제시할 수 있다면 모든 NP 문제들을 P로 풀 수 있을 것이다. 이 질문이 그 유명한 <a href="http://en.wikipedia.org/wiki/P_versus_NP_problem" target="new">P=NP?</a> 문제가 되겠다. 그리고 또 안타깝게도 이 문제는 <a href="http://en.wikipedia.org/wiki/Millennium_Prize_Problems" target="new">Millennium Prize Problems</a>라고 해서 엄청나게 어려운, 상금이 걸려있는 문제 중 하나이다. 즉, 아직도 결론이 나지 않은 문제이다. 하지만 그럼에도 대부분의 사람들이 NP complete는 polynomial time안에 풀 수 없다고 생각하고 있으며, 그 때문에 현재 우리가 사용 중인 모든 보안 알고리듬들은 이 NP completeness에 기반하여 만들어져있다. (정확히는 숫자를 곱하는 것은 쉬우나, 주어진 숫자가 소수인지아닌지 구분하는 것은 NP hard problem이라는 특성을 사용한다. - NP hard는 NP complete만큼 어렵거나 그보다 더 어려운 문제를 의미.) 따라서 일단 해당 문제를 reduction했더니 NP complete problem이 튀어나온다면 그 문제는 polynomial안에 답을 낼 수 없는 문제가 되겠다.</p>


<h5 id="60-4-aa">Approximation Algorithm</h5>


<p>그렇다고 이 문제는 NPC problem이니까 풀지말자! 라고 넘기기에는 세상에 너무나 많은 문제들이 NP complete problem이다. 그래서 정확하지는 않지만 NP complete problem을 풀기위한 여러가지 방법들이 존재한다. (<a href="http://en.wikipedia.org/wiki/NP-complete#Solving_NP-complete_problems" target="new">위키</a> 참고) 그 중에서도 approximation algorithm은 정확한 답을 구하는 것이 아니라 approximated solution을 polynomial안에 얻어내는 알고리듬을 의미한다. 즉, 원래 알고리듬이 x라는 답을 줬을 때, \(\alpha\)-approximation algorithm은 \(\alpha\)x 라는 답을 주게 된다. 자세한 점은 <a href="http://en.wikipedia.org/wiki/Approximation_algorithm" target="new">위키</a> 참고. 즉, NPC problem이 절망적으로 어려운 문제인 것은 맞지만 마냥 절망만 하고 있을 필요는 없다는 의미이며, approximation 말고도 NPC를 해결하는 방법은 여러 방법들이 존재한다. 하지만 일단 가장 중요한 개념 중 하나라고 할 수 있다.</p>


<p>수 많은 경우, Machine Learning 문제를 해결하다보면, 해당 문제가 NP complete problem인 경우가 많이 존재한다. 따라서 절대적인 값을 구하는 것이 불가능하여 어쩔 수 없이 local optimum을 구하는 경우도 있고, 혹은 해당 문제를 정확히 일치하지는 않지만 polynomial time안에 구할 수 있는 문제로 바꾸어 문제를 해결하는 방법도 존재한다. 즉, Machine learning에 대해 공부하기 위해서는 algorithm에 대한 이해가 매우매우 필수적이라고 할 수 있을 것이다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (3) Overfitting]]></title>
    <link href="http://SanghyukChun.github.io/59/"/>
    <updated>2014-08-03T16:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/59</id>
    <content type="html"><![CDATA[<p>Machine Learning Algorithm을 design하고 실제 구현을 하다보면 Overfitting이라는 문제가 꽤나 머리를 아프게 하는 경우가 많다. 아니, Algorithm 자체가 문제 없이 실행이 된다면 대부분의 경우 overfitting이 가장 큰 문제라고 단언해도 좋을 것 같다. 물론 좋은 Algorithm을 design하고 못하고의 문제는 또 다른 문제이기는 하다. 다음 글은 Algorithm에 대해서 써볼까.. 아무튼 이번 글에서는 Overfitting에 대해 좀 다뤄보도록 하겠다.</p>


<h5 id="59-1-overfitting">Overfitting</h5>


<p>overfitting이란 문자 그대로 너무 과도하게 데이터에 대해 모델을 learning을 한 경우를 의미한다. 지금 현재에 대해 잘 설명하면 되는 것 아닌가? 싶을 수 있지만, 우리가 사실 원하는 정보는 기존에 알고 있는 데이터에 대한 것들이 아니라 새롭게 우리가 알게되는 데이터에 대한 것들을 알고 싶은 것인데, 정작 새로운 데이터에 대해서는 하나도 못맞추고, 즉 제대로 설명할 수 없는 경우라면 그 시스템은 그야말로 무용지물이라고 할 수 있을 것이다. 조금 더 자세한 설명은 <a href="http://SanghyukChun.github.io/14#Overfitting" target="new">이전에 적은 포스트</a>로 대체하겠다.</p>


<h5 id="59-2-reg">Regularization</h5>


<p>이런 문제를 해결하기위해 여러가지 시도를 할 수 있다. 먼저 Overfitting이 일어나는 이유는 무엇인가에 대해서 한 번 생각해보자. 먼저 overfitting의 가장 간단한 예시를 하나 생각해보자.</p>


<p><img src="/images/post/59-1.png" width="500"></p>

<p>위 그림에서도 알 수 있 듯, 만약 우리가 주어진 데이터에 비해서 높은 complexity를 가지는 model을 learning하게 된다면 overfitting이 일어날 확률이 높다. 그렇다면 한 가지 가설을 세울 수 있는데, 'complexity가 높을 수록 별로 좋은 모델이 아니다.' 라는 가설이다. 이는 <a href="http://en.wikipedia.org/wiki/Occam%27s_razor" target="new">Occam's razor</a>, 오컴의 면도날이라 하여 문제의 solution은 간단하면 간단할수록 좋다라는 가설과 일맥상통하는 내용이다. 하지만 그렇다고해서 너무 complexity가 낮은 model을 사용한다면 역시 부정확한 결과를 얻게 될 것은 거의 자명해보인다. 그렇기 때문에 우리는 원래 cost function에 complexity와 관련된 penalty term을 추가하여, 어느 정도 '적당한' complexity를 찾을 수 있다. 이를 regularization이라 한다. 이 이외에도 다양한 설명이 있을 수 있기에 <a href="http://en.wikipedia.org/wiki/Regularization_(mathematics)" target="new">위키 링크</a>를 첨부한다.</p>


<p>그리고 이를 Bayesian 관점에서 설명을 할 수도 있다.</p>


<p>\[ p(Y|X) = {p(X|Y) p(Y) \over p(X)} \]</p>


<p><a href="58#58-1-Bayes" target="new">이전 글</a>에서도 설명했던 것 처럼, 만약 우리가 어떤 사전지식, 혹은 prior knowledge가 존재한다면 단순히 observation만 하는 것 보다는 훨씬 더 잘 할 수 있을 것이다. 그리고 다시 Overfitting 문제로 돌아와서, overfitting이 생기는 가장 큰 이유는 너무 지금 데이터, 즉 observation에만 충실했던 것이 그 원인이다. 그러면 당연히 prior가 있으면 이를 해결할 수 있지 않을까? 라는 질문을 던질 수 있을 것이다. 즉, 우리의 prior는 high complexity solution은 나오지 않을 것이다. 어느정도 complexity가 높아지는 것 까지는 용인하지만, (표현할 수 있는 영역이 더 넓어지니까) 하지만 너무 그 complexity가 높아지면 문제가 생길 수 있다. 즉, 그런 high complexity를 가지는 solution이 나올 확률 자체가 매우 낮다. 라는 prior가 있다면 이를 간단하게 해결 할 수 있을 것이며, 이것이 결국 앞에서 봤었던 penalty와 동일하다는 것을 알 수 있다.</p>


<h5 id="59-3-ms">Model Selection</h5>


<p>그런데 제 아무리 우리가 좋은 prior를 넣고, 좋은 penalty term을 design하더라도 만약 우리가 제대로 되지 않은 데이터들을 이용해 learning을 한다면 문제가 생길 수 있다. 마치 장님 코끼리 만지듯 전체 데이터는 엄청나게 많이 분포해있는데 우리가 가진 데이터가 아주 일부분에 대한 정보라면, 혹은 갑자기 그 상황에서 갑자기 노이즈가 팍 튀어서 데이터가 통채로 잘못 들어온다면? 아마 그런 데이터로 learning을 했다가는 sample bias가 일어나게 되어 크게 성능이 저하되게 될 것이다. 사실 위에 complexity라는 말도 결국에는 'Data point 대비 높은 complexity'가 더 정확한 말이다. 이를 최대한 피하기 위하여 우리는 <a href="http://en.wikipedia.org/wiki/Generalization" target="new">generalization</a> 이라는걸 하게 되는데, 다시 말해서 우리의 solution이 specific한 결과만을 주는 것이 아니라 general한 결과를 주도록 하려는 것이다. 이를 위한 여러 방법이 있을 수 있지만, 가장 많이 사용하는 방법은 validation set이라는 것을 사용하는 것이다. 즉, 모든 data를 전부 training에 사용하는 것이 아니라, 일부만 training에 사용하고 나머지를 일종의 validation을 하는 용도로 확인하는 것이다. 만약 우리의 모델이 꽤 괜찮은 모델이고, validation set이 잘 선택이 되었다면 training set에서만큼 validation set에서도 좋은 결과가 나올 수 있을 것이다.</p>


<p>보통 전체 데이터 중에서 training과 validation의 비율은 8:2로 하는 것이 일반적이다. 하지만 만약 validation set이 너무 작다면, 이 마저도 좋은 결과를 내기에는 부족할 수 있다. 이를 해결할 수 있는 컨셉 중에서 cross-validation이라는 컨셉이 있는데, validation set을 하나만 가지는 것이 아니라 여러개의 validation set을 정해놓고 각각의 set에 대해서 learning을 하는 것이다. 예를 들어 우리가 데이터가 X={1,2,3,4,5,6,7,8,9,10} 이 있을 때, 첫 번째 learning에서는 {1,..,8}을 사용해 learning하고 그 다음에는 {2,..,9}까지 learning하는 식으로 모든 permutation에 대해서 learning을 할 수 있을 것이다. 그리고 당연히 이런 방식으로 여러번 learning을 하게되면 그 때 얻어지는 model parameter는 그때그때 달라질텐데, cross-validation은 그 값들을 적당히 사용하여 가장 적절한 parameter를 얻어내는 방식이다. average로 해도 되고, median으로 해도 되고, 여러 방법이 있을 수 있다. cross-validation의 단점은 algorithm의 running time이 데이터의 크기 뿐 아니라 validation을 하기 위한 그 여러 set들에 dependent하다는 것이다. 그리고 데이터가 많아지면 그런 validation set이 엄청나게 많아진다. 정확히는 exponential로 늘어나기 때문에 마냥 모든 데이터에 대해 cross-validation을 하는 것은 불가능하다.</p>


<p>그렇기 때문에 실제로 cross-validation을 할때는 모든 데이터를 사용하지는 않고, 적당히 몇 개의 set을 골라서 여러 번 model parameter를 '적당히' 구하는 방법을 사용한다. 물론 이론적으로 AIC, BIC 등의 개념이 존재하여 이에 맞춰서 모델을 고르는 방법도 존재하지만 (AIC는 Akaike Information Criterion이고 BIC는 Bayesian Information Criterion으로, 둘 다 어떤 'information criteria'를 사용하느냐에 대한 내용이다.) 지금 내가 다루고자 하는 내용에서 좀 벗어나기 때문에 나중에 여유가 되면 이에 대한 글을 작성해보도록하겠다.</p>


<h5 id="59-4-cd">Curse of dimension</h5>


<p>그러나 이게 끝이 아니다. 우리가 싸워 이겨내야할 문제들은 complexity, number of data 뿐 아니라 dimension of data 역시 존재한다. 즉, 우리가 1차원의 데이터를 다루는 것과 10000차원의 데이터를 다루는 것과는 정말 어마어마한 차이가 존재한다는 것이다. 이렇게 차원이 높은 데이터를 다룰 일이 있을까? 하고 약간 막연하게 생각할 수 있지만, 가장 간단한 예로, 100px by 100px 그림은 각각의 픽셀이 하나의 차원이라고 했을 때 10000차원 벡터로 표현이 가능하다. 실제로 머신러닝 분야에서 이미지를 다룰 때는 이런 식으로 처리를 하게 된다. 이 이외에도 high dimensioanl space 상에 존재하는 데이터를 다룰 일은 매우 많이 존재한다.</p>


<p>그렇다면 이런 high dimensional data가 왜 우리가 learning한 system의 성능을 나쁘게 만들까? 정말 간단하게 생각해보자. 만약 우리가 주어진 공간을 regular cell로 나눴다고 가정해보자. 그리고 각각의 cell에 가장 많이 존재하는 class를 그 cell의 class로 생각하여 무조건 그 cell에 존재하는 데이터는 그 class라고 하는 logic을 생각해보자. 당연히 cell의 개수를 무한하게 가져가게 된다면, 그리고 데이터가 무한하다면 이 logic은 반드시 truth로 수렴하게 될 것이다. 이 알고리듬이 제대로 동작하려면 각각의 cell, 혹은 bin이 반드시 차있어야한다. 즉, 비어있는 empty cell이 존재해서는 안된다. 따라서 데이터는 아무리 적어도 cell의 개수만큼은 존재해야한다. 그런데 이렇게 cell을 만들게 될 경우 그 cell의 개수는 dimension이 증가함에 따라 exponential하게 늘어나게 되는 것이다. 예를 들어 우리가 1차원상에서 3개의 bin을 가지고 있다고 하면, 이는 2차원상에서는 9개, 3차원상에서는 27개.. 이렇게 exponentially grow하게 되는 것을 알 수 있다. 이 모델의 parameter들이 exponentially 증가하는 것이다. (아래 슬라이드(<a href="http://cssanalytics.wordpress.com/2013/10/06/random-subspace-optimization-rso/" target="new">출처</a>) 참고)</p>


<p><img src="/images/post/59-2.png" width="600"></p>

<p>따라서 당연히 각각의 bin이 비어있지 않도록 ensure해줄 수 있는 data의 개수 역시 exponentially 하게 늘어나게 되고, 즉 차원이 증가하게 되면 필요한 데이터가 exponentailly하게 늘어나게 된다는 것을 의미한다. 그러나 당연히 우리가 3차원 데이터보다 100000차원 데이터를 exponential하게 더 많이 가지고 있으리라는 법은 없고, 이로 인해 문제가 발생하게 되는 것이다.</p>


<p></p>

<p>그리고 또 문제가 되는 것은 high dimensional space에서 정의되는 metric들로, 특히 2-norm 혹은 euclidean distance의 경우는 그 왜곡이 매우 심하여, 실제 멀리 떨어진 데이터보다 별로 멀리 떨어져있지 않고 각 dimension의 방향으로 약간의 noise가 섞여있는 데이터에 더 큰 distance를 부여하는 등의 문제가 존재한다.</p>


<p>조금 다른 예를 들어보자. 만약 D-dimensional space에서 엄청나게 얇은 구각을 만들었다고 생각해보자. 여기에서 '구' 라는 것은 한 점에 대해 거리가 동일하게 떨어져있는 모든 점 내부의 영역을 의미한다는 것은 당연한 것이고.. 이 구의 부피는 \(V_D(r)=K_D r^D\) 가 될 것이며, \(K_D\)는 그냥 D에 대한 상수라고 생각하면 된다. 그럼 반지름이 1이고 두께가 \(\epsilon\)인 구각의 부피와 반지름이 1인 구의 부피의 비율은 \({V_D(1) - V_D(1-\epsilon) \over V_D(1)} = 1-(1-\epsilon)^D\) 가 될 것이다. 놀랍게도, 만약 very very high dimension D에서는 이 값이 1로 수렴하게 된다. 즉, 매우 얇은 구각의 부피가 구의 부피와 같다는 의미. 혹은 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 그 shell에 존재한다는 희한하고 요상한 의미가 된다.</p>


<p>즉, high dimension은 (1) model의 complexity도 증가시키며 (2) 필요한 데이터의 양도 exponentially 하게 늘어나게 하고 (3) 우리가 기존에 사용하던 metric이 제대로 동작하지 않는 그야말로 끔찍한 환경이라 할 수 있다. 그래서 이런 high dimensional space에서 일어나는 여러 문제점들을 통틀어 Curse of dimension이라 한다.</p>


<p>이를 해결하기 위해서는 결국 feature extraction 등의 기술을 사용하여 dimension을 가장 적절하게 낮추는 것이 바람직하다고 할 수 있다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (2) Probability Theory]]></title>
    <link href="http://SanghyukChun.github.io/58/"/>
    <updated>2014-08-03T14:18:00+09:00</updated>
    <id>http://SanghyukChun.github.io/58</id>
    <content type="html"><![CDATA[<p>지난 글에서 Machine Learning을 일종의 function 처럼 묘사했었는데, 사실 Machine Learning을 Probability density의 관점에서 보는 것도 가능하다. 이 얘기를 하려면 먼저 Probability density를 포함한 전반적인 probability theory에 대해 먼저 다뤄야할 것 같아서 그 주제로 먼저 글을 써보기로 했다. 일단 엄청 기본적인 내용들, 예를 들어서 조건부 확률이 무엇이고, 이런 얘기들은 일단 생략을 하도록 하겠다. 너무 basic한 얘기이고 \(p(X) = \sum_Y p(X,y) \) 이런거나 \(p(X,Y) = p(Y|X) p(X) \) 이런건 너무나도 기초적인 얘기들이니까. 만약 조건부확률을 잘 모른다면 구글링을 통해 먼저 간단한 지식을 습득하고 오기를 권한다.</p>


<p>다만 그 중에서도 약간 중요하게 다룰만한 주제가 있는데, 그게 바로 Bayes' theorem이다. 이 Theorem 자체는 그냥 간단하게 \(p(X|Y) \)와 \(p(Y|X) \)와의 관계를 표현한 것임에 불과하지만, 그 안에 숨겨진 의미가 매우매우 중요하다.</p>


<h5 id="58-1-Bayes">Bayes' Theorem</h5>


<p>\[ p(Y|X) = {p(X|Y) p(Y) \over p(X)} \]</p>


<p>식의 모양은 위와 같다. 이 식이 중요한 이유는 무엇이냐면, 실제 우리가 관측하는 데이터와 실제 일어나는 현상과의 관계를 이어주는 연결고리가 되기 때문이다.</p>


<p>우리가 실제로 관측할 수 있는 데이터와 현상의 관계는 어떻게 되느냐 하면 '이런 현상이 나타났을 때 데이터의 분포' 를 보는 것이다. 무슨 얘기냐, 만약 데이터를 X, 현상을 Y라고 해보자. 그럼 우리가 당연히 얻고 싶은 것은 주어진 데이터 X에 대한 현상 Y일 것이다. 즉 \( p(Y|X) \) 를 우리는 실제로 계산하고 싶은 것이다. 그런데 우리가 확인할 수 있는 데이터는 무엇이냐 하면, 어떤 주어진 현상 Y에 대해 존재하는 데이터 X들의 분포 즉, \( p(X|Y) \) 만을 관측할 수 있다. 예를 들어보자. 만약 어떤 질병에 대한 검사를 하는 상황이라고 가정해보자. 우리가 확인할 수 있는 것은 해당 검사에 대해 양성 판정을 받았는지 아닌지 밖에 확인할 수 없다. 만약 이 검사가 완벽하지 않다면 (즉, 정확도가 90% 정도라면) 실제 우리가 관측하는 검사 결과와 그 사람의 질병 보유 상황이 다를 수 있는 것이다. 즉, 검사 결과를 X라고 하고 실제 병에 걸렸는지 아닌지를 Y라고 한다면 우리가 최종적으로 확인하고 싶은 것은 \( p(Y|X) \), 즉 검사 결과를 보고 이 사람이 진짜 질병을 가질 확률을 알고 싶은 것이지만, 실제 우리가 확인할 수 있는 데이터는 \( p(X|Y) \), 즉 이 사람이 실제 병에 걸렸을 때 제대로 검사가 될 확률 (아까 90%라고 했었던) 뿐이 가지고 있지 않다. 더 자세한 것은 <a href="http://musicetc.wikidot.com/bayes-theorem#toc3" target="new">링크</a>를 참조하길 바란다.</p>


<p>중요한 것은 무엇이냐 하면, 진짜 우리가 관측할 수 있는 데이터만 가지고는 우리가 원하는 추론을 하는 것이 어렵다는 점이다. 이때, 주어진 현상에 대해 나타나는 데이터의 분포 \( p(X|Y) \), 즉 우리의 observation을 일컬어 Likelihood라고 한다. 만약 우리가 아무런 정보도 가지고 있는 상황이 아니라면 언제나 이 값을 최대로 만드는 작업을 통해 가장 optimized된 현상을 찾을 수 있는데 이를 maximum likelihood라 한다. (<a href="http://en.wikipedia.org/wiki/Maximum_likelihood" target="new">위키</a>) 즉, 우리가 관측한 정보만을 가지고, 그 정보가 전부라고 가정한 이후에 그 안에서 모든 optimization을 거쳐 가장 좋은 something을 얻어내는 과정이라 할 수 있다. 이를 Maximum Likelihood Estimation 혹은 MLE라 한다. 이 녀석은 Machine learning을 하면서 정말 많이 나오는 용어이고, 실제 이 방법을 사용해 풀어내는 문제들이 많기 때문에 반드시 숙지해야하는 개념이다.</p>


<p>그런데, 앞서 설명했던 예시와 같이, 항상 MLE가 능사는 아니다. 극단적으로 생각해서, 만약 우리가 동전 던지기를 해서 10번 던져서 10번 tail이 나오면 '이 동전은 tail이 100%로 나오는 동전이다' 라고 예측하는 것이 바로 MLE인 것이다. 이 방법이 잘 될 떄도 많지만, 방금처럼 데이터가 부족한 경우 등에는 좋은 결과를 얻지 못할 수 있다. 만약 우리가 '동전 던지기를 하면 head, tail이 50:50 으로 나온다.' 라는 것을 알고 있다면 조금 더 나은 추론을 하는 것이 가능하지 않을까? 이런 생각에서 나오는 것이 바로 Maximize a posterior, 혹은 MAP이다. Posterior는 앞에서 설명했던 주어진 데이터에 대한 현상의 확률, 즉 \( p(Y|X) \)이다. 간단히 생각해서 Likelihood는 내가 본 데이터에 대한 관측값 만을 의미하는 것이고, Posterior는 관측값과 다른 결과들을 조합하여 나온 조금 더 추론하기에 알맞은 형태? 라고 보면 될 것 같다.</p>


<p>앞서 Bayes' theorem에서 계산했듯, Observation, 혹은 Likelihood를 알고 있을 때 Posterior를 계산하기 위해서는 \(p(X), p(Y)\)가 필요하다. 이때 \(p(Y)\)는 어떤 현상에 대한 사전 정보이다. 즉, 아까 동전 던지기에서 동전을 던졌을 때 head tail이 나올 확률이 0.5라는 것에 대한 사전 정보이다. 이를 prior 라고 한다. 만약 이 값을 알고 있다면 observation이 잘못되어도 이 값이 약간의 보정을 해주는 역할을 할 수 있게 되는 것이다. 그리고 여기에서 데이터의 분포 \(p(X)\)는 일종의 normalization 을 해주는 역할을 하며, 실제 모든 데이터에 대해 \(p(X|Y) p(X)\) 를 계산한 뒤 그 값들을 noralization하는 것과 동일한 효과이기 때문에 이 값에 대해 알 필요는 없다.</p>


<p>정리하자면, Bayes' theorem은 observation(likelihood), 현상에 대한 사전정보 (prior), 주어진 데이터에 대한 현상의 확률 (posterior) 의 관계를 define하는 중요한 역할을 한다고 할 수 있겠다.</p>




<h5 id="58-2-pd">Probability densities</h5>


<p>Probability density, 우리 말로 하면 확률밀도가 되겠다. 간단히 생각하면 주어진 domain에 대해 확률이 어떻게 분포하고 있는지를 나타내는 일종의 function이라 할 수 있겠다. 아마 이것도 고등학교 수학시간에 배우는 것으로 기억하는데.. 그만큼 아주 간단한 개념이다. 자세한건 위키를 <a href="http://en.wikipedia.org/wiki/Probability_density_function" target="new">참고</a>하면 될 것 같다. 그럼 이게 실제로 어떤 의미가 있으며 맨 처음에 probability density 관점에서 machine learning을 볼 수 있다는 것의 의미는 무엇인가?</p>


<p>Probaiblity density라는 녀석은 결국 주어진 데이터들이 어떤 방식으로, 어떤 확률로 분포해있는지를 알려주는 녀석이라 할 수 있다. 예를 들어보자. 만약 우리가 스팸필터를 만들었는데 '광고' 라는 단어가 포함이 되면 해당 메일이 스팸일 확률이 80%이고, '구매' 라는 단어가 포함이 되면 90%, 'Machine Learning' 이라는 단어가 포함되면 스팸일 확률이 10%.. 이런식으로 모든 단어, 모든 domain에 대해 스팸일 확률을 알고 있다면, 혹은 그런 probability density를 찾을 수 있다면, 더 정확히 말하면 probability density function을 찾아낼 수 있다면 우리는 매우 좋은 inference를 할 수 있게 될 것이다.</p>


<p>이제 Machine Learning과의 연계를 지어보자. 어떤 우리가 모르는 probability density function을 가지는 데이터들에 대해, 그 데이터들을 사용해 probability density function을 찾아내는, estimate하는 과정을 일컬어 Density estimation이라 부른다. 이전에는 데이터들을 통해 '함수'를 찾아낸 것이고, 지금은 그 함수가 density function이라는 점이 다른 점이다.</p>




<p>Bayes' theorem 이나 probability density 등 이외에도 probability theory 쪽에서 언급해야할 얘기들이 없는 것은 아니다. 예를 들어서 expectation이라거나.. 하지만 내 생각에 이번 글에서 언급한 두 개는 약간 기본적인 probability theory와는 다르게 조금 머신러닝적인 insight가 필요한 부분이 아닐까해서 조금 강조해서 언급해보았다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (1) Machine Learning이란?]]></title>
    <link href="http://SanghyukChun.github.io/57/"/>
    <updated>2014-08-02T18:48:00+09:00</updated>
    <id>http://SanghyukChun.github.io/57</id>
    <content type="html"><![CDATA[<p>Machine learning이라는 것을 접한지 어느새 거의 1년 반이 넘는 시간이 지났다. 계속 여러 종류의 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning/" target="new">Machine learning과 관련된 글들</a>을 써왔지만, 항상 중구난방이고 제대로 정리가 되지 않은 느낌을 받아서 근래에 다시 머신러닝에 대해 공부를 하는 김에 싹 몰아서 정리해보기로 했다. 8월에 연구실에서 머신러닝 스터디를 하기로 했지만, 그것과는 조금 다르게 내가 생각했을 때 이런 순서로 정리를 하면 되겠다.. 하는 느낌으로 정리를 해볼 생각이다. Bishop 책을 많이 참고했으면 하고.. 특히 이번 ICML에서 느낀거지만, Machine Learning에는 정말 많은 분야가 있는데, 많은 Lecture들에서 어쩔 수 없이 그것들을 전부 커버하지 못하는 점이 너무 아쉬웠다. 그래서 일단 내가 할 수 있는데까지는 정리해보는게 좋지 않을까.. 하는 생각으로 글을 써보려 한다. 이런 글을 써야겠다는 생각은 오래전부터 했는데 정작 실천에 옮기는데에는 시간이 많이 걸렸네.. 그래 무튼 시작해보자.</p>


<p>Machine Learning... 혹은 우리 말로 하면 기계 학습. 이놈이 대체 뭐길래 너도나도 머신러닝 머신러닝 해대는 걸까. 내가 예전 <a href="http://SanghyukChun.github.io/3" target="new">Andrew Ng. 교수의 Machine Learning Lecture를 들으며 정리했던 글</a>에 정의했던바에 따르면, </p>


<blockquote>
    <p>머신러닝은 그 관계를 알 수 없는 수 많은 데이터들 사이에서 관계를 찾아내주는 마법과 같은 기술이다.</p>
</blockquote>


<p>라고 정의했었다. 마법과 같은 기술이라! 어디서 저런 중2병 스멜나는 용어를 가져왔는지 참.. 아무튼 내가 이쪽을 더 공부해보면서 느낀건, 결국에 이 Machine Learning이라는 것은 일종의 Modeling Problem이라는 것이다. 무슨 얘기냐? 머신러닝이란 <b>주어진 데이터</b>에 대해 <b>현상</b>을 <b>가장 잘 설명할 수 있는 관계</b>를 찾아내는 것이라 했었는데, 이 말을 조금 수학적으로 풀어내면 주어진 <b>데이터</b> \(X = (x_1, x_2, x_3, \ldots, x_n)\) 이 있을 때 이 데이터와 실제 <b>현상</b> \(Y = (y_1, y_2, \ldots, y_n) \)에 대한 <b>관계</b> function \(f\)를 찾아야한다. 당연히 우리가 정확한 함수 \(f\)를 찾아낼 수는 없으므로 <b>최대한 잘 설명할 수 있는</b>, 함수 \(f'\)을 찾아내야 한다. 이를 Hypothesis라고 한다. 즉, 이 모든 과정을 일종의 Modeling process로 생각이 가능하다는 뜻이다. 다만 우리의 새로운 process는 for given data에 대해 dependent한 model을 만들어낸다는 점이 독특한 것이다.</p>


<p>스팸필터를 생각해보자. 데이터 X는 메일들이다. 우리가 받는 메일 하나하나가 데이터가 될 수도 있고, 그 메일 안에 있는 단어 하나하나가 될 수도, 혹은 아예 알파벳 하나하나가 될 수도 있다. 그것은 우리가 정하기 나름이니까. 그럼 현상 Y는 무엇일까? 각각의 메일이 스팸인지, 아니면 일반 메일인지 구분하는 구분자, indicator, 혹은 Label, Class가 될 것이다. 마지막으로 가장 잘 설명할 수 있는 함수는 이런 데이터들을 가지고 어떻게 Learning을 할 것인지에 대한 얘기가 될 것이다. 예를 들어 Naïve Bayes라던가, KNN, SVM도 있고, 요새 핫한 Deep learning도 있을 수 있다.</p>


<p>그러면 이 함수로 우리는 무엇을 하고 싶은걸까? 단순히 데이터와 현상의 관계를 함수로 나타내는 것이 무슨 의미가 있지? 이것이 의미가 있는 이유는 우리가 새로 주어진 데이터, 혹은 test data에 대해 새로운 추론, inference를 하는 것이 가능하기 때문이다. 이 메일은 스팸인가? 이런 쇼핑 목록을 가진 사람은 또 뭘 사고 싶어할까? 이런 날씨에는 고속도로가 막힐까 막히지 않을까? 이런 질문들에 대해 앞서 우리가 정의한 방식대로 Machine Learning problem을 풀어 새로운 Hypothesis를 통해 우리는 inference를 내릴 수가 있다. 이런 inference가 맞을 수도 있고 틀릴 수도 있다. 그러나 우리는 최대한 좋은 방법으로 Learning을 한다면 좋은 inference를 할 수 있을 것이라고 생각할 수 있다. 이 '좋은 방법' 을 위해 도입되는 개념이 바로 cost function이다. 내가 원하는 결과와 실제 내가 찾아낸 가설과의 괴리가 얼마나 있는지를 function으로 정의하는 것이다. 가장 간단한 예로, 스팸 필터에서 '스팸이다 아니다'라는 질문이 틀리면 cost function은 1, 맞으면 0이라고 하자. 그렇게 정의하면 이 cost를 최소화 하는 방향으로 model을 learning하면 결국 가장 잘 맞추는 model을 얻을 수 있겠지.</p>


<p>즉, 머신러닝이란, 완전히 raw한 데이터를 다루는 과정, 예를 들면 Dimensionality reduction, Metric Learning 등의 과정에서부터 시작해서, 어떻게 Learning할 것이냐, model이 무엇이냐, 조금 더 구체적으로 얘기하면 어떤 Algorithm을 사용해 이 model을 learning할 것이냐라는 문제를 풀어내고, 마지막으로 어떻게 결정을 내릴 것이냐, decision making을 할 것이냐 하는 이 여러개의 과정으로 나뉘어지는 것이다. 내가 관심있는 부분은 주로 어떤 model을 사용할 것이냐, 또 어떤 방법으로, 어떤 algorithm으로 이 model을 learning할 것이냐에 관심이 있다. 또 추가로 관심이 있는 부분이라면, 어떤 문제가 있을 때 그 문제를 Machine Learning의 관점에서 풀어내는 것도 좋아하고.. 예를 들면 추천 알고리듬 같은게 있겠다.</p>


<p>사실 이렇게 머신러닝을 정의하기에는 조금 어정쩡한 부분이 있는 것이 사실이다. 그럼 Unsupervised Learning은?? Reinforcement Learning은? 그 이외에도 많은 것들을 이 정의만 가지고 설명하기에는 사실 무리가 있는 것이 맞지만, 그래도 이런 컨셉으로 이해를 하게 된다면, 즉 Machine Learning이 근본적으로 model, algorithm 문제라는 것을 이해하게 된다면 이 녀석들에 대한 이해도 빠르게 할 수 있지 않을까 생각된다. 위에 링크 건 <a href="http://SanghyukChun.github.io/3" target="new">예전 글</a>에 이와 관련된 글들이 있으니 궁금하면 읽어보면 될 것 같다.마찬가지 이유로, 요즘 유행하는 Big Data 역시 근본적으로는 Machine Learning의 일부라는 것을 알 수 있다. <a href="http://SanghyukChun.github.io/21" target="new">이에 대한 글</a>도 이전에 적은 적이 있으니 참고바란다. 달라지는 점이라면 algorithm이 굉장히 빨라야 한다하거나, Memory efficient 해야한다거나, 아니면 분산처리가 가능해야한다거나 하는 새로운 challenge들이 존재하기는 하지만, 근본적으로는 같다고 봐도 무방하다. 즉, 이 머신러닝이라는 것을 제대로 이해하고 다룰 수 있다면 정말 많은 문제들을 해결할 수 있다는 의미가 될 것 이다.</p>




<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2014 ICML 후기]]></title>
    <link href="http://SanghyukChun.github.io/55/"/>
    <updated>2014-06-26T16:16:00+09:00</updated>
    <id>http://SanghyukChun.github.io/55</id>
    <content type="html"><![CDATA[<p>이번에 <a href="icml.cc/2014/" target="new">ICML 2014</a>를 다녀왔다. 내 첫 해외 학회이기도 했고, 처음으로 Machine Learning과 관련된 연구를 하는 사람들의 talk을 듣고, 그 사람들이 직접 하는 일들을 많이 볼 수 있어서 개인적으로 많이 고무된 상태로 학회에 참여했었다. 첫 학회를 다녀온 기념으로 학회에서 내가 느낀 점들을 간단하게 정리해보고자 한다. 대충 보자면 아래 리스트 정도가 될 것 같다.</p>


<ul>
<li>항상 열정을 가지고 있고 열심인 사람들</li>
<li>어떤 식으로 연구를 해야겠다라는 생각</li>
<li>Machine Learning이라는 학문의 방대함</li>
<li>Neural Network와 Deep Learnig의 강세</li>
<li>Real Industry와 Machine Learning</li>
</ul>


<p>먼저 사람들. 개인적으로 내 자신에게 살짝 아쉬운 점이라면 학회에서 만난 사람들에게 먼저 다가가서 이야기를 걸거나 할 베짱이 많이 없었다는 것이다. 사실 내가 그렇게 사람들에게 먼저 다가가고 얘기를 나누기에는 내가 알고 있는 지식이 많이 부족함에도 원인이 있기는 하지만 그래도 다들 같은 분야에 관심을 가지고 (세부 관심사는 조금씩 다를 수 있지만) 나와 비슷한 입장을 가진 사람들도 있을 수 있었을텐데 내가 조금 적극적이지 못했던 부분이 있다. 학기 중에 윤준보교수님 수업에서 학회를 가면 그 사람들과 이야기를 많이 나누어보라는 얘기를 해주셨는데 막상 내게 그런 기회가 생기니 할 수 있는 말이 많이 없더라. 조금 더 정진해서 그런 대화에 두려움이 없을 정도의 지식을 쌓아야할텐데. 그런 개인적인 아쉬움은 잠시 넘겨서 생각을 해보자면, ICML에서 만난 사람들은 정말 열정있는 사람들이었다. 고작 15분에서 20분짜리 talk조차 이해하지 못하고 허덕이고 있는 나와는 다르게, 정말 발표자의 talk을 이해하고 그들과 communication하면서 질문하는 모습이 멋져보였다. 1시간 가까이 되는 invited talk이나 key note talk에서도 많은 부분을 놓치지 않으려 노력하는 모습도 멋졌다고 생각한다. 나도 그런 멋지다고 생각한 모습에 한 단계 더 다가가야할텐데.</p>


<p>그리고 그 다음은 역시 연구였다. 내가 대학원생이 되었고, 연구가 나에게 가장 중요한 비중을 차지하게 된 이상, 어떤 연구를 할 것이며 어떻게 연구를 할 것이며.. 혹은 왜 연구를 해야하는 것이냐 등의 물음은 나에게 굉장히 중요한 물음이다. 약간 어느 정도는 간접적으로 그 질문들에 대한 답을 얻을 수 있었던 것 같은데, 어떤 공명감으로 연구를 한다는 느낌보다는 이 문제를 풀어야하겠다는 그런 근본적인 호기심? 같은게 영향을 미치는게 아닌가 싶다. 사실 명확하게 이거다! 싶은 느낌은 잘 들지 않았지만 앞서 말했듯이 다들 열정적으로 임하고 질문 하나하나가 날카롭게 들어가는 모습을 보면서 이런 학회에 publish를 하는 사람들은 어떤 생각으로 다른 사람들의 talk을 듣는지 조금이나마 간접적으로라도 체험할 수 있지 않았나 싶다. 아무튼 진짜 열정적으로 해야한다. 그게 진짜 큰 것 같다.</p>


<p>또 학회에서 놀라웠던 점이라면 Machine Learning이라는 분야 자체가 생각보다도 훨씬 더 방대했다는 점이다. 전체 Track이 6개가 parallel 하게 돌아가면서 전체 다 합쳐서 거의 300개 가까이 되는 talk이 진행이 됐으니까.. (<a href="http://icml.cc/2014/index/article/12.htm" target="new">스케쥴</a>) 단순히 accept된 paper만 많은 것이 아니라 각 track의 주제 또한 너무나도 다양하였다. Networks and Graph-Based Learning, Reinforcement Learning, Bayesian Optimization and Gaussian Processes, Supervised Learning, Neural Networks and Deep Learning, Graphical Models, Bandits, Monte Carlo, Statistical Methods, Structured Prediction, Deep Learning and Vision, Matrix Completion and Graphs, Learning Theory, Clustering and Nonparametrics, Active Learning, Optimization, Large-Scale Learning, Latent Variable Models, Online Learning and Planning, Clustering, Metric Learning and Feature Selection, Optimization, Neural Language and Speech, Graphical Models and Approximate Inference, Online Learning, Monte Carlo and Approximate Inference, Method-Of-Moments and Spectral Methods, Boosting and Ensemble Methods, Matrix Factorization, Nonparametric Bayes, Manifolds, Kernel Methods, Unsupervised Learning and Detection, Crowd-Sourcing, Manifolds and Graphs, Regularization and Lasso, Nearest-Neighbors and Large-Scale Learning, Topic Models, Sparsity, Neural Theory and Spectral Methods, Features and Feature Selection, Time Series and Sequences.... 와 진짜 많다. 물론 이 전체를 또 잘 묶으면 더 줄어들 수 있겠지만 그래도 일단 각각의 Track들이 서로 다른 주제를 가지고 이렇게 많이 있다는 사실이 놀라웠다. 글쎄, 그래도 굳이 크게 나누자면, (1) Learning for Graphical Model (2) Traditional Machine Learning Problems (Bayesian, Supervised Learning...), (3) Optimization (4) Monte Carlo (5) Unsupervised Learning (Clustering, Metric Learning...) (6) Neural Network (7) Others 정도가 아닐까. 모르겠다 너무 많고 내가 모르는 분야가 너무 많아서. 아무튼 정말 Machine Learning이 어마어마하게 큰 분야라는 것을 다시 한 번 느끼게 되었다. 나는 저 많은 Track 중에서 어느 분야에 기여를 할 수 있을까?</p>


<p>꼭 그런건 아니었지만, 전반적으로 Deep learning 과 관련된 talk들. 심지어 'Deep' 이라는 이름이 들어가기만 해도 컨퍼런스 룸이 터질듯한 것을 볼 수 있었다. 정말 요즘 이게 핫하긴 핫하다. 근데 난 이상하게 정말 Deep learning이 싫은데.. 이유를 잘 모르겠다. 가장 practical하게 powerful해서 그렇겠지? 중국에서 해서 그런지는 모르겠지만 Deep learning세션은 중국인들이 바글바글 몰려서 진짜 산만했었다. 그만큼 가장 핫하다는 뜻이고, 중국인들이 이런 실용적인 것들에 무지 관심이 많다는 것을 느꼈다. 이론쪽보다는 확실히 그런 practical 한 세션에 중국인들이 압도적으로 많았다. Deep learning 관련 시스템 쪽도 사람 엄청 많았고.. 진짜 그야말로 Deep Learning의 시대라고 봐도 무방할 정도. 대단하더라.</p>


<p>마찬가지 맥락에서, 굉장히 많은 기업들이 ICML을 찾았다. 구글, Facebook, 아마존, MS, 야후 같은 글로벌 기업은 물론이고 바이두, 알리바바 같은 중국 기업들도 엄청 많았다. 그만큼 머신러닝을 전공한 사람들의 힘이 필요하다는, 그런 사람들에 대한 수요가 확실하구나.. 라는 그런 생각이 들더라. 기업 연구소 특히 MS나 구글 연구소 등에서도 많은 논문들이 나오는걸 보고, 저런 연구소에서 일하는 것도 생각보다는 나쁘지 않을 수도 있다는 그런 생각도 들고 그랬다.</p>


<p>ICML에서 여러모로 많은 자극을 받았다. 재미도 있었고. 내년 ICML은 내가 intivation 되서 갔으면 좋겠다! 나도 좋은 논문을 쓸 수 있었으면.</p>

]]></content>
  </entry>
  
</feed>
