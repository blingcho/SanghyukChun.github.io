<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-Learning | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2015-03-25T02:12:38+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (8) Classification Introduction (Decision Tree, Naïve Bayes, KNN)]]></title>
    <link href="http://SanghyukChun.github.io/64/"/>
    <updated>2015-03-25T02:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/64</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="57-4-ClassML">첫 번째 글</a>에서 설명했던 것 처럼 Machine Learning은 크게 Supervised Learning, Unsupervised Learning 그리고 Reinforcement Learning으로 구분된다. 이 글에서는 그 중 Supervised Learning의 가장 대표적인 예시인 Classification에 대해 다룰 것이며 가장 대표적이고 간단한 세 가지 알고리즘에 대해서 역시 다룰 것이다.</p>


<h5>What is Classification?</h5>


<p><a href="http://en.wikipedia.org/wiki/Statistical_classification">Classification</a>은 <a href="http://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a>의 일종으로, 기존에 존재하는 데이터와 category와의 관계를 learning하여 새로 관측된 데이터의 category를 판별하는 문제이다. 스팸 필터를 예로 들어들어보자. 스팸 필터의 데이터는 이메일이고, category, 혹은 label, class는 spam메일인지 일반 메일인지를 판별하는 것이 될 것이다. 스팸필터는 먼저 스팸 메일, 그리고 일반 메일을 learning을 한 이후, 새로운 데이터 (혹은 메일)이 input으로 들어왔을 때 해당 메일이 스팸인지 일반 메일인지 판별하는 문제를 풀어야하며, 이런 문제를 classification이라고 한다.</p>


<p><a href="57">첫 번째 글</a>에서 머신러닝 문제는 먼저 주어진 데이터에 대한 가정을 하고, 해당 가정을 만족하는 best hypothesis를 찾는 문제라고 언급한 적이 있다. Classification 문제 역시 Machine Learning 문제이므로 데이터에 대한 가정을 먼저 해야하고, best hypothesis를 찾는 과정을 거친다. 따라서 각각의 서로 다른 classification algorithm들은 서로 다른 assumption을 가지고 있으며, 해당 assumption을 가장 잘 만족하는 function parameter를 계산하는 과정이다. 앞으로 설명하게 될 알고리즘들에 대한 설명을 읽을 때 데이터에 대해 어떤 가정을 하였는지 꼼꼼히 확인하면서 읽으면 알고리즘을 이해하기 한결 수월할 것이다.</p>


<h5>Decision Tree</h5>


<p><a href="http://en.wikipedia.org/wiki/Decision_tree">Decision Tree</a>는 가장 단순한 classifier 중 하나이다. 이 Decision Tree의 구조는 매우 단순하다.</p>


<p><img src="/images/post/64-1.png" width="600"></p>

<p>위의 그림은 오늘 외출을 할까 말까를 결정하는 decision tree이다. 이렇듯 decision tree는 tree구조를 가지고 있으며, root에서부터 적절한 node를 선택하면서 진행하다가 최종 결정을 내리게 되는 model이다. Decision tree의 가장 좋은 점은 단순하다는 점이다. 누구나 쉽게 이해할 수 있고, 그렇기 때문에 쉽게 디버깅할 수 있다. 예를 들어 위의 예시에서 습도가 높아도 나갈만하다는 생각이 든다면 맨 왼쪽의 No를 Yes로 바꾸기만 하면 간단하게 로직을 바꿀 수 있다. 그러나 다른 모델들은 그런 점들이 비교적 어렵다. Machine Learning에서 말하는 decision tree는 <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">decision tree learning</a>으로, 일일이 node마다 로직을 사람이 써넣어 만드는 것을 의미하는게 아니라, node 개수, depth, 각각의 node에서 내려야하는 결정 등을 데이터를 통해 learning하는 algorithm들을 사용해 만든 decision tree를 의미한다.</p>


<p>많이 쓰이는 알고리즘들로는 <a href="http://en.wikipedia.org/wiki/ID3_algorithm">ID3</a>, <a href="http://en.wikipedia.org/wiki/C4.5_algorithm">C4.5</a>, <a href="http://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees">CART</a>, <a href="http://en.wikipedia.org/wiki/CHAID">CHAID</a>, <a href="http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines">MARS</a> 등이 있으며, 보통 C4.5를 가장 많이 사용한다.</p>


<p>C4.5는 ID3의 몇 가지 문제점들을 개선한 알고리즘으로, 그 기본 개념은 ID3와 크게 다르지 않다. 추후 이 글 혹은 다른 글에 ID3 알고리즘에 대한 내용을 추가하도록 하겠다.</p>


<h5>Regression Tree and Ensemble method</h5>


<p>Decision tree는 output value가 반드시 binary여야한다는 제약조건이 있기 때문에 스팸 필터 등에서는 사용할 수 있지만, 실제 모든 데이터가 binary만을 output으로 가지지 않으므로 모든 데이터에 사용하려면 변형이 필요하다. <a href="http://en.wikipedia.org/wiki/Decision_tree_learning#Types">Regression tree</a>는 binary가 아니라 real value를 output으로 가지는 모델로, learning하는 방법은 크게 다르지 않다고 한다.</p>


<p>가끔은 하나의 decision tree를 사용하는 것이 아니라 한 번에 여러 개의 decision tree들을 만들어서 각각의 decision tree들이 내리는 결정을 종합적으로 판단하여 (ensemble) 결정을 내리기도 한다. <a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">Bagging decision tree</a>, <a href="http://en.wikipedia.org/wiki/Random_forest">random forest</a>등이 이에 속한다. 이런 식으로 여러 개의 classifier를 사용해 decision을 내리는 방법을 ensemble method라고 하는데, industry에서는 machine learning algorithm의 성능을 높이기 위해서 여러 개의 알고리즘들을 ensemble method를 사용하여 한 번에 같이 사용하기도 한다. 대표적인 예로 <a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a> 등이 있다.</p>


<p>Ensemble method에 대해서는 나중에 따로 다시 설명할 예정이므로 그 글을 참고하면 좋을 것 같다. (링크는 추후 추가 예정)</p>


<h5>Naïve Bayes</h5>


<p><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naïve Bayes Classifier</a>는 <a href="58-1-Bayes">Bayesian rule</a>에 근거한 classifier이다. Naïve Bayes는 일종의 확률 모델로, 약간의 가정을 통해 문제를 간단하게 푸는 방법을 제안한다. 만약 데이터의 feature가 3개 있고, 각각이 binary라고 해보자. 예를 들어 남자인지 여자인지, 성인인지 아닌지, 키가 큰지 작인지 등의 feature를 사용해 사람을 구분해야한다고 생각해보자. 이 경우 적어도 8개의 데이터는 있어야 모든 경우의 수를 설명할 수 있게 된다. 그런데 보통 데이터를 설명하는 feature의 개수는 이보다 훨씬 많은 경우가 많다. 예를 들어 feature가 10개 정도 있고 각각이 binary라면, 제대로 모든 데이터를 설명하기 위해서는 \(2^{10}\), 약 1000개 이상의 데이터가 필요하다. 즉, 필요한 데이터의 개수가 feature 혹은 데이터의 dimension에 exponential하다. 이런 경우 그냥 Bayes rule을 사용해 분류를 하게 되면 overfitting이 되거나 데이터 자체가 부족해 제대로 된 classification을 하기 어려울 수 있다. Naïve bayes는 이런 문제를 해결하기 위해 새로운 가정을 하나 하게 된다. 바로 모든 feature들이 i.i.d.하다는 것이다. i.i.d는 independent and identically distributed의 준말로, 모든 feature들이 서로 independent하며, 같은 분포를 가진다는 의미이다. 당연히 실제로는 feature들이 서로 긴밀하게 관련되어있고 다른 분포를 가질 것이므로 이 가정은 틀린 가정이 될 수 있다. 그러나 만약 모든 feature가 i.i.d.하다고 가정하게 된다면 우리가 필요한 최소한의 데이터 개수는 feature의 개수에 exponential하게 필요한게 아니라 linear하게 필요하게 된다. 간단한 가정으로 모델의 complexity를 크게 줄일 수 있는 것이다. 때문에 Naïve Bayes 뿐 아니라 많은 모델에서 실제 데이터가 그런 분포를 보이지 않더라도 그 데이터의 분포를 특정한 형태로 가정하여 문제를 간단하게 만드는 기술을 사용한다.</p>


<p>조금 더 엄밀하게 수식을 사용해 설명을 해보자. 우리가 가지고 있는 input data 를 \(x = (x_1, \ldots, x_n)\)이라고 가정해보자. 즉 우리는 총 \(n\)개의 feature를 가지고 있다고 가정해보자 (보통 \(n\)은 데이터의 개수를 의미하지만, wikipedia의 notation을 따라가기 위하여 이 글에서도 dimension을 나타내기 위해 \(n\)을 사용하였다). 그리고 Class의 개수는 \(k\)라고 해보자. 우리의 목표는 \(p(C|x_1, \ldots, x_n) = p(C|x)\)를 구하는 것이다. 즉, 1부터 \(k\)까지의 class 중에서 가장 확률이 높은 class를 찾아내어 이를 사용해 classification을 하겠다는 것이다. Bayes rule을 알고 있으므로 이 식을 bayes rule을 사용해 전개하는 것은 간단하다.</p>


<p>$$ p(C|x) = \frac{p(C) p(x|C)}{p(x)} $$</p>


<p>이 때 분모에 있는 데이터의 확률은 normalize term이기 때문에 모든 값을 계산하고 나서 한 번에 계산하면 되므로 우리는 \(p(x,C) = p(C) p(x|C)\), 다시 말해 prior와 likelihood를 계산해야만한다. 그러나 이 값은 joint probability이므로 데이터에서부터 이 값을 알아내기 위해서는 '엄청나게 많은' 데이터가 필요하다. 구체적으로는 앞서 말한 것 처럼 dimension에 exponential하게 많은 데이터 개수를 필요로 한다. 그러나 만약 우리가 x가 모두 indepent하다고 가정한다면 간단하게 다음과 같은 식으로 나타낼 수 있다.</p>


<p>$$ p(C) p(x_1, \ldots, x_n | C) = p(C) p(x_1|C) p(x_2|C) \ldots = p(C) \prod p(x_i|C)$$</p>


<p>따라서 normalize term을 \(Z\)로 표현한다면, 우리가 구하고자 하는 최종 posterior는 \(p(C|x) = \frac{1}{Z} p(C)\prod p(x_i|C)\)로 나타낼 수 있게 된다.</p>


<h5>KNN</h5>


<p><a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K-Nearest Neighbors algorithm (KNN)</a>은 구현하기 어렵지 않으면서도 비교적 나쁘지 않은 성능을 내는 Classification Algorithm 중 하나이다. KNN은 가까운 데이터는 같은 label일 가능성이 크다고 가정하고 새로운 데이터가 들어오면, 그 데이터와 가장 가까운 k개의 데이터를 training set에서 뽑는다. 뽑은 k개의 데이터들의 label을 관측하고 그 중 가장 많은 label을 새로운 데이터의 label로 assign하는 알고리즘이다 (이런 방식을 majority voting이라고 한다). 이때 '가까움'은 Euclidean distance로 측정해도 되고, 다른 metric이나 measure를 사용해도 된다. 이때 distance 혹은 similarity를 측정하기 위해서 반드시 metric을 사용해야하는 것은 아니다. 즉, metric의 세 가지 성질을 만족하지 않는 measure일지라도 두 데이터가 얼마나 '비슷하냐'를 measure할 수 있는 measure라면 KNN에 적용할 수 있다. 아래 그림은 KNN이 어떻게 동작하는지 알 수 있는 간단한 예시이다. 아래 그림을 보면 k를 3으로 골랐을 때 초록색 데이터의 label은 빨강이 되고, k를 5로 골랐을 때는 파란색이 됨을 알 수 있다.</p>


<p><img src="/images/post/64-2.png" width="500"></p>

<p>KNN은 구현하기에도 매우 간단하고 (새로 들어온 점과 나머지 점들간의 distance를 측정한 후 sorting하기만 하면 된다) 성능도 보통 크게 나쁘지 않은 값을 보이기 때문에 간단하게 개발할 필요가 있는 경우에 많이 사용하게 된다. 사실 대부분의 머신러닝 툴박스들은 KNN의 다양한 variation까지 built-in function으로 지원한다. Matlab의 knnclassify가 대표적인 예. 또한 KNN은 <a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Properties">유용한 성질들</a>이 많이 있다. 예를 들어 만약 데이터의 개수가 거의 무한하게 있다면, KNN classifier의 error는 bayes error rate의 두 배로 bound가 된다는 특성이 있다. 즉, 데이터가 엄청나게 많다면, KNN은 상당히 좋은 error bound를 가지게 된다는 것이다. 즉, 단순한 휴리스틱 알고리즘이 아니라 엄밀하게 수학적으로 우수한 알고리즘임을 증명할 수 있는 알고리즘이라는 뜻이다. 또한 distance를 마음대로 바꿀 수 있기 때문에 KNN은 변형하기에도 간단한 편이므로 데이터에 대한 가정을 모델에 반영하여 변형하기에 간편하다는 장점이 있다.</p>


<h5>정리</h5>


<p>가장 간단하게 적용할 수 있는 세 가지 classification algorithm에 대해 훑어보았다. 개인적으로 KNN은 정말 직관적일뿐 아니라 잘 동작하는 알고리즘이기 때문에, 개인적으로 어떤 문제를 해결해야할 때 가장 먼저 이 데이터가 어느 정도 잘 분류되는지 테스트하는 용도로 애용한다. 중요한 점은, 각각의 classification algorithm이 풀려고 하는 '문제' 혹은 model은 서로 다른 가정을 가지고 있으며, 그 가정에 따라 문제를 푸는 방법이 아주 많이 바뀐다는 것이다. 즉, 어떤 새로운 classification algorithm을 만들어야 할 때는 (이는 classification 뿐 아니라 모든 알고리즘을 만들어야 할 때도 마찬가지인데) 먼저 어떤 문제를 풀어야하는지 문제를 정의해야하며, 문제를 정의하기 위해 어떤 모델을 가정해야한다는 것이다. 예를 들어 Naïve Bayes는 데이터의 각각의 feature들이 서로 i.i.d하다는 가정을 하고 있고, KNN은 '가까운' 데이터와 내가 같은 label을 가지고 있을 확률이 높다는 가정을 하고 있다. 이렇듯 Machine learning algorithm을 개발하는 일에서 가장 중요한 것은 좋은 문제를 먼저 정의하는 것에서부터 시작하는 것이 아닐까 생각한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 3월 25일: 글 등록</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li>Hidden Markov Model</li>
<li>Clustering (K-means, Gaussian Mixture Model)</li>
<li>EM algorithm</li>
<li>Dimensionality Reduction (LDA, PCA)</li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li>Neural Network Introduction</li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (7) Convex Optimization]]></title>
    <link href="http://SanghyukChun.github.io/63/"/>
    <updated>2014-09-10T06:28:00+09:00</updated>
    <id>http://SanghyukChun.github.io/63</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>Machine learning 문제를 풀다보면 Objective function을 만들고 그 objective function을 optimize 해야하는 경우가 매우 빈번하게 발생한다. 간단히 생각해서 loss function을 minimize하는 것도 optimization이다. 그렇다면 그런 optimization은 도대체 어떻게 해야하는 것일까. 여러가지 방법이 있겠지만 이번 글에서는 간단한 optimization이라는 것에 대한 컨셉을 다루고, 그 중 특수 케이스인 convex optimization에 대해 다루도록 하겠다.</p>


<h5>Optimization</h5>


<p>대부분의 Optimization은 아래와 같은 식으로 표현할 수 있을 것이다.</p>


<p>$$ \min f(x) \text{ s.t. } g(x) = c $$</p>


<p>여기에서 \(f(x), g(x)\)는 함수이다. 어떤 함수의 optimum point, 즉 그것이 최소이거나 혹은 최대인 지점을 찾는 과정을 optimization이라고 한다고 생각하면 간단할 것이다. 엄청 간단하게 생각해보면 \(f(x)\)는 loss function이고, \(g(x)\)는 일종의 constriants로 생각하면 될 것 같다. Machine learning 문제를 풀다보면 이렇게 optimization을 해야하는 일이 아주 빈번하게 발생하는데, 안타깝게도 항상 이런 function들의 optimum point를 찾을 수 있는 것은 아니다. 가장 간단하게 생각했을 때 이런 point를 찾는 방법은 미분을 하고 그 값이 0이 되는 지점을 찾는 것인데, 안타깝게도 미분 자체가 되지 않는 함수가 존재할 수도 있으며, 미분값이 0이라고 해서 반드시 극점인 것은 아니기 때문이다. (saddle point를 생각해보자.) 따라서 대부분의 경우에 이런 방법으로 극점을 구하는 것은 불가능하며, 매우매우 특수한 일부 경우에 대해서 완전한 optimum을 찾는 것이 알려져 있다. 그리고 그 경우가 바로 convex optimization이다.</p>


<h5>Convex function</h5>


<p>Convex function은 convex한 function을 의미한다. 이때 convex는 한국어로 옮기면 볼록에 가까운데, convex function은 볼록 함수라고 번역할 수 있다. Convex가 무슨 의미인지 차근차근 알아보자. 먼저 <a href="http://en.wikipedia.org/wiki/Convex_set">convex set</a>에 대해 알아보자. 어떤 set이 convex하다는 것의 의미는 그 set에 존재하는 그 어떤 점을 잡아도 그 점들 사이에 있는 모든 점들 역시 그 set에 포함되는 set을 알컬어 convex set이라고 부른다. 따라서 convex한 set을 그림으로 그리게 되면 움푹하게 파인 지점 없이 약간 동글동글한 모양을 하고 있을 것이다. 그렇다면 이제 convex function을 정의해보자. Convex function은 domain이 convex set이며, 함수는 다음과 같은 성질을 만족해야한다.</p>


<p>$$ f(\lambda x_1 + (1-\lambda x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)) \text{ for } \forall x_1, x_2 \in X, \forall \lambda \in [0,1] $$</p>


<p>즉, 아래와 같은 함수는 convex function이다.</p>


<p><img src="/images/post/63-1.png" width="500"></p>

<p>Convex function이 좋은 이유는 반드시 optimal한 값이 하나 밖에 존재하지 않는다는 것이다. 여기에서 내가 optimal point가 하나라고 얘기하지 않은 이유는 아래와 같은 예가 있기 때문이다.</p>


<p><img src="/images/post/63-2.png" width="500"></p>

<p>이 함수는 optimal한 값은 unique하게 존재하지만, 그 값을 가지는 point가 unique하지는 않다.</p>


<p>따라서 unique한 optimal point를 찾기 위해서는 하나의 조건이 더 필요한데, 바로 strictly convex라는 조건이다. 이 조건은 위의 식에서 = 이 빠진 형태이다. 즉 &le; 가 &lt; 으로 바뀌는 것이다.</p>


<p>이런 strictly convex function에 대해서 optimal point가 unique하게 존재한다는 것을 증명할 수 있으며, 증명과정은 크게 어렵지 않으니 <a href="http://math.stackexchange.com/questions/345865/strictly-convex-function-and-well-separated-minimum">링크</a> 등을 참고하면 될 것 같다. 아무튼 strictly convex function은 minimum point가 unique하게 존재하기 때문에, 이런 convex function에 대해서 우리는 어떤 optimization algorithm을 design할 수 있다.</p>


<h5>Convex Optimization</h5>


<p>Convex function \(f, g_1, g_2, ..., g_m\)에 대해 아래와 같은 optimization 문제를 Convex optimization이라 정의한다.</p>


<p>$$ \min f(x) \text{ subject to } g_i (x) \leq 0, \text{ i = 1,...,m}$$</p>


<p>모든 함수들이 convex하기 때문에 이 optimization의 solution은 unique하며, 이런 optimization 문제를 풀 수 있는 방법은 아주아주 많이 존재한다.</p>


<p>이런 convex optimization의 subset으로 linear programming, quadratic programming, semidefinite programming 등이 존재한다. 이 글에서는 그런 특수한 경우는 다루지 않고, 일반적인 convex optimization에서 사용할 수 있는 알고리듬들을 다룰 예정이다. 크게 gradient descent method, newton method, 그리고 lagrange multiplier가 그것이다.</p>


<h5>Gradient Descent Method</h5>


<p><a href="http://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent Method</a>는 미분 가능한 convex function의 optimum point를 찾는 가장 popular한 방법 중 하나이다. 장점이라면 많은 application에 implement하기 매우 간단하고, 모든 차원과 공간으로 확장할 수 있으며, 수렴성이 항상 보장된다는 것이지만, 속도가 느리다는 단점이 존재한다.</p>


<p>아이디어는 매우 간단하다. 어떤 산 위에 우리가 서있다고 가정해보고 우리가 알 수 있는 정보는 내 위치와 내 주변 위치들의 높이 차이밖에 없다고 가정해보자. 만약 내가 산의 가장 낮은 위치로 내려가야하는 상황이라면 어떻게 내려가면 낮은 위치에 도달할 수 있을까? 가장 간단한 방법은 가장 기울기가 가파른 방향을 골라서 내려가는 것이다. 그러다보면 언젠가는 기울기가 0이 되는 지점에 도달하게 될 것이고, 그 지점이 주변에서는 가장 낮은 지점이 될 것이다. 만약 산의 높이가 convex function이라면, 즉 가장 낮은 지점이 unique하다면, 그렇게 도달한 지점이 우리가 원했던 가장 optimal한 지점이라는 것을 알 수 있다. 즉, 이 알고리듬은 매 step마다 현재 위치에서의 가장 가파른 아래로 내려가는 기울기 \(-\nabla f(x^{(k)})\)를 계산하고, 그 방향으로 이동하고, 다시 기울기를 계산하는 방식의 iterative algorithm이다.</p>


<p><img src="/images/post/63-3.png" width="500"></p>

<p>Algorithm description은 다음과 같다.</p>


<p>$$ x^{(k+1)} = x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}) $$</p>


<p>이 때 \(x^{(k)}\) 는 k번 째 iteration에서의 x의 값이며 \(\nabla f(x^{(k)})\) 는 그 지점에서의 gradient 값이다. 이를 조금 더 알고리듬스럽게 기술해보면</p>


<ol>
    <li>set iteration counter k=0, and make an initail guess \(x_0\) for the minimum</li>
    <li>Compute \(-\nabla f(x^{(k)})\)</li>
    <li>Choose \(\eta^{(k)}\) by using some algorithm</li>
    <li>Update \(x^{(k+1)} = x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}))\) and k = k+1</li>
    <li>Go to 2 until \(\nabla f(x^{(k)})) < \epsilon \)</li>
</ol>


<p>만약 function이 convex하다면, 이런 방법으로 x를 계속 update하다보면 적절한 \(\eta\)를 취했을 때 이 algorithm은 global unique optimum으로 수렴하게 된다. 이때 \(\eta\)를 Step size라고 일컫는데, 이 step size를 어떻게 설정하느냐에 따라 알고리듬의 performance가 좌우된다. 만약 step size가 너무 작다면 iteration을 너무 많이 돌아서 전체 performance자체가 저하될 것이다. 그렇다고 step size가 너무 크다면 minima에 converge하는 것이 아니라 그 주변에서 diverge를 할 수도 있다.</p>


<p><img src="/images/post/63-4.png" width="500"></p>

<p>Step size를 고르는 방법은 크게 fixed step size를 취하는 방법과 매번 optimal한 step size를 고르는 방법 두 가지가 존재한다. 먼저 fixed step size에 대해 살펴보자.</p>


<p>앞서 언급했듯, step size를 잘 잡는 것이 중요한데, 너무 큰 step size를 잡게 되면 algorithm이 diverge 하기 때문이다. 다행히도 우리는 어떤 적절한 step size \(\eta\)에 대해 algorithm이 strictly convex function f의 global unique optimum으로 수렴한다는 증명을 할 수 있다.</p>


<p>이 적절한 step size에 대해 설명을 하려면 먼저 L-Lipschitz function이라는 것을 정의해야하는데, 이는 다음과 같이 정의된다.</p>


<p>$$ \|\nabla f(x) - \nabla f(y) \|_2 \leq L \|x-y\|_2, \forall x,y \in R^n $$</p>


<p>만약 f가 L-Lipschitz function이고 어떤 optimum이 존재한다면 fixed step size \(\eta \leq \frac{2}{L}\) 을 취했을 때 gradient descent algorithm이 stationary point로 수렴하게 된다는 것을 증명할 수 있다. 증명은 이 <a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_4_Scribe_Notes.final.pdf">렉쳐노트</a>를 참고하기를 바란다.</p>


<p>다음으로 step size를 계속 update하는 방식(이를 Line search라고 한다)을 살펴보자.</p>


<p>먼저 exact line search라는 것 부터 살펴보자. Exact line search의 algorithm은 다음과 같다</p>


<ol>
    <li>set iteration counter k=0, and make an initail guess \(x_0\) for the minimum</li>
    <li>Compute \(-\nabla f(x^{(k)})\)</li>
    <li>Choose \(\eta^{(k)} = argmin_\eta f(x^{(k)} - \eta \nabla f(x^{(k)}))\)</li>
    <li>Update \(x^{(k+1)} = x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}))\) and k = k+1</li>
    <li>Go to 2 until \(\nabla f(x^{(k)})) < \epsilon \)</li>
</ol>


<p>이 알고리듬은 가장 optimal한 \(\eta\)를 매 순간 찾아준다. 하지만 이 알고리듬은 3번 과정 때문에 practical하지는 못하고, 이 알고리듬을 조금 더 practical하게 보완한 backtracking line search algorithm이 조금 더 많이 쓰인다.</p>


<p>For \(\alpha \in {0, 0.5}, \beta \in {0,1}\)</p>


<ol>
    <li>set iteration counter k=0, and make an initail guess \(x_0\) and choose initial \(\eta=1\)</li>
    <li>Compute \(-\nabla f(x^{(k)})\)</li>
    <li>Update \(\eta^{(k)} = \beta \eta^{(k)}\)</li>
    <li>Go to 2 until \(f(x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}))) \leq f(x^{(k)}) - \alpha \eta^{(k)} \| \nabla f(x^{(k)})) \|^2 \)</li>
    <li>Update \(x^{(k+1)} = x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}))\) and k = k+1</li>
    <li>Go to 1 until \(\nabla f(x^{(k)})) < \epsilon \)</li>
</ol>


<p>보통 \(\alpha\)는 0.01에서 0.3 사이의 값으로 선택하고 \(\beta\)의 값은 0.1에서 0.8 정도로 설정된다.</p>


<p>이론적으로는 line search 쪽이 convergence speed가 훨씬 빠르지만, 실제로는 이를 구하는 것보다 적당한 step size로 값을 고정해놓고 gradient descent를 취하는 방법이 더 쉽고, line search를 위해 update되는 step size에 소모되는 computation이 꽤 크기 때문에 practical하게는 적당한 step size를 고정하는 방법이 훨씬 더 많이 사용된다.</p>


<p>Gradient Descent를 약간 응용하여 변형한 알고리듬으로는 Coordinate Descent method, Steepest descent method 등이 존재한다.</p>


<p>간단하게 개념만 설명하자면, <a href="http://en.wikipedia.org/wiki/Coordinate_descent">Coordinate descent</a>는 각 좌표계 방향에서 한 방향씩으로만 번갈아가면서 gradient descent를 하는 것이다. 예를 들어 n차원 function이 있는 경우, 첫번째 차원에 대해 gradient descent를 계산하고 두번째 세번째.. n번째 차원에 대해 이 과정을 반복한다. 그리고 이 과정을 전체 함수가 converge할 때 까지 반복한다. 이 방법이 좋은 이유는 함수가 비록 convex하지 않더라도, 어떤 특수한 경우에 대해 이 방법을 사용해 optimal point를 얻을 수 있기 때문이다.</p>


<p> <br/>
<img src="/images/post/63-5.jpg" width="500"></p>

<p><a href="http://en.wikipedia.org/wiki/Method_of_steepest_descent">Steepest descent</a>는 간단히 생각하면 norm을 적절한 norm으로 바꿔 더 빠르게 converge를 시키는 방법이다. 이 방법은 saddle point가 존재하는 경우에 유용하게 사용할 수 있다고 한다.</p>


<p><img src="/images/post/63-6.gif" width="500"></p>

<h5>Newton's Method</h5>


<p>Gradient descent는 기울기 정보 즉, 미분을 한 번만 한 값만을 사용하는데 만약 우리가 두 번 미분한 값을 사용할 수 있다면 훨씬 훨씬 빠른 수렴속도를 보이는 알고리듬을 디자인할 수 있을 것이다. <a href="http://en.wikipedia.org/wiki/Newton's_method">Newton's method</a>는 Gradient Descent의 2nd derivative version으로 훨씬 수렴성이 빠르다는 장점을 가지고 있지만, Hessian Matrix를 계산해야하기 떄문에 computation과 memory측면에서 expensive하다는 단점을 가지고 있다.</p>


<p><img src="/images/post/63-7.gif" width="500"></p>

<p>2번 미분 가능한 strongly convex function f에 대해 Newton step은 다음과 같이 기술된다.</p>


<p>$$ \triangle x_{nt} (x) = -\nabla^2 f(x)^{-1} \nabla f(x) $$</p>


<p>이 newton step에 대해 newton's method 알고리듬은 다음과 같이 쓸 수 있다.</p>


<ol>
    <li>initialize</li>
    <li>Compute the newton step \(\triangle x = -\nabla^2 f(x)^{-1} \nabla f(x)\)</li>
    <li>Choose step size \(\eta\)</li>
    <li>Update \(x^+ = x + \eta \triangle x_{nt} (x) \)</li>
    <li>Go to 2 until converge</li>
</ol>


<p>이 알고리듬을 깊게 파고들어가면 할 얘기가 정말 많아지므로 일단 convergence 조건이나, step size를 고르는 방법 등에 대해서는 생략하도록 하겠다. 하지만 이 알고리듬에 대해 크게 두 가지 얘기는 꼭 하고 넘어가야할 것 같은데, 하나는 convergence speed, 그리고 하나는 computation이다.</p>


<p>먼저 이 알고리듬은 매우 빠르게 수렴한다. 실제 convergence rate를 증명했을 때 gradient descent보다 빠르게 수렴할 뿐더러, 실제 practical하게도 (Hessian을 계산할 수 있다면) 아래와 같은 convergence phase를 보인다</p>


<p><img src="/images/post/63-8.png" width="500"></p>

<p>즉 처음에는 linear하게 converge하는 것처럼 보이지만, 시간이 지나면 순식간에 quadratic으로 수렴한다. 생각해보면, newton method는 기울기 뿐 아니라, 기울기의 기울기 정보도 같이 사용하기 때문에, 처음에 기울기가 크게 변하지 않을 때는 빠르게 감소하다가, 갑자기 기울기의 크기가 변하기 시작하면 그에 맞춰서 적절한 newton step을 찾을 수 있기 때문에 아주 빠르게 수렴할 수 있다. 특히 saddle point에 대해 영향을 크게 받지 않기 때문에 gradient descent 보다는 훨씬 훨씬 빠르게 수렴한다.</p>


<p>하지만 Newton method의 근본적인 한계는 바로 Hessian을 계산해야한다는 점이다. 이 Hessian을 계산하는 것 자체도 매우 연산이 복잡할 뿐 아니라, gradient의 제곱 만큼의 공간이 필요하기 때문에 memory 역시 efficient하게 사용하기가 어렵다. 때문에 매우 복잡한 함수에 대해서는 newton method를 사용하기가 매우 힘들다. 예를 들어서 neural network에서 gradient descent method를 사용하면 chain rule을 통해 아주 쉽게 그 값을 계산할 수 있지만, second derivation으로 넘어가는 순간, computation을 해야할 양이 급격하게 증가하고, 안그래도 부족한 메모리가 더 부족하게 되기 때문에 neural network에서는 사용되지 않는 방법이다.</p>


<p>대략 요약하자면, 별 일이 없다면 gradient descent에서 fixed step size를 사용해 converge를 시키는 것이 practical하게는 가장 많이 쓰이고 있다. 하지만, 더 빠른 convergence speed가 필요하다면 line search나 steepest method, newton's method를 한 번쯤은 고려해볼만 할 것이다.</p>


<h5>Lagrange Multiplier</h5>


<p>마지막으로 <a href="http://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multiplier</a>에 대해 살펴보자. Lagrange Multiplier는 constrained optimization을 푸는 매우 popular한 방법 중 하나이다. 원리는 원래 constrained optimization과 같은 optimum point를 가지는 새로운 unconstrained optimization 문제를 만들어서 optimum point를 구하는 것이다. 즉, 다음과 같은 optimization 을 생각해보자</p>


<p>$$ \min f(x) \text{ s.t. } g(x) = c $$</p>


<p>그렇다면 이와 같은 극점을 가지는 다음과 새로운 optimization problem을 만드는 상수 \(\lambda\)가 존재한다.</p>


<p>$$ \min f(x) + \lambda (g(x) - c) $$</p>


<p>보통 c는 상수이기 때문에 극값에 영향을 주지 않으므로 많이 무시된다.</p>


<p>이 문제를 푸는 방법은 여러 개가 있지만, 어렵지 않은 문제인 경우 Lagrange function을 편미분해 0이 되는 값들을 모두 구해 \(\lambda\)를 계산해 대입해서 극값을 찾는 방법을 많이 취한다.</p>


<p>하지만 편미분을 구하기 어려운 경우도 존재하기 때문에 항상 그렇게 푸는 것은 아니고, 이런 문제를 잘 풀 수 있는 여러 알고리듬들이 존재하며, 최근 많이 쓰이는 알고리듬으로는 <a href="http://en.wikipedia.org/wiki/Augmented_Lagrangian_method">ALM</a> (Augumented Lagrange method)가 있다.</p>


<p>Lagrange Multiplier는 풀기 어려운 constrained optimization problem을 그보다 더 풀기 쉬운 unconstrained optimization problem으로 바꿔주기 때문에, 새로운 Lagrange function을 풀기 쉬운 경우에 많이 사용한다.</p>


<h5>Optimization for non-convex function (Local optimum)</h5>


<p>하지만 위에서 서술한 방법들은 오직 convex한 function에만 적용할 수 있는 방법들이므로 우리는 non convex한 함수들은 optimize할 수 없다.. 라고 말해야하지만, 사실 꼭 그렇지만은 않다. Convex function은 global optimum, 즉 해당 식을 만족하는 x를 반드시 찾을 수 있지만, 같은 방법을 적용했을 때 non-convex한 함수는 그럴 수 없다. 하지만 non-convex한 함수이더라도 local optimum 값을 찾을 수는 있다. 때문에 많은 경우에 optimization problem이 convex하지 않다고 하더라도 converge를 하는 것 만으로도 그 문제를 풀었다라고 얘기하는데, 이때, local하게 함수를 봤을 때 convex하다면 gradient descent 등의 방법을 사용해서 local optimum을 계산할 수 있다. 예를 들어서 Neural network에 많이 쓰이는 backpropagation algorithm은 사실 gradient descent method algorithm인데, 따라서 이 알고리듬의 결과는 항상 local optimum으로 converge한다.</p>


<p><img src="/images/post/63-9.jpg" width="500"></p>

<p>따라서 convex하지 않은 optimization일지라하더라도, 일단은 gradient descent 등의 방법을 통해 optimization을 할 수 있다면, local optimum을 구할 수 있다.</p>


<p>하지만 그럼에도 local한 optimum이 엄청나게 많은 경우에는 local optimum으로 converge하는 것이 문제가 되기 때문에 함수를 convex하게 relaxation 시켜서 optimization을 푸는 경우도 존재한다. 특히 원래 문제가 NP-hard인 경우, parameter를 골라야하는 경우 등에 convex relaxation을 많이하는 것 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2014년 9월 10일: 글 등록</li>
<li>2015년 2월 28일: 변경 이력 추가</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex optimization</a> by Boyd</li>
<li><a href="http://stanford.edu/~boyd/cvxbook/bv_cvxslides.pdf">Lecture slides</a> by Boyd and Vandenberghe</li>
<li><a href="http://users.ece.utexas.edu/~cmcaram/EE381V.html">EE381V &ndash; Large Scale Optimization</a> The University of Texas at Austin</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li>Hidden Markov Model</li>
<li>Clustering (K-means, Gaussian Mixture Model)</li>
<li>EM algorithm</li>
<li>Dimensionality Reduction (LDA, PCA)</li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li>Neural Network Introduction</li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (6) Information Theory]]></title>
    <link href="http://SanghyukChun.github.io/62/"/>
    <updated>2014-08-20T00:20:00+09:00</updated>
    <id>http://SanghyukChun.github.io/62</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>Information Theory는 섀년이라는 걸출한 천재가 이룩해낸 매우 뛰어난 이론이다. 이 이론 덕분에 우리는 이렇게 인터넷도 할 수 있고, 무선통신도 할 수 있는 것이다. 이 이론 덕분에 불안정한 noisy한 channel에서도 유한한 시간 안에 우리가 전달하고 싶은 정보를 전부 전달할 수 있다는 것을 확신할 수 있고, 이론적인 한계값까지 도출이 가능한 엄청난 이론이다. Machine Learning에서도 이런 information theory 측면에서 문제를 바라보는 경우가 종종 있는데, Entropy, Mutual Information, KL-divergence 등이 그것이다. 이 글에서는 그런 Machine Learning의 관점에서 많이 쓰이는 기초적인 정보이론의 개념들을 짚고 넘어가볼까 한다.</p>


<h5>Entropy</h5>


<p>엔트로피는 '정보'의 단위라고 할 수 있다. 어떤 distibution p(x)에서 generate되는 discrete random variable x가 있다고 해보자. 이 random variable x가 전달할 수 있는 정보량은 어떻게 계산할 수 있을까. 여기에서 '정보'란 얼만큼의 bit가 있어야 x에 대한 정보를 완벽하게 얻을 수 있는가로 정의해보자. 예를 들어서 fair한 동전 던지기의 정보량은 1이다. 한 비트만 있으면 반드시 그 동전 던지기의 distribution을 서술할 수 있다. 그러나 만약 fair coin이 아니라면 한 면이 나올 확률이 다른 면이 나올 확률보다 상대적으로 더 크기 때문에 한 비트보다도 더 적은 정보를 사용해 값을 맞추는 것이 가능해진다. 이런 정보의 양을 <a href="http://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy</a>라는 것으로 정의하게 되는데, 간단하게 생각하면 열역학2법칙의 그 엔트로피와 동일하다. 즉, 엔트로피가 커질수록 불확실성이 높아지고 정보량은 더 많아진다. Entropy는 \(H(x) = - \sum_x p(x) log_2 p(x) \)로 정의가 되며, 만약 p(x)가 0으로 가면 \(\log_2 p(x)\)는 음의 무한으로 발산하지만, p(x)가 0이 되는 속도가 더 빠르기 때문에 엔트로피는 0이 된다.</p>


<p>그럼 왜 엔트로피는 이런 꼴을 하게 되는 것일까. 만약 우리가 전체 N개의 object들이 있고, 이 object들이 K개의 bin으로 나뉘어져있다고 해보자. 그리고 i번째 bin에 들어갈 수 있는 object의 개수를 \(n_i\)라고 했을 때, object들이 bin에 들어갈 수 있는 permutation의 개수는 \(W = \frac{N!}{\prod_i n_i!}\)와 같으며 이를 multiplicity라고 한다. 엔트로피란 이 multiplicity에 비례하는, 정확히는 log를 취한 값을 엔트로피라고 하게 된다. 즉 Entropy H는 multiplicity W에 대해 다음과 같이 표현된다.</p>


<p>\[H = \frac{1}{N}\ln W = \frac{1}{N}\ln N! - \frac{1}{N}\sum_i \ln n_i ! \]</p>


<p>이때 \(\lim N \to \infty \) 라고 해보자, 그러면 우리는 <a href="http://en.wikipedia.org/wiki/Stirling's_approximation">Stirling 근사</a>를 할 수 있는데 이는 \(\ln N! \simeq N \ln N - N\)으로 주어진다. 이를 대입해서 잘 정리해보면 아래와 같은 식을 얻을 수 있다.</p>


<p>\[H = -\lim_{N \to \infty} \sum_i \left( \frac{n_i}{N} \right) \ln \left( \frac{n_i}{N} \right) = - \sum_i p_i \ln p_i \]</p>


<p>이는 위에서 정의한 엔트로피의 값과 일치한다.</p>


<p>그런데 이 값은 discrete한 random variable에 대해 정의된 값이고 continous한 random variable x에 대해서는 <a href="http://en.wikipedia.org/wiki/Differential_entropy">differencial entropy</a>라는 정의할 수 있다. <a href="http://en.wikipedia.org/wiki/Mean_value_theorem">평균값 정리</a>에 의해서 우리는 다음을 만족하는 value \(x_i\)를 반드시 찾을 수 있다</p>


<p>\[ \int_{i\Delta}^{(i+1)\Delta} p(x) dx = p(x_i) \Delta\]</p>


<p>엔트로피는 discrete한 random variable에 대한 값이었는데, 위 식을 통해 continous variable x를 위의 식을 만족하는 \(x_i\)로 치환하는 방식으로 quantize할 수 있다. 또한 이런 경우 각 \(x_i\)를 관측할 확률이 \(p(x_i)\Delta\)로 계산되므로, 이렇게 했을 경우 엔트로피는 아래와 같이 계산할 수 있다.</p>


<p>\[H_\Delta = -\sum_i p(x_i) \Delta \ln ( p(x_i) \Delta ) = - \sum_i p(x_i) \Delta p(x_i) - \ln \Delta \]</p>


<p>이때, 오른쪽 term은 x에 대한 값이 아니니까 일단 먼저 무시하고, \(\lim \Delta \to 0\)를 취해보자. 이렇게 계산할 경우 아래 식이 얻어진다.</p>


<p>\[\lim_{\Delta \to 0} H_\Delta = \lim_{\Delta \to 0} -\sum_i p(x_i) \Delta \ln ( p(x_i) \Delta ) = -\int p(x) \ln p(x) dx\]</p>


<p>이때, 맨 오른쪽 term을 differencial entropy라고 정의한다. 즉, differencial entropy는 다음과 같이 정의된다.</p>


<p>\[H(x) = -\int p(x) \ln p(x) dx\]</p>


<p>마지막으로 random variable이 x,y 두 개가 있고 이 둘의 joint distribution p(x,y)가 있다고 해보자. 우리가 알고 있는 정보는 x의 value라고 했을 때 우리는 y의 information의 양을 계산할 수 있을까? 이를 <a href="http://en.wikipedia.org/wiki/Conditional_entropy">Conditional Entropy</a>라고 하는데 이때 y에 대해 필요한 additioanl information은 \(p(y|x)\)이며, x와 y의 확률은 p(x,y)이므로 Conditional Entropy는 아래와 같이 정의된다.</p>


<p>\[H(y|x) = - \int \int p(y,x) \ln p(y|x) dy dx\]</p>


<p>이 값은 다음과 같은 chain rule을 항상 만족시킨다.</p>


<p>\[H(x,y) = H(y|x) + H(x)\]</p>


<h5>KL divergence</h5>


<p>어떤 probability distribution p(x)와 p(y)가 있다고 했을 때 이 둘의 차이, 혹은 distance를 정의할 수는 없을까. 예를 들어 p(x)라는 우리가 모르는 unknown distribution이 있을 때, 우리가 추측한 \(p(\hat x)\)와 true distribution p(x)가 얼마나 차이나는지를 계산할 수 있는 방법은 없을까. 만약 우리가 q(x)를 사용해서 x를 transmitting하는 coding scheme을 construct했다고 해보자. 그리고 true distribution을 p(x)였다고 했을 때, q(x)를 사용하였을 때 얼마나 더 많은 정보량이 필요할 것인지 measure할 수 있을 것이다. 이를 <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> 혹은 KL divergence라고 하며 수식은 아래와 같다.</p>


<p>\[KL(p\|\|q) = - \int p(x) \ln q(x) - \left( -\int p(x) \ln p(x) dx \right) \\\\ = -p(x) \ln \left[ \frac{q(x)}{p(x)} \right] dx \]</p>


<p>정확히 얘기하면 이 값은 'distance'가 될 수는 없다. 왜냐하면 distance, 혹은 metric은 symmetric해야하는데 KL divergence는 \(KL(p\|\|q) \neq KL(q\|\|p)\) 이기 때문이다.</p>


<p>KL divergence는 언제나 0 보다 크거나 같은데, 같은 경우는 오직 p(x)와 q(x)가 일치하는 경우 뿐이다. 이를 증명하기 위해서는 convexity 컨셉과 Jensen's inequality를 도입하면 쉽게 증명이 가능하지만, 여기에서는 생갹하도록 하겠다.</p>


<p>중요한 점은, KL divergence는 두 distribution의 차이를 define할 수 있는 좋은 수단 중 하나라는 것이며, 다시 말해 원래 true distribution p(x)와 우리가 estimate한 q(x)가 얼마나 비슷한지를 measure할 수 있는 수단이라는 점이다.</p>


<h5>Mutual Information</h5>


<p><a href="http://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a>은 두 random variable들이 얼마나 mutual dependence한지를 measure하는 방법을 의미한다. 만약 random variable x와 y가 independent하다면 joint distribution p(x,y) 는 p(x,y) = p(x)p(y) 로 주어지게 될 것이며, 만약 둘이 dependent한 경우에는 두 값이 달라질 것이다. 그렇다면 만약 true distribution을 p(x,y)라고 했을 때, 새롭게 우리가 x와 y가 independent하다고 estimate하고 구한 p(x)p(y)와의 KL-divergence를 구할 수 있지 않을까? 당연히 이 값은 x와 y가 independent할 때만 0이고 그 이외에는 항상 0보다 크다. 즉, 두 random variable이 얼마나 mutually dependent한가, 얼마나 Mutual하게 information을 많이 가지고 있느냐를 측정할 수 있는 도구가 되므로 이를 Mutual information이라 한다. 수식으로 표현해보면 아래와 같다.</p>


<p>\[I(x,y) = KL(p(x,y)\|\|p(x)p(y) \\\\ = - \int \int p(x,y) \ln \left( \frac{p(x)p(y)}{p(x,y)} \right) dx dy\]</p>


<p>위의 값을 Mutual Information이라 하며, 이 값은 항상 다음과 같은 관계를 만족시킨다.</p>


<p>\[I(x,y) = H(x) - H(x|y) = H(y) - H(y|x)\]</p>


<h5>Machine Learning and Information Theory</h5>


<p>Entropy는 주어진 bin에 얼마나 비슷한 element들이 들어있는지를 측정하는 척도로 쓰일 수 있으며, decision tree를 learning하는 알고리듬 등에서도 사용할 수 있다. 또한 KL-divergence는 두 distribution과의 거리를 의미하므로, density estimation 관점에서 바라봤을 때 우리가 estimate하는 distribution과 원래 true distribution이 얼마나 유사한지, 우리가 얼마나 잘 density estimation을 했는지 evaluation을 하는 용도 등으로 쓰일 수 있다. 마지막으로 Mutual information을 Bayes perspective에서 바라보게 된다면, 만약 우리가 어떤 데이터 x의 prior p(x)를 관측하고, 새로운 데이터 y를 관측해 얻은 posterior distribution p(y|x)가 있다고 했을 때, Mutual information은 이전 관측 x를 통해 새로운 관측 y의 uncertainty가 얼마나 reduction 되는지를 의미하게 되는 것과 동일하다는 것을 알 수 있다.</p>


<p>정보이론 자체는 Machine Learning과 크게 관계가 없어보이지만, 그 개념들은 생각보다 꽤 많은 부분에서 사용되게 되므로 좀 간략하게 다루게 되었다.</p>


<p>추가: 정보이론이 어떻게 머신러닝에 유용하게 쓰일 수 있는가에 대한 lecture와 book link들 <a href="http://www.inference.phy.cam.ac.uk/itprnn_lectures/">[1]</a>, <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">[2]</a></p>




<h5>변경 이력</h5>


<ul>
<li>2014년 8월 19일: 글 등록</li>
<li>2014년 10월 9일: 정보이론, 머신러닝 렉쳐 및 책 링크 등록</li>
<li>2015년 2월 28일: 변경 이력 추가</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li>Hidden Markov Model</li>
<li>Clustering (K-means, Gaussian Mixture Model)</li>
<li>EM algorithm</li>
<li>Dimensionality Reduction (LDA, PCA)</li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li>Neural Network Introduction</li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (5) Decision Theory]]></title>
    <link href="http://SanghyukChun.github.io/61/"/>
    <updated>2014-08-19T18:13:00+09:00</updated>
    <id>http://SanghyukChun.github.io/61</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>그 어떤 좋은 알고리즘을 선택하더라도, 최종적으로 특정 문제에 대해 inference를 하기 위해서는 decision making을 해야만 한다. 그렇다면 decision은 어떻게 내릴 수 있을까? 라는 질문이 자연스럽게 들 수 있는데, decision theory는 어떻게 decision을 내릴지에 대해 다룬다. 내가 지금 결정한 parameter는 적당한 parameter인가? 예를 들어서 내가 임의의 데이터를 가장 잘 설명할 수 있는 1차함수를 그려야하는 상황이라고 했을 때, 나는 그 '가장 잘 설명할 수 있는' parameter를 어떻게 decide할 수 있을까, 어떻게 decision making을 할 수 있을까. 혹은 Classification problem에서 해당 data가 어떤 class에 속하는지 어떤 방식으로 decision을 내려야할까?</p>


<h5>Decision rule with prior only</h5>


<p>다시 <a href="58#58-1-Bayes">Bayes Rule</a>로 돌아가보자.</p>


<p>\[ p(C|X) = {p(X|C) p(C) \over p(X)} \]</p>


<p>이전 글에서 prior라는 것에 대해 언급했었다. 예를 들어 우리가 어떤 classification을 하는 상황이라 해보자. 만약 주어진 데이터가 class A인지 B인지 구분해야하는 상황이라고 가정을 해보자. 우리가 아는건 prior P(C) 밖에 모르는 상황이라고 해보자. 그렇다면 우리가 내릴 수 있는 가장 합리적인 판단은 무엇일까? 이 상황에서의 가정은 우리가 아는 정보는 오직 prior밖에 없으며, incorrect classification에 대한 cost는 모두 동일하다고 가정한다. 그렇다면 당연히 reasonable한 decision rule은 가장 높은 확률을 가지는 class를 선택하는 것일 것이다. 이유는 가장 확률이 높은 클래스를 C1이라 하면, 언제나 \(P(C1) > P(C2)\) 라는 사실을 알 수 있기 때문이다. 즉, 언제나 C1을 고르는 것이 가장 best하다. 그렇다면 여기에서 문제가 발생하는데, 만약 우리가 모든 decision을 동일하게 내리게 된다면 당연히 좋은 결과가 나올리가 없다는 점이다. 예를 들어 C1이 60%, C2가 40% 존재하는 상황이면, 언제나 error는 0.4가 될 것이며, 언제나 classification result는 C1이 될 것이기 때문이다. 그러나 일단 우리가 prior만 가지고 있다고 가정하고 있기 때문에 이런 상황에서는 언제나 이것이 best가 될 것이다. 그렇다면 이 이외의 다른 정보를 가지고 있다면 어떨까?</p>


<h5>Decision rule with Likelihood</h5>


<p>Observation, 혹은 Likelihood에 대한 정보를 가지고 있다면 분명 상황은 더 나아질 수 있다. 확률로 표시하자면 이 값은 \(P(X|C)\), 즉 주어진 클래스에 대해 관측되는 데이터가 된다. 그리고 당연하게도, 우리가 이 값을 가지고 있다면, Bayes rule에서부터 P(C|X), 혹은 posterior를 계산할 수 있게 된다. 즉, prior와 likelihood를 알고 있으므로, \(C = argmax P(C|X) = argmax {P(X|C) P(C) \over \sum_i P(X|C_i) P(C_i) } \) 를 통해 decision을 내릴 수 있게 된다. 그렇다면 이렇게 선택하는 경우 error는 어떻게 나타나게 될까? 여기에서 error는 classification이 틀린 경우를 의미한다 (number of misclassification). 일단 class가 2개 밖에 없다고 해보자. 그럼 error가 발생하는 경우는 단 두가지인데,</p>


<p>\[P(error|x) = \begin{cases} P(C1|X) \text{ if we decide C2} \\\\ P(C2|X) \text{ if we decide C1} \end{cases} \]</p>


<p>이렇게 두 가지가 될 것이다. 즉, 우리가 P(C|X)를 maximize하는 C를 선택하는 decision rule을 가지고 있다면, P(error)는 간단하게 다음과 같이 계산된다</p>


<p>\[P(error) = \int P(error|X) P(X) dX \]</p>


<p>가 될 것이며, 이 결과를 최소화하는 decision making rule이 우리가 원하는 decision rule이 될 것이다. (당연히 우리는 결과의 error가 최소화되는 것을 원할테니까.) 결론만 놓고 얘기하자면 prior와 likelihood를 모두 알고 있다면 위와 같이 posterior를 maximize하는 것이 가장 이 값을 \(P(error|X) = min[P(C_1|X), ..., P(C_k|X)]\) 로 최소화 시킬 수 있다. (이유는 조금 생각해보면 간단하게 알 수 있다.) 즉, 모든 class의 posterior를 계산하여 주어진 데이터에 대해 얻어질 확률이 가장 큰 class를 선택하는 것과 정확히 같다는 의미이다. (참고로 만약 prior에 대한 정보가 없어서 uniform한 prior를 가정하게 된다면 결국 likelihood만을 놓고 계산한 값과 정확히 일치하게 된다.)</p>


<h5>Loss function</h5>


<p>앞서 설명한 예제는 "misclassification number"를 minimize하는 예제였다. 그러나 실제 많은 application들에서 이런 방식 이외의 다른 approach를 요구하게 된다. 예를 들어서 값이 틀렸을 때 0과 1로 error를 정의하는 것이 아니라, 원래 target data와 우리가 계산한 estimated data의 차이의 제곱의 합들로 표현을 할 수도 있고, 제곱이 아니라 절대값의 합으로 표현할 수도 있다. 즉, 보다 더 generalized된 접근법이 필요한데, 가장 formal하게 많이 쓰이는 decision criteria 중 하나로 <a href="http://en.wikipedia.org/wiki/Loss_function">loss function</a> 혹은 cost function이 있다. Cost function은 여러가지로 정의할 수 있겠지만, 나는 Cost function을 이렇게 정의한다. 우리가 목표로 하는 가장 좋은 결과와 지금 내가 선택한 결과와의 차이. 즉, 내 결과가 optimal한 결과보다 좋지 않으면 않을수록 cost function은 커지고, 당연히 optimal한 결과를 가지게 됐을 때 Cost function의 값이 가장 작아질 것이다. 다만, 그 차이는 여러가지 방법으로 정의할 수 있는데, 앞서 misclassification number와 같은 binary한 차이가 될 수도 있고, 또 앞서 내가 예로 들었던 원래 값과 예상 값의 차이의 제곱의 합.. 등 굉장히 다양하게 cost function을 정의하는 것이 가능하다. 그리고 이런 함수는 true value가 변하지 않고, parameter에 따라 estimated value가 변하게 되므로, loss function은 parameter에 대한 함수로 나타나게 된다. 이때, Loss function은 \(L(\theta, \hat \theta(X))\)로 표기되며, 우리가 찾고자하는 true parameter를 \(\theta\), 주어진 data X에 대해 estimate한 parameter를 \(\hat \theta(X)\)라 하자. 우리의 목표는 가장 적절한 parameter \(\hat \theta(X)\)를 찾는 것이다.</p>


<h5>Minimize Bayes Risk</h5>


<p>자 다시 Decision rule로 돌아가보자. 우리가 minimize하고 싶은 것은 Loss function을 minimize하는 것이지만, 그 값 자체가 true value에 dependent하기 때문에 정확한 값을 구하는 것이 불가능하다. 따라서 주어진 데이터 X에 대해 expectation을 계산하여 이를 해결하게 된다. 주어진 데이터들에 대한 loss function의 expectation은 \(R = \int \int L (\theta, \hat \theta(x)) p(\theta, x) dx d \theta \) 로 표현이 되는데, 이 값은 사실 posterior의 risk, 혹은 conditional risk라고 알려진 \(R = \int L (\theta, \hat \theta(x)) p(\theta | x) d \theta \) 값을 minimize하는 것과 일치한다. Expected Loss, 혹은 Bayes risk를 전개해보면</p>


<p>\[R = \int \int L (\theta, \hat \theta(x)) p(\theta, x) dx d \theta \\\\ = \int \int L (\theta, \hat \theta(x)) p(\theta | x) p(x) dx d \theta \\\\ =  \int \left( \int L (\theta, \hat \theta(x)) p(\theta | x) d \theta \right) p(x)dx  \]</p>


<p>이때, \(\theta\)에 대한 적분 구간이 정확하게 conditional risk와 같으며, x에 대해 summation하는 것은 parameter에 영향을 주지 않으므로 둘이 동일함을 알 수 있다. 이렇게 구해진 expectation loss, Bayes risk (혹은 이와 같은 posterior risk, conditional risk) 를 minimization시키는 방법으로 \(\hat \theta\)를 estimate하는 estimator를 Bayes estimator라고 한다.</p>


<p>그러면 다양한 loss function들에 대해 이 Bayes risk와 Bayes estimator로 얻어지는 parameter를 계산해보자.</p>


<p>간단한 예로 zero-one loss의 expected loss를 구해보자. Zero-one loss는 아래와 같이 주어진다. 이 경우는 class를 구하는 것이므로 \(\theta\)가 아니라 \(C_k\)로 작성하였다.</p>


<p>\[L(C_k, \hat C_k(x)) = \begin{cases} 1 \text{ if } C_k = \hat C_k(x) \\\\ 0 \text{ otherwise } \end{cases} \]</p>


<p>따라서 expectation loss 혹은 그와 동일한 conditional risk \(R = \sum_j L (C_j, \hat C_j(x) p(C_j | x) \) 는 (이 경우는 discrete하므로 integral이 아니라 summation이다.)</p>


<p>\[R = \sum_j L (C_j, \hat C_j(x) p(C_j | x) = \sum_{j \neq i} P(C_j | X) = 1-P(C_i|X) \]</p>


<p>와 같이 얻어지게 된다. 따라서 zero-one loss를 decision rule로 삼게 되면 아래와 같은 결과를 얻게 된다.</p>


<p>\[C_i = argmin R = argmin_{C_k} 1 - P(C_k|X) = argmax_{C_k} P(C_k|X) \]</p>


<p>즉, 0-1 loss를 사용하게 되면 Bayes estimator가 MAP (maximum a posterior)와 같아진다는 사실을 알 수 있다.</p>


<p>이번에는 다른 Loss function을 사용해서 bayes risk를 계산해보자. 만약 우리가 \(L (\theta, \hat \theta(x)) = (\theta - \hat \theta)^2 \) 이라 한다면 어떻게 될까. 먼저 conditional risk는 아래와 같다.</p>


<p>\[R = \int (\theta - \hat \theta)^2 p(\theta | x) d \theta p(x) \]</p>


<p>위의 식을 \(\hat \theta\)에 대해 미분해보면,</p>


<p>\[ \frac{\partial}{\partial \hat \theta} \left[ \int (\theta - \hat \theta)^2 ) p(\theta | x) d\theta \right] \\\\ = \int \frac{\partial}{\partial \hat \theta} \left[ (\theta - \hat \theta)^2 ) p(\theta | x) d\theta \right] \\\\ = -2 \int (\theta - \hat \theta) ) p(\theta | x) d\theta\]</p>


<p>이 derivation을 0으로 만드는 \(\hat \theta\)는 간단하게 \(\hat \theta = \int \theta p(\theta | x) d \theta = E[\theta|x]\)라는 결과를 얻게 된다. 즉, Bayes risk를 가장 minimize하는 \(\hat \theta\)는 주어진 데이터에 대한 parameter의 expectation인 \(E[\theta|x]\)라는 사실을 알 수 있다. 그리고 이를 다시 conditional risk에 대입을 해보면</p>


<p>\[R = \int (\theta - E[\theta|x]^2 p(\theta|x) d\theta = \sigma^2_{\theta|x}\]</p>


<p>즉, risk가 random variable의 variance와 정확히 같다는 것을 알 수 있다. 또한 따라서 Bayes risk가 절대로 0으로 수렴하지 않음도 알 수 있는데, 우리가 구한 결과에 따르면 이 값은 항상 \(\theta|x\)라는 random variable의 variance와 같기 때문에 이 값이 0이라는 얘기는 더 이상 \(\theta|x\)가 random variable이 아니라 deterministic하다는 의미가 되기 때문이다. 따라서 이 값은 항상 0이 아님을 알 수 있다.</p>


<h5>Inference and decision</h5>


<p>다시 Classification problem으로 돌아가보자. 이 classification은 크게 두 가지 step으로 나눌 수 가 있는데 하나가 inference stage이고, 또 하나가 decision stage이다. Inference stage에서는 training data를 사용하여 \(p(C_k|x)\)를 계산하기 위한 model을 learning한다. 그리고 그에 따르는 다음 step인 decision stage에서는 앞서 inference stage에서 계산한 posterior probability를 사용하여 실제 class assignment decision을 내리게 된다. 이런 과정 없이 단순하게 input x에 directly decision을 mapping하는 function을 생각할 수도 있는데, 이런 function은 discriminant function이라 한다.</p>


<p>이런 inference와 decision stage로 classification을 구분하는 approach를 취하게 되면 크게 두 가지 방법으로 decision problem을 접근하는 것이 가능해진다. <a href="http://en.wikipedia.org/wiki/Generative_model">Generative model</a>과 <a href="http://en.wikipedia.org/wiki/Discriminative_model">Discriminative model</a>이 바로 그것이다.</p>


<p>Generative model은 input에서부터 distribution을 직접적으로 혹은 간접적으로 얻어내고 output 역시 마찬가지 방법으로 얻어내게 된다. 조금 더 구체적으로 들어가보자. Generative model은 가장 먼저 inference problem, 정확히는 각각의 class 별로 가지게 되는 class conditional density \(p(x|C_k)\)를 결정하는 inference prblem을 풀게 된다. 또한 class prior probability \(p(C_k)\)도 infer를 하게 된다. 이 두 가지 정보를 사용하여 Bayes rule을 적용하여 posterior probability \(p(C_k|x)\)를 계산하게 된다.</p>


<p>\[ p(C_k|x) = {p(x|C_k) p(C_k) \over p(x)} \]</p>


<p>이때, \(p(x)\)는 단순히 normalize를 해주는 것으로 무시할 수 있다. (자세한건 <a href="58#58-1-Bayes">이전 글</a>에서 설명했으므로 생략.) 따라서 우리가 assume해야하는 값은 \(p(x|C_k)\), 그리고 \(p(C_k)\)인데, 결국 이 둘을 assume하는 것은 \(p(C_k, x)\)를 assume하는 것, 혹은 modeling하는 것과 같다. 즉, generative model은 joint probability를 modeling 하여 sample들을 해당 distribution으로 'generate' 한 결과를 사용하여 decision을 내리는 것이다.</p>


<p>이런 Generative model의 예로는 Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Naïve Bayes, Restricted Boltzmann Machine (RBM) 등이 존재한다.</p>


<p>이에 반해 Discriminative model 은 inference problem에서 posterior class probability \(p(C_k|x)\)를 직접 계산한다. 그리고 decision stage에서 이 posterior probability를 직접 사용하여 decision을 내리게 된다. 즉, 새로 들어온 input x에 대해 모든 class들의 posterior를 계산하여 가장 probability가 높은 class를 assign하는 방식으로 classification을 하게 된다. 위애서 길게 설명했던 Generative model과의 가장 큰 차이는 Generative model은 joint distribution \(p(x, C_k)\)를 계산하여 inference와 decision을 하는데에 반해 Discriminative model은 그것을 직접 계산하여 inference를 한다는 점이 다르다.</p>


<p>Discriminative model의 예는 Logistic regression, Linear discriminant analysis (LDA), Support Vector Machines (SVM), Boosting, Conditional Random Fields (CRF), Linear Regression, Neural Networks 등이 존재한다.</p>


<p>Generative model 과 Discriminative model의 차이점은 아래 그림에서 간단하게 정리되어있다. (<a href="http://sens.tistory.com/408">출처</a>)</p>


<p><img src="/images/post/61-1.jpg" width="600"></p>

<p>즉 데이터 x와 그 데이터들의 class y가 주어졌을 때, Classification을 위해서 필요한 값은 결국 \(p(y|x)\)로 같지만, generative model은 먼저 \(p(x|y), p(y)\)를 modeling하고 이 값들을 learning한 이후에 \(p(y|x) \propto p(x|y) p(y)\)를 통해 데이터 x의 class y의 확률을 계산하게 된다. 반면 discriminative model은 model 자체가 \(p(y|x)\)를 바로 learning하기 때문에 이 값을 바로 사용하면 된다. 따라서 discriminative model은 어떤 식으로 class가 분포해 있는지 사전 정보를 알 필요가 전혀 없지만, generative model을 사용하기 위해서는 이 값을 내가 미리 assume해야만 한다. 간단하게 생각하면 SVM은 대표적인 discriminative model 중 하나인데, SVM의 그 어떤 과정도 주어진 class들이 특정 형태로 분포해야한다는 가정이 전혀 들어있지 않다. 반면 generative model 중 하나인 GMM은 모든 cluster들이, 혹은 class들이 Gaussian의 mixture 형태로 주어진다고 가정하게 된다. 즉, 이 경우는 데이터와 class의 joint probability를 계산하게 되는 것이다.</p>


<p>정리해보면 Generative model은 joint distribution에서부터 sample을 'generative'하는 반면, Discriminative model은 주어진 데이터 x에 대해서만 model을 구하게 된다. 따라서 일반적으로 generative model이 훨씬 더 flexible한 결과를 보인다. 그러나 joint distribution을 직접 계산하는 것은 매우 computation cost가 높은, 즉 complexity가 높은 작업인 경우가 대다수인 반면, posterior probability를 계산하는 것은 상대적으로 더 저렴한 경우가 많다. 그 뿐 아니라, joint distribution을 modeling  하는 것도 대부분 쉽지 않다. 주어진 데이터들이 어떤 joint distribution으로 분포했는지는 modeling을 하지 않고서는 알 수 없기 때문이다. 반면 \(p(y|x)\)는 우리가 관측하는 likelihood 등을 사용하여 바로 계산하는 것이 가능하기 때문에 대부분의 discriminant model은 아무래도 조금 더 데이터가 sparse하게 존재하는 경우라거나, 전체 우리가 그 joint distribution에 대해 감조차 잡을 수 없는 경우에 해야하는 classification에 적합한 결과를 내놓는 경우가 많다고 한다.</p>


<h5>Decision process</h5>


<p>정리하자면, 최종적으로 decision을 내리기 위해서는 (1) Find parameter by minimizing risk (2) Decision by Model 이라는 과정을 거치게 된다. 먼저 minimize risk는 위에서 서술한 bayes risk를 minimization하는 부분으로, 이 부분에서 우리는 parameter들을 learning하게 된다. 이때 Loss function을 어떻게 정의하냐에 따라 parameter의 값이 결정되게 되며, 나중에 다루겠지만, 단순히 loss function을 정의하는 것으로 끝나는 것이 아니라, 이 optimization 문제를 polynomial algorithm을 통해 계산해내어야한다. 실제로 많은 algorithm들이 global optimum으로 수렴하지 않고 local optimum만을 찾아주는 경우가 많다. 이렇게 parameter를 learning한 이후에는 parameter를 사용해 posterior를 계산하게 된다. 이때 Model에 따라 inference를 하는 방법이 달라지게 되는데 discriminative model은 parameter를 사용해 directly하게 posterior를 계산하게 되고, generative model은 joint distribution을 learning하여 posterior를 indirectly하게 유추하게 된다.</p>




<h5>변경 이력</h5>


<ul>
<li>2014년 8월 19일: 글 등록</li>
<li>2015년 2월 28일: 변경 이력 추가</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li>Hidden Markov Model</li>
<li>Clustering (K-means, Gaussian Mixture Model)</li>
<li>EM algorithm</li>
<li>Dimensionality Reduction (LDA, PCA)</li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li>Neural Network Introduction</li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (4) Algorithm]]></title>
    <link href="http://SanghyukChun.github.io/60/"/>
    <updated>2014-08-10T21:17:00+09:00</updated>
    <id>http://SanghyukChun.github.io/60</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>Machine Learning을 제대로 이해하기 위해서는 알고리즘에 대한 이해가 필수적이다. 어떤 알고리즘이 좋은 것이고 어떤 알고리즘이 나쁜 것인지에 대한 구분이 이뤄져야지만 향후 논의하게 될 많은 주제들에 대해 얘기하기 쉬워진다. 어떤 문제가 풀 수 있는 문제이고 어떤 문제가 풀 수 없는 문제인가? 풀 수 있다 없다는 어떻게 정의하는가? 등에 대해 이해해야만 머신러닝 알고리즘들에서 얘기하는 local optimum이나 compuation complexity 등에 대해 이해할 수 있을 것이다. (잘 모르는 개념이더라도 영화 이미테이션 게임을 봤다면 금방 이해할 수 있을 것이다)</p>


<p><a href="57">이전 글</a>에서 머신러닝에서 알고리즘이란 어떤 의미가 있는지를 얘기했었다. 쉽게 생각하면, 우리가 원하는 형태로 모델을 정의한 이후에 그 모델을 어떻게 learning할 것인가, 즉, 어떤 알고리즘을 사용하여 model을 learning할 것인가에 대한 얘기를 하기 위해서는 알고리즘에 대해 반드시 짚고 넘어가야만 한다. 알고리즘 부분에서 반드시 짚고 넘어가야할 부분은 (1) Big O notation (2) P and NP (3) Reduction (4) NP Complete (5) Approximation Algorithm 정도라고 생각하기에 이 글에서는 이 정도 내용을 다루도록 하겠다.</p>


<h5 id="60-1-bigO">Algorithm, Big O notation</h5>


<p>먼저 algorithm이란 무엇인지에 대해 생각해보자. 알고리즘이란 것의 정확한 정의는 <a href="http://en.wikipedia.org/wiki/Algorithm">위키</a>를 참고하길 바란다. 알고리즘에 있어서 중요한 몇 가지를 꼽자면, 먼저 input이 정의가 되어야하며 output을 가져야한다. 즉, 알고리즘은 특정 데이터에 대해 동작해야하며, 해당 데이터에 대한 알고리즘의 결과를 출력해야만한다. 그리고 알고리즘은 반드시 어떤 '목적'을 가지고 있다. 즉, 내가 만약 특정 지점부터 다른 특정 지점으로 이동하는 가장 짧은 path를 찾는 알고리즘을 작성해야만한다면 해당 알고리즘의 목적은 shortest path를 찾는 것이고, input은 임의의 graph와 시작점, 그리고 끝점이 될 것이다. 마지막으로 출력값은 shortest path가 될 것이다. 이런 것을 행할 수 있는 일종의 procedure가 알고리즘이라고 할 수 있다. 하지만 우리는 그냥 임의의 알고리즘이 필요한 것이 아니라 '좋은' 알고리즘이 필요하다. 예를 들어서 Algorithm A는 shortest path를 찾는데 1시간이 걸리고, Algorithm B는 4초가 걸린다면 당연히 B를 사용해야할 것이다. 그렇다면 알고리즘의 좋다 혹은 나쁘다는 무엇으로 구분하느냐, <a href="http://en.wikipedia.org/wiki/Big_O_notation">Big O notation</a>의 역할이 바로 그것을 구분하는 역할을 하는 것인데, 이 notation은 해당하는 알고리즘이 '주어진 input의 크기에 대해' 계산량이 얼마나 필요하느냐를 indicate하는 notation이다. 표기는 O(n) 와 같은 꼴로 표시하게 된다. n은 input의 크기이다. 예를 들어 shortest path면 전체 graph의 node의 개수가 될 것이다. O notation은 일종의 upper limit로, 아무리 최악의 상황에서도 계산량이 O 안에 있는 양보다 적게 걸린다는 의미이다. 또한 만약 소요 시간이 2n 이거나 n+1 이거나 10000000000000000n 이어도 이 알고리즘들은 모두 O(n)이 된다. 이 정도 얘기는 조금만 구글링해도 많이 나오는 얘기니 여기까지만 적고, 진짜 중요한건 'polynomial time'일 것이다. 무슨 얘기이냐하면, 알고리즘의 실행시간이 input의 크기가 늘어나는 것에 대해 polynomial scale로 증가하는 알고리즘이 좋은 알고리즘이라는 뜻이다. 당연히 input에 대해 최대한 적게 증가하는 알고리즘이 좋기는 하지만, \( O(e^n) \) 보다는 \( O(n^4) \) 이 훨씬 더 좋다는 얘기이다. Exponential time이 소요되는 알고리즘은 사실상 거의 무한대의 시간이 걸린다고 봐도 될 정도로 절망적인 computation time을 의미하며, 제대로 활용 가능한 알고리즘이 되려면 그 알고리즘의 computation time은 반드시 polynomial time이어야한다.</p>


<p>Expotentially increase라는 말에는 정말 어마어마한 파괴력이 있다. 이것이 왜 절망적이냐하면, 우리가 linear한 10n 알고리즘을 가지고 있을 때 input의 size가 1, 2, 3 의 순으로 증가하더라도 여전히 computation time은 10, 20, 30이지만, exponential time이 필요한 경우에는 예를 들어 10^n이라고 한다면 10, 100, 1000만큼의 시간이 필요하다. 즉 input이 3배 증가했을 뿐인데 두 알고리즘은 1000/30 = 33.33 배 만큼의 성능차이가 나는 것이다. 만약 input size가 100이라면? \(10^{97}\) 만큼의 차이가 난다. Exponential이라는 것에는 이만큼의 파괴력이 있다. 그러나 polynomial time안에 풀 수 있다면, 100배가 증가했을 때, n과 \(n^2\)의 차이는 100에 불과하다. 이 때문에 polynomial time algorithm은 풀 수 있는 문제로 취급되고, exponential algorithm은 실제로 쓸 수 없는 알고리즘으로 취급받는 것이다. <a href="http://SanghyukChun.github.io/59#59-3-ms">이전 글의 model selection part</a>에서 '그리고 데이터가 많아지면 그런 validation set이 엄청나게 많아진다. 정확히는 exponential로 늘어나기 때문에 마냥 모든 데이터에 대해 cross-validation을 하는 것은 불가능하다.' 라는 표현을 했을 때 exponential 로 증가하는 validation이 불가능하다는 표현을 했던 것이다.</p>


<h5 id="60-2-np">P and NP</h5>


<p>P problem이란 해당하는 문제를 polynomial time안에 풀 수 있는 알고리즘을 제시할 수 있는 문제를 의미한다. 예를 들어 우리에게 주어진 데이터의 개수를 sorting하는 알고리즘은 \(n log n\) 의 시간이 필요하므로 P problem이라 할 수 있다. NP problem은 올바르지는 않지만 진짜 진짜 간단하게, 표현하면, 'polynomial time안에 풀 수 없는 엄청나게 어려운 문제' 라고 할 수 있다. 하지만 이것은 올바르지 않은 정의이며, 심지어 정말 polynomial time안에 풀 수 없는지 조차 아직 확실하지 않다. NP problem의 정확한 정의는 'problem이 주어지고 해당 problem에 대해 어떤 suggested solution이 주어졌을 때 polynomial time안에 그 solution이 맞는 solution인지 아닌지 구분할 수 있는 문제'라고 할 수 있다. 당연히 정확한 답을 polynomial time안에 풀 수 있는 P는 NP의 subset이다. 즉, NP는 P를 포함하는 set이라고 할 수 있다. 그렇다면 NP는 반드시 P보다 크다고 할 수 있을까? 이 문제를 얘기하려면 먼저 Reduction에 대해 다뤄야한다.</p>


<h5 id="60-3-red">Reduction</h5>


<p>Problem X에서 Y로의 Reduction이란 만약 우리가 Problem Y를 풀 수 있는 Algorithm을 가지고 있을 때, 이 algorithm을 사용해 problem X를 풀 수 있는 algorithm을 찾을 수 있다는 것을 의미한다. 즉, 우리가 problem X를 풀기위해서는 problem Y를 풀기만 하면 된다. 즉, 간단히 생각하면 문제 Y가 X보다는 더 어려운 문제라고 생각하면 된다. 만약 우리가 problem Y 를 풀기위한 algorithm 중에서 P인 algorithm을 가지고 있다면, 그리고 reduction을 polynomial time안에 할 수 있다면 우리는 반드시 problem X를 polynomial time안에 풀 수 있을 것이다.</p>


<h5 id="60-4-npc">NP Complete</h5>


<p>NP Complete problem은 모든 NP problem이 해당 problem으로 reduction될 수 있는 문제를 의미한다. 즉, 내가 그 어떤 NP problem을 제시하더라도 반드시 어떤 NP complete problem으로 reduction시키는 것이 가능하다. 그리고 <a href="http://en.wikipedia.org/wiki/Cook%E2%80%93Levin_theorem">Cook-Levin Theorem</a>에서 Boolean SAT problem이 NP problem이라는 것을 증명한다. 그렇다면 당연히 SAT problem을 통해서 다른 NP complete problem들을 찾을 수 있다. 우리가 많이 사용하는 NP complete problem들은 <a href="http://en.wikipedia.org/wiki/Independent_set_problem">Independent set problem</a>, <a href="http://en.wikipedia.org/wiki/Clique_problem">Clique problem</a>, <a href="http://en.wikipedia.org/wiki/Vertex_cover_problem">Vertex Cover problem</a>, <a href="http://en.wikipedia.org/wiki/Set_cover">Set Cover problem</a>, <a href="http://en.wikipedia.org/wiki/Subset_sum_problem">Subset Sum problem</a> <a href="http://en.wikipedia.org/wiki/Hamiltonian_path_problem">Hamiltonian path problem</a>, <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem">Travelling salesman problem</a>, <a href="http://en.wikipedia.org/wiki/Graph_coloring_problem">Graph Coloring problem</a> 등이 있다. 해당 문제들의 전체 목록은 <a href="http://en.wikipedia.org/wiki/NP-complete#NP-complete_problems">위키</a>에서도 볼 수 있다.</p>


<p>그렇다면 당연히 필연적으로 할 수 있는 질문은, 'NP complete problem을 polynomial time안에 풀 수 있는가?' 라는 질문이 되겠다. 당연히 위에 서술한 그 어떤 문제 중에서 단 하나라도 polynomial solution을 제시할 수 있다면 모든 NP 문제들을 P로 풀 수 있을 것이다. 이 질문이 그 유명한 <a href="http://en.wikipedia.org/wiki/P_versus_NP_problem">P=NP?</a> 문제가 되겠다. 그리고 또 안타깝게도 이 문제는 <a href="http://en.wikipedia.org/wiki/Millennium_Prize_Problems">Millennium Prize Problems</a>라고 해서 엄청나게 어려운, 상금이 걸려있는 문제 중 하나이다. 즉, 아직도 결론이 나지 않은 문제이다. 하지만 그럼에도 대부분의 사람들이 NP complete는 polynomial time안에 풀 수 없다고 생각하고 있으며, 그 때문에 현재 우리가 사용 중인 모든 보안 알고리즘들은 이 NP completeness에 기반하여 만들어져있다. (정확히는 숫자를 곱하는 것은 쉬우나, 주어진 숫자가 소수인지아닌지 구분하는 것은 NP hard problem이라는 특성을 사용한다. - NP hard는 NP complete만큼 어렵거나 그보다 더 어려운 문제를 의미.) 따라서 일단 해당 문제를 reduction했더니 NP complete problem이 튀어나온다면 그 문제는 polynomial안에 답을 낼 수 없는 문제가 되겠다.</p>


<h5 id="60-4-aa">Approximation Algorithm</h5>


<p>그렇다고 이 문제는 NPC problem이니까 풀지말자! 라고 넘기기에는 세상에 너무나 많은 문제들이 NP complete problem이다. 그래서 정확하지는 않지만 NP complete problem을 풀기위한 여러가지 방법들이 존재한다. (<a href="http://en.wikipedia.org/wiki/NP-complete#Solving_NP-complete_problems">위키</a> 참고) 그 중에서도 approximation algorithm은 정확한 답을 구하는 것이 아니라 approximated solution을 polynomial안에 얻어내는 알고리즘을 의미한다. 즉, 원래 알고리즘이 x라는 답을 줬을 때, \(\alpha\)-approximation algorithm은 \(\alpha\)x 라는 답을 주게 된다. 자세한 점은 <a href="http://en.wikipedia.org/wiki/Approximation_algorithm">위키</a> 참고. 즉, NPC problem이 절망적으로 어려운 문제인 것은 맞지만 마냥 절망만 하고 있을 필요는 없다는 의미이며, approximation 말고도 NPC를 해결하는 방법은 여러 방법들이 존재한다. 하지만 일단 가장 중요한 개념 중 하나라고 할 수 있다.</p>


<p>수 많은 경우, Machine Learning 문제를 해결하다보면, 해당 문제가 NP complete problem인 경우가 많이 존재한다. 따라서 절대적인 값을 구하는 것이 불가능하여 어쩔 수 없이 local optimum을 구하는 경우도 있고, 혹은 해당 문제를 정확히 일치하지는 않지만 polynomial time안에 구할 수 있는 문제로 바꾸어 문제를 해결하는 방법도 존재한다. 즉, Machine learning에 대해 공부하기 위해서는 algorithm에 대한 이해가 매우매우 필수적이라고 할 수 있을 것이다.</p>




<h5>변경 이력</h5>


<ul>
<li>2014년 8월 10일: 글 등록</li>
<li>2015년 2월 28일: 변경 이력 추가, 문장 표현 등 수정 (알고리듬->알고리즘)</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li>Hidden Markov Model</li>
<li>Clustering (K-means, Gaussian Mixture Model)</li>
<li>EM algorithm</li>
<li>Dimensionality Reduction (LDA, PCA)</li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li>Neural Network Introduction</li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
</feed>
