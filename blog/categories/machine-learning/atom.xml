<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-Learning | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2015-09-14T20:06:17+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recurrent Neural Network Regularization]]></title>
    <link href="http://SanghyukChun.github.io/89/"/>
    <updated>2015-09-14T19:26:00+09:00</updated>
    <id>http://SanghyukChun.github.io/89</id>
    <content type="html"><![CDATA[<p>최근 머신러닝 분야에서 가장 주목받고 있는 분야는 Deep learning이다. Recurrent Neural Network, 혹은 RNN은 Deep learning 연구 분야 중에서도 요즘 가장 활발하게 연구가 진행되고 있는 분야 중 하나이다. RNN은 sequencial data를 처리하는데에 적합한 형태로 디자인 되어있으며, 현재 language model, speech recognition, machine translation 등에서 우수한 결과를 성취하고 있는 neural network model 중 하나이다. 이 논문은 popular한 RNN 중 하나인 LSTM 모델을 regularization 시켜서 보다 기존 결과들보다 더 잘 동작하는 결과를 제안한다.</p>




<h5>Motivation: Regularization of Recurrent Neural Network</h5>




<p>RNN이 sequencial data에 대해 꽤 좋은 성능을 보이고 있는 것은 사실이지만, RNN을 regularization하는 방법은 많이 제안되어있지 않은 상황이다. 기존 MLP (Multi-Layer Perceptron, 혹은 feed-forward network) 쪽에서는 Dropout, ReLU 등의 컨셉들이 연구되면서 상당한 발전이 있었지만, 아직 RNN에는 Dropout조차 제대로 적용되지 않고 있다고 한다. Dropout을 붙이면 오히려 성능이 떨어지기 때문에 아직은 LSTM (Long-Short-Term-Memory) 모델에 dropout없이 연구가 진행되고 있는 모양이다. 이 논문에서는 LSTM에 dropout을 RNN의 특성을 잘 살린 형태로 적용하여 dropout이 잘 동작하도록 하는 방법을 제안한다.</p>




<h5 id="RNN-intro">RNN Introduction</h5>




<p>본 논문을 소개하기 전에 먼저 RNN에 대해 간단하게 설명을 하고 넘어가도록 하겠다. 이름에서도 알 수 있듯 일반적인 Feed-forward network와 RNN의 차이는 recurrent한 loop의 존재 유무이다. 이는 자기 자신을 향한 self-loop일 수도 있고, 아니면 cycle 형태이거나 아니면 undirected edge의 형태일 수도 있다. 보통 일반적으로 RNN이라 하면 아래 그림과 같이 hidden unit에 self-loop이 있는 형태를 일컫는 듯하다. (출처: Bengio Deep Learning book)</p>


<p><img src="/images/post/89-1.png" width="600"></p>

<p>이 그림에서도 알 수 있듯 self-loop의 존재는 RNN으로 하여금 자연스럽게 historical data를 현재 decision에 반영하도록 만들어준다. 즉, RNN 모델은 마치 HMM 등의 sequencial data를 처리하는 모델들처럼 동작하는 것이다. 실제 learning을 할 때는 시간에 대해 self-loop를 'unfold' 하여 마치 weight를 공유하는 deep layer를 연산하듯 update한다. RNN에 대한 더 자세한 설명은 추후 다른 포스팅을 통해 다룰 수 있도록 하겠다.</p>




<h5>Long-Short-Term-Memory (LSTM) Architecture</h5>




<p>기존 vanilla RNN은 long-term dependency를 가지도록 learnig을 하게되면 gradient vanishing이나 exploding 문제에 직면하기 쉬워진다. 이유는 dependency를 더 long-term으로 가져갈수록 gradient 값이 시간에 따른 곱하기 형태가 되어 gradient growth가 exponential해지기 때문이다 (역시 위와 마찬가지로 나중에 더 자세하게 다루도록 하겠다). 때문에 이를 해결하기 위한 아이디어 중 하나로 LSTM이라는 것이 존재한다.</p>




<p>LSTM은 historical information을 저장하기 위한 다소 복잡한 dynamics를 가지고 있다. "long term" memory라는 것은 memory cell (\(c_t\))에 저장되며, 시간에 따라, 그리고 주어진 input data에 따라 저장해둔 information을 얼마나 간직하고 있을지 forget gate (\(f_t\)) 라는 것을 통해 결정하게 된다. LSTM을 그림으로 표현하면 아래 그림과 같다. (출처: 논문)</p>


<p><img src="/images/post/89-2.png" width="600"></p>

<p>이를 수식으로 한 번 나타내어보자. 먼저 몇 가지 notation을 정의해보자. 먼저 모든 state는 n-dimension이라고 가정하자. \(h_t^l \in \mathbb R^n \)은 layer \(l\)의 timestamp \(t\)일 때의 hidden state라고 하자. 그리고 \(T_{n,m}: \mathbb R^n \to \mathbb R^m\) 을 n차원에서 m차원으로 가는 affine transform이라고 해보자. 예를 들어 parameter \(W\)와 \(b\)로 나타내어지는 \(Wx + b\)도 \(T_{n,m}\)에 포함된다 (즉, 이 논문에서는 복잡한 weight와 bias에 대한 식을 T라는 notation으로 간단하게 치환했다고 생각하면 된다). 마지막으로 \(\odot\)을 두 벡터의 element-wise multiplication이라고 정의해보자. 이렇게 notation을 정의하고 나면 일반적인 vanilla RNN을 다음과 같이 과거의 hidden state와 현재 hidden state의 이전 layer의 state로부터 현재 hidden state를 표현하는 function으로 표현할 수 있다.</p>


<p>\[\mbox{RNN: } h_t^{l-1}, h_{t-1}^l \to h_t^l, \mbox{ where } h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l). \]</p>


<p>이때, function \(f\)는 RNN의 경우 sigmoid나 tanh 함수 중 하나로 선택하는 것이 일반적이다. 그럼 이번에는 LSTM을 수식으로 표현해보자.</p>


<p><img class="center" src="/images/post/89-3.png" width="250"></p>

<p>앞에서 언급했던 LSTM의 graphical representation을 살펴보면서 수식을 읽어보면 어렵지 않게 이해할 수 있을 것이다.</p>




<h5>Dropout Regularization for LSTM</h5>




<p>저자들은 RNN에 Dropout을 붙였을 때 잘 동작하지 않는 이유가 dropout이 지워버리면 안되는 과거 information까지 전부 지워버리기 때문이라고 주장한다. 때문에 RNN에 Dropout을 적용하기 위해서는 recurrent connection이 아닌 connection 들에 대해서만 Dropout을 적용해야한다고 논문에서는 주장하고 있다. 아래 식에 조금 더 자세하게 적혀있다. 이때 \(\mathbf D\) 는 dropout operator라는 것으로, 주어진 argument의 random subset을 0으로 만들어버리는 operator이다. 즉, \(\mathbf D (h)\) 라고 한다면 vector \(h\) 중 random하게 고른 일부를 (보통 50%) 0으로 설정하라는 뜻이다.</p>


<p><img class="center" src="/images/post/89-4.png" width="250"></p>

<p>이를 그림으로 표현하면 아래와 같다. 이때 실선은 일반적인 connection이고, 점선이 dropout으로 연결된 connection을 의미한다. (출처: 논문)</p>


<p><img src="/images/post/89-5.png" width="600"></p>

<p>위 그림에서도 알 수 있듯, 과거에서부터 propagation되는 information은 언제나 100% 보존되지만, 아래 layer에서 위 layer로 전달되는 information은 특정 확률로 dropout에 의해 corruption되어 진행된다. 이때, 맨 아래 data layer로부터 맨 위 \(L\)번째 layer까지 information이 전달되는 동안 점선으로 그려진 connection은 정확하게 \(L+1\) 번 만큼만 지나게 된다. 만약 recurrent connection까지 dropout을 적용했다면, 이 횟수는 \(L+1\) 보다 항상 같거나 클 것이며, 더 long-term dependency를 가질수록 그 효과가 더 강해져서 우리가 원하는 과거 정보는 거의 다 희석되고, 결과적으로 안좋은 결과를 얻게 될 확률이 높아질 것이다.</p>


<p><img src="/images/post/89-6.png" width="600"></p>

<p>위 그림은 어떻게 information이 time t-2로부터 t+2까지 전달되는지 그 flow를 표현한 것이다. 굵은 선이 information path를 나타내는데, 앞서 설명한 것 처럼 이런 information flow는 data layer로부터 decision layer까지 정확하게 \(L+1\) 번만 dropout의 영향을 받게 된다. 반면 standard dropout을 적용했더라면 information이 더 많은 dropout들에 의해 영향을 받아서 LSTM이 정보를 더 긴 시간 동안 저장할 수 없도록 만들게 되는 것이다. 때문에 recurrent connection에 dropout을 적용하지 않는 것 만으로도 LSTM에서 좋은 regularization 효과를 얻을 수 있는 것이다.</p>




<h5>Experiments</h5>




<p>논문에서는 총 4개의 실험을 진행한다. Language Modeling (Penn Tree Bank - PTB dataset), Speech Recognition, Machine Translation 그리고 마지막으로 Image Caption Generation이 그것이다. 결과는 순서대로 아래 Table 1,2,3,4에 나열되어있다.</p>


<p><img src="/images/post/89-7.png" width="600"></p>

<h5>Summary of Recurrent Neural Network Regularization</h5>


<ul>
<li>Motivation: Dropout을 RNN에 그냥 적용하면 성능이 좋지 않다</li>
<li>Idea: Dropout을 모든 connection에 적용하는 대신 non recurrent connection에만 dropout을 적용하자</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1409.2329">Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. &ldquo;Recurrent neural network regularization.&rdquo; arXiv preprint arXiv:1409.2329 (2014).</a></li>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (18) Neural Network Introduction]]></title>
    <link href="http://SanghyukChun.github.io/74/"/>
    <updated>2015-09-13T23:13:00+09:00</updated>
    <id>http://SanghyukChun.github.io/74</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>최근 Machine Learning 분야에서 가장 뜨거운 분야는 누가 뭐래도 Deep Learning이다. 엄청나게 많은 사람들이 관심을 가지고 있고, 공부하고 응용하고 있지만, 체계적으로 공부할 수 있는 자료가 많이 없다는 것이 개인적으로 조금 안타깝다. 이제 막 각광받기 시작한지 10년 정도 지났고, 매년 새로운 자료들이 쏟아져나오기 때문에 책이나 정리된 글을 찾기가 쉽지가 않다. 그러나 Deep Learning은 결국 artificial neural network를 조금 더 복잡하게 만들어놓은 모델이고, 기본적인 neural network에 대한 이해만 뒷받침된다면 자세한 내용들은 천천히 탑을 쌓는 것이 가능하다고 생각한다. 이 글에서는 neural network의 가장 기본적인 model에 대해 다루고, model paramter를 update하는 algorithm인 backpropagation에 대해서 다룰 것이다. 조금 더 advanced한 topic들은 이 다음 글에서 다룰 예정이다. 이 글의 일부 문단은 <a href="http://SanghyukChun.github.io/blog/categories/neural-network/">이전 글들</a>을 참고하였다.</p>


<h5>Motivation of Neural Network</h5>


<p>이름에서부터 알 수 있듯 neural network는 사람의 뇌를 본 따서 만든 머신러닝 모델이다 (참고: 원래 neural network의 full name은 artificial neural network이지만, 일반적으로 neural network라고 줄여서 부른다). 본격적으로 neural network에 대해 설명을 시작하기 전에 먼저 인간보다 컴퓨터가 훨씬 잘 할 수 있는 일들이 무엇이 있을지 생각해보자.</p>


<ul>
<li>1부터 10000000까지 숫자 더하기</li>
<li>19312812931이 소수인지 아닌지 판별하기</li>
<li>주어진 10000 by 10000 matrix 의 determinant값 계산하기</li>
<li>800 페이지 짜리 책에서 &lsquo;컴퓨터&rsquo; 라는 단어가 몇 번 나오는지 세기</li>
</ul>


<p>반면 인간이 컴퓨터보다 훨씬 잘 할 수 있는 일들에 대해 생각해보자</p>


<ul>
<li>다른 사람과 상대방이 말하고자하는 바를 완벽하게 이해하면서 내가 하고 싶은 말을 상대도 이해할 수 있도록 전달하기</li>
<li>주어진 사진이 고양이 사진인지 강아지 사진인지 판별하기</li>
<li>사진으로 찍어보낸 문서 읽고 이해하기</li>
<li>주어진 사진에서 얼마나 많은 물체가 있는지 세고, 사진에 직접 표시하기</li>
</ul>


<p>컴퓨터가 잘 할 수 있는 0과 1로 이루어진 사칙연산이다. 기술의 발달로 인해 지금은 컴퓨터가 예전보다도 더 빠른 시간에, 그리고 더 적은 전력으로 훨씬 더 많은 사칙연산을 처리할 수 있다. 반면 사람은 사칙연산을 컴퓨터만큼 빠르게 할 수 없다. 인간의 뇌는 오직 빠른 사칙연산만을 처리하기 위해 만들어진 것이 아니기 때문이다. 그러나 인지, 자연어처리 등의 그 이상의 무언가를 처리하기 위해서는 사칙연산 그 너머의 것들을 할 수 있어야하지만 현재 컴퓨터로는 인간의 뇌가 할 수 있는 수준으로 그런 것들을 처리할 수 없다.</p>


<p>예를 들어 아래와 같이 주어진 사진에서 각각의 물체를 찾아내는 문제를 생각해보자 (출처: <a href="http://www.engadget.com/2014/09/08/google-details-object-recognition-tech/">링크</a>). 사람에게는 너무나 간단한 일이지만, 컴퓨터가 처리하기에는 너무나 어려운 일이다. 어떻게 어디부터 어디까지가 'tv or monitor'라고 판단할 수 있을까? 컴퓨터에게 사진은 단순한 0과 1로 이루어진 픽셀 데이터에 지나지 않기 때문에 이는 아주 어려운 일이다.</p>


<p><img class="center" src="/images/post/74-2.jpg" width="400"></p>

<p>그렇기 때문에 자연언어처리, 컴퓨터 비전 등의 영역에서는 인간과 비슷한 성능을 내는 시스템을 만들 수만 있다면 엄청난 기술적 진보가 일어날 수 있을 것이다. 그렇기 때문에 인간의 능력을 쫓아가는 것 이전에, 먼저 인간의 뇌를 모방해보자라는 아이디어를 낼 수 있을 것이다. Neural Network는 이런 모티베이션으로 만들어진 간단한 수학적 모델이다. 우리는 이미 인간의 뇌가 엄청나게 많은 뉴런들과 그것들을 연결하는 시냅스로 구성되어있다는 사실을 알고 있다. 또한 각각의 뉴런들이 activate되는 방식에 따라서 다른 뉴런들도 activate 되거나 activate되지 않거나 하는 등의 action을 취하게 될 것이다. 그렇다면 이 사실들을 기반으로 다음과 같은 간단한 수학적 모델을 정의하는 것이 가능하다.</p>


<h5>Model of Neural Network: neuron, synapse, activation function</h5>


<p>먼저 뉴런들이 node이고, 그 뉴런들을 연결하는 시냅스가 edge인 네트워크를 만드는 것이 가능하다. 각각의 시냅스의 중요도가 다를 수 있으므로 edge마다 weight를 따로 정의하게 되면 아래 그림과 같은 형태로 네트워크를 만들 수 있다. (출처: <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>)</p>


<p><img src="/images/post/74-1.png" width="600"></p>

<p>보통 neural network는 directed graph이다. 즉, information propagation이 한 방향으로 고정된다는 뜻이다. 만약 undirected edge를 가지게 되면, 혹은 동일한 directed edge가 양방향으로 주어질 경우, information propagation이 recursive하게 일어나서 결과가 조금 복잡해진다. 이런 경우를 recurrent neural network (RNN)이라고 하는데, 과거 데이터를 저장하는 효과가 있기 때문에 최근 음성인식 등의 sequencial data를 처리할 때 많이 사용되고 있다. 이번 ICML 2015에서도 RNN 논문이 많이 발표되고 있고, 최근들어 연구가 활발한 분야이다. 이 글에서는 일단 가장 간단한 'multi layer perceptron (MLP)'라는 구조만 다룰 것인데, 이 구조는 directed simple graph이고, 같은 layer들 안에서는 서로 connection이 없다. 즉, self-loop와 parallel edge가 없고, layer와 layer 사이에만 edge가 존재하며, 서로 인접한 layer끼리만 edge를 가진다. 즉, 첫번째 layer와 네번째 layer를 직접 연결하는 edge가 없는 것이다. 앞으로 layer에 대한 특별한 언급이 없다면 이런 MLP라고 생각하면 된다. 참고로 이 경우 information progation이 'forward'로만 일어나기 때문에 이런 네트워크를 feed-forward network라고 부르기도 한다.</p>


<p>다시 일반적인 neural network에 대해 생각해보자. 실제 뇌에서는 각기 다른 뉴런들이 activate되고, 그 결과가 다음 뉴런으로 전달되고 또 그 결과가 전달되면서 최종 결정을 내리는 뉴런이 activate되는 방식에 따라 정보를 처리하게 된다. 이 방식을 수학적 모델로 바꿔서 생각해보면, input 데이터들에 대한 activation 조건을 function으로 표현하는 것이 가능할 것이다. 이것을 activate function이라고 정의한다. 가장 간단한 activation function의 예시는 들어오는 모든 input 값을 더한 다음, threshold를 설정하여 이 값이 특정 값을 넘으면 activate, 그 값을 넘지 못하면 deactivate되도록 하는 함수일 것이다. 일반적으로 많이 사용되는 여러 종류의 activate function이 존재하는데, 몇 가지를 소개해보도록 하겠다. 편의상 \(t = \sum_i w_i * x_i\) 라고 정의하겠다. (참고로, 일반적으로는 weight 뿐 아니라 bais도 고려해야한다. 이 경우 \(t = \sum_i (w_i * x_i + b_i) \)로 표현이 되지만, 이 글에서는 bais는 weight와 거의 동일하기 때문에 무시하고 진행하도록 하겠다. - 예를 들어 항상 값이 1인 \(x_0\)를 추가한다면 \(w_0\)가 bais가 되므로, 가상의 input을 가정하고 weight와 bais를 동일하게 취급하여도 무방하다.)</p>




<ul>
    <li><p>sigmoid function: \(f(t) = \frac{1}{1+ e^{-t}}\)</p></li>
    <li><p>tanh function: \(f(t) = \frac{e^t - e^{-t}}{e^t + e^{-t} }\)</p></li>
    <li><p>absolute function: \(f(t) = \|t\|\)</p></li>
    <li><p>ReLU function: \(f(t) = max(0, t)\)</p></li>
</ul>




<p>보통 가장 많이 예시로 드는 activation function으로 sigmoid function이 있다. (출처는 위의 <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>와 같음)</p>


<p><img class="center" src="/images/post/74-4.png" width="300"></p>

<p>이 함수는 미분이 간단하다거나, 실제 뉴런들이 동작하는 것과 비슷하게 생겼다는 등의 이유로 과거에는 많이 사용되었지만, 별로 practical한 activation function은 아니고, 실제로는 ReLU를 가장 많이 사용한다 (2012년 ImageNet competition에서 우승했던 <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> publication을 보면, ReLU와 dropout을 쓰는 것이 그렇지 않은 것보다 훨씬 더 우수한 결과를 얻는다고 주장하고 있다. 이에 대한 자세한 내용은 다른 포스트를 통해 보충하도록 하겠다). 참고로 neuron을 non-linearity라고 부르기도 하는데, 그 이유는 activation function으로 linear function을 사용하게 되면 아무리 여러 neuron layer를 쌓는다고 하더라도 그것이 결국 하나의 layer로 표현이 되기 때문에 non-linear한 activation function을 사용하기 때문이다.</p>


<p>따라서 이 모델은 처음에 node와 edge로 이루어진 네트워크의 모양을 정의하고, 각 node 별 activation function을 정의한다. 이렇게 정해진 모델을 조절하는 parameter의 역할은 edge의 weight가 맡게되며, 가장 적절한 weight를 찾는 것이 이 수학적 모델을 train할 때의 목표가 될 것이다.</p>




<h5>Inference via Neural Network</h5>


<p>먼저 모든 paramter가 결정되었다고 가정하고 neural network가 어떻게 결과를 inference하는지 살펴보도록하자. Neural network는 먼저 주어진 input에 대해 다음 layer의 activation을 결정하고, 그것을 사용해 그 다음 layer의 activation을 결정한다. 이런 식으로 맨 마지막까지 결정을 하고 나서, 맨 마지막 decision layer의 결과를 보고 inference를 결정하는 것이다 (아래 그림 참고, 빨간 색이 activate된 뉴런이다).</p>


<p><img src="/images/post/74-3.png" width="600"></p>

<p> 이때, classification이라고 한다면 마지막 layer에 내가 classification하고 싶은 class 개수만큼 decision node를 만든 다음 그 중 하나 activate되는 값을 선택하는 것이다. 예를 들어 0부터 9까지 손글씨 데이터를 (MNIST라는 유명한 dataset이 있다) classification해야한다고 생각해보자. 그 경우는 0부터 9까지 decision이 총 10개이므로 마지막 decision layer에는 10개의 neuron이 존재하게 되고 주어진 데이터에 대해 가장 activation된 크기가 큰 decision을 선택하는 것이다.</p>




<h5 id="backprop">Backpropagation Algorithm</h5>


<p>마지막으로 이제 weight를 어떻게 찾을 수 있는지 weight paramter를 찾는 알고리즘에 대해 알아보자. 먼저 한 가지 알아두어야 할 점은 activation function들이 non-linear하고, 이것들이 서로 layer를 이루면서 복잡하게 얽혀있기 때문에 neural network의 weight optimization이 non-convex optimization이라는 것이다. 따라서 일반적인 경우에 neural network의 paramter들의 global optimum을 찾는 것은 불가능하다. 그렇기 때문에 보통 gradient descent 방법을 사용하여 적당한 값까지 수렴시키는 방법을 사용하게 된다.</p>


<p>Neural network (이 글에서는 multi-layer feed-forward network)의 parameter를 update하기 위해서는 backpropagation algorithm이라는 것을 주로 사용하는데, 이는 단순히 neural network에서 gradient descent를 chain rule을 사용하여 단순화시킨 것에 지나지 않는다 (Gradient descent에 대해서는 이전에 쓴 <a href="http://SanghyukChun.github.io/63">Convex Optimization글</a>에서 자세히 다루고 있으니 참고하면 좋을 것 같다). 모든 optimization 문제는 target function이 정의되어야 풀 수 있다. Neural network에서는 마지막 decision layer에서 우리가 실제로 원하는 target output과 현재 network가 produce한 estimated output끼리의 loss function을 계산하여 그 값을 minimize하는 방식을 취한다. 일반적으로 많이 선택하는 loss에는 다음과 같은 함수들이 있다. 이때 우리가 원하는 <a class="red tip" title="만약 MNIST라면 d=10">d-dimensional</a> target output을 \(t=[t_1, \ldots, t_d]\)로, estimated output을 \(x=[x_1, \ldots, x_d]\) 로 정의해보자.</p>




<ul>
    <li><p>sum of squares (Euclidean) loss: \(\sum_{i=1}^d (x_i - t_i)^2 \)</p></li>
    <li><p>softmax loss: \(-\sum_{i=1}^d t_i * \log (\frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }) \)</p></li>
    <li><p>cross entropy loss: \(\sum_{i=1}^d [ -t_i \log x_i - (1-t_i) \log (1-x_i) ]\)</p></li>
    <li><p>hinge loss: \(\max(0,1-t \cdot x)\), 이때 \(\cdot\)은 내적을 의미한다.</p></li>
</ul>




<p>상황에 따라 조금씩 다른 loss function을 사용하지만, classification에 대해서는 보통 softmax loss가 gradient의 값이 numerically stable하기 때문에 softmax loss를 많이 사용한다. 이렇게 loss function이 주어진다면, 이 값을 주어진 paramter들에 대해 gradient를 구한 다음 그 값들을 사용해 parameter를 update하기만 하면 된다. 문제는, 일반적인 경우에 대해 이 paramter 계산이 엄청 쉬운 것만은 아니라는 것이다.</p>


<p>Backpropagtaion algorithm은 chain rule을 사용해 gradient 계산을 엄청 간단하게 만들어주는 알고리즘으로, 각각의 paramter의 grdient를 계산할 때 parallelization도 용이하고, 알고리즘 디자인만 조금 잘하면 memory도 많이 아낄 수 있기 때문에 실제 neural network update는 이 backpropagtaion 알고리즘을 사용하게 된다.</p>


<p>Gradient descent method를 사용하기 위해서는 현재 parameter에 대한 gradient를 계산해야하지만, 네트워크가 복잡해지면 그 값을 바로 계산하는 것이 엄청나게 어려워진다. 그 대신 backpropataion algorithm에서는 먼저 현재 paramter를 사용하여 loss를 계산하고, 각각의 parameter들이 해당 loss에 대해 얼마만큼의 영향을 미쳤는지 chain rule을 사용하여 계산하고, 그 값으로 update를 하는 방법이다. 따라서 backpropagation algorithm은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>


<h6>Phase 1: Propagation</h6>


<ol>
    <li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 'forward' propagation이라 한다.)</li>
    <li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 'back' propagation이라 한다.)</li>
</ol>


<h6>Phase 2: Weight update</h6>


<ol>
    <li>Chain rule을 사용해 paramter들의 gradient를 계산한다.</li>
</ol>


<p>이때, chain rule을 사용한다는 의미는 아래 그림에서 나타내는 것처럼, 앞에서 계산된 gradient를 사용해 지금 gradient 값을 update한다는 의미이다. (그림은 bengio의 <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">deep learning book</a> <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/mlp.html">Ch6</a> 에서 가져왔다.)</p>


<p><img src="/images/post/74-6.png" width="400"></p>

<p>두 그림 모두 \(\frac{\partial z}{\partial x}\)를 구하는 것이 목적인데, 직접 그 값을 계산하는 대신, \(y\) layer에서 이미 계산한 derivative인 \(\frac{\partial z}{\partial y}\)와 \(y\) layer와 \(x\)에만 관계있는 \(\frac{\partial y}{\partial x}\)를 사용하여 원하는 값을 계산하고 있다. 만약 \(x\) 아래에 \(x^\prime\)이라는 parameter가 또 있다면, \(\frac{\partial z}{\partial x}\)와 \(\frac{\partial x}{\partial x^\prime}\)을 사용하여 \(\frac{\partial z}{\partial x^\prime}\)을 계산할 수 있는 것이다. 때문에 우리가 backpropagation algorithm에서 필요한 것은 내가 지금 update하려는 paramter의 바로 전 variable의 derivative와, 지금 paramter로 바로 전 variable을 미분한 값 두 개 뿐이다. 이 과정을 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -> hidden k, hidden k -> hidden k-1, ... hidden 2 -> hidden 1, hidden 1 -> input의 과정을 거치면서 계속 weight가 update되는 것이다. 예를 들어서 decision layer와 가장 가까운 weight는 직접 derivative를 계산하여 구할 수 있고, 그보다 더 아래에 있는 layer의 weight는 그 바로 전 layer의 weight와 해당 layer의 activation function의 미분 값을 곱하여 계산할 수 있다. 이해가 조금 어렵다면 아래의 <a href="74#example">예제</a>를 천천히 읽어보기를 권한다.</p>


<p>이 과정을 맨 위에서 아래까지 반복하면 전체 gradient를 구할 수 있고, 이 gradient를 사용해 parameter들을 update할 수 있다. 이렇게 한 번의 iteration이 진행되고, 충분히 converge했다고 판단할 때 까지 이런 iteration을 계속 반복하는 것이 feed-forward network의 parameter를 update하는 방법이다.</p>


<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network">링크</a>)</p>


<p><img src="/images/post/42-1.png" width="600"></p>

<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 '역으로' 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>




<h5>Stochastic Gradient Descent</h5>


<p>Gradient를 계산했으니 이제 직접 Gradient Descent를 써서 parameter만 update하면 된다. 그러나 문제가 하나 있는데, 일반적으로 neural network의 input data의 개수가 엄청나게 많다는 것이다. 때문에 정확한 gradient를 계산하기 위해서는 모든 training data에 대해 gradient를 전부 계산하고, 그 값을 평균 내어 정확한 gradient를 구한 다음 '한 번' update해야한다. 그러나 이런 방법은 너무나도 비효율적이기 때문에 Stochastic Gradient Descent (SGD) 라는 방법을 사용해야한다.</p>


<p>SGD는 모든 데이터의 gradient를 평균내어 gradient update를 하는 대신 (이를 'full batch'라고 한다), 일부의 데이터로 'mini batch'를 형성하여 한 batch에 대한 gradient만을 계산하여 전체 parameter를 update한다. Convex optimization의 경우, 특정 조건이 충족되면 SGD와 GD가 같은 global optimum으로 수렴하는 것이 증명되어있지만, neural network는 convex가 아니기 때문에 batch를 설정하는 방법에 따라 수렴하는 조건이 바뀌게 된다. Batch size는 일반적으로 메모리가 감당할 수 있을 정도까지 최대한 크게 잡는 것 같다.</p>




<h5 id="example">Backpropagation Algorithm: example</h5>


<p>이전에 chain rule로 gradient를 계산한다고 언급했었는데, 실제 이 chain rule이 어떻게 적용되는지 아래의 간단한 예를 통해 살펴보도록하자. 이때 계산의 편의를 위해 각각의 neuron은 sigmoid loss를 가지고 있다고 가정하도록 하겠다.</p>


<p><img src="/images/post/74-5.png" width="600"></p>

<p>이때 각각의 neuron의 input으로 들어가는 값을 \(in_{o_5}\), output으로 나가는 값을 \(out_{h_3}\)와 같은 식으로 정의해보자 (이렇게 된다면 in과 out은 \(out_{h_3} = \sigma(in_{h_3})\) 으로 표현 가능하다. - 이때 \(\sigma\)는 sigmoid function). 먼저 error를 정의하자. error는 가장 간단한 sum of square loss를 취하도록 하겠다. 우리가 원하는 target을 \(t\)라고 정의하면 loss는 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)가 될 것이다 (1/2는 미분한 값을 깔끔하게 쓰기 위해 붙인 상관없는 값이므로 무시해도 좋다). 그리고 우리가 원하는 값들은 \(\frac{\partial E}{\partial w_{13}}, \frac{\partial E}{\partial w_{14}}, \ldots, \frac{\partial E}{\partial w_{46}}\)이 될 것이다. 이제 가장 먼저 \(\frac{\partial E}{\partial w_{35}}\) 부터 계산해보자.</p>


<p>\[\frac{\partial E}{\partial w_{35}} = \frac{\partial E}{\partial out_{o_5}} * \frac{\partial out_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial w_{35}}. \]</p>


<p>즉, 우리가 원하는 derivative를 계산하기 위해서는 세 개의 다른 derivative (\(\frac{\partial E}{\partial out_{o_5}}, \frac{\partial out_{o_5}}{\partial in_{o_5}}, \frac{\partial in_{o_5}}{\partial w_{35}}\))를 계산해야한다. 각각을 구하는 방법은 다음과 같다.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{o_5}}\): error를 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)라고 정의했으므로, \(\frac{\partial E}{\partial out_{o_5}} = out_{o_5} - t_5\)이다. - 이때 \(out_{o_5}\)와 \(t_5\)는 weight update이전 propagation step에서 계산된 값이다.</p></li>
    <li><p>\(\frac{\partial out_{o_5}}{\partial in_{o_5}}\): \(o_5\)는 sigmoid activation function을 사용하므로 \(out_{o_5} = \sigma(in_{o_5})\)이다. 또한 sigmoid function의 미분 값은 \(\frac{\partial \sigma(x)}{\partial x} = \sigma(x) (1 - \sigma(x))\)으로 주어지므로, 이 값을 대입하면 \(\frac{\partial out_{o_5}}{\partial in_{o_5}} = out_{o_5} (1 - out_{o_5})\)가 된다. - 역시 여기에서도 미리 계산한 \(out_{o_5}\)를 사용한다.</p></li>
    <li>\(\frac{\partial in_{o_5}}{\partial w_{35}}\): \(o_5\)로 들어온 값의 총 합은 앞선 layer의 output과 \(o_5\)로 들어오는 weight를 곱하면 되므로 \(in_{o_5} = w_{35} out_{h_3} + w_{45} out_{h_4}\)이고, 이것을 통해 \(\frac{\partial in_{o_5}}{\partial w_{35}} = out_{h_3}\)가 됨을 알 수 있다. - \(out_{h_3}\) 역시 이전 propagation에서 계산된 값이다.</li>
</ul>


<p>따라서 \(\frac{\partial E}{\partial w_{35}}\)의 derivative 값은 위의 세 값을 모두 곱한 것으로 계산 할 수 있다. 그림으로 표현하면 아래와 같은 그림이 될 것이다. 즉, 'backward' 방향으로 derivative에 대한 정보를 'propagation'하면서 parameter의 derivative를 계산하는 것이다. 마찬가지 방법으로 \(w_{36}, w_{45}, w_{46}\)에 대한 derivative도 계산할 수 있다.</p>


<p><img src="/images/post/74-7.png" width="300"></p>

<p>그럼 이번에는 그 전 layer의 paramter들 중 하나인 \(w_{13}\)의 derivative를 계산해보자. 이번에 계산할 과정도 위와 비슷한 그림으로 표현해보면 아래와 같다.</p>


<p><img src="/images/post/74-8.png" width="300"></p>

<p>그러면 이제 \(\frac{\partial E}{\partial w_{13}}\)을 구해보자.</p>


<p>\[\frac{\partial E}{\partial w_{13}} = \frac{\partial E}{\partial out_{h_3}} * \frac{\partial out_{h_3}}{\partial in_{h_3}} * \frac{\partial in_{h_3}}{\partial w_{13}}.\]</p>


<p>마찬가지로 각각을 구하는 방법에 대해 적어보자.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{h_3}}\): \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)를 \(E = E_{o_5} + E_{o_6}\)로 decompose 하면 이 미분 식은 \(\frac{\partial E_{o_5}}{\partial out_{h_3}} + \frac{\partial E_{o_6}}{\partial out_{h_3}}\)로 쓸 수 있다. 각각의 계산은 다음과 같다.</p></li>
    <ul>
        <li><p>\(\frac{\partial E_{o_5}}{\partial out_{h_3}} = \frac{\partial E_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial out_{h_3}}\)으로 쓸 수 있다. 이 중 앞의 값인 \(\frac{\partial E_{o_5}}{\partial in_{o_5}}\)은 이미 전 과정에서 계산했던 \(\frac{\partial E}{\partial out_{o_5}}\)과 \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)의 곱으로 계산가능하다. 뒤의 값은 \(\frac{\partial in_{o_5}}{\partial out_{h_3}} = w_{35}\)이므로 간단하게 계산할 수 있다.</p></li>
        <li><p>\(\frac{\partial E_{o_6}}{\partial out_{h_3}}\)도 위와 같은 방법으로 연산이 가능하다.</p></li>
    </ul>
    <li><p>\(\frac{\partial out_{h_3}}{\partial in_{h_3}}\): \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)와 같다. 따라서 \(out_{h_3} (1-out_{h_3})\)이다.</p></li>
    <li><p>\(\frac{\partial in_{h_3}}{\partial w_{13}}\): \(\frac{\partial in_{o_5}}{\partial w_{35}}\)와 같다. 따라서 \(out_{i_1}\)이다.</p></li>
</ul>


<p>이렇게 \(\frac{\partial E}{\partial out_{h_3}}\)에서는 앞에서 계산했던 값들을 재활용하고, 아래의 값들은 activation function과 network의 topological property에 맞는 derivative를 곱하는 방식으로 \(\frac{\partial E}{\partial w_{13}}\)을 구할 수 있다.</p>


<p>이렇듯 backpropagation algorithm은 forward propagation을 통해 필요한 값들을 미리 저장해두고, backward propagation이 진행되면서 위에서부터 loss에 대한 derivative를 하나하나 계산해나가면서 다음 layer에서 바로 전 layer에서 계산한 값들과 각 neuron 별로 추가적으로 필요한 derivative들을 곱해나가면서 weight의 derivative를 계산하는 알고리즘이다.</p>


<p>이렇게 한 번 전체 gradient를 계산한 다음에는 learning rate를 곱하여 전체 parameter의 값을 update한 다음, 다시 처음부터 이 과정을 반복한다. 보통 에러가 감소하는 속도를 관측하면서 '이 정도면 converge한 것 같다' 하는 수준까지 돌린다.</p>


<p>익숙해지려면 다소 시간이 걸리지만, 개념적으로 먼저 'error를 먼저 계산하고, 그 값을 아래로 전달해나가면서 바로 전 layer에서 계산한 미분값들을 사용해 현재 layer의 미분값을 계산한 다음, 그 값을 사용해 다음 layer의 미분값을 계산한다.' 라고 개념만 이해해두고 다시 차근차근 chain rule을 계산해나가면서 계산하면 조금 편하게 익숙해 질 수 있을 것이다.</p>




<h5>정리</h5>


<p>Deep learning을 다루기 위해서는 가장 먼저 aritifitial neural network의 model에 대한 이해와 gradient descent라는 update rule에 대한 이해가 필수적이다. 이 글에서는 가장 기초적이라고 생각하는 feed-forward network의 model을 먼저 설명하고, paramter를 update하는 gradient descent algorithm의 일종인 backpropagation에 대한 개념적인 설명을 다루었다. 조금 어려울 수 있는 내용이니 다른 글들을 계속 참고하면서 보면 좋을 것 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 13일: 글 등록</li>
<li>2015년 9월 14일: 오타수정, SGD 내용 추가 등</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
<li>wiki (<a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks">링크</a>)</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Batch Normalization]]></title>
    <link href="http://SanghyukChun.github.io/88/"/>
    <updated>2015-08-25T21:25:00+09:00</updated>
    <id>http://SanghyukChun.github.io/88</id>
    <content type="html"><![CDATA[<p>Batch Normalization은 현재 <a href="http://image-net.org/challenges/LSVRC/2015/">ImageNet competition</a>에서 state-of-art (Top-5 error: 4.9%)를 기록하고 있는 Neural Network model의 기본 아이디어이다. 이 글에서는 arXiv에 제출된 (그리고 ICML 2015에 publish된) <a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 논문을 리뷰하고, batch normalization이 어떤 기술이고, 어떤 원리에 의해 작동하는지 등에 대해 다룰 것이다.</p>


<h5>Motivation: Deep learning의 속도를 어떻게 더 빠르게 만들 수 있을까?</h5>


<p>Deep learning이 잘 동작하고, 뛰어난 성능을 보인다는 것은 이제 누구나 알고 있다. 그러나 여전히 deep learning은 굉장히 시간이 오래 걸리는 작업이고, 그만큼 computation power도 많이 필요로 한다. 그 동안의 연구 결과를 보면, converge한 것 처럼 보이더라도 더 많이 돌리게 된다면 더 좋은 결과로 수렴한다는 것을 알 수 있는 만큼, deep neural network의 train 속도를 높이는 것은 전체적인 성능 향상에 도움이 될 것이다.</p>


<p>보통 Deep learning을 train할 때에는 stochastic gradient descent (SGD) method를 사용한다. SGD의 속도를 높이는 가장 naive한 방법은 learning rate를 높이는 것이지만, 높은 learning rate는 보통 gradient vanishing 혹은 gradient exploding problem을 야기한다는 문제가 있다.</p>


<p>Gradient vanishing은 backpropagation algorithm에서 아래 layer로 내려갈수록, 현재 parameter의 gradient를 계산했을 때 앞에서 받은 미분 값들이 곱해지면서 그 값이 거의 없어지는 (vanish하는) 현상을 의미한다. Gradient exploding은 learning rate가 너무 높아 diverge하는 현상을 말한다. Learning rate의 값이 크면 이 두 가지 현상이 발생할 확률이 높기 때문에 우리는 보통 작은 learning rate를 고르게 된다. 그러나 우리는 이미 일반적으로 learning rate의 값이 diverge하지 않을 정도로 크면 gradient method의 converge 속도가 향상된다는 것을 알고 있다. 따라서 이 논문이 던지는 질문은 다음과 같다. 자연스럽게 나오는 궁금증은 Gradient vanishing/exploding problem이 발생하지 않도록 하면서 learning rate 값을 크게 설정할 수 있는 neural network model을 design할 수 있는가?</p>


<h5>Internal Covariate Shift: learning rate의 값이 작아지는 이유</h5>


<p>Gradient vanishing problem이 발생하는 이유에 대해서는 여러가지 설명이 가능하지만 (exploding은 그냥 우리가 값을 작게 설정하여 해결할 수 있다) 이 논문에서는 internal covariate shift라는 개념을 제안한다. Covariate shift는 machine learning problem에서 아래 그림과 같이 train data와 test data의 data distribution이 다른 현상을 의미한다. 아래 그림 참고 (<a href="http://blog.bigml.com/2014/01/03/simple-machine-learning-to-detect-covariate-shift/">출처</a>)</p>


<p><img src="/images/post/88-1.jpg" width="500"></p>

<p>이 논문에서는 단순히 train/test input data의 distribution이 변하는 것 뿐 아니라, 각각의 layer들의 input distribution이 training 과정에서 일정하지 않기 때문에 문제가 발생한다고 주장하며, 이렇게 각각의 layer들의 input distribution이 consistent하지 않은 현상을 internal convariate shift라고 정의한다. 이 논문에서 이것이 문제가 된다고 주장하는 이유는, 각각의 layer parameter들은 현재 layer에 들어오는 input data 뿐만 아니라 다른 model parameter들에도 영향을 받기 때문이라고한다. 즉, gradient vanishing problem이 발생하는 이유를 backpropagation 과정에서 아래로 내려갈수록 이전 gradient들의 영향이 더 커져서 지금 parameter가 거의 update되지 않는다고 설명하는 것과 같은 맥락이다.</p>


<p>기존에는 이런 현상을 방지하기 위하여 ReLU neuron을 사용하거나 (Nair & Hinton, 2010), cafeful initialization을 사용하거나 (Bengio & Glorot, 2010; Saxe et al., 2013), leanring rate를 작게 취하는 등의 전략을 사용했지만, 그런 방법이 아닌 다른 방법을 통해 internal covariate shift 문제가 해결이 된다면 더 높은 learning rate를 선택하여 learning 속도를 빠르게하는 것이 가능할 것이다.</p>


<p></p>

<h5>Navie approach: Whitening</h5>


<p>따라서 이 논문의 목표는 internal covariate shift를 줄이는 것이다. 그렇다면 internal covariate shift는 어떻게 줄일 수 있을까? 이 논문에서는 엄청 간단하게 input distribution을 zero mean, unit variance를 가지는 normal distribution으로 normalize 시키는 것으로 문제를 해결하며, 이를 whitening이라한다 (LeCun 1998, Wiesler & Ney 2011). 주어진 column data \(X\in R^{d\times n}\)에 대해 whitening transform은 다음과 같다.</p>


<p>\[\hat{X} = Cov(X)^{-1/2} X, Cov(X) = E[( X - E[X] ) ( X - E[X] )^\top ].\]</p>


<p>그러나 이런 naive한 approach에서는 크게 두 가지 문제점들이 발생하게 된다.</p>


<ol>
<li>multi variate normal distribution으로 normalize를 하려면 inverse의 square root를 계산해야 하기 때문에 필요한 계산량이 많다.</li>
<li>mean과 variance 세팅은 어떻게 할 것인가? 전체 데이터를 기준으로 mean/variance를 training마다 계산하면 계산량이 많이 필요하다.</li>
</ol>


<p>따라서 이 논문에서는 이런 문제점들을 해결할 수 있으면서, 동시에 everywhere differentiable하여 backpropagation algorithm을 적용하는 데에 큰 문제가 없는 간단한 simplification을 제안한다.</p>


<h5>Batch Noramlization Transform</h5>


<p>앞서 제시된 문제점들을 해결하기 위하여 이 논문에서는 두 가지 approach를 제안한다.</p>


<ol>
<li>각 차원들이 서로 independent하다고 가정하고 각 차원 별로 따로 estimate를 하고 그 대신 표현형을 더 풍성하게 해 줄 linear transform도 함께 learning한다</li>
<li>전체 데이터에 대해 mean/variance를 계산하는 대신 지금 계산하고 있는 batch에 대해서만 mean/variance를 구한 다음 inference를 할 때에만 real mean/variance를 계산한다</li>
</ol>


<p>먼저 naive approach에서 covariance matrix의 inverse square root를 계산해야했던 이유는 모든 feature들이 서로 correlated되었다고 가정했기 때문이지만, 각각이 independent하다고 가정함으로써, 단순 scalar 계산만으로 normalization이 가능해진다. 이를 수식으로 표현하면 다음과 같다.</p>


<p></p>

<p>d dimensional data \(x = (x^{(1)}, x^{(2)}, \ldots, x^{(d)})\)에 대해 각각의 차원 \(k\) 마다 다음과 같은 식을 계산하여 \(\hat x\)를 계산한다</p>


<p>\[\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}.\]</p>


<p>그러나 이렇게 correlation을 무시하고 learning하는 경우 각각의 관계가 중요한 경우 제대로 되지 못한 training을 하게 될 수도 있으므로 이를 방지하기 위한 linear transform을 각각의 dimension \(k\)마다 learning해준다. 이 transform은 scaling과 shifting을 포함한다.</p>


<p>\[y^{(k)} = \gamma \hat{x}^{(k)} + \beta.\]</p>


<p>이때 parameter \(\gamma, \beta\)는 neural network를 train하면서 마치 weight를 update하듯 같이 update하는 model parameter이다.</p>


<p>두 번째로, 전체 데이터의 expectation을 계산하는 대신 주어진 mini-batch의 sample mean/variance를 계산하여 대입한다.</p>


<p></p>

<p>이제 앞서 설명한 두 가지 simplification을 적용하여 다음과 같은 batch normalization transform이라는 것을 정의할 수 있다.</p>


<p><img src="/images/post/88-2.png" width="500"></p>

<p>이때, backpropagation에 사용되는 \(\gamma, \beta\) 그리고 layer를 위한 chain rule은 다음과 같이 계산된다.</p>


<p><img src="/images/post/88-4.png" width="500"></p>

<h5>Train/Inference with BN network</h5>


<p>앞에서 batch normalization transform을 각각의 layer input을 normalization하는데에 사용할 것이라는 설명을 했었다. 다시말해서 BN network는 기존 network에서 각각의 layer input 앞에 batch normalization layer라는 layer를 추가한 것과 구조가 동일하다.</p>


<p><img src="/images/post/88-5.png" width="500"></p>

<p>이때 자세한 알고리즘은 다음과 같다.</p>


<p><img src="/images/post/88-3.png" width="500"></p>

<p>주의해야할 점 하나는 train 과정에서는 mini-batch의 sample mean/variance를 사용하여 BN transform을 계산하였지만, inference를 할 때에도 같은 규칙을 적용하게 되면 mini-batch 세팅에 따라 inference가 변할 수도 있기 때문에 각각의 test example마다 deterministic한 결과를 얻기 위하여 sample mean/variance 대신 그 동안 저장해둔 sample mean/variance들을 사용하여 unbiased mean/variance estimator를 계산하여 이를 BN transform에 이용한다.</p>


<h5>BN network의 장점</h5>


<p>저자들이 주장하는 BN network의 장점은 크게 두 가지이다.</p>


<ol>
<li>더 큰 learning rate를 쓸 수 있다. internal covariate shift를 감소시키고, parameter scaling에도 영향을 받지 않고, 더 큰 weight가 더 작은 gradient를 유도하기 때문에 parameter growth가 안정화되는 효과가 있다.</li>
<li>Training 과정에서 mini-batch를 어떻게 설정하느냐에 따라 같은 sample에 대해 다른 결과가 나온다. 따라서 더 general한 model을 learning하는 효과가 있고, drop out, l2 regularization 등에 대한 의존도가 떨어진다.</li>
</ol>


<p>논문을 살펴보면 BN transform이 scale invariant하고, 큰 weight에 대해 작은 gradient가 유도되기 때문에 paramter growth를 안정화시키는 효과가 있다는 언급이 있다. 또한 regularization효과를 더 강화하기 위하여 매 mini-batch마다 training data를 shuffling하여 input으로 넣는데, 이때 한 mini-batch 안에서는 같은 데이터가 중복으로 나오지 않도록 shuffling하여 대입한다.</p>


<h5>실험</h5>


<p>먼저 BN network가 주장하는대로 잘 동작하는지 보여주는 실험이다.</p>


<p><img src="/images/post/88-6.png" width="500"></p>

<p>가장 왼쪽은 MNIST data에 대해 BN을 쓴 것과 쓰지 않은 것의 convergence speed를 비교한 것이며, 다음 그림들은 BN을 사용했을 때와 사용하지 않았을 때, internal covariate shift가 어떻게 변화하는지를 보여주는 것이다. 한 뉴런의 training 동안 activation value의 변화를 plot한 것으로, 가운데 있는 선이 평균 값이고, 위 아래가 variance를 의미한다고 생각하면 된다. BN을 사용하면 처음부터 끝까지 거의 비슷한 distribution을 가진다는 것을 알 수 있다.</p>


<p></p>

<p>다음으로 single network에 대해 inception network와의 성능을 비교한 실험이다</p>


<p><img src="/images/post/88-7.png" width="500"></p>

<p>세팅은 전부 같고 inception network와 비교하여 BN이 추가되었는지 여부와 learning rate가 몇 배인지 (x5는 5배의 leanring rate를 취한 것이다) 여부만 다르게 설정하였음에도 불구하고 convergence speed와 심지어 최종 max acc까지 차이가 나는 것을 볼 수 있다.</p>


<p>이런 결과들을 기반으로 약간의 paramter tunning을 거쳐 아래와 같은 ImageNet state-of-art를 기록했다고 한다.</p>


<p><img src="/images/post/88-8.png" width="500"></p>

<h5>BN 네트워크 성능 accelerating하기</h5>


<p>BN을 추가하는 것 만으로 성능 개선이 엄청나게 일어나는 것은 아니며 다음과 같은 parameter tunning이 추가로 필요하다고 한다.
1. learning rate 값을 키운다 (0.0075 &ndash;> 0.045, 5)
2. drop out을 제거한다 (BN이 regularization 효과가 있기 때문이라고 한다)
3. l2 weight regularization을 줄인다 (BN이 regularization 효과가 있기 때문이라고 한다)
4. learning rate decay를 accelerate한다 (6배 더 빠르게 가속한다)
5. local response normalization을 제거한다 (BN에는 적합하지 않다고 한다)
6. training example의 per-batch shuffling을 추가한다 (BN이 regularization 효과를 증폭시키기 위함이다)
7. photometric distortion을 줄인다 (BN이 속도가 더 빠르고 더 적은 train example을 보게 되기 때문에 실제 데이터에 더 집중한다고 한다)</p>

<p>이런 parameter tunning이 추가로 이루어지고 나면, 기존 neural network보다 ImageNet에서 훨씬 좋은 성능을 내는 neural network를 구성할 수 있다고 한다.</p>


<h5>Summary of BN network</h5>


<ul>
<li>아이디어: 각 nonlinearity의 input으로 BN transform을 추가한다</li>
<li>BN transform은 다음과 같은 두 가지 simplification으로 구성된다

<ul>
<li>각 feature들의 correlation을 무시하고 각각 따로 normalize 하고 각각에 대한 linear transform을 같이 learning한다</li>
<li>Mini-batch의 sample mean/variance로 normalize 한다</li>
</ul>
</li>
<li>BN network의 train/inference에서는 다음과 같은 특이점이 있다

<ul>
<li>Train/inference의 forward rule이 다르다 (각각이 사용하는 mean/variance가 다르다)</li>
<li>Train 과정에서 mini-batch를 (중복없이) shuffling하여 train시킨다</li>
</ul>
</li>
<li>BN network를 사용함으로써 다음과 같은 효과를 볼 수 있다

<ul>
<li>Learning rate를 큰 값으로 설정할 수 있어 converge가 빠르다</li>
<li>Generalized model을 learning하는 효과가 있다</li>
</ul>
</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, Sergey Ioffe, Christian Szegedy, ICML 2015</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (16) Dimensionality Reduction (PCA, LDA)]]></title>
    <link href="http://SanghyukChun.github.io/72/"/>
    <updated>2015-06-17T05:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/72</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p>Machine Learning problem을 풀다보면, 종종 high dimensional 데이터를 다뤄야할 일이 생긴다. 그런데 dimension 높은 데이터를 다루다보면 여러 문제가 발생하는데, 높은 dimension으로 인해 생기는 대표적인 문제가 <a href="http://SanghyukChun.github.io/59#59-4-cd">예전 글</a>에서 다뤘던 Curse of dimensionality이다. 또한 많은 algorithm들에서 dimension이 complexity에 영향을 주는 경우가 많으므로 높은 dimension은 알고리즘의 성능에 악영향을 미치는 경우가 많다. 그렇기 때문에 많은 경우 데이터의 dimension이 높다면 다양한 방식의 dimenionality reduction 기술을 적용해 데이터의 차원을 낮추는 작업을 한다. 일반적으로 많이 사용하는 Dimensionality Reduction으로는 LDA와 PCA가 있으며, ICA, CCA 등의 방법도 종종 사용되며, 그 이외에도 RBM, Auto-encoder 등의 Neural network와 관련된 모델들도 존재한다. 이 글에서는 가장 많이 사용되는 방법들인 LDA와 PCA에 대해서만 다룰 것이다.</p>


<h5>Recall: Curse of Dimensonality</h5>


<p>Curse of Dimensionality는 데이터의 차원이 높아질수록 발생하는 여러 문제들을 통틀어 일컫는 말이다. 이런 문제가 발생하는 이유는 차원이 높아질수록 우리가 일반적으로 사용하는 Euclidean distance가 예상치 못한 방식으로 동작하기 때문이다. 예를 들어 d-차원 공간에서 임의의 점으로부터 거리가 1인 점들을 모아놓은 공간을 생각해봤을 때, d가 점점 커지면 커질수록 그 구의 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 shell에 존재한다는 것을 이미 예전 글에서 증명한바 있다. 다시 말해서 아주 높은 차원의 데이터는 우리가 원하지 않는 방향으로 움직일 가능성이 크다.</p>


<h5>Feature Extraction</h5>


<p>많은 상황에서 차원의 크기는 feature의 개수를 의미한다. 예를 들어 키, 몸무게, 나이, 성별이라는 네 가지 정보를 가지고 클러스터링을 한다고 생각해보자. 이 경우 데이터의 차원은 4이다. 그런데 아마도 이 네 가지 정보 이외에도 소득, 학력, 자산크기 등의 정보 등을 추가로 사용해 클러스터링을 한다면 더 좋은 클러스터링이 가능할지도 모른다. 문제는, 모든 feature가 전부 의미있는 feature는 아닐 수 있다는 것이다. 몸무게 정보를 사용하여 클러스터링한 것과 사용하지 않고 클러스터링한 것 중에서 몸무게 정보를 사용하지 않고 클러스터링 한 것이 더 좋을 수도 있다는 것이다. 이렇게 주어진 정보들 중에서 정말 의미 있는 feature를 뽑아내는 과정을 feature extration이라고 한다. 가장 간단한 feature extraction은 모든 feature를 사용해보기도 하고 사용해보지 않기도 하면서 \(2^d\) 개의 조합을 모두 확인해보는 것이다. 그러나 이 방법은 차원의 크기에 exponential할 뿐 아니라, 만약 기존 feature가 highly correlate되어있고, 여러 개의 feature를 묶어서 한 feature로 만들어야 성능이 좋아지는 경우 등에 대해 좋은 성능을 내기 어렵다. 따라서 이런 상황에서도 dimensionality reduction 방법을 사용해 feature를 뽑아낼 수 있다. 만약 우리가 100개의 feature를 가지고 있을 때, '가장 좋은' 30개의 feature만 뽑기 위해서 30차원으로 dimensionality reduction을 하는 것이다.</p>


<h5>Dimensionality Reduction</h5>


<p>데이터의 차원을 낮춘다는 것의 의미는, 현재 데이터가 존재하는 차원에서 그보다 낮은 다른 차원으로 데이터들을 mapping시키는 map을 찾는다는 것과 같다고 할 수 있다. 이때 어떤 임의의 차원에서 그보다 낮은 임의의 낮은 차원으로 가는 mapping은 셀 수 없이 많다. 그렇다면 우리는 어떤 mapping을 선택해야할까? 예를 들어 가장 간단한 방법으로, d 차원 데이터를 d' 차원으로 보내고 싶을 때, 앞에서 설명했던 간단한 방법처럼 임의의 d' 개의 축을 골라서 그 축만 사용하는 방법도 있을 수 있고, <a class="red tip" title="projection이라고도 한다.">d에서 d'으로 가는 linear map</a>을 임의로 하나 고르는 것도 가능하다. 물론 non linear map도 가능하지만, 이 글에서 다룰 LDA와 PCA 두 가지 방법은 모두 linear mapping을 찾는 알고리즘이다. LDA는 supervised learning이며, PCA는 unsupervised learning에 해당하게 된다. 모든 문제에서 데이터는 matrix \(X\)로 표기되며, 데이터의 차원은 d이고, 개수는 n개이다. 따라서 \(X\)는 d by n matrix가 된다.</p>


<h5>LDA</h5>


<p><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis (LDA)</a>는 Dimensionality reduction만을 위한 방법은 아니다. LDA로 약어가 표시되는 것들이 꽤 많아서 (예: Latent Dirichelt Allocation) 이 모델을 처음 제안한 사람의 이름을 따서 Fisher's LDA 라고 부르기도 한다. LDA는 여러 클래스가 존재할 때 그 클래스들을 최대한 잘 분리시키는 projection을 찾는다. 철학은 굉장히 단순한데, projection 시킨 데이터들에서 같은 클래스에 속하는 데이터들의 variance는 최대한 줄이고 (\(\sigma_{within}\)), 각 데이터들의 평균 값들의 variance는 최대한 키워서 (\(\sigma_{between}\)) 클래스들끼리 최대한 멀리 떨어지게 만드는 것이다. 이를 수식으로 표현하면 다음과 같은 수식을 얻을 수 있다.</p>


<p>\[S = \frac{\sigma_{between}^2}{\sigma_{within}^2}\]</p>


<p>일단 가장 간단한 상황인 클래스가 2개일 때의 상황만 고려해보도록하자. 참고로 LDA의 결과는 항상 클래스 개수 - 1 개까지의 벡터 밖에 찾을 수 없기 때문에, 이 상황에서 우리가 찾게 될 projection은 1차원 projection을 찾는 것이므로 간단하게 vector \(w\)로 기술하도록 하겠다. 클래스가 2개 뿐이라면, 위의 식은 엄청 간단한 수식으로 바뀌게 된다. 먼저 \(\sigma_{between}\)은 데이터가 단 두 개 뿐이기 때문에 간단하게 \( (w \cdot \mu_1 - w \cdot \mu_2 )^2 \)으로 표현이 된다. 이때, \(\mu_1, \mu_2\)는 각각 1번째 클래스와 2번째 클래스에 속한 데이터들의 평균 값이다. 다음으로 \(\sigma_{within}\) 역시 어렵지 않게 계산할 수가 있다. Projection을 하게 되면 데이터의 variance는 \(w^\top \Sigma w\)로 표현이 되기 때문에, 1번 클래스와 2번 클래스에 대해 이 값을 계산하고 더해주기만 하면 된다. 식을 정리해보면</p>


<p>\[w = \arg\min_w \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_w\frac{(w \cdot \mu_1 - w \cdot \mu_2 )^2}{w^\top \Sigma_1 w + w^\top \Sigma_2 w} \]</p>


<p>그리고 위 식을 w에 대해 미분하고 좀 정리해보면 \(w \propto    (\Sigma_1 + \Sigma_2)^{-1}(\mu_1 - \mu_2) \) 라는 식을 얻을 수 있다.</p>


<h5>Multiclass LDA</h5>


<p></p>

<p>그러면 클래스가 2개보다 많을 때, 벡터 \(w\)가 아닌 subspace \(U\)를 찾는 과정은 어떻게 되는지 살펴보도록하자. 다시 \(\sigma_{within}\)과 \(\sigma_{between}\)을 살펴보자. 먼저 \(\sigma_{between}\)은 위와 비슷한 형태로 표현할 수 없다. 그러나 우리가 각각의 클래스의 평균을 \(\mu_i\)라고 정의한다면, 그냥 이 값들의 variance를 계산하기만 하면 된다. 이 variance는 \(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\)로 표현이 된다. 이때 \(\mu\)는 모든 평균들의 평균이다. 이 variance가 있으므로, projection하여 얻는 variance도 앞 뒤에 proejction matrix를 곱해 쉽게 계산할 수 있다. \(\sigma_{within}\)은 위와 비슷한 방식으로 \(\sigma_{within} = U^\top \left(\sum_i \Sigma_i \right) U \)로 표현할 수 있다. 이때 \(\Sigma_i\)는 i번째 클래스의 variance이다. 이 식을 정리해보면 다음과 같은 식을 얻는다.</p>


<p>\[U = \arg\min_U \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_U \frac{U^\top \left(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\right) U}{U^\top \left(\sum_i \Sigma_i \right) U} = \arg\max_U \frac{U^\top A U}{U^\top B} \]</p>


<p>\(A\)와 \(B\)는 각각 괄호 안에 있는 값을 의미한다. 위 식에서 보게 되면, 분자에 해당하는 부분이 rank가 클래스 개수 - 1 이기 때문에, 우리가 이 식을 풀었을 때도 \(U\)의 rank가 클래스 개수 - 1이 되므로 LDA가 구할 수 있는 subspace의 축 개수가 클래스 개수 - 1로 제한되는 것이다. 이 식을 풀기 위해서는 eigenvalue를 계산하는 것으로 간단하게 식을 풀 수 있다. 지금부터 왜 이 식이 eigenvalue를 푸는 것으로 해결이 가능한지를 살펴보자.</p>


<h5>General Eigenvector problem</h5>


<p>문제를 간단하게 하기 위해 전체 subspace가 아닌 vector \(w\)만 고려해보도록 하자. 따라서 식은</p>


<p>\[\min_w \frac{w^\top A w}{w^\top B w}\]</p>


<p>로 표현 가능하다. 이제 이 식을 w에 대해서 미분해보면 다음과 같은 식을 얻게 된다</p>


<p>\[\left(w^\top B w\right)^2 \big[ 2A w \left( w^\top B w \right) - 2B w \left( w^\top A w \right) \big] = 0\]</p>


<p>이를 잘 정리하면</p>


<p>\[Aw = \frac{w^\top A w} {w^\top B w} B w\]</p>


<p>로 표현이 되고, 우리가 원래 풀려고 했던 \(\frac{w^\top A w} {w^\top B w}\)를 \(\lambda\)라고 정의하면 식이 \(A w = \lambda B w\)라는 엄청 간단한 식이 나오게 된다. 이런 형태를 만족하는 \(\lambda\)를 찾는 문제를 general eigenvalue problem이라 부른다. 만약 \(B\)가 Identity matrix라면 우리가 아는 일반 eigenvalue problem이 된다. 따라서 원래 문제가 \(\lambda\)의 minimum을 찾는 것이었으니 이제 가장 작은 general eigenvalue를 찾기만 하면 우리가 풀고 싶었던 문제를 풀 수 있는 것이다.</p>


<h5>PCA</h5>


<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>는 Dimensionality reduction의 가장 대표적인 방법 중 하나이다. PCA는 projection된 데이터의 variance가 최대화되는 projection matrix를 찾는 문제이다. 그런데 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Low-rank_matrix_approximation">Eckart–Young theorem</a>에 의해서, 이 문제의 답이 데이터 \(X\)에 대한 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition(SVD)</a>으로 계산할 수 있다는 것이 알려져 있다. 그래서 PCA를 variance를 maximize하는 원래 정의대로 설명이 하는 경우도 많고, low rank matrix 문제로 설명하는 경우도 있다. 이 글에서는 low rank matrix estimation 문제로 설명하도록 하겠다. 이 문제의 objective는 아래 식과 같다.</p>


<p>\[\min_{rank(Z)=k} \| X - Z \|_F^2\]</p>


<p>이때 \(\|A\|_F\)는 <a href="http://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a>을 의미한다. 이 norm은 matrix의 모든 element들의 제곱을 더한 값이며, \(\| A \|_F^2 = {\tt tr} (A A^\top) = {\tt tr} (A^\top A) \) 라는 특성이 알려져있다. 이제 위의 식에서 \(Z\)를 UV decomposition한 후 대입해보면 다음과 같은 식을 얻는다.</p>


<p>\[\min_{U\in R^{d \times k}, V\in R^{n \times k}, U^\top U = I} \|X - UV^\top\|_F^2\]</p>


<p>이 식을 \(V\)에 대해 미분하면 optimal한 V는 \(V=X^\top U\)로 표현이 됨을 알 수 있으며, 이를 위의 식에 대입한 후, Frobenius norm의 성질을 잘 활용하면 아래와 같은 식이 나온다.</p>


<p>\[\max_{U\in R^{d \times k}, U^\top U = I} {\tt tr} (U^\top X X^\top U)\]</p>


<p>가 되며, 이 식의 답은 SVD를 통해 계산할 수 있다는 것을 알 수 있다. 그러나 만약 \(X\)의 mean이 0가 아니게 되면 UV decomposition을 하는 과정에서 문제가 생기게 되서 같은 방식으로 깔끔하게 구할 수가 없다. 이 과정은 다소 복잡하므로 생략하고 결과만 얘기하면, 그냥 \(\hat X = X - \frac{1}{n} X \mathbf 1\)을 SVD하면 된다. 뒤에 있는 term은 그냥 X의 평균값이다.</p>


<p>정리하면, 임의의 데이터 X에 대한 PCA는 다음과 같이 구할 수 있다.</p>


<ol>
    <li>데이터 \(X\)의 empirical mean을 계산한 후 모든 데이터에서 평균을 빼준다.</li>
    <li>새로 만들어진 데이터 \(\hat X\)의 가장 큰 singular value부터 k번째 큰 singular value까지에 대응하는 singular vector들을 구한다. (SVD를 통해)</li>
    <li>뽑아낸 k개의 singular vector로 U를 구성하고 return한다.</li>
</ol>


<p>쉽지만 그만큼 강력하다. 하지만 PCA의 경우 Frobenius norm의 제곱값을 사용하므로 각 element들이 norm을 계산할 때 한 번 제곱되고, 다시 전체에 제곱을 취할 때 또 제곱이 취해지므로 조금이라도 원래 데이터와 다른 outlier가 존재하게 된다면 그 효과가 굉장히 극적으로 증폭되기 때문에 noise에 취약하다. 이를 방지하기 위해 objective의 norm을 1-norm으로 바꾸거나 하는 등의 robust PCA 연구도 활발하게 진행되고 있다.</p>


<h5>정리</h5>


<p>Dimensionality Reduction은 매우 유용하고 많이 쓰이는 툴이다. 특히 PCA는 굉장히 빈번하게 사용되고 알고리즘도 매우 간단하기 때문에 알아두면 쓰임새가 많다. 이 글에서는 LDA와 PCA만 다뤘지만, ICA, CCA, RBM 등 굉장히 많은 dimensionality reduction 기술들이 존재한다. 이 중 RBM은 추후에 다시 한 번 자세하게 설명하도록 하겠다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 17일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Nie, Feiping, Jianjun Yuan, and Heng Huang. &ldquo;Optimal mean robust principal component analysis.&rdquo; Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (14) EM algorithm]]></title>
    <link href="http://SanghyukChun.github.io/70/"/>
    <updated>2015-06-14T03:50:00+09:00</updated>
    <id>http://SanghyukChun.github.io/70</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM 알고리즘</a>은 <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 하나이다. 굉장히 많은 probabilistic 모델을 풀기 위해 널리 사용되는 알고리즘 중 하나이며, iterative한 알고리즘 중 하나이다. Clustering에서 다뤘던 GMM은 물론이고, HMM, RBM 등의 문제를 해결하는데 있어서도 사용되는 알고리즘이다. 이 글에서는 EM 알고리즘이 무엇인지, latent variable이 존재하는 probabilistic model은 무엇이며 어떤 장점이 있는지를 다룰 것이며, EM 알고리즘의 의미와 더 나아가 이 알고리즘이 어떻게 MLE나 MAP문제를 해결하는지에 대해 다룰 것이다.</p>


<h5>Probabilistic model having latent variable</h5>


<p>EM 알고리즘에 대해 다루기 전에 먼저 latent variable을 가지고 있는 probabilistic model에 대해 설명하도록 하겠다. Latent variable은 우리가 본래 가지고 있는 random variable이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다. 예를 들어, 아래 그림과 같은 Grapical model을 고려해보자. 이때 우리가 관측할 수 있는 random variable은 paramter \(\theta\)로 parameterized 되어있는 \(\mathbf X\) 하나이고, \(\mathbf Z\)은 우리가 관측할 수 없는 hidden variable이라고 해보자.</p>


<p><img src="/images/post/70-1.png" width="200"></p>

<p>만약 위의 grapical model에서 \(\mathbf X\)의 maximum likelihood를 계산하고 싶다면 어떻게 해야할까? 먼저 \(\mathbf X\)의 maximum likelihood는 다음과 같이 표현된다.</p>


<p>\[\max_{\theta} p(\mathbf X | \theta) = \sum_{\mathbf Z} p(\mathbf X,Z | \theta). \]</p>


<p>문제를 조금 더 간단하게 하기 위하여 위의 식에서 \(\mathbf Z\)는 discrete variable이라고 정의하였다. 이 문제에서 우리가 가정할 것이 하나있다. 바로 marginal distribution \(p(\mathbf X | \theta)\)를 직접 계산하는 것이 매우 까다롭다는 것이다. 이때, \(\mathbf Z\)는 우리 마음대로 정할 수 있는 latent variable이기 때문에, joint distribution \(p(\mathbf X,Z | \theta)\)가 marginal distribution보다 쉬운 \(\mathbf Z\)를 잡는 것이 가능하다.</p>


<h5>Decomposition of log-likelihood</h5>


<p>만약 우리가 latent variable \(\mathbf Z\)의 marginal distribution을 \(q(\mathbf Z)\)라고 정의한다면, 앞에서 설명한 log-likelihood를 다음과 같이 decompose할 수 있다.</p>


<p>\[\ln p(\mathbf X | \theta) = \mathcal L(q,\theta) + ~\mbox{KL}(q\|p),\]
이때, \(\mathcal L(q,\theta)\)와 \(\mbox{KL}(q\|p)\)는 다음과 같이 정의된다. \[\mathcal L(q,\theta) = \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf X, \mathbf Z | \theta)}{q(\mathbf Z)} ~\mbox{and}~ \mbox{KL}(q\|p) = - \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf Z | \mathbf X, \theta)}{q(\mathbf Z)}. \]</p>


<p>위의 식에서 \(\mathcal L(q,\theta)\)는 hidden variable \(\mathbf Z\)의 marginal distribution \(q(\mathbf Z)\)의 functional이고, \(\mbox{KL}(q\|p)\)는 \(q,p\)의 KL divergence를 의미한다. 이렇게 log-likelihood를 decompose하게 되면, 한 쪽에는 random variable \(\mathbf X, \mathbf Z\)의 joint distribution, 그리고 또 한 쪽은 conditional distribution으로 표현이 된다는 것을 알 수 있다. 또한 KL divergence의 특성에서부터 재미있는 사실을 하나 더 유추할 수 있는데, 바로 KL divergence가 반드시 0보다 크거나 같기 때문에 \(\mathcal L(q,\theta)\)이 곧 log-likelihood의 lower bound가 된다는 사실이다. 이를 그림으로 나타내면 아래 그림의 오른쪽 그림과 같다. (일단 \(theta^{\mbox{old}}, theta^{\mbox{new}}\)는 무시하자)</p>


<p><img src="/images/post/70-2.png" width="500"></p>

<h5>EM algorithm</h5>


<p>위와 같은 사실로부터 lower bound가 maximum이 되도록하는 \(\theta\)와 \(q(\mathbf Z)\)의 값을 찾고, 그에 해당하는 log-likelihood의 값을 찾는 알고리즘을 설계하는 것이 가능할 것이다. 만약 \(\theta\)와 \(q(\mathbf Z)\)를 jointly optimize하는 문제가 어려운 문제라면 이 문제를 해결하는 가장 간단한 방법은 둘 중 한 variable을 고정해두고 나머지를 update한 다음, 나머지 variable을 같은 방식으로 update하는 alternating method일 것이다. EM 알고리즘은 이런 아이디어에서부터 시작하게 된다. EM 알고리즘은 E-step과 M-step 두 가지 단계로 구성된다. 각각의 step에서는 앞서 설명한 방법처럼 \(\theta\)와 \(q(\mathbf Z)\)를 번갈아가면서 한 쪽은 고정한채 나머지를 update한다. 이런 alternating update method는 한 번에 수렴하지 않기 때문에, EM 알고리즘은 E-step과 M-step을 알고리즘이 수렴할 때 까지 반복하는 iterative 알고리즘이 된다.</p>


<p>현재 우리가 가지고 있는 parater \(\theta\)의 값을 \(\theta^{\mbox{old}}\)라고 정의해보자. EM 알고리즘의 E-step은 먼저 \(\theta^{\mbox{old}}\) 값을 고정해두고 \(\mathcal L(q,\theta)\)의 값을 최대로 만드는 \(q(\mathbf Z)\)의 값을 찾는 과정이다. 이 과정은 매우 간단하게 계산 수 있는데, 그 이유는 log-likelihood \(\ln p(\mathbf X | \theta^{\mbox{old}})\)의 값이 \(q(\mathbf Z)\) 값과 전혀 관계가 없기 때문에, 항상 \(\mathcal L(q,\theta)\)를 최대로 만드는 조건은 KL divergence가 0이 되는 상황이기 때문이다. KL divergence는 \(q(\mathbf Z) = p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\) 인 상황에서 0이 되기 때문에, \(q(\mathbf Z)\)에 posterior distribution \(p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\)을 대입하는 것으로 해결할 수 있다. 따라서 E-step은 언제나 KL-divergence를 0으로 만들고, lower bound와 likelihood의 값을 일치시키는 과정이 된다.</p>


<p>E-step에서 \(\theta^{\mbox{old}}\)을 고정하고 \(q(\mathbf Z)\)에 대한 optimization 문제를 풀었으므로 M-step에서는 그 반대로, \(q(\mathbf Z)\)를 고정하고 log-likelihood를 가장 크게 만드는 새 paramter \(\theta^{\mbox{new}}\)을 찾는 optimization 문제를 푸는 단계가 된다. E-step에서는 update하는 variable과 log-likelihood가 서로 무관했기 때문에 log-likelihood가 증가하지 않았지만, M-step에서는 \(\theta\)가 log-likelihood에 직접 영향을 미치기 때문에 log-likelihood 자체가 증가하게 된다. 또한 M-step에서 \(\theta^{\mbox{old}}\)가 \(\theta^{\mbox{new}}\)로 바뀌었기 때문에 E-step에서 구했던 \(p(\mathbf Z)\)로는 더 이상 KL-divergence가 0이 되지 않는다. 따라서 다시 E-step을 진행시켜 KL-divergence를 0으로 만들고, log-likelihood의 값을 M-step을 통해 키우는 과정을 계속 반복해야만한다.</p>


<p>위에 나왔던 그림에서 왼쪽이 E-step을 의미하고, 오른쪽 그림이 M-step을 의미한다. E-step을 의미하는 왼쪽 그림에서 KL divergence는 0이 되고, lower bound인 functional과 log-likelihood의 값이 같아진다. 오른쪽 그림은 M-step을 표현하고 있으며, \(\theta\)가 update되면서 log-likelihood의 값이 증가하게 되지만, 더 이상 KL divergence의 값이 0이 아니게 된다. 이 과정을 더 이상 값이 변화하지 않을 때 까지 충분히 많이 돌리게 되면 이 값은 log-likelihood의 어떤 값으로 수렴하게 될 것이다. 그리고 매 step마다 항상 optimal한 값으로 진행하기 때문에 이 값은 log-likelihood의 local optimum으로 수렴하게 된다는 사실까지 알 수 있다. EM algorithm은 아래와 같은 그림으로 표현할 수 있다.</p>


<p><img src="/images/post/70-3.png" width="500"></p>

<p>각 curve는 \(\theta\) 값이 고정이 되어있을 때 \(q(\mathbf Z)\)에 대한 lower bound \(\mathcal L(q,\theta)\)의 값을 의미한다. 매 E-step마다 고정된 \(\theta\)에 대해 \(p(\mathbf Z)\)를 풀게 되는데, 이는 곧 log-likelihood와 curve의 접점을 찾는 과정과 같다. 또한 M-step에서는 \(\theta\) 값 자체를 현재 값보다 더 좋은 지점으로 update시켜서 curve 자체를 이동시키는 것이다. 이런 과정을 계속 반복하면 알고리즘은 언젠가 local optimum으로 수렴하게 될 것이다. Local optimum에 수렴한다는 성질은 얼핏보면 나빠보일 수도 있지만, 이 글의 도입부에서 latent variable이 introduce되는 이유 자체가 원래 log-likelihood를 계산하는 것이 불가능에 가깝기 때문이었다는 사실을 돌이켜본다면, latent variable을 잘 잡기만 한다면 반드시 local optimum으로 수렴하는 EM 알고리즘은 매우 훌륭한 알고리즘이라는 사실을 알 수 있다. 즉, 아예 문제를 풀지 못하는 것 보다는 local optimum으로 수렴하는 것이 훨씬 좋다.</p>


<h5>Pratical issues</h5>


<p>대부분의 probabilistic model의 MLE 혹은 MAP는 EM 알고리즘을 사용하면 구할 수 있다. 그러나 EM 알고리즘이 항상 잘 동작하는 것은 아닌데, E-step 혹은 M-step의 optimization 문제를 푸는 것이 어려운 상황이 그러하다. E-step은 posterior를 계산하는 과정이므로 크게 문제가 되는 경우는 많지 않지만, M-step은 \(\theta\)에 대한 optimization 문제를 풀어야하는 과정인데, 이 과정에서 문제가 발생하는 경우가 많다. 예를 들어 모든 \(\theta\)를 한 번에 jointly optimize하는 것이 어려워 또 다른 alternative method를 사용해야할 수 도 있다. 그렇게 되면 iterative 알고리즘 안에 nested iterative 알고리즘이 발생하게 되어 전체 알고리즘의 수렴 속도가 매우 느려지게 된다. 가장 단순하게 이 문제를 해결하는 방법으로는 nested iterative 알고리즘을 완전히 푸는 것이 아니라, 수렴여부와 관계없이 iteration을 조금만 돌리고 다시 E-step을 구하고, 다시 M-step을 정확히 푸는 대신 iteration을 몇 번만 돌리는 등의 방식이 있을 것이다. 일반적인 경우에는 이런 방식이 수렴하지 않지만, 몇몇 경우에는 이런 방식이 local optimum에 수렴한다는 것이 증명되어있다. 가장 대표적인 예가 RBM을 푸는 Contrastive Divergence이다. 이에 대한 더 자세한 설명은 추후에 Deep learning에 대해 다루는 글에서 더 자세히 다루도록 하겠다.</p>


<h5>정리</h5>


<p>EM 알고리즘은 latent variable이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 가장 대표적인 알고리즘이다. 본문에서는 MLE를 계산하는 과정만 다뤘지만, MAP도 비슷한 방식으로 구할 수 있다. Latent variable을 쉽게 잡기만한다면, 아무리 풀기 어려운 문제일지라도 구하고자 하는 문제의 local optimum에 수렴한다는 좋은 성질을 가지고 있기 때문에 매우 다양한 모델에서 이 알고리즘을 사용해 문제를 해결하고는 한다. 그러나 간혹 특히 M-step의 optimization을 푸는 과정에서 시간이 너무 오래 걸리는 문제가 발생하는 경우가 생길 수 있는데, 이런 경우 몇 가지 휴리스틱을 사용해 문제를 해결하기도 한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 14일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

]]></content>
  </entry>
  
</feed>
