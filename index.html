
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>README</title>
  <meta name="author" content="Sanghyuk Chun">

  
  <meta name="description" content="Playing Atari With Deep Reinforcement Learning (NIPS 2013) Sep 15th, 2015 | Comments 이번에 리뷰할 논문은 Google DeepMind가 NIPS 2013에 발표한 Deep Learning과 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://SanghyukChun.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/layout480.css" media="only screen and (max-width : 500px)" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="README" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
	<script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/bootstrap.js" type="text/javascript"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">



<script>
$(function() {
	$('.tip').attr('data-toggle','tooltip');
	$('.tip').attr('data-placement','top');
	$('.tip').tooltip();
});
</script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-42711199-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  
  <div id="main">
  	<header role="banner"><hgroup>
  <h1><a id="blog-title" href="/">README&nbsp;&nbsp; </a>
  
    <span style="white-space:nowrap;">SanghyukChun's Blog</span>
  
  </h1>
  <div class="clear"></div>
</hgroup>

</header>
  	<nav role="navigation"><ul class="main-navigation list-inline">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/search">Search</a></li>
  <li><a href="http://sanghyuk.kaist.ac.kr/">Homepage</a></li>
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
</ul>
</nav>
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/90/">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-09-15T19:56:00+09:00" pubdate data-updated="true">Sep 15<span>th</span>, 2015</time>
        
         | <a href="/90/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>이번에 리뷰할 논문은 Google DeepMind가 NIPS 2013에 발표한 Deep Learning과 Reinforcement Learning을 결합한 <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> 이라는 논문이다. 그 전에도 DNN과 RL을 결합하려는 시도는 있었지만, 거의 사람이 플레이하는 수준으로 의미있는 수준까지 도달한 work은 이 work이 처음인듯 하다. 글을 시작하기 전에 먼저 이 논문에서 한 결과부터 살펴보자. Atari라는 게임 콘솔을 가지고 이 논문의 method를 적용한 결과이다. 2분 5초부터 시작되는 터널링 전략이 진짜 걸작이다.</p>




<div style="text-align: center;"><iframe width="420" height="315" align="middle" src="https://www.youtube.com/embed/iqXKQf2BOSE" frameborder="0" allowfullscreen></iframe></div>




<p>충격적인 사실은 다른 hand-coding feature나 parameter 튜닝 없이 오직! vision data만 사용해서 이런 결과를 냈다는 사실이다. 빨간 공이 object고  빨간 판이 내가 움직이는거라는 기본적인 hand-craft feature 조차 없이 이런 결과를 냈다는 것이다. Deep leanring이 RL에서 뛰어난 성과를 보이지 못한 이유가 주로 데이터에 관련된 것이었음을 생각해보면 대단한 결과라고 할 수 있다. 최근에는 이 work을 기반으로 <a href="http://arxiv.org/abs/1406.6247">Image Attention (NIPS 2015)</a> 이라고 부르는 work도 나온 것 같다. Image Attention 논문은 다음에 정리해보도록하겠다.</p>




<h5>Background</h5>


<p>이 논문은 RL 중에서도 MDP에 초점을 맞추고 있으며 (사실 MDP가 아닌 RL은 거의 없다고 봐도 무방하지만) 그 중에서도 model-free technique 중 하나인 <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning algorithm</a>을 neural network를 통해 해결하고 있다. 이 글은 RL에 대해 어느 정도 기초적인 지식이 있다고 가정하고 쓸 것이기 때문에 조금 더 자세한 내용은 추후 작성할 RL 관련 포스트나 다른 reference들을 참고하면서 읽으면 좋을 것 같다.</p>


<p>Reinforcement Learning을 위해서는 먼저 환경 \(\mathcal E\)을 정의해야한다. 이 논문에서는 Atari 에뮬레이터가 환경이 될 것이다. Atari 에뮬레이터 환경을 구성하는 요소는 action의 sequence들, observe하는 화면과 최종 reward (점수)가 될 것이다. 그림으로 표현하면 다음과 같은 식이다.</p>


<p><img src="/images/post/90-6.png" width="600"></p>

<p>매 시간마다 agent는 legal game action \(\mathcal A = \{1, \ldots, K\}\) 중에서 action \(a_t\)를 하나 선택한다. 예를 들어 아래 그림과 같은 컨트롤러의 버튼 중 어떤 버튼을 누를 거인지를 결정하는 것이다. 옆으로 움직이는 버튼, 기타 다른 버튼들 하나하나가 \(\mathcal A\)의 element이며, 그 중 하나가 action \(a_t\)가 되는 것이다.</p>


<p><img class="center" src="/images/post/90-1.png" width="200"></p>

<p>게임을 하는 동안 컨트롤러의 버튼을 누르는 action들이 모이게 되면 현재 점수에 어떻게든 영향을 주게 되고, 그 결과로 최종 점수가 결정된다. 즉, Atari 게임 환경에서 reward \(r_t\)는 게임 score이며, 현재 내가 선택한 action은 바로 reward에 반영되는 것이 아니라 엄청나게 나중에 반영될 수도 있는 것이다.</p>


<p>또한 게임 조작을 통해 변화하는 것 중 우리가 관측할 수 있는 것은 실제 게임 화면의 pixel 값들 뿐이다 (이 논문에서는 time stamp \(t\)에서의 픽셀 값을 \(x_t\)라고 정의하였다). 때문에 이 논문에서는 vision 데이터를 사용해서 state를 정의하는데, state를 정의하는 방식이 상당히 재미있다. 간단하게 \(x_t\)를 state로 삼으면 될 것 같지만, 실제로는 화면 하나만 보고 알 수 있는 정보가 제한적이고 현재 상태를 정확하게 판단하기 위해서는 vision정보와 내가 행한 action을 포함한 과거 history들까지 모두 있지 않으면 안되기 때문에 이 논문은 state \(s_t\)를 action과 image의 sequence로 정의한다. 예를 들어 아래 스크린샷에서 공은 왼쪽으로 움직일까 오른쪽으로 움직일까? 만약 관성을 implement한 게임이라면 (움직이는 버튼에서 손을 떼도 조금 움직이는 게임이라면) 과연 play agent는 위로 올라가고 있을까 아니면 아래로 내려가고 있을까 그것도 아니면 멈춰있을까? 이렇듯 한 time stamp에 대한 vision 데이터로는 파악할 수 있는 데이터가 너무 제한적이기 때문에 모든 history가 반드시 필요하다.</p>


<p><img class="center" src="/images/post/90-2.png" width="300"></p>

<p>다시 말해서, \(s_t  = x_1, a_1, \ldots, a_{t-1}, x_t\) 가 된다. 이런 방식으로 modeling을 하게 되면 policy 혹은 strategy를 learning할 때 모든 과거 sequence를 고려해서 strategy가 결정되게 된다. 게임은 언젠가 끝나게 되어있기 때문에 모든 \(s_t\)는 finite한 길이를 가지고 있으며 (비록 어마어마하게 크지만) \(s_t\)의 domain 역시 유한하다. 따라서 이 모델은 엄청나게 large하지만 어쨌거나 finite한 Markov dicision process(MDP)가 된다.</p>


<p>우리의 목표는 에뮬레이터에 agent가 어떤 strategy를 통해 게임을 조작하여, 최종적으로 게임이 끝났을 때 게임에서 가장 높은 점수를 획득하는 것이다. 즉, reward에 대한 수학적 정의만 있다면 이 문제는 간단한 optimization 문제가 된다. 게임은 보통 시간 마다 reward를 받는다. 하지만 일반적으로 시간이 오래 지날수록 해당 reward의 가치는 점점 내려가는데 이를 고려하기 위하여 discount factor \(\gamma\)가 정의된다. 시간 \(t\)의 reward를 t라고 한다면 time \(T\)에서의 discount factor를 고려한 future reward는 다음과 같이 정의한다.</p>


<p>\[R_t = \sum_{t^\prime=t}^T \gamma^{t^\prime-t} r_{t^\prime}.\]</p>


<p>Reward function을 정의했으니 Q-function (action-value function) 역시 정의할 수 있다. \(\pi\)를 \(s_t\)에서 \(a_t\)를 mapping하는 policy function이라하면, optimal Q-function \(Q^*\)는 다음과 같이 정의할 수 있다.</p>


<p>\[Q^*(s,a) = \max_\pi \mathbb E \big[ R_t \big| s_t = s, a_t = a, \pi \big]. \]</p>


<p>MDP에서는 이 optimal action value function 혹은 optimal Q-function 하나만 제대로 알고 있다면 언제나 주어진 state에 대해 가장 \(Q^*\)의 값을 크게 만드는 action을 고르는 간단한 policy만으로도 반드시 항상 optimal한 action을 고를 수 있다는 이론적 결과가 있기 때문에 Q-function은 매우매우 중요하다. 이 논문에서도 \(Q^*\)를 찾는 neural network를 만들어서 문제를 해결하고 있으니, Q-function에 대한 이해가 필수적이다.</p>


<p>다시 본문으로 돌아와서, Optimal Q-function은 <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>이라는 중요한 특성을 따르게 된다. 이 equation을 intuition은 대충 이런식이다: seqeunce \(s^\prime\)의 다음 time stamp에서의 optimal Q-function \(Q^* (s^\prime, a^\prime)\) 값이 모든 action \(a^\prime\)에 대해서 알려져 있다면, optimal strategy는 \(r + \gamma Q^* (s^\prime, a^\prime)\)의 expected value를 maximize하는 것이라는 것이다. 이를 수식으로 표현하면 다음과 같다.</p>


<p>\[Q^*(s,a) = \mathbb E_{s^\prime \sim \mathcal E} \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].\]</p>


<p>많은 RL algorithm 들에서는 Q-function을 estimate하기 위하여 이 Bellman equation을 value iteration algorithm이라는 것을 통해 iterative update하여 구하게 된다. Value iteration algorithm에서는 매 \(i\) 번쨰 iteration마다 다음과 같은 procedure를 수행하게 된다.</p>


<p>\[Q_{i+1} (s,a) = \mathbb E \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].\]</p>


<p>이런 value iteration algorithm은 MDP에서 \(Q_i \to Q^* \mbox{ as } i\to\infty\)라는 것이 알려져 있다. 그러나 이런 방식은 이론적으로는 의미가 있을지 몰라도 실제로는 완전히 impractical한데, 이 과정을 모든 seqeunce에 대해 독립적으로 시행해야하기 때문이다. 따라서 보통은 action-value function을 다음과 같은 식으로 적절한 approximator를 사용하여 approximation한다.</p>


<p>\[Q(s,a;\theta) \simeq Q^* (s,a).\]</p>


<p>보통 linear function으로 approximation하지만, 간혹 non-linear function으로 모델링하는 경우도 있다. 예를 들어 deep network를 쓰는 방법이 있는데, 이 논문은 기존 방법들에 비해 RL에 적합한 Deep Network 모델을 제안하여 뛰어난 non-linear function approximator를 제안한다.</p>


<p>일반적으로 RL에 deep learning approach를 바로 적용했을 때 몇 가지 이슈가 생기게 되는데, 먼저 deep learning을 하기 위해서는 엄청나게 많은 hand labelled training data가 필요하지만, RL에서는 모든 state와 action에 대한 labelled data가 없기 때문에 이를 어떻게 handle해야할지를 모델에서 고려해야만한다. 또한 현재까지 연구된 많은 deep learning structure들은 data가 i.i.d.하다고 가정하지만, 실제 RL 환경에서는 state들이 엄청나게 correlated되어있기 때문에 제대로 된 learning이 어렵다. 예를 들어 위 핑퐁 게임 화면에서 한 프레임 더 지난 화면과 지금 화면은 정말 엄청나게 높은 correlation을 가지지만, standard feed-forward deep learning은 그것을 처리할 수 있는 모델이 아니기 때문에 문제가 발생한다. 이 논문은 그 문제를 experience replay라는 것으로 해결하는데, 자세한 내용은 다음 section에서 다루도록 하겠다.</p>




<h5>Deep Q-Learning</h5>


<p>앞에서 정의한 Q-function을 modeling한 network를 (\(Q(s,a;\theta) \simeq Q^* (s,a)\), 이때 \(\theta\)는 weight나 bias 등 neural network의 model parameter들) 이 논문에서는 Q-network라고 부르고 있다. 만약 parameter가 \(\theta\)가 정해진다면, 우리는 state \(s\)와 action \(a\)를 Q-network에 넣어서 forward pass를 돌리게 되면 해당하는 Q-value를 얻을 수 있을 것이다. 따라서 parameter가 정해진다면 주어진 \(s\)에 대해 모든 \(a\)에서의 Q-value를 얻을 수 있고, 이를 통해서 \(Q^*\)의 값과 그것을 achieve하는 action \(a^*\)를 구하는 것도 가능해진다. 다시 말해서 이 Q-network를 올바른 방향으로 update하는 알고리즘을 design하기만 한다면 주어진 문제를 해결할 수 있는 것이다. 이 논문에서는 Q-network가 우리가 원하는 목적대로 train되도록 하기 위하여 i 번째 iteration에서 아래와 같은 loss function을 가지도록 design한다. 이 논문은 현재 Q-network가 항상 target Q-value에 가까워지도록 loss를 설정함으로써 마치 value iteration이 converge하듯 Q-network의 update가 converge할 때 까지 iterative algorithm을 돌리도록 하는 것이다. 이때 \(y_i\)는 iteration i의 target value이고, \(\rho(s,a)\)는 sequence \(s\)와 action \(a\)의 probability distribution이며, 이를 이 논문에서는 behaviour distribution이라고 정의한다.</p>


<p>\[L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot)} \bigg[ \big(y_i - Q(s,a;w_i) \big)^2 \bigg], \]</p>


<p>\[\mbox{where, }y_i = \mathbb E_{s^\prime \sim \mathcal E} \bigg[r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime;w_{i-1}) ~\big|~ s, a \bigg]. \]</p>


<p>주의할 점은, optimization 과정에서 parameter \(\theta\)가 update되는 동안 loss function \(L_i(\theta_i)\) 의 이전 iteration paramter \(\theta_{i-1}\)은 고정된다는 것이다. 이를 &#8216;freeze target Q-network&#8217; 라고 부르는데, 이렇게하는 이유는 supervised learning과는 다르게, target의 값이 \(\theta\)의 값에 (민감하게) 영향을 받기 때문에 stable한 learning을 위하여 \(\theta\)값을 고정하는 것이다. 이건 아래에서 조금 더 자세하게 설명하도록 하겠다. 이제 loss function을 정의되었으므로 gradient값만 있다면 그 값을 사용해서 backpropagation을 돌리면 쉽게 update할 수 있다. 이 network의 loss의 gradient는 다음과 같이 구할 수 있다.</p>


<p>\[\nabla_{\theta_i} L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot); s^\prime \sim \mathcal E} \bigg[ \big( r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime; \theta_{i-1}) - Q(s,a;\theta_i) \big) \nabla_{\theta_i} Q(s,a;\theta_i)\bigg]\]</p>


<p>하지만 이렇게 build한 Deep RL을 바로 사용할 수는 없고, 아래와 같은 몇 가지 이슈들을 처리해야한다.</p>


<ul>
<li>Deep Learning은 데이터가 i.i.d.하다고 가정하지만 실제 RL input의 데이터는 sequential하고 highly correlated 되어있다.</li>
<li>Policy 변화에 따른 (이 경우는 w의 변화에 따른) Q-value의 변화량이 너무 크기 때문에 policy가 oscillate하기 쉽다.</li>
<li>위와 같은 세팅에서는 reward와 Q-value의 값이 엄청나게 커질 수 있기 때문에 stable한 SGD 업데이트가 어려워진다.</li>
</ul>


<p>첫 번째 이슈는 이미 전 문단에서 간단하게 언급했었으니 생략한다. 두 번째 문제도 크게 어렵지 않게 생각할 수 있는데, 게임을 하는 방식을 아주 조금만 바꾸더라도 게임의 결과가 완전히 크게 바뀌기 때문에 이런 현상이 발생한다. 핑퐁 게임에서 움직이는 속도를 조금 늦춘다거나 했다가는 바로 한 점을 잃게 될 것이다. 또한 앞에서 설명한 것 처럼 supervised learning과는 다르게 target의 값이 parameter에 영향을 아주 민감하게 받기 때문에, 이 값을 고정해주는 과정이 필요하다. 마지막 조건은 좀 practical한 이슈인데, Q-value의 값이 얼마나 커질지 모르기 때문에 stable update가 힘들 수도 있다. 이 논문에서는 다음과 같은 세 가지 방법으로 각각의 issue를 handling한다</p>


<ul>
<li>Experience replay</li>
<li>Freeze target Q-network</li>
<li>Clip reward or normalize network adaptively to sensible range</li>
</ul>


<p>이 중에서 두 번째 idea는 이미 설명했고 (update하는 동안 target을 계산하기 위해 사용하는 paramter를 고정), 세 번째 idea는 reward의 값을 [-1,0,1] 중에서 하나만 선택하도록 강제하는 아이디어이다. 즉, 내가 100점을 얻거나 10000점을 얻거나 항상 reward는 +1 이다. &#8216;highest&#8217; score를 얻는 것은 불가능하지만, 이렇게 설정함으로써 조금 더 stable한 update가 가능해진다. 그리고 그와는 별개로 모든 게임에 적용가능한 DQL을 learning할 수 있다는 장점도 있다. 실제로 실험에서는 모든 게임을 단 하나의 네트워크로만 learning해서 기존의 모든 방법을 beating한다.</p>


<p>그럼 이제 마지막으로 이 논문의 핵심 아이디어라고 할 수 있는 experience replay에 대해 살펴보자. Experience replay는 agent의 experine를 각 time stamp마다 다음과 같은 튜플 형태로 메모리 \(\mathcal D = e_1, \ldots, e_N\) 에 저장한 후 이를 다시 사용하는 것이다.</p>


<p>\[e_t = (s_t, a_t, r_t, s_{t+1}).\]</p>


<p>Experience replay는 이렇게 experience \(e_t\)를 메모리 \(\mathcal D\)에 저장해두었다가, 일부를 uniformly random하게 sample하여 mini-batch를 구성한 다음 parameter \(\theta\)를 mini-batch에 대해 backpropagation으로 update하는 과정을 의미한다.</p>


<p>Experience replay를 사용함으로써 data의 correlation을 깰 수 있고, 조금 더 i.i.d.한 세팅으로 network를 train할 수 있게 된다. 또한 방대한 과거 데이터가 한 번만 update되고 버려지는 비효율적 접근이 대신에, 지속적으로 추후 update에도 영향을 줄 수 있도록 접근하기 때문에 데이터 사용도 훨씬 효율적이라는 장점이 있다. 실제로 실험에서는 메모리 용량의 한계 때문에 bucket을 \(N\)으로 고정하고, FIFO 형태로 저장을 한 모양이다.</p>


<p>Experience replay가 끝난 후 agent는 action을 \(\epsilon\)-greedy policy라는 것을 사용해 선택하고 실행한다. 이 방법은 action을 \(\epsilon\)의 확률로 random하게 고르거나 \(1-\epsilon\)의 확률로 MDP의 optimal action selection criteria인 \(a_t = \arg\max_a Q^*(s_t, a;\theta) \)로 고르는 policy를 의미한다.</p>


<p>참고로, arbitrary length의 input을 다루는 것이 general feed-forward network에서는 어렵기 때문에, 이 논문에서는 function \(\phi\)라는 것을 정의해서 모든 \(s_t\)의 length를 fix한다. 이 알고리즘을 수식적으로 기술하면 다음과 같이 기술할 수 있다.</p>


<p><img src="/images/post/90-3.png" width="600"></p>

<p>Equation 3은 위에서 증명한 gradient 값이고, 함수 \(\phi\)는 다음과 같다: 주어진 history 중에서 가장 마지막 4개의 frame을 stack으로 쌓는 것. 이때 각각의 frame은 원래 128색 210 by 160 픽셀으로 구성되어있지만, gray-scale로 만들고 110 by 84로 down sampling한 후 84 by 84로 크롭한다. 이때 크롭을 하는 이유는 주어진 툴에서 정사각형 사진만 GPU 연산이 되기 때문이라고..</p>


<p>이렇게 experience replay라는 아이디어를 사용한 DQL은 몇 가지 이점이 있다.</p>


<ul>
<li>각각의 experience가 potentially 많은 weight update에 reuse되기 때문에 experience를 weight update 한 번에만 사용하는 기존 방법보다 훨씬 data efficiency하다.</li>
<li>두 번째로, mini-batch를 만드는 sampling 과정을 통해 데이터들 간의 high correlation을 효율적으로 관리하고, 이를 통해 보다 효율적인 update를 할 수 있다. 이 방법은 random하게 sample을 뽑아서 mini-batch로 구성하기 때문에 이런 high correlation을 break해서 update의 효율성을 높이기 때문이다.</li>
<li>마지막으로 이 방법을 통해 parameter를 update하게 되면 다음 training을 위한 data sample을 어느 정도 determine할 수 있다. 예를 들어서 내가 지금 오른쪽으로 움직이는 쪽으로 action을 고른다면 다음 sample들은 내가 오른쪽에 있는 상태의 sample들이 dominate하게 나올 것이라고 예측할 수 있다. 따라서 이 방법을 통해 training을 위한 다음 데이터를 무작정 뽑는 것이 아니라 현재 action을 고려하여 효율적으로 뽑을 수 있다.</li>
</ul>


<p>몇 가지 주의점이라면 freeze target Q-network, 혹은 off-policy가 반드시 필요하다는 점 정도가 있겠다. 한계점으로는 메모리의 한계 때문에 앞에서 말한 것처럼 모든 history를 저장하지 못한다는 점과, uniform sampling을 사용하기 때문에 모든 과거 experience가 동일한 weight를 가진다는 점이다. 이 논문에서는 조금 더 wise한 sampling을 하게 되면 성능 향상이 있을 수도 있다고 언급하고 있다.</p>




<h5>Model Architecture of DQL and Experiment</h5>


<p>이제 구체적으로 어떤 neural network 모델을 사용해 learning을 하게 될지 알아보자. 항상 관심있게 살펴봐야 할 내용은 (1) input data는 무엇인가 (2) output data는 무엇인가 (3) 구체적인 network 구조는 어떻게 되는가 정도가 있겠다. 이 논문에서는 input으로 \(\phi(s_t)\)를 받고, output으로 가능한 모든 action에 대한 Q-value를 출력한다. 즉, 버튼이 4개 있다면 output은 4개이고, 12개 있다면 output은 12개이다. 실제 논문에서는 게임 종류에 따라 action을 4개에서 18개 사이에서 고른 것 같다. 이렇게 모델을 고르게 되면 \(Q^*\)를 단 한번의 forward pass 만으로 구할 수 있다는 장점이 있기 때문에 이 논문에서는 이러한 방법을 선택하였다. 구체적인 네트워크는 CNN을 사용한다. 인풋 데이터는 \(\phi(s_t)\)를 넣는다고 했으므로 84 by 84로 크롭한 후에 4개의 history를 stack으로 쌓은 데이터가 들어오게 된다. 그러니까 대충 다음과 같은 식이다.</p>


<p><img src="/images/post/90-5.png" width="600"></p>

<p>이 네트워크를 사용한 자세한 실험 결과는 다음 표에 나와있다. 각각의 숫자는 모두 reward를 의미하기 때문에 값이 클 수록 좋은 결과이다.</p>


<p>
<img src="/images/post/90-4.png" width="600"></p>

<p>Breakout, Enduro, Pong은 심지어 사람보다도 좋은 것을 알 수 있고, Space Invaders를 제외하면 기존 모델들보다 best 뿐 아니라 avg 까지 뛰어난 것을 볼 수 있다. 논문에서는 Q*bert, Seaquest, Spae Invaders 등의 게임에서 사람의 performance에 한참 미치지 못하는 이유로 이 게임들은 전략이 엄청나게 긴 time scale로 필요하기 때문에 조금 더 challenge한 문제라고 주장하고 있다. 아마 하드웨어의 발달과 모델의 발달로 언젠가는 극복할 수 있을 것으로 보인다.</p>




<h5>Summary of DQL</h5>


<ul>
<li>문제 정의 자체가 흥미롭다. 특히 state space를 sequence of iamges and action으로 구성했다는 점이 흥미롭다.</li>
<li>State에 대한 hand-craft feature가 전혀 없다. 오직 이미지 sequence만을 사용해서 CNN으로 feature를 자동으로 만들어내는 방법으로 이를 해결하고 있다는 점이 흥미롭다.</li>
<li>Q-function을 learning하는 neural network를 구성하였는데 몇 가지 stable update를 위하여 &lsquo;off-policy&#8217;를 사용하고, &#8216;experience replay&rsquo; 기법을 사용한다.</li>
<li>Experience replay는 매 시간마다 experience tuple (e_t)를 메모리에 저장하고, 메모리에서 (e_t)를 uniformly sample하여 뽑아 mini-batch를 구성하고 이를 (off-policy를 적용한 채로 혹은 target network를 freeze하고) 사용해 parameter를 update하는 아이디어이다.</li>
<li>서로 다른 Atari 게임 7개에 대한 policy를 learning하기 위해 단 하나의 neural network만을 사용했고, 그 결과가 기존 결과를 outperform한다.</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1312.5602">Mnih, Volodymyr, et al. &ldquo;Playing atari with deep reinforcement learning.&rdquo; NIPS (2013).</a></li>
<li><a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf">Deep Reinforcement Learning &ndash; ICLR 2015 tutorial</a></li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/89/">Recurrent Neural Network Regularization</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-09-14T19:26:00+09:00" pubdate data-updated="true">Sep 14<span>th</span>, 2015</time>
        
         | <a href="/89/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>이번에 review할 논문은 <a href="http://arxiv.org/abs/1409.2329">Recurrent neural network regularization</a>이라는 논문이다. 아직 학회나 저널에 publish된 논문은 아니지만 ICLR 2015 review를 기다리는 모양. 이 논문은 최근 머신러닝 분야에서 가장 주목받고 있는 분야는 Deep learning, 그 중에서도 요즘 가장 활발하게 연구 중인 Recurrent Neural Network, 혹은 RNN에 관한 논문이다. RNN은 sequencial data를 처리하는데에 적합한 형태로 디자인 되어있으며, 현재 language model, speech recognition, machine translation 등에서 우수한 결과를 성취하고 있는 neural network model 중 하나이다. 이 논문은 popular한 RNN 중 하나인 LSTM 모델을 regularization 시켜서 보다 기존 결과들보다 더 잘 동작하는 결과를 제안한다.</p>




<h5>Motivation: Regularization of Recurrent Neural Network</h5>


<p>RNN이 sequencial data에 대해 꽤 좋은 성능을 보이고 있는 것은 사실이지만, RNN을 regularization하는 방법은 많이 제안되어있지 않은 상황이다. 기존 MLP (Multi-Layer Perceptron, 혹은 feed-forward network) 쪽에서는 Dropout, ReLU 등의 컨셉들이 연구되면서 상당한 발전이 있었지만, 아직 RNN에는 Dropout조차 제대로 적용되지 않고 있다고 한다. Dropout을 붙이면 오히려 성능이 떨어지기 때문에 아직은 LSTM (Long-Short-Term-Memory) 모델에 dropout없이 연구가 진행되고 있는 모양이다. 이 논문에서는 LSTM에 dropout을 RNN의 특성을 잘 살린 형태로 적용하여 dropout이 잘 동작하도록 하는 방법을 제안한다.</p>




<h5 id="RNN-intro">RNN Introduction</h5>


<p>본 논문을 소개하기 전에 먼저 RNN에 대해 간단하게 설명을 하고 넘어가도록 하겠다. 이름에서도 알 수 있듯 일반적인 Feed-forward network와 RNN의 차이는 recurrent한 loop의 존재 유무이다. 이는 자기 자신을 향한 self-loop일 수도 있고, 아니면 cycle 형태이거나 아니면 undirected edge의 형태일 수도 있다. 보통 일반적으로 RNN이라 하면 아래 그림과 같이 hidden unit에 self-loop이 있는 형태를 일컫는 듯하다. (출처: Bengio Deep Learning book)</p>


<p><img src="/images/post/89-1.PNG" width="600"></p>

<p>이 그림에서도 알 수 있듯 self-loop의 존재는 RNN으로 하여금 자연스럽게 historical data를 현재 decision에 반영하도록 만들어준다. 즉, RNN 모델은 마치 HMM 등의 sequencial data를 처리하는 모델들처럼 동작하는 것이다. 실제 learning을 할 때는 시간에 대해 self-loop를 &#8216;unfold&#8217; 하여 마치 weight를 공유하는 deep layer를 연산하듯 update한다. RNN에 대한 더 자세한 설명은 추후 다른 포스팅을 통해 다룰 수 있도록 하겠다.</p>




<h5>Long-Short-Term-Memory (LSTM) Architecture</h5>


<p>기존 vanilla RNN은 long-term dependency를 가지도록 learnig을 하게되면 gradient vanishing이나 exploding 문제에 직면하기 쉬워진다. 이유는 dependency를 더 long-term으로 가져갈수록 gradient 값이 시간에 따른 곱하기 형태가 되어 gradient growth가 exponential해지기 때문이다 (역시 위와 마찬가지로 나중에 더 자세하게 다루도록 하겠다). 때문에 이를 해결하기 위한 아이디어 중 하나로 LSTM이라는 것이 존재한다.</p>


<p>LSTM은 historical information을 저장하기 위한 다소 복잡한 dynamics를 가지고 있다. &#8220;long term&#8221; memory라는 것은 memory cell (\(c_t\))에 저장되며, 시간에 따라, 그리고 주어진 input data에 따라 저장해둔 information을 얼마나 간직하고 있을지 forget gate (\(f_t\)) 라는 것을 통해 결정하게 된다. LSTM을 그림으로 표현하면 아래 그림과 같다. (출처: 논문)</p>


<p><img src="/images/post/89-2.PNG" width="600"></p>

<p>이를 수식으로 한 번 나타내어보자. 먼저 몇 가지 notation을 정의해보자. 먼저 모든 state는 n-dimension이라고 가정하자. \(h_t^l \in \mathbb R^n \)은 layer \(l\)의 timestamp \(t\)일 때의 hidden state라고 하자. 그리고 \(T_{n,m}: \mathbb R^n \to \mathbb R^m\) 을 n차원에서 m차원으로 가는 affine transform이라고 해보자. 예를 들어 parameter \(W\)와 \(b\)로 나타내어지는 \(Wx + b\)도 \(T_{n,m}\)에 포함된다 (즉, 이 논문에서는 복잡한 weight와 bias에 대한 식을 T라는 notation으로 간단하게 치환했다고 생각하면 된다). 마지막으로 \(\odot\)을 두 벡터의 element-wise multiplication이라고 정의해보자. 이렇게 notation을 정의하고 나면 일반적인 vanilla RNN을 다음과 같이 과거의 hidden state와 현재 hidden state의 이전 layer의 state로부터 현재 hidden state를 표현하는 function으로 표현할 수 있다.</p>


<p>\[\mbox{RNN: } h_t^{l-1}, h_{t-1}^l \to h_t^l, \mbox{ where } h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l). \]</p>


<p>이때, function \(f\)는 RNN의 경우 sigmoid나 tanh 함수 중 하나로 선택하는 것이 일반적이다. 그럼 이번에는 LSTM을 수식으로 표현해보자.</p>


<p><img class="center" src="/images/post/89-3.PNG" width="250"></p>

<p>앞에서 언급했던 LSTM의 graphical representation을 살펴보면서 수식을 읽어보면 어렵지 않게 이해할 수 있을 것이다.</p>




<h5>Dropout Regularization for LSTM</h5>


<p>저자들은 RNN에 Dropout을 붙였을 때 잘 동작하지 않는 이유가 dropout이 지워버리면 안되는 과거 information까지 전부 지워버리기 때문이라고 주장한다. 때문에 RNN에 Dropout을 적용하기 위해서는 recurrent connection이 아닌 connection 들에 대해서만 Dropout을 적용해야한다고 논문에서는 주장하고 있다. 아래 식에 조금 더 자세하게 적혀있다. 이때 \(\mathbf D\) 는 dropout operator라는 것으로, 주어진 argument의 random subset을 0으로 만들어버리는 operator이다. 즉, \(\mathbf D (h)\) 라고 한다면 vector \(h\) 중 random하게 고른 일부를 (보통 50%) 0으로 설정하라는 뜻이다.</p>


<p><img class="center" src="/images/post/89-4.PNG" width="250"></p>

<p>이를 그림으로 표현하면 아래와 같다. 이때 실선은 일반적인 connection이고, 점선이 dropout으로 연결된 connection을 의미한다. (출처: 논문)</p>


<p><img src="/images/post/89-5.PNG" width="600"></p>

<p>위 그림에서도 알 수 있듯, 과거에서부터 propagation되는 information은 언제나 100% 보존되지만, 아래 layer에서 위 layer로 전달되는 information은 특정 확률로 dropout에 의해 corruption되어 진행된다. 이때, 맨 아래 data layer로부터 맨 위 \(L\)번째 layer까지 information이 전달되는 동안 점선으로 그려진 connection은 정확하게 \(L+1\) 번 만큼만 지나게 된다. 만약 recurrent connection까지 dropout을 적용했다면, 이 횟수는 \(L+1\) 보다 항상 같거나 클 것이며, 더 long-term dependency를 가질수록 그 효과가 더 강해져서 우리가 원하는 과거 정보는 거의 다 희석되고, 결과적으로 안좋은 결과를 얻게 될 확률이 높아질 것이다.</p>


<p><img src="/images/post/89-6.png" width="600"></p>

<p>위 그림은 어떻게 information이 time t-2로부터 t+2까지 전달되는지 그 flow를 표현한 것이다. 굵은 선이 information path를 나타내는데, 앞서 설명한 것 처럼 이런 information flow는 data layer로부터 decision layer까지 정확하게 \(L+1\) 번만 dropout의 영향을 받게 된다. 반면 standard dropout을 적용했더라면 information이 더 많은 dropout들에 의해 영향을 받아서 LSTM이 정보를 더 긴 시간 동안 저장할 수 없도록 만들게 되는 것이다. 때문에 recurrent connection에 dropout을 적용하지 않는 것 만으로도 LSTM에서 좋은 regularization 효과를 얻을 수 있는 것이다.</p>




<h5>Experiments</h5>


<p>논문에서는 총 4개의 실험을 진행한다. Language Modeling (Penn Tree Bank - PTB dataset), Speech Recognition, Machine Translation 그리고 마지막으로 Image Caption Generation이 그것이다. 결과는 순서대로 아래 Table 1,2,3,4에 나열되어있다.</p>


<p><img src="/images/post/89-7.png" width="600"></p>

<h5>Summary of Recurrent Neural Network Regularization</h5>


<ul>
<li>Motivation: Dropout을 RNN에 그냥 적용하면 성능이 좋지 않다</li>
<li>Idea: Dropout을 모든 connection에 적용하는 대신 non recurrent connection에만 dropout을 적용하자</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1409.2329">Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. &ldquo;Recurrent neural network regularization.&rdquo; arXiv preprint arXiv:1409.2329 (2014).</a></li>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/74/">Machine Learning 스터디 (18) Neural Network Introduction</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-09-13T23:13:00+09:00" pubdate data-updated="true">Sep 13<span>th</span>, 2015</time>
        
         | <a href="/74/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h5>들어가며</h5>


<p>최근 Machine Learning 분야에서 가장 뜨거운 분야는 누가 뭐래도 Deep Learning이다. 엄청나게 많은 사람들이 관심을 가지고 있고, 공부하고 응용하고 있지만, 체계적으로 공부할 수 있는 자료가 많이 없다는 것이 개인적으로 조금 안타깝다. 이제 막 각광받기 시작한지 10년 정도 지났고, 매년 새로운 자료들이 쏟아져나오기 때문에 책이나 정리된 글을 찾기가 쉽지가 않다. 그러나 Deep Learning은 결국 artificial neural network를 조금 더 복잡하게 만들어놓은 모델이고, 기본적인 neural network에 대한 이해만 뒷받침된다면 자세한 내용들은 천천히 탑을 쌓는 것이 가능하다고 생각한다. 이 글에서는 neural network의 가장 기본적인 model에 대해 다루고, model paramter를 update하는 algorithm인 backpropagation에 대해서 다룰 것이다. 조금 더 advanced한 topic들은 이 다음 글에서 다룰 예정이다. 이 글의 일부 문단은 <a href="/blog/categories/neural-network/">이전 글들</a>을 참고하였다.</p>


<h5>Motivation of Neural Network</h5>


<p>이름에서부터 알 수 있듯 neural network는 사람의 뇌를 본 따서 만든 머신러닝 모델이다 (참고: 원래 neural network의 full name은 artificial neural network이지만, 일반적으로 neural network라고 줄여서 부른다). 본격적으로 neural network에 대해 설명을 시작하기 전에 먼저 인간보다 컴퓨터가 훨씬 잘 할 수 있는 일들이 무엇이 있을지 생각해보자.</p>


<ul>
<li>1부터 10000000까지 숫자 더하기</li>
<li>19312812931이 소수인지 아닌지 판별하기</li>
<li>주어진 10000 by 10000 matrix 의 determinant값 계산하기</li>
<li>800 페이지 짜리 책에서 &lsquo;컴퓨터&rsquo; 라는 단어가 몇 번 나오는지 세기</li>
</ul>


<p>반면 인간이 컴퓨터보다 훨씬 잘 할 수 있는 일들에 대해 생각해보자</p>


<ul>
<li>다른 사람과 상대방이 말하고자하는 바를 완벽하게 이해하면서 내가 하고 싶은 말을 상대도 이해할 수 있도록 전달하기</li>
<li>주어진 사진이 고양이 사진인지 강아지 사진인지 판별하기</li>
<li>사진으로 찍어보낸 문서 읽고 이해하기</li>
<li>주어진 사진에서 얼마나 많은 물체가 있는지 세고, 사진에 직접 표시하기</li>
</ul>


<p>컴퓨터가 잘 할 수 있는 0과 1로 이루어진 사칙연산이다. 기술의 발달로 인해 지금은 컴퓨터가 예전보다도 더 빠른 시간에, 그리고 더 적은 전력으로 훨씬 더 많은 사칙연산을 처리할 수 있다. 반면 사람은 사칙연산을 컴퓨터만큼 빠르게 할 수 없다. 인간의 뇌는 오직 빠른 사칙연산만을 처리하기 위해 만들어진 것이 아니기 때문이다. 그러나 인지, 자연어처리 등의 그 이상의 무언가를 처리하기 위해서는 사칙연산 그 너머의 것들을 할 수 있어야하지만 현재 컴퓨터로는 인간의 뇌가 할 수 있는 수준으로 그런 것들을 처리할 수 없다.</p>


<p>예를 들어 아래와 같이 주어진 사진에서 각각의 물체를 찾아내는 문제를 생각해보자 (출처: <a href="http://www.engadget.com/2014/09/08/google-details-object-recognition-tech/">링크</a>). 사람에게는 너무나 간단한 일이지만, 컴퓨터가 처리하기에는 너무나 어려운 일이다. 어떻게 어디부터 어디까지가 &#8216;tv or monitor&#8217;라고 판단할 수 있을까? 컴퓨터에게 사진은 단순한 0과 1로 이루어진 픽셀 데이터에 지나지 않기 때문에 이는 아주 어려운 일이다.</p>


<p><img class="center" src="/images/post/74-2.jpg" width="400"></p>

<p>그렇기 때문에 자연언어처리, 컴퓨터 비전 등의 영역에서는 인간과 비슷한 성능을 내는 시스템을 만들 수만 있다면 엄청난 기술적 진보가 일어날 수 있을 것이다. 그렇기 때문에 인간의 능력을 쫓아가는 것 이전에, 먼저 인간의 뇌를 모방해보자라는 아이디어를 낼 수 있을 것이다. Neural Network는 이런 모티베이션으로 만들어진 간단한 수학적 모델이다. 우리는 이미 인간의 뇌가 엄청나게 많은 뉴런들과 그것들을 연결하는 시냅스로 구성되어있다는 사실을 알고 있다. 또한 각각의 뉴런들이 activate되는 방식에 따라서 다른 뉴런들도 activate 되거나 activate되지 않거나 하는 등의 action을 취하게 될 것이다. 그렇다면 이 사실들을 기반으로 다음과 같은 간단한 수학적 모델을 정의하는 것이 가능하다.</p>


<h5>Model of Neural Network: neuron, synapse, activation function</h5>


<p>먼저 뉴런들이 node이고, 그 뉴런들을 연결하는 시냅스가 edge인 네트워크를 만드는 것이 가능하다. 각각의 시냅스의 중요도가 다를 수 있으므로 edge마다 weight를 따로 정의하게 되면 아래 그림과 같은 형태로 네트워크를 만들 수 있다. (출처: <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>)</p>


<p><img src="/images/post/74-1.png" width="600"></p>

<p>보통 neural network는 directed graph이다. 즉, information propagation이 한 방향으로 고정된다는 뜻이다. 만약 undirected edge를 가지게 되면, 혹은 동일한 directed edge가 양방향으로 주어질 경우, information propagation이 recursive하게 일어나서 결과가 조금 복잡해진다. 이런 경우를 recurrent neural network (RNN)이라고 하는데, 과거 데이터를 저장하는 효과가 있기 때문에 최근 음성인식 등의 sequencial data를 처리할 때 많이 사용되고 있다. 이번 ICML 2015에서도 RNN 논문이 많이 발표되고 있고, 최근들어 연구가 활발한 분야이다. 이 글에서는 일단 가장 간단한 &#8216;multi layer perceptron (MLP)&#8217;라는 구조만 다룰 것인데, 이 구조는 directed simple graph이고, 같은 layer들 안에서는 서로 connection이 없다. 즉, self-loop와 parallel edge가 없고, layer와 layer 사이에만 edge가 존재하며, 서로 인접한 layer끼리만 edge를 가진다. 즉, 첫번째 layer와 네번째 layer를 직접 연결하는 edge가 없는 것이다. 앞으로 layer에 대한 특별한 언급이 없다면 이런 MLP라고 생각하면 된다. 참고로 이 경우 information progation이 &#8216;forward&#8217;로만 일어나기 때문에 이런 네트워크를 feed-forward network라고 부르기도 한다.</p>


<p>다시 일반적인 neural network에 대해 생각해보자. 실제 뇌에서는 각기 다른 뉴런들이 activate되고, 그 결과가 다음 뉴런으로 전달되고 또 그 결과가 전달되면서 최종 결정을 내리는 뉴런이 activate되는 방식에 따라 정보를 처리하게 된다. 이 방식을 수학적 모델로 바꿔서 생각해보면, input 데이터들에 대한 activation 조건을 function으로 표현하는 것이 가능할 것이다. 이것을 activate function이라고 정의한다. 가장 간단한 activation function의 예시는 들어오는 모든 input 값을 더한 다음, threshold를 설정하여 이 값이 특정 값을 넘으면 activate, 그 값을 넘지 못하면 deactivate되도록 하는 함수일 것이다. 일반적으로 많이 사용되는 여러 종류의 activate function이 존재하는데, 몇 가지를 소개해보도록 하겠다. 편의상 \(t = \sum_i w_i * x_i\) 라고 정의하겠다. (참고로, 일반적으로는 weight 뿐 아니라 bais도 고려해야한다. 이 경우 \(t = \sum_i (w_i * x_i + b_i) \)로 표현이 되지만, 이 글에서는 bais는 weight와 거의 동일하기 때문에 무시하고 진행하도록 하겠다. - 예를 들어 항상 값이 1인 \(x_0\)를 추가한다면 \(w_0\)가 bais가 되므로, 가상의 input을 가정하고 weight와 bais를 동일하게 취급하여도 무방하다.)</p>




<ul>
    <li><p>sigmoid function: \(f(t) = \frac{1}{1+ e^{-t}}\)</p></li>
    <li><p>tanh function: \(f(t) = \frac{e^t - e^{-t}}{e^t + e^{-t} }\)</p></li>
    <li><p>absolute function: \(f(t) = \|t\|\)</p></li>
    <li><p>ReLU function: \(f(t) = max(0, t)\)</p></li>
</ul>




<p>보통 가장 많이 예시로 드는 activation function으로 sigmoid function이 있다. (출처는 위의 <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>와 같음)</p>


<p><img class="center" src="/images/post/74-4.png" width="300"></p>

<p>이 함수는 미분이 간단하다거나, 실제 뉴런들이 동작하는 것과 비슷하게 생겼다는 등의 이유로 과거에는 많이 사용되었지만, 별로 practical한 activation function은 아니고, 실제로는 ReLU를 가장 많이 사용한다 (2012년 ImageNet competition에서 우승했던 <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> publication을 보면, ReLU와 dropout을 쓰는 것이 그렇지 않은 것보다 훨씬 더 우수한 결과를 얻는다고 주장하고 있다. 이에 대한 자세한 내용은 다른 포스트를 통해 보충하도록 하겠다). 참고로 neuron을 non-linearity라고 부르기도 하는데, 그 이유는 activation function으로 linear function을 사용하게 되면 아무리 여러 neuron layer를 쌓는다고 하더라도 그것이 결국 하나의 layer로 표현이 되기 때문에 non-linear한 activation function을 사용하기 때문이다.</p>


<p>따라서 이 모델은 처음에 node와 edge로 이루어진 네트워크의 모양을 정의하고, 각 node 별 activation function을 정의한다. 이렇게 정해진 모델을 조절하는 parameter의 역할은 edge의 weight가 맡게되며, 가장 적절한 weight를 찾는 것이 이 수학적 모델을 train할 때의 목표가 될 것이다.</p>




<h5>Inference via Neural Network</h5>


<p>먼저 모든 paramter가 결정되었다고 가정하고 neural network가 어떻게 결과를 inference하는지 살펴보도록하자. Neural network는 먼저 주어진 input에 대해 다음 layer의 activation을 결정하고, 그것을 사용해 그 다음 layer의 activation을 결정한다. 이런 식으로 맨 마지막까지 결정을 하고 나서, 맨 마지막 decision layer의 결과를 보고 inference를 결정하는 것이다 (아래 그림 참고, 빨간 색이 activate된 뉴런이다).</p>


<p><img src="/images/post/74-3.png" width="600"></p>

<p> 이때, classification이라고 한다면 마지막 layer에 내가 classification하고 싶은 class 개수만큼 decision node를 만든 다음 그 중 하나 activate되는 값을 선택하는 것이다. 예를 들어 0부터 9까지 손글씨 데이터를 (MNIST라는 유명한 dataset이 있다) classification해야한다고 생각해보자. 그 경우는 0부터 9까지 decision이 총 10개이므로 마지막 decision layer에는 10개의 neuron이 존재하게 되고 주어진 데이터에 대해 가장 activation된 크기가 큰 decision을 선택하는 것이다.</p>




<h5 id="backprop">Backpropagation Algorithm</h5>


<p>마지막으로 이제 weight를 어떻게 찾을 수 있는지 weight paramter를 찾는 알고리즘에 대해 알아보자. 먼저 한 가지 알아두어야 할 점은 activation function들이 non-linear하고, 이것들이 서로 layer를 이루면서 복잡하게 얽혀있기 때문에 neural network의 weight optimization이 non-convex optimization이라는 것이다. 따라서 일반적인 경우에 neural network의 paramter들의 global optimum을 찾는 것은 불가능하다. 그렇기 때문에 보통 gradient descent 방법을 사용하여 적당한 값까지 수렴시키는 방법을 사용하게 된다.</p>


<p>Neural network (이 글에서는 multi-layer feed-forward network)의 parameter를 update하기 위해서는 backpropagation algorithm이라는 것을 주로 사용하는데, 이는 단순히 neural network에서 gradient descent를 chain rule을 사용하여 단순화시킨 것에 지나지 않는다 (Gradient descent에 대해서는 이전에 쓴 <a href="/63">Convex Optimization글</a>에서 자세히 다루고 있으니 참고하면 좋을 것 같다). 모든 optimization 문제는 target function이 정의되어야 풀 수 있다. Neural network에서는 마지막 decision layer에서 우리가 실제로 원하는 target output과 현재 network가 produce한 estimated output끼리의 loss function을 계산하여 그 값을 minimize하는 방식을 취한다. 일반적으로 많이 선택하는 loss에는 다음과 같은 함수들이 있다. 이때 우리가 원하는 <a class="red tip" title="만약 MNIST라면 d=10">d-dimensional</a> target output을 \(t=[t_1, \ldots, t_d]\)로, estimated output을 \(x=[x_1, \ldots, x_d]\) 로 정의해보자.</p>




<ul>
    <li><p>sum of squares (Euclidean) loss: \(\sum_{i=1}^d (x_i - t_i)^2 \)</p></li>
    <li><p>softmax loss: \(-\sum_{i=1}^d \bigg[ t_i \log \big(\frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) + (1 - t_i) \log \big(1 - \frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) \bigg] \)</p></li>
    <li><p>cross entropy loss: \(\sum_{i=1}^d [ -t_i \log x_i - (1-t_i) \log (1-x_i) ]\)</p></li>
    <li><p>hinge loss: \(\max(0,1-t \cdot x)\), 이때 \(\cdot\)은 내적을 의미한다.</p></li>
</ul>




<p>상황에 따라 조금씩 다른 loss function을 사용하지만, classification에 대해서는 보통 softmax loss가 gradient의 값이 numerically stable하기 때문에 softmax loss를 많이 사용한다. 이렇게 loss function이 주어진다면, 이 값을 주어진 paramter들에 대해 gradient를 구한 다음 그 값들을 사용해 parameter를 update하기만 하면 된다. 문제는, 일반적인 경우에 대해 이 paramter 계산이 엄청 쉬운 것만은 아니라는 것이다.</p>


<p>Backpropagtaion algorithm은 chain rule을 사용해 gradient 계산을 엄청 간단하게 만들어주는 알고리즘으로, 각각의 paramter의 grdient를 계산할 때 parallelization도 용이하고, 알고리즘 디자인만 조금 잘하면 memory도 많이 아낄 수 있기 때문에 실제 neural network update는 이 backpropagtaion 알고리즘을 사용하게 된다.</p>


<p>Gradient descent method를 사용하기 위해서는 현재 parameter에 대한 gradient를 계산해야하지만, 네트워크가 복잡해지면 그 값을 바로 계산하는 것이 엄청나게 어려워진다. 그 대신 backpropataion algorithm에서는 먼저 현재 paramter를 사용하여 loss를 계산하고, 각각의 parameter들이 해당 loss에 대해 얼마만큼의 영향을 미쳤는지 chain rule을 사용하여 계산하고, 그 값으로 update를 하는 방법이다. 따라서 backpropagation algorithm은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>


<h6>Phase 1: Propagation</h6>


<ol>
    <li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 &#8216;forward&#8217; propagation이라 한다.)</li>
    <li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 &#8216;back&#8217; propagation이라 한다.)</li>
</ol>


<h6>Phase 2: Weight update</h6>


<ol>
    <li>Chain rule을 사용해 paramter들의 gradient를 계산한다.</li>
</ol>


<p>이때, chain rule을 사용한다는 의미는 아래 그림에서 나타내는 것처럼, 앞에서 계산된 gradient를 사용해 지금 gradient 값을 update한다는 의미이다. (그림은 bengio의 <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">deep learning book</a> <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/mlp.html">Ch6</a> 에서 가져왔다.)</p>


<p><img src="/images/post/74-6.png" width="400"></p>

<p>두 그림 모두 \(\frac{\partial z}{\partial x}\)를 구하는 것이 목적인데, 직접 그 값을 계산하는 대신, \(y\) layer에서 이미 계산한 derivative인 \(\frac{\partial z}{\partial y}\)와 \(y\) layer와 \(x\)에만 관계있는 \(\frac{\partial y}{\partial x}\)를 사용하여 원하는 값을 계산하고 있다. 만약 \(x\) 아래에 \(x^\prime\)이라는 parameter가 또 있다면, \(\frac{\partial z}{\partial x}\)와 \(\frac{\partial x}{\partial x^\prime}\)을 사용하여 \(\frac{\partial z}{\partial x^\prime}\)을 계산할 수 있는 것이다. 때문에 우리가 backpropagation algorithm에서 필요한 것은 내가 지금 update하려는 paramter의 바로 전 variable의 derivative와, 지금 paramter로 바로 전 variable을 미분한 값 두 개 뿐이다. 이 과정을 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -> hidden k, hidden k -> hidden k-1, &#8230; hidden 2 -> hidden 1, hidden 1 -> input의 과정을 거치면서 계속 weight가 update되는 것이다. 예를 들어서 decision layer와 가장 가까운 weight는 직접 derivative를 계산하여 구할 수 있고, 그보다 더 아래에 있는 layer의 weight는 그 바로 전 layer의 weight와 해당 layer의 activation function의 미분 값을 곱하여 계산할 수 있다. 이해가 조금 어렵다면 아래의 <a href="74#example">예제</a>를 천천히 읽어보기를 권한다.</p>


<p>이 과정을 맨 위에서 아래까지 반복하면 전체 gradient를 구할 수 있고, 이 gradient를 사용해 parameter들을 update할 수 있다. 이렇게 한 번의 iteration이 진행되고, 충분히 converge했다고 판단할 때 까지 이런 iteration을 계속 반복하는 것이 feed-forward network의 parameter를 update하는 방법이다.</p>


<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network">링크</a>)</p>


<p><img src="/images/post/42-1.png" width="600"></p>

<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 &#8216;역으로&#8217; 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>




<h5>Stochastic Gradient Descent</h5>


<p>Gradient를 계산했으니 이제 직접 Gradient Descent를 써서 parameter만 update하면 된다. 그러나 문제가 하나 있는데, 일반적으로 neural network의 input data의 개수가 엄청나게 많다는 것이다. 때문에 정확한 gradient를 계산하기 위해서는 모든 training data에 대해 gradient를 전부 계산하고, 그 값을 평균 내어 정확한 gradient를 구한 다음 &#8216;한 번&#8217; update해야한다. 그러나 이런 방법은 너무나도 비효율적이기 때문에 Stochastic Gradient Descent (SGD) 라는 방법을 사용해야한다.</p>


<p>SGD는 모든 데이터의 gradient를 평균내어 gradient update를 하는 대신 (이를 &#8216;full batch&#8217;라고 한다), 일부의 데이터로 &#8216;mini batch&#8217;를 형성하여 한 batch에 대한 gradient만을 계산하여 전체 parameter를 update한다. Convex optimization의 경우, 특정 조건이 충족되면 SGD와 GD가 같은 global optimum으로 수렴하는 것이 증명되어있지만, neural network는 convex가 아니기 때문에 batch를 설정하는 방법에 따라 수렴하는 조건이 바뀌게 된다. Batch size는 일반적으로 메모리가 감당할 수 있을 정도까지 최대한 크게 잡는 것 같다.</p>




<h5 id="example">Backpropagation Algorithm: example</h5>


<p>이전에 chain rule로 gradient를 계산한다고 언급했었는데, 실제 이 chain rule이 어떻게 적용되는지 아래의 간단한 예를 통해 살펴보도록하자. 이때 계산의 편의를 위해 각각의 neuron은 sigmoid loss를 가지고 있다고 가정하도록 하겠다.</p>


<p><img src="/images/post/74-5.png" width="600"></p>

<p>이때 각각의 neuron의 input으로 들어가는 값을 \(in_{o_5}\), output으로 나가는 값을 \(out_{h_3}\)와 같은 식으로 정의해보자 (이렇게 된다면 in과 out은 \(out_{h_3} = \sigma(in_{h_3})\) 으로 표현 가능하다. - 이때 \(\sigma\)는 sigmoid function). 먼저 error를 정의하자. error는 가장 간단한 sum of square loss를 취하도록 하겠다. 우리가 원하는 target을 \(t\)라고 정의하면 loss는 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)가 될 것이다 (1/2는 미분한 값을 깔끔하게 쓰기 위해 붙인 상관없는 값이므로 무시해도 좋다). 그리고 우리가 원하는 값들은 \(\frac{\partial E}{\partial w_{13}}, \frac{\partial E}{\partial w_{14}}, \ldots, \frac{\partial E}{\partial w_{46}}\)이 될 것이다. 이제 가장 먼저 \(\frac{\partial E}{\partial w_{35}}\) 부터 계산해보자.</p>


<p>\[\frac{\partial E}{\partial w_{35}} = \frac{\partial E}{\partial out_{o_5}} * \frac{\partial out_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial w_{35}}. \]</p>


<p>즉, 우리가 원하는 derivative를 계산하기 위해서는 세 개의 다른 derivative (\(\frac{\partial E}{\partial out_{o_5}}, \frac{\partial out_{o_5}}{\partial in_{o_5}}, \frac{\partial in_{o_5}}{\partial w_{35}}\))를 계산해야한다. 각각을 구하는 방법은 다음과 같다.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{o_5}}\): error를 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)라고 정의했으므로, \(\frac{\partial E}{\partial out_{o_5}} = out_{o_5} - t_5\)이다. - 이때 \(out_{o_5}\)와 \(t_5\)는 weight update이전 propagation step에서 계산된 값이다.</p></li>
    <li><p>\(\frac{\partial out_{o_5}}{\partial in_{o_5}}\): \(o_5\)는 sigmoid activation function을 사용하므로 \(out_{o_5} = \sigma(in_{o_5})\)이다. 또한 sigmoid function의 미분 값은 \(\frac{\partial \sigma(x)}{\partial x} = \sigma(x) (1 - \sigma(x))\)으로 주어지므로, 이 값을 대입하면 \(\frac{\partial out_{o_5}}{\partial in_{o_5}} = out_{o_5} (1 - out_{o_5})\)가 된다. - 역시 여기에서도 미리 계산한 \(out_{o_5}\)를 사용한다.</p></li>
    <li>\(\frac{\partial in_{o_5}}{\partial w_{35}}\): \(o_5\)로 들어온 값의 총 합은 앞선 layer의 output과 \(o_5\)로 들어오는 weight를 곱하면 되므로 \(in_{o_5} = w_{35} out_{h_3} + w_{45} out_{h_4}\)이고, 이것을 통해 \(\frac{\partial in_{o_5}}{\partial w_{35}} = out_{h_3}\)가 됨을 알 수 있다. - \(out_{h_3}\) 역시 이전 propagation에서 계산된 값이다.</li>
</ul>


<p>따라서 \(\frac{\partial E}{\partial w_{35}}\)의 derivative 값은 위의 세 값을 모두 곱한 것으로 계산 할 수 있다. 그림으로 표현하면 아래와 같은 그림이 될 것이다. 즉, &#8216;backward&#8217; 방향으로 derivative에 대한 정보를 &#8216;propagation&#8217;하면서 parameter의 derivative를 계산하는 것이다. 마찬가지 방법으로 \(w_{36}, w_{45}, w_{46}\)에 대한 derivative도 계산할 수 있다.</p>


<p><img src="/images/post/74-7.png" width="300"></p>

<p>그럼 이번에는 그 전 layer의 paramter들 중 하나인 \(w_{13}\)의 derivative를 계산해보자. 이번에 계산할 과정도 위와 비슷한 그림으로 표현해보면 아래와 같다.</p>


<p><img src="/images/post/74-8.png" width="300"></p>

<p>그러면 이제 \(\frac{\partial E}{\partial w_{13}}\)을 구해보자.</p>


<p>\[\frac{\partial E}{\partial w_{13}} = \frac{\partial E}{\partial out_{h_3}} * \frac{\partial out_{h_3}}{\partial in_{h_3}} * \frac{\partial in_{h_3}}{\partial w_{13}}.\]</p>


<p>마찬가지로 각각을 구하는 방법에 대해 적어보자.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{h_3}}\): \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)를 \(E = E_{o_5} + E_{o_6}\)로 decompose 하면 이 미분 식은 \(\frac{\partial E_{o_5}}{\partial out_{h_3}} + \frac{\partial E_{o_6}}{\partial out_{h_3}}\)로 쓸 수 있다. 각각의 계산은 다음과 같다.</p></li>
    <ul>
        <li><p>\(\frac{\partial E_{o_5}}{\partial out_{h_3}} = \frac{\partial E_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial out_{h_3}}\)으로 쓸 수 있다. 이 중 앞의 값인 \(\frac{\partial E_{o_5}}{\partial in_{o_5}}\)은 이미 전 과정에서 계산했던 \(\frac{\partial E}{\partial out_{o_5}}\)과 \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)의 곱으로 계산가능하다. 뒤의 값은 \(\frac{\partial in_{o_5}}{\partial out_{h_3}} = w_{35}\)이므로 간단하게 계산할 수 있다.</p></li>
        <li><p>\(\frac{\partial E_{o_6}}{\partial out_{h_3}}\)도 위와 같은 방법으로 연산이 가능하다.</p></li>
    </ul>
    <li><p>\(\frac{\partial out_{h_3}}{\partial in_{h_3}}\): \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)와 같다. 따라서 \(out_{h_3} (1-out_{h_3})\)이다.</p></li>
    <li><p>\(\frac{\partial in_{h_3}}{\partial w_{13}}\): \(\frac{\partial in_{o_5}}{\partial w_{35}}\)와 같다. 따라서 \(out_{i_1}\)이다.</p></li>
</ul>


<p>이렇게 \(\frac{\partial E}{\partial out_{h_3}}\)에서는 앞에서 계산했던 값들을 재활용하고, 아래의 값들은 activation function과 network의 topological property에 맞는 derivative를 곱하는 방식으로 \(\frac{\partial E}{\partial w_{13}}\)을 구할 수 있다.</p>


<p>이렇듯 backpropagation algorithm은 forward propagation을 통해 필요한 값들을 미리 저장해두고, backward propagation이 진행되면서 위에서부터 loss에 대한 derivative를 하나하나 계산해나가면서 다음 layer에서 바로 전 layer에서 계산한 값들과 각 neuron 별로 추가적으로 필요한 derivative들을 곱해나가면서 weight의 derivative를 계산하는 알고리즘이다.</p>


<p>이렇게 한 번 전체 gradient를 계산한 다음에는 learning rate를 곱하여 전체 parameter의 값을 update한 다음, 다시 처음부터 이 과정을 반복한다. 보통 에러가 감소하는 속도를 관측하면서 &#8216;이 정도면 converge한 것 같다&#8217; 하는 수준까지 돌린다.</p>


<p>익숙해지려면 다소 시간이 걸리지만, 개념적으로 먼저 &#8216;error를 먼저 계산하고, 그 값을 아래로 전달해나가면서 바로 전 layer에서 계산한 미분값들을 사용해 현재 layer의 미분값을 계산한 다음, 그 값을 사용해 다음 layer의 미분값을 계산한다.&#8217; 라고 개념만 이해해두고 다시 차근차근 chain rule을 계산해나가면서 계산하면 조금 편하게 익숙해 질 수 있을 것이다.</p>




<h5>정리</h5>


<p>Deep learning을 다루기 위해서는 가장 먼저 aritifitial neural network의 model에 대한 이해와 gradient descent라는 update rule에 대한 이해가 필수적이다. 이 글에서는 가장 기초적이라고 생각하는 feed-forward network의 model을 먼저 설명하고, paramter를 update하는 gradient descent algorithm의 일종인 backpropagation에 대한 개념적인 설명을 다루었다. 조금 어려울 수 있는 내용이니 다른 글들을 계속 참고하면서 보면 좋을 것 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 13일: 글 등록</li>
<li>2015년 9월 14일: 오타수정, SGD 내용 추가 등</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
<li>wiki (<a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks">링크</a>)</li>
</ul>


<hr>


<p><a href="/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="/57">Machine Learning이란?</a></li>
<li><a href="/58">Probability Theory</a></li>
<li><a href="/59">Overfitting</a></li>
<li><a href="/60">Algorithm</a></li>
<li><a href="/61">Decision Theory</a></li>
<li><a href="/62">Information Theory</a></li>
<li><a href="/63">Convex Optimzation</a></li>
<li><a href="/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/88/">Batch Normalization (ICML 2015)</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-08-25T21:25:00+09:00" pubdate data-updated="true">Aug 25<span>th</span>, 2015</time>
        
         | <a href="/88/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Batch Normalization은 현재 <a href="http://image-net.org/challenges/LSVRC/2015/">ImageNet competition</a>에서 state-of-art (Top-5 error: 4.9%)를 기록하고 있는 Neural Network model의 기본 아이디어이다. 이 글에서는 arXiv에 제출된 (그리고 ICML 2015에 publish된) <a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 논문을 리뷰하고, batch normalization이 어떤 기술이고, 어떤 원리에 의해 작동하는지 등에 대해 다룰 것이다.</p>


<h5>Motivation: Deep learning의 속도를 어떻게 더 빠르게 만들 수 있을까?</h5>


<p>Deep learning이 잘 동작하고, 뛰어난 성능을 보인다는 것은 이제 누구나 알고 있다. 그러나 여전히 deep learning은 굉장히 시간이 오래 걸리는 작업이고, 그만큼 computation power도 많이 필요로 한다. 그 동안의 연구 결과를 보면, converge한 것 처럼 보이더라도 더 많이 돌리게 된다면 더 좋은 결과로 수렴한다는 것을 알 수 있는 만큼, deep neural network의 train 속도를 높이는 것은 전체적인 성능 향상에 도움이 될 것이다.</p>


<p>보통 Deep learning을 train할 때에는 stochastic gradient descent (SGD) method를 사용한다. SGD의 속도를 높이는 가장 naive한 방법은 learning rate를 높이는 것이지만, 높은 learning rate는 보통 gradient vanishing 혹은 gradient exploding problem을 야기한다는 문제가 있다.</p>


<p>Gradient vanishing은 backpropagation algorithm에서 아래 layer로 내려갈수록, 현재 parameter의 gradient를 계산했을 때 앞에서 받은 미분 값들이 곱해지면서 그 값이 거의 없어지는 (vanish하는) 현상을 의미한다. Gradient exploding은 learning rate가 너무 높아 diverge하는 현상을 말한다. Learning rate의 값이 크면 이 두 가지 현상이 발생할 확률이 높기 때문에 우리는 보통 작은 learning rate를 고르게 된다. 그러나 우리는 이미 일반적으로 learning rate의 값이 diverge하지 않을 정도로 크면 gradient method의 converge 속도가 향상된다는 것을 알고 있다. 따라서 이 논문이 던지는 질문은 다음과 같다. 자연스럽게 나오는 궁금증은 Gradient vanishing/exploding problem이 발생하지 않도록 하면서 learning rate 값을 크게 설정할 수 있는 neural network model을 design할 수 있는가?</p>


<h5>Internal Covariate Shift: learning rate의 값이 작아지는 이유</h5>


<p>Gradient vanishing problem이 발생하는 이유에 대해서는 여러가지 설명이 가능하지만 (exploding은 그냥 우리가 값을 작게 설정하여 해결할 수 있다) 이 논문에서는 internal covariate shift라는 개념을 제안한다. Covariate shift는 machine learning problem에서 아래 그림과 같이 train data와 test data의 data distribution이 다른 현상을 의미한다. 아래 그림 참고 (<a href="http://blog.bigml.com/2014/01/03/simple-machine-learning-to-detect-covariate-shift/">출처</a>)</p>


<p><img src="/images/post/88-1.jpg" width="500"></p>

<p>이 논문에서는 단순히 train/test input data의 distribution이 변하는 것 뿐 아니라, 각각의 layer들의 input distribution이 training 과정에서 일정하지 않기 때문에 문제가 발생한다고 주장하며, 이렇게 각각의 layer들의 input distribution이 consistent하지 않은 현상을 internal convariate shift라고 정의한다. 이 논문에서 이것이 문제가 된다고 주장하는 이유는, 각각의 layer parameter들은 현재 layer에 들어오는 input data 뿐만 아니라 다른 model parameter들에도 영향을 받기 때문이라고한다. 즉, gradient vanishing problem이 발생하는 이유를 backpropagation 과정에서 아래로 내려갈수록 이전 gradient들의 영향이 더 커져서 지금 parameter가 거의 update되지 않는다고 설명하는 것과 같은 맥락이다.</p>


<p>기존에는 이런 현상을 방지하기 위하여 ReLU neuron을 사용하거나 (Nair & Hinton, 2010), cafeful initialization을 사용하거나 (Bengio & Glorot, 2010; Saxe et al., 2013), leanring rate를 작게 취하는 등의 전략을 사용했지만, 그런 방법이 아닌 다른 방법을 통해 internal covariate shift 문제가 해결이 된다면 더 높은 learning rate를 선택하여 learning 속도를 빠르게하는 것이 가능할 것이다.</p>


<p></p>

<h5>Navie approach: Whitening</h5>


<p>따라서 이 논문의 목표는 internal covariate shift를 줄이는 것이다. 그렇다면 internal covariate shift는 어떻게 줄일 수 있을까? 이 논문에서는 엄청 간단하게 input distribution을 zero mean, unit variance를 가지는 normal distribution으로 normalize 시키는 것으로 문제를 해결하며, 이를 whitening이라한다 (LeCun 1998, Wiesler & Ney 2011). 주어진 column data \(X\in R^{d\times n}\)에 대해 whitening transform은 다음과 같다.</p>


<p>\[\hat{X} = Cov(X)^{-1/2} X, Cov(X) = E[( X - E[X] ) ( X - E[X] )^\top ].\]</p>


<p>그러나 이런 naive한 approach에서는 크게 두 가지 문제점들이 발생하게 된다.</p>


<ol>
<li>multi variate normal distribution으로 normalize를 하려면 inverse의 square root를 계산해야 하기 때문에 필요한 계산량이 많다.</li>
<li>mean과 variance 세팅은 어떻게 할 것인가? 전체 데이터를 기준으로 mean/variance를 training마다 계산하면 계산량이 많이 필요하다.</li>
</ol>


<p>따라서 이 논문에서는 이런 문제점들을 해결할 수 있으면서, 동시에 everywhere differentiable하여 backpropagation algorithm을 적용하는 데에 큰 문제가 없는 간단한 simplification을 제안한다.</p>


<h5>Batch Noramlization Transform</h5>


<p>앞서 제시된 문제점들을 해결하기 위하여 이 논문에서는 두 가지 approach를 제안한다.</p>


<ol>
<li>각 차원들이 서로 independent하다고 가정하고 각 차원 별로 따로 estimate를 하고 그 대신 표현형을 더 풍성하게 해 줄 linear transform도 함께 learning한다</li>
<li>전체 데이터에 대해 mean/variance를 계산하는 대신 지금 계산하고 있는 batch에 대해서만 mean/variance를 구한 다음 inference를 할 때에만 real mean/variance를 계산한다</li>
</ol>


<p>먼저 naive approach에서 covariance matrix의 inverse square root를 계산해야했던 이유는 모든 feature들이 서로 correlated되었다고 가정했기 때문이지만, 각각이 independent하다고 가정함으로써, 단순 scalar 계산만으로 normalization이 가능해진다. 이를 수식으로 표현하면 다음과 같다.</p>


<p></p>

<p>d dimensional data \(x = (x^{(1)}, x^{(2)}, \ldots, x^{(d)})\)에 대해 각각의 차원 \(k\) 마다 다음과 같은 식을 계산하여 \(\hat x\)를 계산한다</p>


<p>\[\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}.\]</p>


<p>그러나 이렇게 correlation을 무시하고 learning하는 경우 각각의 관계가 중요한 경우 제대로 되지 못한 training을 하게 될 수도 있으므로 이를 방지하기 위한 linear transform을 각각의 dimension \(k\)마다 learning해준다. 이 transform은 scaling과 shifting을 포함한다.</p>


<p>\[y^{(k)} = \gamma \hat{x}^{(k)} + \beta.\]</p>


<p>이때 parameter \(\gamma, \beta\)는 neural network를 train하면서 마치 weight를 update하듯 같이 update하는 model parameter이다.</p>


<p>두 번째로, 전체 데이터의 expectation을 계산하는 대신 주어진 mini-batch의 sample mean/variance를 계산하여 대입한다.</p>


<p></p>

<p>이제 앞서 설명한 두 가지 simplification을 적용하여 다음과 같은 batch normalization transform이라는 것을 정의할 수 있다.</p>


<p><img src="/images/post/88-2.png" width="500"></p>

<p>이때, backpropagation에 사용되는 \(\gamma, \beta\) 그리고 layer를 위한 chain rule은 다음과 같이 계산된다.</p>


<p><img src="/images/post/88-4.png" width="500"></p>

<h5>Train/Inference with BN network</h5>


<p>앞에서 batch normalization transform을 각각의 layer input을 normalization하는데에 사용할 것이라는 설명을 했었다. 다시말해서 BN network는 기존 network에서 각각의 layer input 앞에 batch normalization layer라는 layer를 추가한 것과 구조가 동일하다.</p>


<p><img src="/images/post/88-5.png" width="500"></p>

<p>이때 자세한 알고리즘은 다음과 같다.</p>


<p><img src="/images/post/88-3.png" width="500"></p>

<p>주의해야할 점 하나는 train 과정에서는 mini-batch의 sample mean/variance를 사용하여 BN transform을 계산하였지만, inference를 할 때에도 같은 규칙을 적용하게 되면 mini-batch 세팅에 따라 inference가 변할 수도 있기 때문에 각각의 test example마다 deterministic한 결과를 얻기 위하여 sample mean/variance 대신 그 동안 저장해둔 sample mean/variance들을 사용하여 unbiased mean/variance estimator를 계산하여 이를 BN transform에 이용한다.</p>


<h5>BN network의 장점</h5>


<p>저자들이 주장하는 BN network의 장점은 크게 두 가지이다.</p>


<ol>
<li>더 큰 learning rate를 쓸 수 있다. internal covariate shift를 감소시키고, parameter scaling에도 영향을 받지 않고, 더 큰 weight가 더 작은 gradient를 유도하기 때문에 parameter growth가 안정화되는 효과가 있다.</li>
<li>Training 과정에서 mini-batch를 어떻게 설정하느냐에 따라 같은 sample에 대해 다른 결과가 나온다. 따라서 더 general한 model을 learning하는 효과가 있고, drop out, l2 regularization 등에 대한 의존도가 떨어진다.</li>
</ol>


<p>논문을 살펴보면 BN transform이 scale invariant하고, 큰 weight에 대해 작은 gradient가 유도되기 때문에 paramter growth를 안정화시키는 효과가 있다는 언급이 있다. 또한 regularization효과를 더 강화하기 위하여 매 mini-batch마다 training data를 shuffling하여 input으로 넣는데, 이때 한 mini-batch 안에서는 같은 데이터가 중복으로 나오지 않도록 shuffling하여 대입한다.</p>


<h5>실험</h5>


<p>먼저 BN network가 주장하는대로 잘 동작하는지 보여주는 실험이다.</p>


<p><img src="/images/post/88-6.png" width="500"></p>

<p>가장 왼쪽은 MNIST data에 대해 BN을 쓴 것과 쓰지 않은 것의 convergence speed를 비교한 것이며, 다음 그림들은 BN을 사용했을 때와 사용하지 않았을 때, internal covariate shift가 어떻게 변화하는지를 보여주는 것이다. 한 뉴런의 training 동안 activation value의 변화를 plot한 것으로, 가운데 있는 선이 평균 값이고, 위 아래가 variance를 의미한다고 생각하면 된다. BN을 사용하면 처음부터 끝까지 거의 비슷한 distribution을 가진다는 것을 알 수 있다.</p>


<p></p>

<p>다음으로 single network에 대해 inception network와의 성능을 비교한 실험이다</p>


<p><img src="/images/post/88-7.png" width="500"></p>

<p>세팅은 전부 같고 inception network와 비교하여 BN이 추가되었는지 여부와 learning rate가 몇 배인지 (x5는 5배의 leanring rate를 취한 것이다) 여부만 다르게 설정하였음에도 불구하고 convergence speed와 심지어 최종 max acc까지 차이가 나는 것을 볼 수 있다.</p>


<p>이런 결과들을 기반으로 약간의 paramter tunning을 거쳐 아래와 같은 ImageNet state-of-art를 기록했다고 한다.</p>


<p><img src="/images/post/88-8.png" width="500"></p>

<h5>BN 네트워크 성능 accelerating하기</h5>


<p>BN을 추가하는 것 만으로 성능 개선이 엄청나게 일어나는 것은 아니며 다음과 같은 parameter tunning이 추가로 필요하다고 한다.
1. learning rate 값을 키운다 (0.0075 &ndash;> 0.045, 5)
2. drop out을 제거한다 (BN이 regularization 효과가 있기 때문이라고 한다)
3. l2 weight regularization을 줄인다 (BN이 regularization 효과가 있기 때문이라고 한다)
4. learning rate decay를 accelerate한다 (6배 더 빠르게 가속한다)
5. local response normalization을 제거한다 (BN에는 적합하지 않다고 한다)
6. training example의 per-batch shuffling을 추가한다 (BN이 regularization 효과를 증폭시키기 위함이다)
7. photometric distortion을 줄인다 (BN이 속도가 더 빠르고 더 적은 train example을 보게 되기 때문에 실제 데이터에 더 집중한다고 한다)</p>

<p>이런 parameter tunning이 추가로 이루어지고 나면, 기존 neural network보다 ImageNet에서 훨씬 좋은 성능을 내는 neural network를 구성할 수 있다고 한다.</p>


<h5>Summary of BN network</h5>


<ul>
<li>아이디어: 각 nonlinearity의 input으로 BN transform을 추가한다</li>
<li>BN transform은 다음과 같은 두 가지 simplification으로 구성된다

<ul>
<li>각 feature들의 correlation을 무시하고 각각 따로 normalize 하고 각각에 대한 linear transform을 같이 learning한다</li>
<li>Mini-batch의 sample mean/variance로 normalize 한다</li>
</ul>
</li>
<li>BN network의 train/inference에서는 다음과 같은 특이점이 있다

<ul>
<li>Train/inference의 forward rule이 다르다 (각각이 사용하는 mean/variance가 다르다)</li>
<li>Train 과정에서 mini-batch를 (중복없이) shuffling하여 train시킨다</li>
</ul>
</li>
<li>BN network를 사용함으로써 다음과 같은 효과를 볼 수 있다

<ul>
<li>Learning rate를 큰 값으로 설정할 수 있어 converge가 빠르다</li>
<li>Generalized model을 learning하는 효과가 있다</li>
</ul>
</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, Sergey Ioffe, Christian Szegedy, ICML 2015</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/72/">Machine Learning 스터디 (16) Dimensionality Reduction (PCA, LDA)</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-06-17T05:21:00+09:00" pubdate data-updated="true">Jun 17<span>th</span>, 2015</time>
        
         | <a href="/72/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h5>들어가며</h5>


<p>Machine Learning problem을 풀다보면, 종종 high dimensional 데이터를 다뤄야할 일이 생긴다. 그런데 dimension 높은 데이터를 다루다보면 여러 문제가 발생하는데, 높은 dimension으로 인해 생기는 대표적인 문제가 <a href="/59#59-4-cd">예전 글</a>에서 다뤘던 Curse of dimensionality이다. 또한 많은 algorithm들에서 dimension이 complexity에 영향을 주는 경우가 많으므로 높은 dimension은 알고리즘의 성능에 악영향을 미치는 경우가 많다. 그렇기 때문에 많은 경우 데이터의 dimension이 높다면 다양한 방식의 dimenionality reduction 기술을 적용해 데이터의 차원을 낮추는 작업을 한다. 일반적으로 많이 사용하는 Dimensionality Reduction으로는 LDA와 PCA가 있으며, ICA, CCA 등의 방법도 종종 사용되며, 그 이외에도 RBM, Auto-encoder 등의 Neural network와 관련된 모델들도 존재한다. 이 글에서는 가장 많이 사용되는 방법들인 LDA와 PCA에 대해서만 다룰 것이다.</p>


<h5>Recall: Curse of Dimensonality</h5>


<p>Curse of Dimensionality는 데이터의 차원이 높아질수록 발생하는 여러 문제들을 통틀어 일컫는 말이다. 이런 문제가 발생하는 이유는 차원이 높아질수록 우리가 일반적으로 사용하는 Euclidean distance가 예상치 못한 방식으로 동작하기 때문이다. 예를 들어 d-차원 공간에서 임의의 점으로부터 거리가 1인 점들을 모아놓은 공간을 생각해봤을 때, d가 점점 커지면 커질수록 그 구의 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 shell에 존재한다는 것을 이미 예전 글에서 증명한바 있다. 다시 말해서 아주 높은 차원의 데이터는 우리가 원하지 않는 방향으로 움직일 가능성이 크다.</p>


<h5>Feature Extraction</h5>


<p>많은 상황에서 차원의 크기는 feature의 개수를 의미한다. 예를 들어 키, 몸무게, 나이, 성별이라는 네 가지 정보를 가지고 클러스터링을 한다고 생각해보자. 이 경우 데이터의 차원은 4이다. 그런데 아마도 이 네 가지 정보 이외에도 소득, 학력, 자산크기 등의 정보 등을 추가로 사용해 클러스터링을 한다면 더 좋은 클러스터링이 가능할지도 모른다. 문제는, 모든 feature가 전부 의미있는 feature는 아닐 수 있다는 것이다. 몸무게 정보를 사용하여 클러스터링한 것과 사용하지 않고 클러스터링한 것 중에서 몸무게 정보를 사용하지 않고 클러스터링 한 것이 더 좋을 수도 있다는 것이다. 이렇게 주어진 정보들 중에서 정말 의미 있는 feature를 뽑아내는 과정을 feature extration이라고 한다. 가장 간단한 feature extraction은 모든 feature를 사용해보기도 하고 사용해보지 않기도 하면서 \(2^d\) 개의 조합을 모두 확인해보는 것이다. 그러나 이 방법은 차원의 크기에 exponential할 뿐 아니라, 만약 기존 feature가 highly correlate되어있고, 여러 개의 feature를 묶어서 한 feature로 만들어야 성능이 좋아지는 경우 등에 대해 좋은 성능을 내기 어렵다. 따라서 이런 상황에서도 dimensionality reduction 방법을 사용해 feature를 뽑아낼 수 있다. 만약 우리가 100개의 feature를 가지고 있을 때, &#8216;가장 좋은&#8217; 30개의 feature만 뽑기 위해서 30차원으로 dimensionality reduction을 하는 것이다.</p>


<h5>Dimensionality Reduction</h5>


<p>데이터의 차원을 낮춘다는 것의 의미는, 현재 데이터가 존재하는 차원에서 그보다 낮은 다른 차원으로 데이터들을 mapping시키는 map을 찾는다는 것과 같다고 할 수 있다. 이때 어떤 임의의 차원에서 그보다 낮은 임의의 낮은 차원으로 가는 mapping은 셀 수 없이 많다. 그렇다면 우리는 어떤 mapping을 선택해야할까? 예를 들어 가장 간단한 방법으로, d 차원 데이터를 d&#8217; 차원으로 보내고 싶을 때, 앞에서 설명했던 간단한 방법처럼 임의의 d&#8217; 개의 축을 골라서 그 축만 사용하는 방법도 있을 수 있고, <a class="red tip" title="projection이라고도 한다.">d에서 d&#8217;으로 가는 linear map</a>을 임의로 하나 고르는 것도 가능하다. 물론 non linear map도 가능하지만, 이 글에서 다룰 LDA와 PCA 두 가지 방법은 모두 linear mapping을 찾는 알고리즘이다. LDA는 supervised learning이며, PCA는 unsupervised learning에 해당하게 된다. 모든 문제에서 데이터는 matrix \(X\)로 표기되며, 데이터의 차원은 d이고, 개수는 n개이다. 따라서 \(X\)는 d by n matrix가 된다.</p>


<h5>LDA</h5>


<p><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis (LDA)</a>는 Dimensionality reduction만을 위한 방법은 아니다. LDA로 약어가 표시되는 것들이 꽤 많아서 (예: Latent Dirichelt Allocation) 이 모델을 처음 제안한 사람의 이름을 따서 Fisher&#8217;s LDA 라고 부르기도 한다. LDA는 여러 클래스가 존재할 때 그 클래스들을 최대한 잘 분리시키는 projection을 찾는다. 철학은 굉장히 단순한데, projection 시킨 데이터들에서 같은 클래스에 속하는 데이터들의 variance는 최대한 줄이고 (\(\sigma_{within}\)), 각 데이터들의 평균 값들의 variance는 최대한 키워서 (\(\sigma_{between}\)) 클래스들끼리 최대한 멀리 떨어지게 만드는 것이다. 이를 수식으로 표현하면 다음과 같은 수식을 얻을 수 있다.</p>


<p>\[S = \frac{\sigma_{between}^2}{\sigma_{within}^2}\]</p>


<p>일단 가장 간단한 상황인 클래스가 2개일 때의 상황만 고려해보도록하자. 참고로 LDA의 결과는 항상 클래스 개수 - 1 개까지의 벡터 밖에 찾을 수 없기 때문에, 이 상황에서 우리가 찾게 될 projection은 1차원 projection을 찾는 것이므로 간단하게 vector \(w\)로 기술하도록 하겠다. 클래스가 2개 뿐이라면, 위의 식은 엄청 간단한 수식으로 바뀌게 된다. 먼저 \(\sigma_{between}\)은 데이터가 단 두 개 뿐이기 때문에 간단하게 \( (w \cdot \mu_1 - w \cdot \mu_2 )^2 \)으로 표현이 된다. 이때, \(\mu_1, \mu_2\)는 각각 1번째 클래스와 2번째 클래스에 속한 데이터들의 평균 값이다. 다음으로 \(\sigma_{within}\) 역시 어렵지 않게 계산할 수가 있다. Projection을 하게 되면 데이터의 variance는 \(w^\top \Sigma w\)로 표현이 되기 때문에, 1번 클래스와 2번 클래스에 대해 이 값을 계산하고 더해주기만 하면 된다. 식을 정리해보면</p>


<p>\[w = \arg\min_w \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_w\frac{(w \cdot \mu_1 - w \cdot \mu_2 )^2}{w^\top \Sigma_1 w + w^\top \Sigma_2 w} \]</p>


<p>그리고 위 식을 w에 대해 미분하고 좀 정리해보면 \(w \propto    (\Sigma_1 + \Sigma_2)^{-1}(\mu_1 - \mu_2) \) 라는 식을 얻을 수 있다.</p>


<h5>Multiclass LDA</h5>


<p></p>

<p>그러면 클래스가 2개보다 많을 때, 벡터 \(w\)가 아닌 subspace \(U\)를 찾는 과정은 어떻게 되는지 살펴보도록하자. 다시 \(\sigma_{within}\)과 \(\sigma_{between}\)을 살펴보자. 먼저 \(\sigma_{between}\)은 위와 비슷한 형태로 표현할 수 없다. 그러나 우리가 각각의 클래스의 평균을 \(\mu_i\)라고 정의한다면, 그냥 이 값들의 variance를 계산하기만 하면 된다. 이 variance는 \(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\)로 표현이 된다. 이때 \(\mu\)는 모든 평균들의 평균이다. 이 variance가 있으므로, projection하여 얻는 variance도 앞 뒤에 proejction matrix를 곱해 쉽게 계산할 수 있다. \(\sigma_{within}\)은 위와 비슷한 방식으로 \(\sigma_{within} = U^\top \left(\sum_i \Sigma_i \right) U \)로 표현할 수 있다. 이때 \(\Sigma_i\)는 i번째 클래스의 variance이다. 이 식을 정리해보면 다음과 같은 식을 얻는다.</p>


<p>\[U = \arg\min_U \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_U \frac{U^\top \left(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\right) U}{U^\top \left(\sum_i \Sigma_i \right) U} = \arg\max_U \frac{U^\top A U}{U^\top B} \]</p>


<p>\(A\)와 \(B\)는 각각 괄호 안에 있는 값을 의미한다. 위 식에서 보게 되면, 분자에 해당하는 부분이 rank가 클래스 개수 - 1 이기 때문에, 우리가 이 식을 풀었을 때도 \(U\)의 rank가 클래스 개수 - 1이 되므로 LDA가 구할 수 있는 subspace의 축 개수가 클래스 개수 - 1로 제한되는 것이다. 이 식을 풀기 위해서는 eigenvalue를 계산하는 것으로 간단하게 식을 풀 수 있다. 지금부터 왜 이 식이 eigenvalue를 푸는 것으로 해결이 가능한지를 살펴보자.</p>


<h5>General Eigenvector problem</h5>


<p>문제를 간단하게 하기 위해 전체 subspace가 아닌 vector \(w\)만 고려해보도록 하자. 따라서 식은</p>


<p>\[\min_w \frac{w^\top A w}{w^\top B w}\]</p>


<p>로 표현 가능하다. 이제 이 식을 w에 대해서 미분해보면 다음과 같은 식을 얻게 된다</p>


<p>\[\left(w^\top B w\right)^2 \big[ 2A w \left( w^\top B w \right) - 2B w \left( w^\top A w \right) \big] = 0\]</p>


<p>이를 잘 정리하면</p>


<p>\[Aw = \frac{w^\top A w} {w^\top B w} B w\]</p>


<p>로 표현이 되고, 우리가 원래 풀려고 했던 \(\frac{w^\top A w} {w^\top B w}\)를 \(\lambda\)라고 정의하면 식이 \(A w = \lambda B w\)라는 엄청 간단한 식이 나오게 된다. 이런 형태를 만족하는 \(\lambda\)를 찾는 문제를 general eigenvalue problem이라 부른다. 만약 \(B\)가 Identity matrix라면 우리가 아는 일반 eigenvalue problem이 된다. 따라서 원래 문제가 \(\lambda\)의 minimum을 찾는 것이었으니 이제 가장 작은 general eigenvalue를 찾기만 하면 우리가 풀고 싶었던 문제를 풀 수 있는 것이다.</p>


<h5>PCA</h5>


<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>는 Dimensionality reduction의 가장 대표적인 방법 중 하나이다. PCA는 projection된 데이터의 variance가 최대화되는 projection matrix를 찾는 문제이다. 그런데 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Low-rank_matrix_approximation">Eckart–Young theorem</a>에 의해서, 이 문제의 답이 데이터 \(X\)에 대한 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition(SVD)</a>으로 계산할 수 있다는 것이 알려져 있다. 그래서 PCA를 variance를 maximize하는 원래 정의대로 설명이 하는 경우도 많고, low rank matrix 문제로 설명하는 경우도 있다. 이 글에서는 low rank matrix estimation 문제로 설명하도록 하겠다. 이 문제의 objective는 아래 식과 같다.</p>


<p>\[\min_{rank(Z)=k} \| X - Z \|_F^2\]</p>


<p>이때 \(\|A\|_F\)는 <a href="http://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a>을 의미한다. 이 norm은 matrix의 모든 element들의 제곱을 더한 값이며, \(\| A \|_F^2 = {\tt tr} (A A^\top) = {\tt tr} (A^\top A) \) 라는 특성이 알려져있다. 이제 위의 식에서 \(Z\)를 UV decomposition한 후 대입해보면 다음과 같은 식을 얻는다.</p>


<p>\[\min_{U\in R^{d \times k}, V\in R^{n \times k}, U^\top U = I} \|X - UV^\top\|_F^2\]</p>


<p>이 식을 \(V\)에 대해 미분하면 optimal한 V는 \(V=X^\top U\)로 표현이 됨을 알 수 있으며, 이를 위의 식에 대입한 후, Frobenius norm의 성질을 잘 활용하면 아래와 같은 식이 나온다.</p>


<p>\[\max_{U\in R^{d \times k}, U^\top U = I} {\tt tr} (U^\top X X^\top U)\]</p>


<p>가 되며, 이 식의 답은 SVD를 통해 계산할 수 있다는 것을 알 수 있다. 그러나 만약 \(X\)의 mean이 0가 아니게 되면 UV decomposition을 하는 과정에서 문제가 생기게 되서 같은 방식으로 깔끔하게 구할 수가 없다. 이 과정은 다소 복잡하므로 생략하고 결과만 얘기하면, 그냥 \(\hat X = X - \frac{1}{n} X \mathbf 1\)을 SVD하면 된다. 뒤에 있는 term은 그냥 X의 평균값이다.</p>


<p>정리하면, 임의의 데이터 X에 대한 PCA는 다음과 같이 구할 수 있다.</p>


<ol>
    <li>데이터 \(X\)의 empirical mean을 계산한 후 모든 데이터에서 평균을 빼준다.</li>
    <li>새로 만들어진 데이터 \(\hat X\)의 가장 큰 singular value부터 k번째 큰 singular value까지에 대응하는 singular vector들을 구한다. (SVD를 통해)</li>
    <li>뽑아낸 k개의 singular vector로 U를 구성하고 return한다.</li>
</ol>


<p>쉽지만 그만큼 강력하다. 하지만 PCA의 경우 Frobenius norm의 제곱값을 사용하므로 각 element들이 norm을 계산할 때 한 번 제곱되고, 다시 전체에 제곱을 취할 때 또 제곱이 취해지므로 조금이라도 원래 데이터와 다른 outlier가 존재하게 된다면 그 효과가 굉장히 극적으로 증폭되기 때문에 noise에 취약하다. 이를 방지하기 위해 objective의 norm을 1-norm으로 바꾸거나 하는 등의 robust PCA 연구도 활발하게 진행되고 있다.</p>


<h5>정리</h5>


<p>Dimensionality Reduction은 매우 유용하고 많이 쓰이는 툴이다. 특히 PCA는 굉장히 빈번하게 사용되고 알고리즘도 매우 간단하기 때문에 알아두면 쓰임새가 많다. 이 글에서는 LDA와 PCA만 다뤘지만, ICA, CCA, RBM 등 굉장히 많은 dimensionality reduction 기술들이 존재한다. 이 중 RBM은 추후에 다시 한 번 자세하게 설명하도록 하겠다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 17일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Nie, Feiping, Jianjun Yuan, and Heng Huang. &ldquo;Optimal mean robust principal component analysis.&rdquo; Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.</li>
</ul>


<hr>


<p><a href="/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="/57">Machine Learning이란?</a></li>
<li><a href="/58">Probability Theory</a></li>
<li><a href="/59">Overfitting</a></li>
<li><a href="/60">Algorithm</a></li>
<li><a href="/61">Decision Theory</a></li>
<li><a href="/62">Information Theory</a></li>
<li><a href="/63">Convex Optimzation</a></li>
<li><a href="/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/70/">Machine Learning 스터디 (14) EM Algorithm</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-06-14T03:50:00+09:00" pubdate data-updated="true">Jun 14<span>th</span>, 2015</time>
        
         | <a href="/70/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h5>들어가며</h5>


<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM 알고리즘</a>은 <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 하나이다. 굉장히 많은 probabilistic 모델을 풀기 위해 널리 사용되는 알고리즘 중 하나이며, iterative한 알고리즘 중 하나이다. Clustering에서 다뤘던 GMM은 물론이고, HMM, RBM 등의 문제를 해결하는데 있어서도 사용되는 알고리즘이다. 이 글에서는 EM 알고리즘이 무엇인지, latent variable이 존재하는 probabilistic model은 무엇이며 어떤 장점이 있는지를 다룰 것이며, EM 알고리즘의 의미와 더 나아가 이 알고리즘이 어떻게 MLE나 MAP문제를 해결하는지에 대해 다룰 것이다.</p>


<h5>Probabilistic model having latent variable</h5>


<p>EM 알고리즘에 대해 다루기 전에 먼저 latent variable을 가지고 있는 probabilistic model에 대해 설명하도록 하겠다. Latent variable은 우리가 본래 가지고 있는 random variable이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다. 예를 들어, 아래 그림과 같은 Grapical model을 고려해보자. 이때 우리가 관측할 수 있는 random variable은 paramter \(\theta\)로 parameterized 되어있는 \(\mathbf X\) 하나이고, \(\mathbf Z\)은 우리가 관측할 수 없는 hidden variable이라고 해보자.</p>


<p><img src="/images/post/70-1.png" width="200"></p>

<p>만약 위의 grapical model에서 \(\mathbf X\)의 maximum likelihood를 계산하고 싶다면 어떻게 해야할까? 먼저 \(\mathbf X\)의 maximum likelihood는 다음과 같이 표현된다.</p>


<p>\[\max_{\theta} p(\mathbf X | \theta) = \sum_{\mathbf Z} p(\mathbf X,Z | \theta). \]</p>


<p>문제를 조금 더 간단하게 하기 위하여 위의 식에서 \(\mathbf Z\)는 discrete variable이라고 정의하였다. 이 문제에서 우리가 가정할 것이 하나있다. 바로 marginal distribution \(p(\mathbf X | \theta)\)를 직접 계산하는 것이 매우 까다롭다는 것이다. 이때, \(\mathbf Z\)는 우리 마음대로 정할 수 있는 latent variable이기 때문에, joint distribution \(p(\mathbf X,Z | \theta)\)가 marginal distribution보다 쉬운 \(\mathbf Z\)를 잡는 것이 가능하다.</p>


<h5>Decomposition of log-likelihood</h5>


<p>만약 우리가 latent variable \(\mathbf Z\)의 marginal distribution을 \(q(\mathbf Z)\)라고 정의한다면, 앞에서 설명한 log-likelihood를 다음과 같이 decompose할 수 있다.</p>


<p>\[\ln p(\mathbf X | \theta) = \mathcal L(q,\theta) + ~\mbox{KL}(q\|p),\]
이때, \(\mathcal L(q,\theta)\)와 \(\mbox{KL}(q\|p)\)는 다음과 같이 정의된다. \[\mathcal L(q,\theta) = \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf X, \mathbf Z | \theta)}{q(\mathbf Z)} ~\mbox{and}~ \mbox{KL}(q\|p) = - \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf Z | \mathbf X, \theta)}{q(\mathbf Z)}. \]</p>


<p>위의 식에서 \(\mathcal L(q,\theta)\)는 hidden variable \(\mathbf Z\)의 marginal distribution \(q(\mathbf Z)\)의 functional이고, \(\mbox{KL}(q\|p)\)는 \(q,p\)의 KL divergence를 의미한다. 이렇게 log-likelihood를 decompose하게 되면, 한 쪽에는 random variable \(\mathbf X, \mathbf Z\)의 joint distribution, 그리고 또 한 쪽은 conditional distribution으로 표현이 된다는 것을 알 수 있다. 또한 KL divergence의 특성에서부터 재미있는 사실을 하나 더 유추할 수 있는데, 바로 KL divergence가 반드시 0보다 크거나 같기 때문에 \(\mathcal L(q,\theta)\)이 곧 log-likelihood의 lower bound가 된다는 사실이다. 이를 그림으로 나타내면 아래 그림의 오른쪽 그림과 같다. (일단 \(theta^{\mbox{old}}, theta^{\mbox{new}}\)는 무시하자)</p>


<p><img src="/images/post/70-2.png" width="500"></p>

<h5>EM algorithm</h5>


<p>위와 같은 사실로부터 lower bound가 maximum이 되도록하는 \(\theta\)와 \(q(\mathbf Z)\)의 값을 찾고, 그에 해당하는 log-likelihood의 값을 찾는 알고리즘을 설계하는 것이 가능할 것이다. 만약 \(\theta\)와 \(q(\mathbf Z)\)를 jointly optimize하는 문제가 어려운 문제라면 이 문제를 해결하는 가장 간단한 방법은 둘 중 한 variable을 고정해두고 나머지를 update한 다음, 나머지 variable을 같은 방식으로 update하는 alternating method일 것이다. EM 알고리즘은 이런 아이디어에서부터 시작하게 된다. EM 알고리즘은 E-step과 M-step 두 가지 단계로 구성된다. 각각의 step에서는 앞서 설명한 방법처럼 \(\theta\)와 \(q(\mathbf Z)\)를 번갈아가면서 한 쪽은 고정한채 나머지를 update한다. 이런 alternating update method는 한 번에 수렴하지 않기 때문에, EM 알고리즘은 E-step과 M-step을 알고리즘이 수렴할 때 까지 반복하는 iterative 알고리즘이 된다.</p>


<p>현재 우리가 가지고 있는 parater \(\theta\)의 값을 \(\theta^{\mbox{old}}\)라고 정의해보자. EM 알고리즘의 E-step은 먼저 \(\theta^{\mbox{old}}\) 값을 고정해두고 \(\mathcal L(q,\theta)\)의 값을 최대로 만드는 \(q(\mathbf Z)\)의 값을 찾는 과정이다. 이 과정은 매우 간단하게 계산 수 있는데, 그 이유는 log-likelihood \(\ln p(\mathbf X | \theta^{\mbox{old}})\)의 값이 \(q(\mathbf Z)\) 값과 전혀 관계가 없기 때문에, 항상 \(\mathcal L(q,\theta)\)를 최대로 만드는 조건은 KL divergence가 0이 되는 상황이기 때문이다. KL divergence는 \(q(\mathbf Z) = p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\) 인 상황에서 0이 되기 때문에, \(q(\mathbf Z)\)에 posterior distribution \(p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\)을 대입하는 것으로 해결할 수 있다. 따라서 E-step은 언제나 KL-divergence를 0으로 만들고, lower bound와 likelihood의 값을 일치시키는 과정이 된다.</p>


<p>E-step에서 \(\theta^{\mbox{old}}\)을 고정하고 \(q(\mathbf Z)\)에 대한 optimization 문제를 풀었으므로 M-step에서는 그 반대로, \(q(\mathbf Z)\)를 고정하고 log-likelihood를 가장 크게 만드는 새 paramter \(\theta^{\mbox{new}}\)을 찾는 optimization 문제를 푸는 단계가 된다. E-step에서는 update하는 variable과 log-likelihood가 서로 무관했기 때문에 log-likelihood가 증가하지 않았지만, M-step에서는 \(\theta\)가 log-likelihood에 직접 영향을 미치기 때문에 log-likelihood 자체가 증가하게 된다. 또한 M-step에서 \(\theta^{\mbox{old}}\)가 \(\theta^{\mbox{new}}\)로 바뀌었기 때문에 E-step에서 구했던 \(p(\mathbf Z)\)로는 더 이상 KL-divergence가 0이 되지 않는다. 따라서 다시 E-step을 진행시켜 KL-divergence를 0으로 만들고, log-likelihood의 값을 M-step을 통해 키우는 과정을 계속 반복해야만한다.</p>


<p>위에 나왔던 그림에서 왼쪽이 E-step을 의미하고, 오른쪽 그림이 M-step을 의미한다. E-step을 의미하는 왼쪽 그림에서 KL divergence는 0이 되고, lower bound인 functional과 log-likelihood의 값이 같아진다. 오른쪽 그림은 M-step을 표현하고 있으며, \(\theta\)가 update되면서 log-likelihood의 값이 증가하게 되지만, 더 이상 KL divergence의 값이 0이 아니게 된다. 이 과정을 더 이상 값이 변화하지 않을 때 까지 충분히 많이 돌리게 되면 이 값은 log-likelihood의 어떤 값으로 수렴하게 될 것이다. 그리고 매 step마다 항상 optimal한 값으로 진행하기 때문에 이 값은 log-likelihood의 local optimum으로 수렴하게 된다는 사실까지 알 수 있다. EM algorithm은 아래와 같은 그림으로 표현할 수 있다.</p>


<p><img src="/images/post/70-3.png" width="500"></p>

<p>각 curve는 \(\theta\) 값이 고정이 되어있을 때 \(q(\mathbf Z)\)에 대한 lower bound \(\mathcal L(q,\theta)\)의 값을 의미한다. 매 E-step마다 고정된 \(\theta\)에 대해 \(p(\mathbf Z)\)를 풀게 되는데, 이는 곧 log-likelihood와 curve의 접점을 찾는 과정과 같다. 또한 M-step에서는 \(\theta\) 값 자체를 현재 값보다 더 좋은 지점으로 update시켜서 curve 자체를 이동시키는 것이다. 이런 과정을 계속 반복하면 알고리즘은 언젠가 local optimum으로 수렴하게 될 것이다. Local optimum에 수렴한다는 성질은 얼핏보면 나빠보일 수도 있지만, 이 글의 도입부에서 latent variable이 introduce되는 이유 자체가 원래 log-likelihood를 계산하는 것이 불가능에 가깝기 때문이었다는 사실을 돌이켜본다면, latent variable을 잘 잡기만 한다면 반드시 local optimum으로 수렴하는 EM 알고리즘은 매우 훌륭한 알고리즘이라는 사실을 알 수 있다. 즉, 아예 문제를 풀지 못하는 것 보다는 local optimum으로 수렴하는 것이 훨씬 좋다.</p>


<h5>Pratical issues</h5>


<p>대부분의 probabilistic model의 MLE 혹은 MAP는 EM 알고리즘을 사용하면 구할 수 있다. 그러나 EM 알고리즘이 항상 잘 동작하는 것은 아닌데, E-step 혹은 M-step의 optimization 문제를 푸는 것이 어려운 상황이 그러하다. E-step은 posterior를 계산하는 과정이므로 크게 문제가 되는 경우는 많지 않지만, M-step은 \(\theta\)에 대한 optimization 문제를 풀어야하는 과정인데, 이 과정에서 문제가 발생하는 경우가 많다. 예를 들어 모든 \(\theta\)를 한 번에 jointly optimize하는 것이 어려워 또 다른 alternative method를 사용해야할 수 도 있다. 그렇게 되면 iterative 알고리즘 안에 nested iterative 알고리즘이 발생하게 되어 전체 알고리즘의 수렴 속도가 매우 느려지게 된다. 가장 단순하게 이 문제를 해결하는 방법으로는 nested iterative 알고리즘을 완전히 푸는 것이 아니라, 수렴여부와 관계없이 iteration을 조금만 돌리고 다시 E-step을 구하고, 다시 M-step을 정확히 푸는 대신 iteration을 몇 번만 돌리는 등의 방식이 있을 것이다. 일반적인 경우에는 이런 방식이 수렴하지 않지만, 몇몇 경우에는 이런 방식이 local optimum에 수렴한다는 것이 증명되어있다. 가장 대표적인 예가 RBM을 푸는 Contrastive Divergence이다. 이에 대한 더 자세한 설명은 추후에 Deep learning에 대해 다루는 글에서 더 자세히 다루도록 하겠다.</p>


<h5>정리</h5>


<p>EM 알고리즘은 latent variable이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 가장 대표적인 알고리즘이다. 본문에서는 MLE를 계산하는 과정만 다뤘지만, MAP도 비슷한 방식으로 구할 수 있다. Latent variable을 쉽게 잡기만한다면, 아무리 풀기 어려운 문제일지라도 구하고자 하는 문제의 local optimum에 수렴한다는 좋은 성질을 가지고 있기 때문에 매우 다양한 모델에서 이 알고리즘을 사용해 문제를 해결하고는 한다. 그러나 간혹 특히 M-step의 optimization을 푸는 과정에서 시간이 너무 오래 걸리는 문제가 발생하는 경우가 생길 수 있는데, 이런 경우 몇 가지 휴리스틱을 사용해 문제를 해결하기도 한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 14일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
</ul>


<hr>


<p><a href="/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="/57">Machine Learning이란?</a></li>
<li><a href="/58">Probability Theory</a></li>
<li><a href="/59">Overfitting</a></li>
<li><a href="/60">Algorithm</a></li>
<li><a href="/61">Decision Theory</a></li>
<li><a href="/62">Information Theory</a></li>
<li><a href="/63">Convex Optimzation</a></li>
<li><a href="/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/69/">Machine Learning 스터디 (13) Clustering (K-means, Gaussian Mixture Model)</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-03-25T03:20:00+09:00" pubdate data-updated="true">Mar 25<span>th</span>, 2015</time>
        
         | <a href="/69/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h5>들어가며</h5>


<p><a href="57-4-ClassML">첫 번째 글</a>에서 설명했던 것 처럼 Machine Learning은 크게 Supervised Learning, Unsupervised Learning 그리고 Reinforcement Learning으로 구분된다. 앞서 이미 그 중 <a href="/64">Supervised Learning</a>을 간략하게 다룬 글이 있었고, 이 글에서는 그 중 Unsupervised Learning의 가장 대표적인 예시인 Clustering 대해 다룰 것이며 가장 대표적이고 간단한 두 가지 알고리즘에 대해서 역시 다룰 것이다.</p>


<h5>What is Clustering?</h5>


<p><a href="http://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a>은 <a href="http://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning</a>의 일종으로, label 데이터 없이 주어진 데이터들을 가장 잘 설명하는 cluster를 찾는 문제이다. 왜 클러스터링이 필요할까? Classification을 하기 위해서는 데이터와 각각의 데이터의 label이 필요하지만, 실제로는 데이터는 존재하지만 그 데이터의 label이나 category가 무엇인지 알 수 없는 경우가 많기 때문에 classfication이 아닌 다른 방법을 통해 데이터들을 설명해야하는 경우가 발생한다. 아래 그림은 클러스터링이 어떤 것인지 잘 보여주는 그림이다.</p>


<p><img src="/images/post/69-1.png" width="500"></p>

<p>우리에게 처음 주어진 것은 왼쪽 파란 데이터이다. 각각의 데이터에 대한 정보는 아무 것도 없는 상태에서 주어진 데이터들을 가장 잘 설명하는 클러스터를 찾아내는 것이 클러스터링의 목적이다. 따라서 클러스터링은 대부분 Optimization 문제를 푸는 경우가 많다. 이는 클러스터링 뿐 아니라 다른 많은 unsupervised learning에서도 역시 마찬가지이다.</p>


<p><a href="/57">첫 번째 글</a>과 <a href="/64">이전 글</a>에서 머신러닝 문제는 먼저 주어진 데이터에 대한 가정을 하고, 해당 가정을 만족하는 best hypothesis를 찾는 문제라고 언급한 적이 있다. Clustering 문제 역시 Machine Learning 문제이므로 데이터에 대한 가정을 먼저 해야하고, best hypothesis를 찾는 과정을 거친다. 따라서 각각의 서로 다른 clustering algorithm들은 서로 다른 assumption을 가지고 있으며, 해당 assumption을 가장 잘 만족하는 function parameter를 계산하는 과정이다. 앞으로 설명하게 될 알고리즘들에 대한 설명을 읽을 때 데이터에 대해 어떤 가정을 하였는지 꼼꼼히 확인하면서 읽으면 알고리즘을 이해하기 한결 수월할 것이다.</p>


<h5>K-means</h5>


<p>클러스터를 정의하는 방법에는 여러 가지가 있을 수 있지만, 가장 간단한 정의 중 하나는 클러스터 내부에 속한 데이터들이 서로 &#8216;가깝다&#8217;라고 정의하고, &#8216;가장 가까운&#8217; 내부 거리를 가지는 클러스터를 고르는 것이다. <a href="http://en.wikipedia.org/wiki/K-means_clustering">K-means</a>는 같은 클러스터에 속한 데이터는 서로 &#8216;가깝다&#8217; 라고 가정한다. 이때 각각의 클러스터마다 &#8216;중심&#8217;이 하나씩 존재하고, 각각의 데이터가 그 중심과 &#8216;얼마나 가까운가&#8217;를 cost로 정의한다. K-means는 이렇게 정의된 cost를 가장 줄이는 클러스터를 찾는 알고리즘이다. 수식으로 적으면 다음과 같다.</p>


<p>$$ \min_{b,w} \sum_i^n \sum_j^k w_{ij} \| x_i - b_j \|_2^2 \text{ s.t. } \sum_j w_{ij} = 1, \forall j$$</p>


<p>데이터는 \(n\)개 있으며 클러스터는 \(k\)개 있다고 가정했다. 이때, \(b_j\)는 \(j\) 번째 클러스터의 &#8216;중심&#8217;을 의미하며, \(w_{ij}\)는 \(i\) 번째 데이터가 \(j\) 번째 클러스터에 속하면 1, 아니면 0을 가지는 binary variable이다. 또한 뒤에 붙는 조건은 반드시 각 데이터 별로 한 개의 클러스터에 assign이 되어야한다는 constraint이다.</p>


<p>이 문제는 풀기 쉬운 문제가 아니다. Binary variable \(w_{ij}\) 때문에, 모든 cluster 조합을 하나하나 확인해야만 optimal한 값을 구할 수 있다. 즉, jointly optimize하는 것이 매우 어렵다. 그러나 재미있게도 \(b\)와 \(w\) 둘 중 하나를 고정하고 나머지 하나를 update하는 것은 매우 간단하다. 나머지 값이 고정되었을 때 \(b_j\)의 optimal값은 \(j\) 번째 클러스터의 &#8216;mean&#8217;을 계산하는 것이다 (이 때문에 &#8216;\(k\)&#8217; 개의 &#8216;mean&#8217;을 찾는다고 해서 k-means 알고리즘이다). \(w_{ij}\)의 optimal 값은 모든 데이터 i에 대해, 각각 모든 클러스터 중에서 \(x_i - b_j\)가 가장 작은 클러스터에 assign하는 것이 optimal한 solution이다. 이렇듯 만약 다른 변수 하나를 정확하게 알고 있다고 생각하면 아주 간단한 방법으로 alternative optimization이 가능하다. 사실 이 개념은 예전에 적은 <a href="/63">convex optimization</a>글에서 잠깐 언급했던 coordinate descent 방법과 거의 유사하다. K-means 알고리즘은 이렇게 \(b\)와 \(w\)를 alternative하게 계속 update하면서 \(b\)와 \(w\)가 더 이상 바뀌지 않을 때 까지 계산을 반복하는 알고리즘이다. 안타깝게도 K-means는 global optimum에 수렴하지 않고 local한 optimum에 수렴하므로 initialization에 매우매우 취약하다는 단점이 존재한다.</p>


<p>또한 여담으로 K-means objective function에 사용한 \(\ell_2\) norm의 제곱이 outlier 혹은 noise에 매우 취약하기 때문에 조금 더 outlier에 덜 sensitive한 &#8216;robust한&#8217; norm을 사용하는 방법도 존재한다. 예를 들어 \(\ell_2\) norm의 제곱을 \(\ell_1\) norm으로 바꾸면 &#8216;mean&#8217; 대신에 &#8216;median&#8217;을 찾는 문제로 바뀌게 된다. 이를 <a href="http://en.wikipedia.org/wiki/K-medians_clustering">k-median</a>이라고 부른다. 그 밖에도 k-means의 robustness를 개선하기 위한 다양한 방법들이 개발이 되어있지만 이 글에서는 다루지 않도록 하겠다.</p>


<h5>Gaussian Mixture Model</h5>


<p>K-means algorithm의 key idea는 &#8216;alternative update&#8217;이다. 즉, coordinate wise로 다른 변수들을 고정한 채로 &#8216;alternative&#8217;하게 변수들을 update함으로써 jointly optimization을 할 수 없는 문제를 푸는 것이다. 비록 그 결과가 global하지 않은 local에 converge하더라도, 찾지 못하는 것보다는 훨씬 낫기 때문에 실제로 이런 방법이 많이 쓰인다. 이번 섹션에서는 이런 방법을 사용하는 또 다른 알고리즘을 하나 소개하도록 하겠다.</p>


<p>Gaussian Mixture Model, Mixture of Gaussian, GMM, 혹은 MoG는 데이터가 &#8216;Gaussian&#8217;의 &#8216;Mixture&#8217;로 구성이 되어있다고 가정한다. 보통 GMM이라고 많이 부르며, 이 글에서 다루는 GMM은 가장 optimal한 GMM을 찾는 알고리즘을 의미한다. 즉, 데이터가 \(k\)개의 gaussian으로 구성되어있다고 했을 때, 가장 데이터를 잘 설명하는 \(k\)개의 평균과 covariance를 찾는 알고리즘이다. 아래 그림은 3개의 gaussian으로 구성되어있다고 가정하고 그 gaussian 분포들을 찾은 결과이다.</p>


<p><img src="/images/post/69-2.png" width="500"></p>

<p>모든 machine learning 문제는 &#8216;performance measure&#8217;를 가진다고 예전에 얘기한 적이 있다. GMM의 performance measure는 log likelihood function이다. 즉, 주어진 paramter에 대해 데이터 X의 확률을 가장 크게 만드는 parameter를 찾는 것이 목표가 된다. log likelihood는 \(\ln p(X|\theta)\)로 정의가 된다. 우리가 찾아야하는 parameter \(\theta\)는 각각의 gaussian의 평균 \(\mu_j\), covariance \(\Sigma_j\), 그리고 마지막으로 각각의 데이터가 각각의 gaussian에 속할 확률 \(\pi_j\)로 구성된다. 따라서 주어진 \(\mu_j, \Sigma_j\)에 대한 \(x_i\)의 multinomial gaussian distribution을 \(N(x_i|\mu_j, \Sigma_j)\)라고 정의한다면 log likelihood function은 다음과 같이 기술할 수 있다.</p>


<p>$$ \ln p(X|\pi, \mu, \Sigma) = \sum_i^n \ln \sum_j^k \pi_j N (x_i | \mu_j, \Sigma_j) $$</p>


<p>그러나 역시 이 문제도 jointly update가 매우매우 어려운 문제이다. 그러나 K-means와 비슷하게도 \(\pi\)를 고정하고 \(\mu, \Sigma\)를 계산하는 것은 쉬우며, 그 반대 역시 쉽다. 따라서 비슷하게 alternative update를 통해 문제를 해결하는 알고리즘을 어렵지 않게 디자인 할 수 있다. 역시 이 알고리즘도 global로 수렴하지 않고 local로만 수렴하게 된다. GMM을 풀기 위해서 사용되는 알고리즘 중 가장 유명한 알고리즘으로는 <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">&#8216;EM&#8217; 알고리즘</a>이 존재한다. 이 알고리즘에 대해서는 <a href="/70">다음 글</a>에서 더 자세하게 설명하도록 하고, 이 글에서는 EM 알고리즘이라는 것을 알고 있는 상태에서 어떻게 각 step을 해결할 수 있는지에 대해서만 다루도록 하겠다. E-step에서는 현재 주어진 \(\mu, \Sigma, \pi\)들을 사용해 가장 &#8216;expectation&#8217;이 높은 latent variable의 값을 찾아내며, M-step에서는 새로 estimate된 latent variable을 사용해 그 값을 maximize하는 \(\mu, \Sigma, \pi\)를 찾는다. EM 알고리즘은 E-step과 M-step이 계속 번갈아 진행되며, 더 이상 값이 변하지 않을 때 까지 반복된다. K-means를 이런 관점으로 바라보게 된다면, cluster information \(w_{ij}\)를 update하는 과정이 E-step, \(b\)를 update하는 과정을 M-step이라고 할 수 있다. (그러나 K-means을 푸는 알고리즘은 엄밀하게 말하면 EM 알고리즘이 아니다)</p>


<p>사실 엄밀하게 설명하면 GMM을 푸는 EM에서 E-step 때 update하는 것은 정확하게 \(\pi\)와 같은 것은 아니다. EM으로 이 문제를 풀기 위해서 우리는 새로운 &#8216;latent&#8217; variable을 introduce해야한다. latent variable은 쉽게 생각하면 graphical model에서 hidden variable에 해당하는 값이다. 다음에 EM 알고리즘에 대해 자세히 다룰 때 다시 설명하겠지만, 이렇게 latent variable을 설정하는 이유는 어떤 특정 variable의 marginal distribution을 optimize하는 것은 어려울 때, latent variable을 사용해 그 variable과 latent variable의 joint distribution을 다루는 것은 간단할 수 있기 때문이다. GMM에서는 latent variable \(z\)를 introduce하게 된다. \(z\)는 \(k\)-ary variable로, \(z\)의 \(j\) 번째 dimension인 \(z_j\)는 Binary random variable이며, \(p(z_j=1) = pi_j, \) where \(\sum_j z_j = 1\) and \(\sum_j \pi_j = 1\) 이라는 조건을 가지고 있다. \(z\)의 marginal probability는 \(p(z) = \prod_j \pi_j^{z_j}\)로 어렵지 않게 계산할 수 있으며, 비슷하게 주어진 데이터 \(x\)에 대한 conditional distribution 역시 간단하게 다음과 같이 표현된다. \(p(x|z) = \prod_j N(x|\mu_j, \Sigma_j)^{z_j}\).</p>


<p>따라서 앞선 식들로부터 joint distribution을 얻을 수 있고, 그 값을 marginalize해 다음과 같은 결과를 얻을 수 있다.</p>


<p>$$ p(x) = \sum_z p(x,z) = \sum_z p(z)p(x|z) = \sum_j \pi_j N(x|\mu_j, \Sigma_j) $$</p>


<p>정리하자면, 각각의 data point \(x_i\) in GMM의 graphical representation은 다음과 같이 표현할 수 있다.</p>


<p><img src="/images/post/69-3.png" width="300"></p>

<p>위의 결과들을 토대로 이제 간단하게 EM algorithm을 돌릴 수 있다. 먼저 E step에서는 \(p(z_j = 1|x)\)를 계산할 것이다. 각 데이터에 대해 \(j\) 번째 클러스터에 속할 확률, 혹은 posterior를 계산하는 과정이다. 이 값은 Bayes rule을 통해 간단하게 다음과 같이 계산할 수 있다.</p>


<p>$$ p(z_j = 1|x) = \frac{p(z_j=1)p(x|z_j=1)}{\sum_j^k p(z_j=1)p(x|z_j=1)} = \frac{\pi_j N(x|\mu_j, \Sigma_j)}{\sum_j^k \pi_j N(x|\mu_j \Sigma_j)} $$</p>


<p>다음으로 \(z\)를 fix했을 때 다음과 같이 나머지 paramter를 계산할 수 있다.</p>


<p>$$ \mu_j = \frac{1}{\sum_i p(z_j=1|x)} \sum_i p(z_{ij}=1|x) x_i$$</p>


<p>$$ \Sigma_j = \frac{1}{\sum_i p(z_j=1|x)} \sum_i p(z_{ij}=1|x) (x_i-\mu_j) (x_i-\mu_j)^\top$$</p>


<p>$$ \pi_j = \frac{\sum_i p(z_j=1|x)}{N}$$</p>


<p>정리하자면, GMM을 풀기 위한 EM 알고리즘은, 먼저 각각의 데이터가 어느 클러스터에 속할지에 대한 정보를 update해 (\(z\)를 업데이트 하여) expectation을 계산하고, 다음으로 업데이트 된 정보들을 사용해 나머지 값들로 가장 log likelihood를 최대화하는 parameter들을 (\(\mu, \Sigma, \pi\)를) 찾아낸다. 알고리즘을 돌리면 아래처럼 iteration이 지날 때 마다 점점 좋은 값을 찾아준다. (출처: <a href="http://kipl.tistory.com/64">Geometry & Recognition :: Gaussian Mixture Model & K-means</a>)</p>


<p><img src="/images/post/69-4.gif" width="500"></p>

<p>EM 알고리즘에 대한 심도있는 이해 없이 이 글을 이해하는 것은 조금 어려울 수 있다. 이 글이 잘 이해가 되지 않는다면 먼저 EM 알고리즘에 대해 설명한 <a href="/70">다음 글</a>을 읽어본 다음 다시 읽어보기를 권한다.</p>


<h5>정리</h5>


<p>Clustering은 unsupervised learning 분야에서 가장 활발히 연구되는 분야 중 하나이다. 이 글에서는 여러 종류의 클러스터링 알고리즘 중에서 optimization function을 (1) 거리 기반으로 세우고 그것을 푸는 알고리즘과 (2) 확률과 확률분포를 기반으로 세우고 그것을 푸는 알고리즘을 소개하였다. 특히 GMM은 다음 글에서 다룰 주제인 EM algorithm과 밀접하게 관련되는 내용이므로 한 번 쯤은 책이나 렉쳐노트를 정독하는 것을 권한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 3월 25일: 글 등록</li>
<li>2015년 6월 14일: EM 알고리즘 링크 추가 및 설명 변경</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
<li><a href="http://kipl.tistory.com/64">Geometry &amp; Recognition :: Gaussian Mixture Model &amp; K-means</a></li>
</ul>


<hr>


<p><a href="/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="/57">Machine Learning이란?</a></li>
<li><a href="/58">Probability Theory</a></li>
<li><a href="/59">Overfitting</a></li>
<li><a href="/60">Algorithm</a></li>
<li><a href="/61">Decision Theory</a></li>
<li><a href="/62">Information Theory</a></li>
<li><a href="/63">Convex Optimzation</a></li>
<li><a href="/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/64/">Machine Learning 스터디 (8) Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-03-25T02:10:00+09:00" pubdate data-updated="true">Mar 25<span>th</span>, 2015</time>
        
         | <a href="/64/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h5>들어가며</h5>


<p><a href="/57#57-4-ClassML">첫 번째 글</a>에서 설명했던 것 처럼 Machine Learning은 크게 Supervised Learning, Unsupervised Learning 그리고 Reinforcement Learning으로 구분된다. 이 글에서는 그 중 Supervised Learning의 가장 대표적인 예시인 Classification에 대해 다룰 것이며 가장 대표적이고 간단한 세 가지 알고리즘에 대해서 역시 다룰 것이다.</p>


<h5>What is Classification?</h5>


<p><a href="http://en.wikipedia.org/wiki/Statistical_classification">Classification</a>은 <a href="http://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a>의 일종으로, 기존에 존재하는 데이터와 category와의 관계를 learning하여 새로 관측된 데이터의 category를 판별하는 문제이다. 스팸 필터를 예로 들어들어보자. 스팸 필터의 데이터는 이메일이고, category, 혹은 label, class는 spam메일인지 일반 메일인지를 판별하는 것이 될 것이다. 스팸필터는 먼저 스팸 메일, 그리고 일반 메일을 learning을 한 이후, 새로운 데이터 (혹은 메일)이 input으로 들어왔을 때 해당 메일이 스팸인지 일반 메일인지 판별하는 문제를 풀어야하며, 이런 문제를 classification이라고 한다.</p>


<p><a href="/57">첫 번째 글</a>에서 머신러닝 문제는 먼저 주어진 데이터에 대한 가정을 하고, 해당 가정을 만족하는 best hypothesis를 찾는 문제라고 언급한 적이 있다. Classification 문제 역시 Machine Learning 문제이므로 데이터에 대한 가정을 먼저 해야하고, best hypothesis를 찾는 과정을 거친다. 따라서 각각의 서로 다른 classification algorithm들은 서로 다른 assumption을 가지고 있으며, 해당 assumption을 가장 잘 만족하는 function parameter를 계산하는 과정이다. 앞으로 설명하게 될 알고리즘들에 대한 설명을 읽을 때 데이터에 대해 어떤 가정을 하였는지 꼼꼼히 확인하면서 읽으면 알고리즘을 이해하기 한결 수월할 것이다.</p>


<h5>Decision Tree</h5>


<p><a href="http://en.wikipedia.org/wiki/Decision_tree">Decision Tree</a>는 가장 단순한 classifier 중 하나이다. 이 Decision Tree의 구조는 매우 단순하다.</p>


<p><img src="/images/post/64-1.png" width="600"></p>

<p>위의 그림은 오늘 외출을 할까 말까를 결정하는 decision tree이다. 이렇듯 decision tree는 tree구조를 가지고 있으며, root에서부터 적절한 node를 선택하면서 진행하다가 최종 결정을 내리게 되는 model이다. Decision tree의 가장 좋은 점은 단순하다는 점이다. 누구나 쉽게 이해할 수 있고, 그렇기 때문에 쉽게 디버깅할 수 있다. 예를 들어 위의 예시에서 습도가 높아도 나갈만하다는 생각이 든다면 맨 왼쪽의 No를 Yes로 바꾸기만 하면 간단하게 로직을 바꿀 수 있다. 그러나 다른 모델들은 그런 점들이 비교적 어렵다. Machine Learning에서 말하는 decision tree는 <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">decision tree learning</a>으로, 일일이 node마다 로직을 사람이 써넣어 만드는 것을 의미하는게 아니라, node 개수, depth, 각각의 node에서 내려야하는 결정 등을 데이터를 통해 learning하는 algorithm들을 사용해 만든 decision tree를 의미한다.</p>


<p>많이 쓰이는 알고리즘들로는 <a href="http://en.wikipedia.org/wiki/ID3_algorithm">ID3</a>, <a href="http://en.wikipedia.org/wiki/C4.5_algorithm">C4.5</a>, <a href="http://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees">CART</a>, <a href="http://en.wikipedia.org/wiki/CHAID">CHAID</a>, <a href="http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines">MARS</a> 등이 있으며, 보통 C4.5를 가장 많이 사용한다.</p>


<p>C4.5는 ID3의 몇 가지 문제점들을 개선한 알고리즘으로, 그 기본 개념은 ID3와 크게 다르지 않다. 추후 이 글 혹은 다른 글에 ID3 알고리즘에 대한 내용을 추가하도록 하겠다.</p>


<h5>Regression Tree and Ensemble method</h5>


<p>Decision tree는 output value가 반드시 binary여야한다는 제약조건이 있기 때문에 스팸 필터 등에서는 사용할 수 있지만, 실제 모든 데이터가 binary만을 output으로 가지지 않으므로 모든 데이터에 사용하려면 변형이 필요하다. <a href="http://en.wikipedia.org/wiki/Decision_tree_learning#Types">Regression tree</a>는 binary가 아니라 real value를 output으로 가지는 모델로, learning하는 방법은 크게 다르지 않다고 한다.</p>


<p>가끔은 하나의 decision tree를 사용하는 것이 아니라 한 번에 여러 개의 decision tree들을 만들어서 각각의 decision tree들이 내리는 결정을 종합적으로 판단하여 (ensemble) 결정을 내리기도 한다. <a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">Bagging decision tree</a>, <a href="http://en.wikipedia.org/wiki/Random_forest">random forest</a>등이 이에 속한다. 이런 식으로 여러 개의 classifier를 사용해 decision을 내리는 방법을 ensemble method라고 하는데, industry에서는 machine learning algorithm의 성능을 높이기 위해서 여러 개의 알고리즘들을 ensemble method를 사용하여 한 번에 같이 사용하기도 한다. 대표적인 예로 <a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a> 등이 있다.</p>


<p>Ensemble method에 대해서는 나중에 따로 다시 설명할 예정이므로 그 글을 참고하면 좋을 것 같다. (링크는 추후 추가 예정)</p>


<h5>Naïve Bayes</h5>


<p><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naïve Bayes Classifier</a>는 <a href="58-1-Bayes">Bayesian rule</a>에 근거한 classifier이다. Naïve Bayes는 일종의 확률 모델로, 약간의 가정을 통해 문제를 간단하게 푸는 방법을 제안한다. 만약 데이터의 feature가 3개 있고, 각각이 binary라고 해보자. 예를 들어 남자인지 여자인지, 성인인지 아닌지, 키가 큰지 작인지 등의 feature를 사용해 사람을 구분해야한다고 생각해보자. 이 경우 적어도 8개의 데이터는 있어야 모든 경우의 수를 설명할 수 있게 된다. 그런데 보통 데이터를 설명하는 feature의 개수는 이보다 훨씬 많은 경우가 많다. 예를 들어 feature가 10개 정도 있고 각각이 binary라면, 제대로 모든 데이터를 설명하기 위해서는 \(2^{10}\), 약 1000개 이상의 데이터가 필요하다. 즉, 필요한 데이터의 개수가 feature 혹은 데이터의 dimension에 exponential하다. 이런 경우 그냥 Bayes rule을 사용해 분류를 하게 되면 overfitting이 되거나 데이터 자체가 부족해 제대로 된 classification을 하기 어려울 수 있다. Naïve bayes는 이런 문제를 해결하기 위해 새로운 가정을 하나 하게 된다. 바로 모든 feature들이 i.i.d.하다는 것이다. i.i.d는 independent and identically distributed의 준말로, 모든 feature들이 서로 independent하며, 같은 분포를 가진다는 의미이다. 당연히 실제로는 feature들이 서로 긴밀하게 관련되어있고 다른 분포를 가질 것이므로 이 가정은 틀린 가정이 될 수 있다. 그러나 만약 모든 feature가 i.i.d.하다고 가정하게 된다면 우리가 필요한 최소한의 데이터 개수는 feature의 개수에 exponential하게 필요한게 아니라 linear하게 필요하게 된다. 간단한 가정으로 모델의 complexity를 크게 줄일 수 있는 것이다. 때문에 Naïve Bayes 뿐 아니라 많은 모델에서 실제 데이터가 그런 분포를 보이지 않더라도 그 데이터의 분포를 특정한 형태로 가정하여 문제를 간단하게 만드는 기술을 사용한다.</p>


<p>조금 더 엄밀하게 수식을 사용해 설명을 해보자. 우리가 가지고 있는 input data 를 \(x = (x_1, \ldots, x_n)\)이라고 가정해보자. 즉 우리는 총 \(n\)개의 feature를 가지고 있다고 가정해보자 (보통 \(n\)은 데이터의 개수를 의미하지만, wikipedia의 notation을 따라가기 위하여 이 글에서도 dimension을 나타내기 위해 \(n\)을 사용하였다). 그리고 Class의 개수는 \(k\)라고 해보자. 우리의 목표는 \(p(C|x_1, \ldots, x_n) = p(C|x)\)를 구하는 것이다. 즉, 1부터 \(k\)까지의 class 중에서 가장 확률이 높은 class를 찾아내어 이를 사용해 classification을 하겠다는 것이다. Bayes rule을 알고 있으므로 이 식을 bayes rule을 사용해 전개하는 것은 간단하다.</p>


<p>$$ p(C|x) = \frac{p(C) p(x|C)}{p(x)} $$</p>


<p>이 때 분모에 있는 데이터의 확률은 normalize term이기 때문에 모든 값을 계산하고 나서 한 번에 계산하면 되므로 우리는 \(p(x,C) = p(C) p(x|C)\), 다시 말해 prior와 likelihood를 계산해야만한다. 그러나 이 값은 joint probability이므로 데이터에서부터 이 값을 알아내기 위해서는 &#8216;엄청나게 많은&#8217; 데이터가 필요하다. 구체적으로는 앞서 말한 것 처럼 dimension에 exponential하게 많은 데이터 개수를 필요로 한다. 그러나 만약 우리가 x가 모두 indepent하다고 가정한다면 간단하게 다음과 같은 식으로 나타낼 수 있다.</p>


<p>$$ p(C) p(x_1, \ldots, x_n | C) = p(C) p(x_1|C) p(x_2|C) \ldots = p(C) \prod p(x_i|C)$$</p>


<p>따라서 normalize term을 \(Z\)로 표현한다면, 우리가 구하고자 하는 최종 posterior는 \(p(C|x) = \frac{1}{Z} p(C)\prod p(x_i|C)\)로 나타낼 수 있게 된다.</p>


<h5>KNN</h5>


<p><a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K-Nearest Neighbors algorithm (KNN)</a>은 구현하기 어렵지 않으면서도 비교적 나쁘지 않은 성능을 내는 Classification Algorithm 중 하나이다. KNN은 가까운 데이터는 같은 label일 가능성이 크다고 가정하고 새로운 데이터가 들어오면, 그 데이터와 가장 가까운 k개의 데이터를 training set에서 뽑는다. 뽑은 k개의 데이터들의 label을 관측하고 그 중 가장 많은 label을 새로운 데이터의 label로 assign하는 알고리즘이다 (이런 방식을 majority voting이라고 한다). 이때 &#8216;가까움&#8217;은 Euclidean distance로 측정해도 되고, 다른 metric이나 measure를 사용해도 된다. 이때 distance 혹은 similarity를 측정하기 위해서 반드시 metric을 사용해야하는 것은 아니다. 즉, metric의 세 가지 성질을 만족하지 않는 measure일지라도 두 데이터가 얼마나 &#8216;비슷하냐&#8217;를 measure할 수 있는 measure라면 KNN에 적용할 수 있다. 아래 그림은 KNN이 어떻게 동작하는지 알 수 있는 간단한 예시이다. 아래 그림을 보면 k를 3으로 골랐을 때 초록색 데이터의 label은 빨강이 되고, k를 5로 골랐을 때는 파란색이 됨을 알 수 있다.</p>


<p><img src="/images/post/64-2.png" width="500"></p>

<p>KNN은 구현하기에도 매우 간단하고 (새로 들어온 점과 나머지 점들간의 distance를 측정한 후 sorting하기만 하면 된다) 성능도 보통 크게 나쁘지 않은 값을 보이기 때문에 간단하게 개발할 필요가 있는 경우에 많이 사용하게 된다. 사실 대부분의 머신러닝 툴박스들은 KNN의 다양한 variation까지 built-in function으로 지원한다. Matlab의 knnclassify가 대표적인 예. 또한 KNN은 <a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Properties">유용한 성질들</a>이 많이 있다. 예를 들어 만약 데이터의 개수가 거의 무한하게 있다면, KNN classifier의 error는 bayes error rate의 두 배로 bound가 된다는 특성이 있다. 즉, 데이터가 엄청나게 많다면, KNN은 상당히 좋은 error bound를 가지게 된다는 것이다. 즉, 단순한 휴리스틱 알고리즘이 아니라 엄밀하게 수학적으로 우수한 알고리즘임을 증명할 수 있는 알고리즘이라는 뜻이다. 또한 distance를 마음대로 바꿀 수 있기 때문에 KNN은 변형하기에도 간단한 편이므로 데이터에 대한 가정을 모델에 반영하여 변형하기에 간편하다는 장점이 있다.</p>


<h5>정리</h5>


<p>가장 간단하게 적용할 수 있는 세 가지 classification algorithm에 대해 훑어보았다. 개인적으로 KNN은 정말 직관적일뿐 아니라 잘 동작하는 알고리즘이기 때문에, 개인적으로 어떤 문제를 해결해야할 때 가장 먼저 이 데이터가 어느 정도 잘 분류되는지 테스트하는 용도로 애용한다. 중요한 점은, 각각의 classification algorithm이 풀려고 하는 &#8216;문제&#8217; 혹은 model은 서로 다른 가정을 가지고 있으며, 그 가정에 따라 문제를 푸는 방법이 아주 많이 바뀐다는 것이다. 즉, 어떤 새로운 classification algorithm을 만들어야 할 때는 (이는 classification 뿐 아니라 모든 알고리즘을 만들어야 할 때도 마찬가지인데) 먼저 어떤 문제를 풀어야하는지 문제를 정의해야하며, 문제를 정의하기 위해 어떤 모델을 가정해야한다는 것이다. 예를 들어 Naïve Bayes는 데이터의 각각의 feature들이 서로 i.i.d하다는 가정을 하고 있고, KNN은 &#8216;가까운&#8217; 데이터와 내가 같은 label을 가지고 있을 확률이 높다는 가정을 하고 있다. 이렇듯 Machine learning algorithm을 개발하는 일에서 가장 중요한 것은 좋은 문제를 먼저 정의하는 것에서부터 시작하는 것이 아닐까 생각한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 3월 25일: 글 등록</li>
</ul>


<hr>


<p><a href="/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="/57">Machine Learning이란?</a></li>
<li><a href="/58">Probability Theory</a></li>
<li><a href="/59">Overfitting</a></li>
<li><a href="/60">Algorithm</a></li>
<li><a href="/61">Decision Theory</a></li>
<li><a href="/62">Information Theory</a></li>
<li><a href="/63">Convex Optimzation</a></li>
<li><a href="/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="/74">Neural Network Introduction</a></li>
<li>Deep Learning</li>
<li>Reinforcement Learning</li>
</ul>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/86/">블룸버그 폰 인터뷰 후기</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-03-14T03:57:00+09:00" pubdate data-updated="true">Mar 14<span>th</span>, 2015</time>
        
         | <a href="/86/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>운이 좋게도, 교수님의 추천을 받아 블룸버그 software internship을 지원하게 되었다. 지원은 작년 12월에 하였고, 인터뷰는 1월 말부터 시간을 계속 조율하다가 오늘에서야 phone interview를 보게 되었다. 블룸버그가 뉴욕에 있다보니 현지 시간으로 2시에 면접인데 나는 새벽 3시.. 그나마 섬머타임이 실행되서 3시였지 안그랬으면 4시였다. 늦은 시간이지만 잠깐 시간을 내서 이 여운이 사라지기 전에 기록을 남겨볼까 한다.</p>


<p>먼저 phone interview는 생각보다도 더 어렵다. 일단 전화라는게 상대방의 말을 명료하게 전달해주지 못하는게 문제가 된다. 안그래도 긴장해서 영어가 잘 안들리는 상황에서 전화 연결 상황까지 안좋으니 더더더 긴장할 수 밖에 없었다. 하지만 이런 상황에도 못알아들었으면 당당하게 다시 말해달라고 하면 인터뷰어가 친절하게 대답해주니까 너무 겁먹을 필요는 없더라. 그리고 내가 phone interview에서 코딩을 위해 사용한 <a href="https://www.hackerrank.com/">HackerRank</a>라는 녀석이 구글 독스처럼 서로 글씨를 적으면 바로바로 상대방이 볼 수 있는 방식이라 말을 하면서 동시에 타이핑을 하는 것으로 충분히 내 부족한 영어를 메꿀 수도 있었다.</p>


<p>맨 처음 전화를 받고나서는 내 resume를 기반으로 몇 가지 질문들이 들어왔다. 너 이런 연구했는데 이거에 대해서 간단하게 설명할 수 있니? 여기에서 한 work은 어떤 work이니? 이 회사에서 일할 때 어떤 일들을 했니? 등의 질문들이었는데 아무래도 내 영어가 길지 못해 충분히 설명하지 못한게 아쉬운 요소였다. 시간을 재본건 아닌데 대충 10분 정도 레쥬메 스캔을 한 것 같다.</p>


<p>레쥬메 스캔이 끝나자마자 바로 코딩 문제로 넘어갔다. 여러 언어를 사용할 수 있는 것 같던데, 나는 Python을 사용해서 코딩을 했다. 내가 가장 멋진 코드를 쓸 수 있는건 (당연히 MATLAB을 제외하면) Ruby이지만, 내가 Ruby를 손에 안잡은지 너무 오래되었기 때문에 그나마 최근에도 가끔 사용하는 Python을 쓰기로 했다. C나 JAVA보다는 Python이 내 장점을 어필하기에 그나마 조금 나을 것 같아서. 그리고 사실 C랑 자바도 까먹었다. 질문은 두 가지 였는데, 하나는 여러 개의 list를 intersection하는 function을 짜라는 문제였고, 또 하나는 캐시를 구현하는거였다. 첫 번째 문제를 듣자마자 그간 코딩을 게을리한 것을 후회하게 되더라. 분명 어렵지 않은 문제인데 갑자기 코딩을 하려니 좋은 방법이 잘 떠오르지 않더라. 구글링을 하면 간단하게 해결할 수 있는 문제지만 그럴 수 있는 상황은 아니니까. 두 번째 문제는 듣자마자 OS에서 배웠던 내용이라 기뻐했는데 막상 어떤 데이터스트럭쳐를 사용해서 어떻게 빠르게 할 수 있는지 설명하려니까 그냥 딕셔너리를 써서 sorting한다는 대답 밖에 할 수 없었다.</p>


<p>첫 번째 문제를 정확하게 기술해보면 Input list들은 [ [1,2,3,&#8230;], [2,3,4,&#8230;], [5,6,7,&#8230;], &#8230; ] 처럼 생긴 list of list로 들어오고, output은 그 list들의 intersection을 구하는 문제였다. 내가 제안한 방법은 functional language 처럼 문제를 푸는 방법이었다. 먼저 두 개의 list를 비교하는 function을 만들고, 그것을 reduce했다. 대략 아래와 같은 느낌</p>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>f = lambda x,y : [a for a in x if a in y]
</span><span class='line'>print reduce(f,L)</span></code></pre></td></tr></table></div></figure>


<p>코드 퀄리티는 아주 만족한다. 처음에 문제를 듣자마자 그냥 for loop으로 일단 돌아가게만 다 풀어버릴까 고민도 했었는데 그래도 그것보다는 이게 훨씬 아름답고 functional한 철학에도 맞고 여러모로 내가 추구하는 이상적인 코드에 가깝다. 문제는 여기까지 가는데에 시간이 너무 오래걸렸다는 것. 처음에 열심히 헤메느라 점수 다 까먹었을 것 같다. List comprehension을 사용하면 Lambda를 금방 정의할 수 있는데 그 생각을 못해서 for loop으로 naive한 것을 먼저 만들고 그걸 lambda로 넣으려고 하고 막 그랬는데.. 암튼 좀 헤메다가 위 처럼 문제를 해결했다. 그런데 저 List comprehension도 내가 naive way라면서 일단 element wise로 다 비교해보자고 하면서 넣은거라 interviewer가 내가 코딩을 끝내자마자 바로 이 방법을 더 좋게 만들 수 없는지 물어보더라. Sorting이 되어있냐고 물어보고, 되어있지 않다고 하길래 일단 각각을 sorting했다고 가정하고 filter를 사용해서 개선할 수 있다고 하고 내가 코드를 적으려고 했는데 시간이 부족하다면서 (이미 여기에서부터 15분 남음) 스킵하고 넘어갔다. 내가 적고 싶었던 솔루션은 아래와 같았다. 진짜 잘 동작하는지모르고, complexity가 진짜 더 낮은지 생각해봐야함. 근데 아마 O(n)이 O(log n)이 되어서 더 빠를거임.</p>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for l in L:
</span><span class='line'>  l = sort(l)
</span><span class='line'>f = lambda x,y : [a for a in x if a in filter(lambda b : b &gt;= a, y)]
</span><span class='line'>print reduce(f,L)</span></code></pre></td></tr></table></div></figure>


<p>사실 for loop쓴 sorting도 한 줄에 쓸 수 있을 것 같지만 귀찮으니까 넘어가야지. 으 딱 15초만 더 줬으면 바로 타이핑해서 보여줬을텐데. 시간이 많이 부족하다그래서 그냥 넘어갔다.</p>


<p>두 번째 문제는 input이 어떻게 생겼는지 물어보다가 대답이 내가 원하는 대답이 아니라 그냥 내가 stream을 새로 정의했다. S = [1,2,3,4,1,2,1,4,2,1,&#8230;] 같은 list로 들어온다고 가정하고, 내가 할 일은 이 리스트를 앞에서부터 읽으면서 캐쉬에 넣다가, 캐쉬 메모리가 모자라면 가장 오래 전에 마지막으로 사용된 element를 drop하는 algorithm을 짜는거였다. 이런 초 쉬운걸 기억이 안나서&#8230; 일단 내 대답은 dictionary를 만드는거였다. 참고로 이 문제는 시간이 없어서 말로 때움. 스트림별로 key를 가지는 dictionary를 만들고 value는 키에 해당하는 스트림이 들어왔을 때의 시간 값을 넣는다. 그리고 메모리가 모자라면 dictionary를 보고 가장 오래된 시간을 쓰는 녀석을 지워낸다.. 가 내 솔루션이었는데, 가장 오래된 녀석을 찾는게 log라서 그걸 더 빠르게 할 수 없냐는 질문이 들어왔음. (구체적으로 log라고 한건 아니고, 그때그때 찾는게 비싸니까 더 빠르게 할 수 없냐는 질문) 내가 기억하는 O(1)짜리 data structure가 큐랑 스택 밖에 없어서 그걸 사용해서 막 삽질을 하다가 결국 시간도 모자라고해서 거기에서 멈췄다. 솔직히 이건 시간 더 줬어도 내가 명석하게 해결하지 못했을 듯 ㅠㅠ 이런거 안한지 너무 오래됐다..</p>


<p>마지막에 다 끝나고 질문있냐 물어봐서 블룸버그에서 어떤 언어를 쓰냐 물어봤더니 팀마다 다르단다. 본인은 팀에서 C++랑 Javascript를 사용한다고. 파이썬이나 루비를 사용하는 팀도 있다고 한다. 사실 내가 지금 software internship으로 들어가게 되면 도대체 어떤 position으로 들어가게 되는건지 알 수가 없어서 (pure developer인지 researcher인지 어느 정도 level로 코딩하는지..) 내가 어떤 언어를 쓰게 될지는 모르겠지만, 최소한 내가 가서 고를 수 있는 일말의 여지가 있다면 내가 최대한 잘 할 수 있는 팀으로 배정되면 된다는 결론을 내릴 수 있었다. 뭐 될지는 모르겠지만. 결과는 2~3주 뒤에 나온다고 했으니 또 막 메일 왔다갔다하다보면 한 달 예상해본다. Optimal case라면 2주 뒤인 3월 말에 ICML 리뷰를 보고 결과도 대충 알 수 있을거고 블룸버그 결과도 같이 나오는 셈이다. 이번에는 좀 좋은 결과가 있었으면 좋겠는데..</p>


<p>사실 크게 별 생각안하고 코딩하면서 떨지만 않게 적당히 인터넷에서 <a href="https://sites.google.com/site/steveyegge2/five-essential-phone-screen-questions">코딩 인터뷰 관련 글</a>이나 <a href="http://www.bogotobogo.com/python/python_interview_questions.php">문제들</a> 푸는 정도로 몸풀고 인터뷰를 봤는데 굉장히 좋은 경험이 되었다. 영어로 official software engineer job recuiting phone interview라니, 정말 중요한 경험인데 굉장히 어린 나이에 운좋게 경험할 수 있었다. 무엇보다 학위를 마치고 장래에 미국에서 job을 구할 생각을 하고 있는 상황에서 이런 좋은 회사와 phone interview 과정을 겪어보는 것만으로도 진짜 좋은 경험이 되었다. 가서 일을 해보면 훨씬 더 좋은 경험이 될 것 같지만, 그건 내가 결정하는게 아니니 일단은 두고 봐야겠지.</p>

</div>
  
  

<hr>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/85/">2015년 02월 27일 새벽 5시 반</a></h1>
    
    
      <p class="meta text-right mB50">
        








  


<time datetime="2015-02-27T05:34:00+09:00" pubdate data-updated="true">Feb 27<span>th</span>, 2015</time>
        
         | <a href="/85/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>2월 6일. 드디어 짧게 보면 반 년 넘게, 길게 보면 1년이 넘게 진행해온 연구로 논문을 제출했다. 제출한 학회는 ICML. 처음으로 쓰는 논문이고, 처음으로 내가 1저자가 되어 제출하는 논문인데 처음부터 너무 좋은 학회에 제출하게 되었다. 여러모로 운이 좋았다. Author feedback period는 3월 말이나 되어야하고, 최종 decision notification은 4월 25일이다. 만약 accept이 된다면 7월에 프랑스 Lille에서 내 연구를 수 많은 사람들 앞에서 발표하게 된다. 아직 결과가 나오려면 2달 가까이 남았기 때문에 연구에 대한 압박감은 많이 사라졌다. 다만, 추가적으로 더 해보고 싶은 것들이 있어 추가 실험이나 앞으로 시도해볼 분야의 review paper를 읽어야 하는데, 쉽사리 의욕이 나지 않는다.</p>


<p>Paper를 제출하고, 약 2주 반 정도의 휴가를 다녀왔다. 원래는 블로그에 ML study 포스트도 하루에 한 개씩 올리고, 책도 많이 읽고 Machine Learning 공부를 처음부터 천천히 할 수 있는 가장 좋은 시기라고 생각했었는데, 중간에 설 연휴도 있었고 잠깐 외국도 다녀오고 하니까 그럴 수 있는 시간이 하나도 없었다. 아마 머신러닝 공부, 그리고 관련 포스팅은 전부 학기 중에 수업도 들으면서 동시에 진행하게 될 것 같다.</p>


<p>쉬는 것에도 관성이 있는 것 같다. 어제 다시 학교로 돌아왔는데 아직까지 하루 종일 일이 손에 잡히지 않는다.</p>


<p>1월부터 2월초까지는 논문을 쓰는 것에 내 모든 역량을 쏟아부었다. 그래서 그런지 나에게는 이번 겨울이 너무나 짧게 느껴진다. 논문 쓰느라 한참 바쁘게 지내고 잠깐 외국다녀오니 겨울이 끝나있었다. 마찬가지로 2015년도 너무 급작스럽게 찾아왔다. 벌써 3월을 눈 앞에 두고있다니. 남들은 연말에 하거나 연초에 하는 2015년 계획을 오늘에서야 세우게 되었다. 사실 올해 목표 예년과 크게 다르지 않다. 올해 목표는 딱 두 가지인데, 하나는 체중 감량이고 또 하나는 제2외국어 공부이다. 체중감량은 워낙 오래전부터 내세우던 목표이고 제대로 성공한 적이 한 번도 없기 때문에 이번에는 정말 제대로 해봐야겠다. 혼자 계속 운동하다가 정체기가 오는 것 같으면 바로 PT를 받아볼 생각이다. 바로 PT를 받기에는 학교 헬스장이 워낙 시설이 좋아서 돈이 조금 아깝다. 그리고 내가 운동이나 식이요법을 &#8216;몰라서&#8217; 못하는게 아니라 귀찮거나 의지가 부족해서 안하는 것이기 때문에 의지가 부족해졌다고 느낄 때 쯤에 PT를 받아야겠다. 제2외국어공부는 여러가지를 생각해보았으나 역시 내가 빠른 시간에 금방 배울 수 있는 언어는 일본어이다. 오늘 저녁 먹기 전에 학원에 가서 상담받고 등록해야겠다.</p>


<p>한 번 연구를 해보니까 자신감이 생기기도 하지만 반대로 내가 이런 작업을 또 해낼 수 있을까 두렵기도 하다. 다행히 지금 나는 해야 할 일이 명확하니 내가 해야 할 일에 최선을 다해야겠다.</p>

</div>
  
  

<hr>
    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/archives">Blog Archives</a>
    
  </div>
</div>
<!--
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/90/">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a>
      </li>
    
      <li class="post">
        <a href="/89/">Recurrent Neural Network Regularization</a>
      </li>
    
      <li class="post">
        <a href="/74/">Machine Learning 스터디 (18) Neural Network Introduction</a>
      </li>
    
      <li class="post">
        <a href="/88/">Batch Normalization (ICML 2015)</a>
      </li>
    
      <li class="post">
        <a href="/72/">Machine Learning 스터디 (16) Dimensionality Reduction (PCA, LDA)</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/SanghyukChun">@SanghyukChun</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'SanghyukChun',
            count: 3,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>
//&#8211;>
    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Sanghyuk Chun -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sanghyukchun';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=182012898639519&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
