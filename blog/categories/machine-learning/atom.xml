<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-Learning | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2015-10-26T01:44:46+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML2015)]]></title>
    <link href="http://SanghyukChun.github.io/93/"/>
    <updated>2015-10-19T18:20:00+09:00</updated>
    <id>http://SanghyukChun.github.io/93</id>
    <content type="html"><![CDATA[<p>주어진 이미지에 대한 설명을 하는 문장, 혹은 캡션을 생성하는 문제를 image caption 문제라고 한다. 이 문제는 여러 가지 문제들이 복합적으로 얽혀있는 문제라고 할 수 있는데, 먼저 이미지가 어떤 것에 대한 이미지인지 판별하기 위하여 object recognition을 정확하게 할 수 있어야한다. 그 다음에는 detect한 object들 사이의 관계를 추론하여 이미지가 나타내는 event가 무엇인지 알아내어야 하고, 마지막으로 event를 caption으로, 즉 natural language로 재생성해야한다. 얼핏 생각하면 간단한 문제같지만 자그마치 비전과 NLP 분야의 핵심 문제들이 복합적으로 얽혀있는 복잡한 문제인 것이다. <s>방금 너희들이 본 건 간단해 보이지만 자그마치 3개의 퀑 기술이 합쳐진 컴비네이션!</s> 예를 들어보자.</p>


<p></p>

<p><img class="center" src="/images/post/93-1.jpg" width="400"></p>

<p>이 사진을 보고 우리는 '오바마가 청소부와 인사를 하고 있다' 라고 바로 문장으로 풀어낼 수 있지만, 이런 문제를 푸는 알고리즘을 디자인하기 위해서는 먼저 이 사진에서 오바마와 청소부라는 핵심 object를 detect 해야한다. 그러나 아직까지는 어떤 object가 중요한지 알 수 없으므로, 먼저 복도에 있는 보좌관들, 기둥, 바닥, 뒤에 보이는 계단 난간, 전등, 복도 등등을 모두 detect 해야한다. 그러므로, caption을 만들기 위해서 가장 먼저 우리는 높은 수준의 object detection과 segmentation을 필요로 하다. 다음으로는 segmentation된 정보를 사용해서 여러 event를 알아내야 한다. '오바마와 청소부가 인사를 하고 있다' '보좌관이 복도를 걷고 있다' '한 남자가 책을 읽고 있다' '사람이 기둥 뒤에 서있다' <s>'기둥 뒤에 공간이 있다'</s> 등의 여러 event를 추정해야하고, 그 중에서 가장 가능성이 높은 event를 판별해야한다. 마지막으로 해당 event를 설명하는 문장을 generate해야한다.</p>


<p>현재 caption generation 문제를 해결하는 방법들 중에서 가장 널리 쓰이고 있고, 가장 잘 동작하는 (state-of-art) 방법들은 neural network를 사용한 접근 방식들이다. Neural network를 사용하지 않은 기존 접근 방법은 크게 두 가지가 있었다. 하나는 object detection과 attribute discovery를 먼저 진행한 후에, 그것들을 사용해 미리 만들어놓은 caption template을 채우는 것이고, 또 하나는 비슷한 이미지 들의 caption 데이터를 사용해 지금 image에 적합한 caption으로 수정하는 방식이었다고 한다. Reference들을 보면 약 2010년부터 2013년 정도까지 연구가 활발하게 진행되었던 모양이지만, 지금은 전부 neural network 기반의 work에게 밀려서 사용되지 않는다고 한다.</p>


<p>현재 state-of-art를 찍고 있는 Neural network 기반 image description generator 모델들은 주로 2014년쯤부터 활발하게 연구가 진행되고 있다. Image caption 문제를 해결하기 위해 기존 deep learning 연구 그룹들은 마치 image를 하나의 language처럼 취급하고 실제 언어로 'translate' 하는 concept을 도입해서 문제를 machine traslation의 연장선으로 바라보는 접근 방법을 취한다고 한다. 그래서 대부분의 neural network 기반의 work들은 machine translation에서 사용하는 encoder-decoder 아이디어를 활용하여 caption generation을 한다고 한다. 보통 encoder-decoder 과정에서 이미지 하나를 그대로 사용하는 대신, CNN을 사용하여 이미지 하나를 single feature vector로 표현하고, 그 feature vector를 model에서 사용하는 방식을 취하고 있다.</p>


<p>올해 초, 기존 state-of-art를 뛰어넘는 RNN visual attention 기반 caption generation model이 <a href="http://arxiv.org/abs/1502.03044">Xu, Kelvin, et al. "Show, attend and tell: Neural image caption generation with visual attention." ICML 2015</a>이라는 work을 통해 제안되었다. 이 논문은 기본적으로 기존의 방법들처럼 encoder-decoder 개념을 사용하지만, 추가로 visual attention이라는 개념을 caption generator에 도입하여 image caption 문제를 해결한다. Attention이란 사람이 시각 정보를 처리할 때 일부 데이터에 'focus'하면서 계속 focus되는 대상이 움직이는 현상을 일컫는다. 최근 RNN을 사용하여 visual attention을 반영한 새로운 모델 들이 여기저기에서 등장하고 있는데, 이는 기존 CNN 기반 접근 방법들이 모든 이미지 픽셀을 그대로 사용하는 것과 대조적이라 할 수 있다. 참고로, 이미 앞선 <a class="tip" title="Recurrent Models of Visual Attention (NIPS 2014)" href="http://SanghyukChun.github.io/91">다른 work</a>에서도 visual attention이라는 개념을 사용해 classification 문제를 해결했었다.</p>


<p>이 논문의 가장 큰 contribution은 visual attention을 "hard" attention과 "soft" attention, 두 가지 attention machanism을 제안하고 새로운 방식의 two attention-based image caption generator 모델을 제안했다는 것이다.</p>


<ul>
<li>&ldquo;Soft&rdquo; attention은 deterministic machanism으로, standard back-propagation 방법으로 train할 수 있기 때문에 전체 모델이 end-to-end로 learning된다. Soft attention model은 hard attention model의 approximation model이라고 생각하면 된다.</li>
<li>&ldquo;Hard&rdquo; attention은 stochastic mechanism이며, reinforcement learning으로 train할 수 있다. Hard attention model은 매 iteration마다 데이터를 sampling을 해야하고, reinforcement learning과 neural network 부분이 분리되어있어 end-to-end learning이 아니라는 단점이 있다.</li>
</ul>


<p>(+ 이 논문을 처음 읽을 때는 두 가지 모델을 '섞어서' 한 모델에서 soft와 hard attention이 복합적으로 작용하는 모델을 만드는 것이라고 생각했었지만, 논문을 자세히 읽어보니, 먼저 hard attention을 제안한 후에, 이 모델의 approximation version으로 soft attention이라는 모델을 추가로 제안한 것이었다.)</p>


<p>그럼 이제 모델이 구체적으로 어떻게 구성이 되어있는지 자세하게 알아보도록 하자.</p>




<h5>Image Caption Generation with Attention Mechanism: Model details</h5>




<p>먼저 이 논문에서 제안하는 caption generation task를 정의하자. 이 논문은 caption의 길이를 \(C\)로, 사용할 수 있는 단어의 개수를 \(K\)로 고정시킨채 문제를 해결한다. 이 정의에 따라 caption을 vector \(y\)로 표현할 수 있다.</p>




<p>\[ y = \{y_1, \ldots, y_c \}, y_i \in \mathbb R^K. \]</p>




<p>이 식에서 각 \(y_i\)는 단어 하나를 의미한다. 즉, 이 논문의 목적은 '적절한' caption vector \(y\)를 생성하는 것이다. 이 논문은 '적절한' caption vector를 hard loss와 soft loss 두 가지 loss function을 사용해 정의하고 있으며, 여기에서 attention 개념이 사용된다. 자세한 설명은 아래에서 마저 설명하도록 하겠다.</p>




<p>이제 자세한 모델 설명을 해보자. 앞에서도 잠깐 언급했듯, 이 논문 역시 다른 기존 deep learning caption generator model들처럼 image에서 caption을 생성하는 과정을 image라는 언어에서 caption이라는 언어로 'translatation' 하는 개념을 사용한다. 따라서 이 논문은 machine translation의 encoder-decoder 개념을 사용하게 되는데, encoder는 우리가 잘 알고 있는 CNN을 사용하고, decoder로 RNN, 정확히는 LSTM을 사용하게 된다. 이 논문의 핵심이라고 할 수 있는 attention 개념은 LSTM에서 사용된다.</p>


<p>이 논문에서 제안하는 모델을 그림으로 표현하면 다음과 같다.</p>


<p><img class="center" src="/images/post/93-8.png" width="450"></p>

<h5>Encoder: CNN</h5>




<p>Encoder CNN은 주어진 이미지를 input으로 받아, output으로 feature vector \(a\)를 내보낸다. 이 CNN의 마지막 layer는 총 \(L\) 개의 filter로 이루어져있으며, 각각의 filter마다 \(D\) 개의 neuron을 가지도록 설계하였다. 즉, 다음과 같이 쓸 수 있다</p>




<p>\[ a = \{ a_1, \ldots, a_L \}, a_i \in \mathbb R^D. \]</p>




<p>이 논문에서는 encoder를 위한 CNN으로 VGG network를 선택하였는데, 이 네트워크는 바로 <a href="http://SanghyukChun.github.io/92">전 글</a>에서 다뤘으니 자세한 설명은 생략하도록 하겠다. 19 layer짜리를 사용한 것 같고, VGG11 layer로 pre-training만 시키고 fine-tunning은 하지 않은 상태로 사용했다고 한다. 당연한 얘기지만, VGG 네트워크말고도 다른 네트워크도 사용가능하다.</p>




<h5>Decoder: LSTM</h5>




<p>이 논문은 decoder로 LSTM을 사용한다. 이 LSTM은 매 time stamp \(t\) 마다 caption vector \(y\)의 한 element \(y_t\)를 생성한다. 즉, 전체 'unfold' 하게되는 시간은 caption의 길이 \(C\)와 같다. 즉 이 LSTM은 한 time stamp \(t\) 마다 바로 전 hidden state \(h_{t-1}\)과 바로 전에 generate된 단어 \(y_{t-1}\)을 input으로 받아서 지금 time stamp에 해당하는 단어 \(y_t\)를 생성하는 것이다. 이 논문에서 사용하는 LSTM 모델은 다음과 같다.</p>


<p><img class="center" src="/images/post/93-2.png" width="450"></p>

<p>LSTM에 대한 자세한 설명은 생략하도록 하겠다. 추후 다른 포스트를 통해 LSTM 자체에 대해 자세히 다뤄보도록하겠다. \(T_{s,t}: \mathbb R^s \to \mathbb R^t\)를  간단한 affine transformation이라고 정의해보자 (\(T_{n,m} (x) = W x + b\)라는 의미이다). 그러면 LSTM은 다음과 같이 간단하게 표현할 수 있다.</p>


<p><img class="center" src="/images/post/93-3.png" width="300"></p>

<p>이때 1번 식의 \(i_t,f_t ,c_t ,o_t ,h_t \)는 각각 input, forget, memory, output, hidden state를 의미한다. 이 논문은 LSTM의 initial memory state와 hidden state를 \(a\)의 평균 \(\bar a = \frac{1}{L}\sum_i^L a_i\)을 input으로 하는 두 개의 MLP (\(f_{init,c}\) \(f_{init,h}\))로 estimate한다고 한다.</p>


<p>그럼 이제 LSTM cell 하나에 input으로 들어오는 \(Ey_{t-1}, h_{t-1}, \hat z_t\)에 대해 알아보자. \(h_{t-1}\)은 바로 전 hidden state이니 제외하고, \(Ey_{t-1} \)는 \(t-1\) 시점에서 생성된 caption \(y_{t-1}\)을 embedding matrix \(E \in \mathbb R^{m \times K}\)로 embedding한 \(m\) dimensional vector이다. \(E\)는 맨 처음에 randomly initialize를 한 이후 train 과정에서 update되는 parameter이다. 마지막으로 \(\hat z \in \mathbb R^D\)는 context vector라고 하는데, 이 context vector는 attention model들에 의해서 결정된다.</p>




<p>Context vector \(\hat z_t\)는 CNN encoder output \(a\)와 바로 전 hidden state \(h_{t-1}\)에 의해 다음과 같이 결정된다.</p>




<p>\[ \hat z_t = \phi (a, \alpha_t), \mbox{ where } \alpha_{ti} = \frac{\exp(f_{att}(a_i, h_{t-1}))}{\sum_{k=1}^L \exp(f_{att}(a_k, h_{t-1}) )}. \]</p>




<p>먼저 \(\alpha_t\)는 time \(t\)에서의 \(a\)의 weight vector를 의미하며, \(\alpha_{ti}\)는 time \(t\)에서의 \(a\)의 \(i\)번째 element \(a_i\)에 해당하는 weight value값이다. 이때 weight란, 우리가 주어진 annotation (CNN의 output) 중에서 어느 location에 focus를 맞출 것인지, 혹은 어떤 것이 중요하지 않은지를 결정하는 값으로, 모델에서 'attention' 개념이 적용된 부분이다. 위의 식에서 알 수 있듯, softmax로 정의가 되기 때문에, weight \(\alpha_t\)의 element-wise summation은 1이다. \(f_{att}\)는 attention model이라는 것으로, weight vector \(\alpha\)를 계산하기 위한 모델이며, 이 논문은 이 모델을 hard와 soft 두 가지로 정의하였다. \(\phi\) function은 주어진 \(a\)와 그것의 weight vector \(\alpha_t\)를 사용해 \(\hat z_t\)를 계산하기 위한 function이다. 정리해보면 다음과 같다.</p>




<ul>
    <li><p>\(\alpha_t\): \(a\)의 weight vector로, 어디에 'attend' 할지 결정하는 값. 모두 더하면 1.</p></li>
    <li><p>\(f_{att}\): \(a\)와 \(h_{t-1}\)을 사용해 weight vector \(\alpha\)를 계산하기 위한 attention model.</p></li>
    <li><p>\(\phi\): \(a\)와 \(\alpha_t\)를 받아 \(\hat z\)를 계산하는 mechanism (예: \(\phi(a,\alpha_t) = \sum_i\alpha_{ti} a\)).</p></li>
</ul>




<p>마지막으로, 모델이 주어졌을 때, time \(t\)에서의 단어 \(y_t\)는 다음과 같이 바로 전 context vector \(\hat z_{t-1}\)와 그 동안의 LSTM의 state를 저장하고 있는 hidden state \(h_t\), 그리고 바로 직전 단어 \(y_{t-1}\)에 관련된 확률에 의해 결정된다.</p>




<p>\[p (y_t | a, y^{t-1}_1) \propto \exp(L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)). \]</p>




<p>여기에서 \(L_o \in \mathbb R^{K \times m}, L_h \in \mathbb R^{m \times n} L_z \in \mathbb R^{m \times D}, E \in \mathbb R^{m \times K} \)는 train과정에서 learning하는 parameter들이다.</p>




<p>이제 이 논문의 핵심인 attention model \(f_{att}\)들을 살펴보도록하자.</p>




<h5>Stochastic "Hard" Attention</h5>




<p>캡션 모델이 i번째 단어를 생성하기 위해 focus attention해야 하는 위치를 \(s_t\)라고 하는 location variable을 사용해 표현해보자. 우리가 원하는 벡터는 정확하게 focus해야하는 부분 한 부분만 값이 1이고 나머지는 전부 0인 벡터이다.</p>


<p>첫 번째 attention model인 stochastic "hard" attention은 attention location \(s_t\)를 latent variable로 취급한 다음, 이 값을 \(\alpha_t\)로 multinoulli distiribution parameterize시킨다. 다시 말하면, 주어진 시간 \(t\)에서, \(s_t\)의 \(i\)번째 element s_{ti}의 값이 1이 될 확률이 \(\alpha_{ti}\)이 되는 것이며, 주어진 시간 \(t\)에서 모든 \(i\)에 대해 \(\alpha_{ti}\)를 더하면 그 값은 1이 된다. 즉, \(\sum_i \alpha_{ti} = 1\) 이다. \(s_t\)와 \(\alpha_t\)를 정의하게 되면 \(\hat z_t\)라는 새로운 random variable을 다음과 같이 정의할 수 있다.</p>




<p>\[p(s_{ti} = 1 | s_{j &#60; t, a}) = \alpha_{ti} \mbox{ and } \hat z_t = \sum_i s_{ti} a_i. \]</p>




<p>우리의 목표는 주어진 feature vector \(a\)에 대해 가장 확률이 높은 caption \(y\)를 고르는 것이다. 이 작업은 간단하게 maximum log likelihood \(\max_y \log p(y|a)\)를 계산하는 것으로 구할 수 있는데, 이 값을 직접 계산하는 대신, 앞에서 정의한 attention location \(s_t\)를 사용하게 된다면 log likelihood의 lower bound를 다음과 같이 계산할 수 있으며, 이 값을 새로운 objective function \(L_s\)로 정의한다.</p>




<p>\[ L_s = \sum_s p(s|a) \log p(y|s,a) \leq \log \sum_s p(s|a) p(y|s,a) = \log p(y|a).\]</p>




<p>\(L_s\)가 log likelihood의 lower bound이므로, 이 값을 증가시키게 되면 likelihood 역시 함께 증가할 것이다. 따라서 log-likelihood의 maximum값을 구하는 대신, \(L_s\)의 maximum 값을 구하는 것으로 문제를 대략적으로 풀 수 있다 (하지만 엄밀하게 증명해본 것은 아니지만, 아마도 \(L_s\)가 local optimum으로 converge한다고 해서 원래 log likelihood가 converge할 것 같지는 않기 때문에 정확한 문제의 solution을 찾게 되는 것은 아닌 것 같다. 그러나 이미 neural network 쪽 algorithm들이 그러하듯, 정확한 답보다는 그 답을 향해 진행하는 것이 훨씬 중요하기 때문에 이 work에서는 큰 문제가 될 것 같지는 않다). Parameter \(W\)에 대한 \(L_s\)의 미분값은 다음과 같이 주어지며, 이 값을 사용하면 gradient descent를 통해 \(\max L_s\) 문제를 해결할 수 있다. </p>




<p>\[ \frac{\partial L}{\partial W} = \sum_s p(s|a) \left[ \frac{\partial p(y| s,a)}{\partial W} + \log p(y|s,a)\frac{\partial p(s|a)}{\partial W} \right]. \]</p>




<p>이 미분 값을 직접 구하기 위해서는 모든 attention location \(s\)에 대해 summation 기호 안에 있는 연산을 계산해야하기 때문에 computation이 간단하지 않다. 이미 많은 기존 deep learning approach들에서 '정확한' 값을 구하는 데에 시간이 오래걸린다면, 그냥 '적당히' 빠르게 근사하는 것이 더 낫다는 것이 알려져 있는 만큼, 이 논문에서는 정확한 값을 계산하는 대신 Monte Carlo based sampling을 사용해 이 값을 다음과 같이 근사하고 있다. 이때, \(\tilde s_t \sim \mbox{Multinouli}_L (\alpha)\)로 주어진 값이다.</p>




<p>\[ \frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y| \tilde s^n,a)}{\partial W} + \log p(y| \tilde s^n,a)\frac{\partial p(\tilde s^n |a)}{\partial W} \right]. \]</p>




<p>Monte Carlo based sampling을 사용해 gradient를 근사하게 되면, 굉장히 효율적이고 빠르게 gradient를 근사할 수 있지만, 그렇게 계산된 gradient는 variance가 크기 때문에 이를 handle하기 위한 추가적인 아이디어들이 도입되게 된다. 먼저 moving average를 사용한다. 이 논문에서는 moving average baseline을 다음과 같이 이전 log likelihood들의 exponential decay를 사용한 합으로 표현하였다.</p>




<p>\[ b_k = 0.9 \times b_{k-1} + 0.1 \times \log p (y | \tilde s_k, a). \]</p>




<p>여기에 또 varinace를 줄이기 위하여 entroy term \(H[s]\)를 더한다. 그뿐 아니라 주어진 이미지에 0.5의 확률로 sampled attention location \(\tilde s\)의 값을 \(\tilde s\)의 기대값인 \(\alpha\)로 설정한다. 이 방법들을 사용하게 되면 stochastic attention learning algorithm의 robustness를 증대시킬 수 있다고 한다. 이 방법들을 모두 섞으면, 기존의 gradient는 다음과 같이 바뀐다.</p>




<p>\[ \frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y| \tilde s^n,a)}{\partial W} + \lambda_r (\log( p(y|\tilde s^n, a) - b)\frac{\partial p(\tilde s^n |a)}{\partial W} + \lambda_e \frac{H[\partial \tilde s^n]}{\partial W} \right]. \]</p>




<p>이렇게 구해진 식은, reinforcement learning의 update rule과 같다. 그렇기 때문에 hard visual attention을 reinforcement learning을 사용해 learning할 수 있다고 이야기하는 것이다. Action은 attention의 위치를 고르는 것이 될 것이고, reward는 log-likelihood의 lower bound인 \(L_s\)가 된다.</p>


<p>Hard attention model은 매 순간마다 \(\hat z\)를 'hard choice'를 통해 계산하게 된다. 여기에서 hard choice란, 주어진 시간 \(t\)에서 \(\alpha\)로 parameterize된 multinouilli distribution에서 \(a_i\)를 sampling하여 얻게되는 choice를 의미한다.</p>




<h5>Deterministic "Soft" Attention</h5>


<p>Hard attention은 train phase의 매 timestamp마다 attention location \(s_t\)를 매 번 sampling해줘야하기 때문에, 이 논문에서는 hard attention 대신 soft attention이라는 개념을 추가로 도입한다. Soft attention은 stochastic하게 매 번 sampling을 하는 대신 deterministic하게 context vecot \(\hat z_t\)을 계산한다. Soft attention \(\phi\)는 다음과 같이 표현된다.</p>


<p></p>

<p>\[\phi (\{ a_i \}, \{ \alpha_i \}) = \sum_i^L \alpha_i a_i. \]</p>




<p>이렇게 표현되는 이유는 \(\hat z_t\)의 expectation이 \(\alpha, a\)로 다음과 같이 직접 계산할 수 있기 때문이라고 한다.</p>




<p>\[ \mathbb E_{p(s_t|a)}p[\hat z_t] = \sum_{i=1}^L \alpha_{ti} a_i. \]</p>




<p>따라서 이 방법을 취하게 되면 전체 모델이 좀 더 smooth해지고, differentiable해지기 때문에 back-propagation을 사용해서 end-to-end로 learning이 가능하다는 장점이 있다. Deterministic attention, 혹은 soft attention 역시 앞에서 구한 likelihood \(p(y|a)\)를 \(s_t\)를 사용하여 구한 approximation을 optimizing하는 것으로 구할 수 있다. 이 논문에 따르면 \(h_t\)가 stochatic context vector \(\hat z_t\)를 tanh를 사용한 linear projection이기 때문에, \(\mathbb E_{p(s_t|a)[h_t]}\)를 1st order Taylor approximation하게 되면 이 값은 \(\hat z_t\)의 expectation인 \(\mathbb E_{p(s_t|a)}p[\hat z_t]\)를 사용하여 forward propagation을 한 번 진행하여 \(h_t\)를 구한 값과 같아진다고 한다. \(n_t = L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)\)로 정의하고 (\(p (y_t | a, y^{t-1}_1)\)를 approximation했을 때 쓰인 값과 같다), \(n_{ti}\)를 \(a_i\)를 사용하여 계산한 random variable \(\hat z_t\)를 대입하여 구한 \(n_{t}\)값이라고 해보자. 이 값들을 사용하여 이 논문은 k번째 word prediction을 위한 NWGM (Normalized Weighted Geometric Mean)이라는 것을 다음과 같이 정의한다.</p>




<p>\[NWGM[p(y_t = k | a)] = \frac{\prod_i \exp(n_{tki})^{p(s_{ti}=1|a)}}{\sum_j \prod_i \exp(n_tji)^{p(s_{ti}=1|a)}} = \frac{\exp(\mathbb E_{p(s_t|a)}[n_{tk}])}{\sum_j\exp(\mathbb E_{p(s_t|a)}[n_{tj}])}.\]</p>




<p>정의에 따라 \(\mathbb E [n_t] = L_o(E y_{t-1} + L_h h_t + L_z \hat z_t)\)이 되므로, 이 식을 통해 우리는 caption prediction을 위한 NWGM이 expected context vector를 사용하여 approximate된다는 것을 알 수 있다. 이전의 다른 work에 의하면, NWGM은 (softmax acivation일 때만) \(\mathbb E [ p(y_t = k | a)]\)로 근사가 된다고 한다. 이 말은 바꿔 말하면, 모든 가능한 attention location \(s_t\)에 대해 구한 output의 expectation이 expected context vector \(\mathbb E [\hat z_t]\)를 사용하여 간단한 feedforward propagation으로 계산된다는 의미가 된다.</p>




<p>또한 이 논문에서는 doubly stochastic attention이라는 개념도 같이 제안한다. \(\alpha\)의 정의에 따라 우리는 \(\sum_i \alpha_{ti} = 1\) 이라는 관계식을 가진다. 이 논문이 주장하는 것은, \(\sum_t \alpha_{ti} \approx 1\) 이라는 조건을 하나 더 추가하는 것이 실제 실험결과 더 좋은 성능을 낸다는 것이다. 그 이유는 모델이 모든 이미지의 모든 부분을 전체 기간 동안 보는 것을 방해하기 때문에 더 focus된 attention이 가능하기 때문이라고 한다. 그리고 추가로, soft attention model에 </p>




<p>결론적으로, 이 모델은 아래와 같은 negative log-likelihood를 minimize하는 방식으로 end-to-end learning을 할 수 있다.</p>


<p>\[ L_d = -\log (p(y|x)) + \lambda \sum_i^L \left(1 - \sum_t^C \alpha_{ti} \right)^2. \]</p>




<h5>Experiment</h5>


<p>다음 그림은 실제 hard attention과 soft attention이 어떻게 동작하는지 잘 보여주는 그림이다. 가장 왼쪽의 주어진 이미지에 대해 soft attnetion과 hard attention 모두 'a bird flying over a body of water.' 이라는 caption을 생성했는데, soft와 hard attention 각각 경우에 대해 caption generation 모델이 어느 곳을 attend하도록 동작했는지 표현되어있다.</p>


<p><img src="/images/post/93-5.png" width="600"></p>

<p>위의 그림이 deterministic하게 attention을 계산하는 soft attention이다. 정확한 'attention location'이 존재하는 것이 아니라, 하얗게 mapping된 부분을 전반적으로 attend한다고 생각하면 된다. 반면 아래에 있는 hard attention을 보면, 매 번 정확한 attention location을 고르는 것을 알 수 있다. 이 과정을 매 번 확률적으로, sampling based approximaiton을 취하기 때문에 hard attention model은 stochastic machanism이다.</p>


<p>다음 그림을 통해, 실제 각 word 별로 어느 곳을 attend하는지 대략적으로 확인할 수 있다.</p>


<p></p>

<p><img src="/images/post/93-6.png" width="600"></p>

<p>Attend하는 위치가 비교적 굉장히 정확한 곳을 고르는 것을 알 수 있다. 좀 더 많은 예시가 gif로 <a href="http://kelvinxu.github.io/projects/capgen.html">이 링크</a>에 업로드되어있으니 참고하면 좋을 것 같다. 이 논문은 성공했을 때 뿐 아니라 실패했을 경우의 attention도 아래 그림과 같이 기재하여두었다.</p>


<p><img src="/images/post/93-7.png" width="600"></p>

<p>이 사진들을 통해 저자들은, 이 모델이 attend하는 위치는 잘 골랐으나, 각 object를 조금 잘못 classification했기 때문에 실패하는 경우가 나온다고 주장하고 있다. 예를 들어 위 실패 경우를 보면 바이올린을 스케이트 보드라고 했거나, 돛을 서핑보드라고 분류하여 잘못된 caption 결과가 나왔음을 알 수 있다.</p>


<p>좀 더 많은 이미지 데이터셋에 대해 hard attention과 soft attention을 사용한 caption generator의 성능은 다음 표에 잘 나타나있다. 점수는 높을수록 좋다.</p>


<p><img src="/images/post/93-4.png" width="600"></p>

<p>기존에 알려진 모델들에 비해 attention based model들의 성능이 훨씬 좋은 것을 알 수 있다.</p>




<h5>Summary of Show, Attend and Tell</h5>


<ul>
<li>Image caption 문제는 상당히 어려운 문제이며, 기존 deep learning 기법들은 machine translation에서 사용하는 encoder-decoder concept를 사용해 문제를 해결한다.</li>
<li>이 논문 역시 encoder-decoder 개념을 사용하지만, decoder에서 attention이라는 개념을 추가로 사용하여 새로운 모델을 제안하고 있다.</li>
<li>Encoder는 CNN을 사용한다. 실제 실험에서는 CNN 모델로 VGG 네트워크를 선택하여 사용하고 있다.</li>
<li>Decoder는 RNN, 정확히 말하면 LSTM을 사용하는데, 바로 전 state h, 바로 전 caption word y, 그리고 attention model을 통해 생성되는 context vector z가 LSTM cell의 input이 된다.</li>
<li>Context vector z는 hard attention과 soft attention 두 가지 방법 중에 한 가지 방법을 선택하여 생성하게 된다. Hard attention은 stochastic machanism이고, soft attention은 deterministic machanism이다.</li>
<li>Hard attention은 먼저 location variable s를 정의하고, 이것을 사용해 log-likelhood의 lower bound Ls를 계산한다. Ls를 optimization하기 위해 gradient를 구해야하는데, 이 값을 정확하게 구하는 것이 까다롭기 때문에 Monte Carlo based sampling approximation을 사용해 문제를 해결하게 된다. 이 update rule은 reinforcement learning의 update rule과 일치한다.</li>
<li>Soft attention은 매 iteration마다 sampling을 하는 대신, s의 확률 alpha를 직접 사용하여 z를 계산한다.</li>
<li>Attention based caption generation model은 기존 image caption generation 모델들에 비해 훨씬 좋은 성능을 보인다.</li>
</ul>


<h5>Reference</h5>


<ul>
    <li><p><a href="http://arxiv.org/abs/1406.6247">Mnih, Volodymyr, Nicolas Heess, and Alex Graves. "Recurrent models of visual attention." Advances in Neural Information Processing Systems. 2014.</a></p></li>
    <li><a href="http://kelvinxu.github.io/projects/capgen.html">http://kelvinxu.github.io/projects/capgen.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[고흐의 그림을 따라그리는 Neural Network, A Neural Algorithm of Artistic Style (2015)]]></title>
    <link href="http://SanghyukChun.github.io/92/"/>
    <updated>2015-10-14T00:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/92</id>
    <content type="html"><![CDATA[<p>얼마 전, <a href="http://arxiv.org/abs/1508.06576 ">'A Neural Algorithm of Artistic Style '</a> 이라는 이름의 충격적인 논문이 arXiv에 업로드되었다. 기술 전문 잡지나 신문이 아닌 스브스뉴스 같은 일반적인 기사를 보도하는 매체에서도 보도가 되었을 정도로 요즘 꽤 이슈가 되고 있는 논문이다.</p>


<ul>
<li><a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: &ldquo;이건 &lsquo;반 고흐'의 그림이 아닌 '컴퓨터'의 그림입니다.&rdquo;</a></li>
<li><a href="http://www.dailymail.co.uk/sciencetech/article-3214634/The-algorithm-learn-copy-artist-Neural-network-recreate-snaps-style-Van-Gogh-Picasso.html">DailyMail: &ldquo;The algorithm that can learn to copy ANY artist: Neural network can recreate your snaps in the style of Van Gogh or Picasso&rdquo;</a></li>
<li><a href="http://www.theguardian.com/technology/2015/sep/02/computer-algorithm-recreates-van-gogh-painting-picasso">The Guardian: &ldquo;Computer algorithm recreates Van Gogh painting in one hour&rdquo;</a></li>
</ul>


<p>자고로 백문이 불여일견이라, 이게 도대체 무슨 contribution이 있길래 사람들의 이목이 쏠리고 있는지 논문에 첨부되어있는 그림을 먼저 보자.</p>


<p><img src="/images/post/92-1.jpg" width="600"></p>

<p>이 그림들은 모두 사람이 그린 것이 아니라 neural network를 사용하여 generate한 것이다. 이 논문은 제목 그대로, 'artistic style'을 learning하는 neural network algorithm을 제안한다. 여기에서 artistic style이라는 것을 어떻게 정의하였는지는 나중에 조금 더 자세히 살펴보도록하자. 위 그림은 이 논문에서 제안한 알고리즘을 사용하여 report한 결과이다. 원본이 되는 A가 독일의 튀빙겐이라는 곳에서 찍은 '사진'이다. B부터 F는 유명한 거장들의 그림 'style'과 A의 'content'를 가지는 그림을 generate한 결과이다. 순서대로 B는 J.M.W. 터너의 &#60;미노타우르스 호의 난파&#62;, C는 그 유명한 빈센트 반 고흐의 &#60;별이 빛나는 밤&#62;을, D는 뭉크의 &#60;절규&#62;, E는 피카소의 &#60;앉아 있는 나체의 여성&#62;, F는 칸딘스키의 &#60;구성 VII&#62;이다. 놀랍게도 알고리즘을 통해 얻은 그림은 원본 사진의 content는 거의 그대로 보존하면서, 동시에 다른 그림의 style을 특징을 잘 살려서 가지고 있다.</p>


<p>이 짧은 논문이 사람들에게 얼마나 큰 충격을 주었는지는 길게 적지 않아도 알 수 있을 것이라고 생각한다. 논문이 나오고 얼마 지나지 않아 <a href="https://github.com/jcjohnson">jcjohson</a> 이라는 github user가 torch 기반으로 만든 <a href="https://github.com/jcjohnson/neural-style">'neural style'</a>이라는 프로젝트를 github에 공개하였다. 논문이 정말 좋은 결과를 낸 것인지 사람들이 이런 저런 사진과 그림들을 사용해 실험해본 결과, 논문에서 이야기하는 것 처럼 실제로 아무 사진을 적당히 골라서 적당한 그림을 넣어주면 사진의 내용은 보존한 채로 질감만 바꿔서 출력해주는 것을 알 수 있었다. 아래 그림은 <a href="https://github.com/jcjohnson/neural-style">'neural style'</a>을 사용해 금문교 사진과 여러 예술가들의 그림을 사용해 generate한 결과이다.</p>


<p><img src="/images/post/92-3.png" width="600"></p>

<p>이 논문이 나온게 9월 말이었는데, 벌써 한국 개발팀에서 스마트폰 app까지 개발했을 정도로 관심이 뜨겁다. (<a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: 이건 '반 고흐'의 그림이 아닌 '컴퓨터'의 그림입니다.</a>)</p>


<p><img class="center" src="/images/post/92-4.jpg" width="300"></p>

<p>꽤나 흥미로운 논문인 만큼, 어떤 아이디어를 사용했고, 어떤 방법론을 사용했는지까지 한 번 차근차근 살펴보도록 하자.</p>




<h5>Content &#38; Style Reconstruction using CNN</h5>




<p>Deep learning이 지금처럼 급부상하게 된 배경에는 (비전 분야를 중심으로 한) <a class="red tip" title="Convolutional Neural Network">CNN</a>의 엄청난 힘이 있었다. CNN이 비전에서 월등한 성능을 내는 이유를 여러가지로 설명할 수 있겠지만, 일반적으로는 CNN은 각각의 layer가 'feature'의 의미를 지니기 때문이라고 설명한다. 각각의 layer가 feature를 생성해내고, 이 feature들이 hierarchy하게 쌓이면서 더 높은 layer로 갈수록 더 좋은 feature를 만들어낸다는 것이다. CNN은 이 feature를 hard-coding하여 뽑아내는대신, 데이터에서부터 '가장 좋은' 최종 feature를 만들도록 학습시키기 때문에 아주 좋은 feature를 사용해 perceptron 등의 간단한 classifier로 높은 performance를 얻게 되는 것이다 (CNN에 익숙하지 않다면 <a href="http://SanghyukChun.github.io/75/#75-cnn">CNN에 대해 설명했었던 이전 글</a>을 참고하면 좋을 것 같다). 그렇기 때문에 각각의 convolution layer의 output은 흔히 feature map으로 표현이 된다.</p>


<p>주어진 이미지에서 feature를 뽑아내는 것은 CNN을 통하여 지금까지 항상 하던 일이었다. 그렇다면 반대로 할 수도 있지 않을까? 즉, CNN의 중간 feature map을 사용하여 원래 이미지를 복원하는 작업을 하는 것이다. 이렇게 feature map에서부터 이미지를 reconstruction 할 수만 있다면, deep CNN에서 layer를 지면서 어떤 재미있는 일들이 벌어지고 있는지 사람이 직접 눈으로 확인할 수 있을 것이다. 이런 visualization에 대한 motivation 때문에 그 동안 CNN의 convolution layer에서 원래 이미지를 reconstruction하는 작업들은 꾸준하게 제안되어 왔다. 그 중 가장 유명한 work으로 다음과 같은 work이 있다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1412.0035">Mahendran, Aravindh, and Andrea Vedaldi. &ldquo;Understanding deep image representations by inverting them.&rdquo; arXiv preprint arXiv:1412.0035 (2014).</a></li>
</ul>


<p>이 논문은 주어진 feature map에서 image를 복원하는 방법을 제안한다. 단순히 이미지를 복원하는 것이 아니라, 이미지를 특정 목적에 맞게 변형하는 work도 진행되어왔다. 대표적인 예가 아래 논문과 Google DeepMind의 <a href="http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html">Deep Dream</a>이다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1412.1897">Nguyen, Anh, Jason Yosinski, and Jeff Clune. &ldquo;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.&rdquo; arXiv preprint arXiv:1412.1897 (2014).</a></li>
</ul>


<p>이 논문은 deep CNN이 거의 100% 확률로 오답을 발생시키도록 이미지를 조작한 work이다. 예를 들어 주어진 이미지가 '새' 라는 label을 가지고 있다고 판별했을 때, '문어'라는 label을 100% 로 가지도록 이미지를 조작하는 것이다. 사람이 봤을 때는 여전히 '새' 사진이지만, CNN은 '문어'라고 판별해버리는 것이다.</p>


<p>이렇듯, 다양한 목적으로 CNN이 이미 주어져 있을 때, 특정 목적에 따라 이미지를 update하는 방법론들은 이미 예전부터 연구가 계속 진행되어왔다. 위에 링크한 3개의 work은 꽤 흥미로운 주제들이기 때문에 나중에 또 따로 포스팅할 수 있도록 하겠다. 이 논문의 contribution은 각 convolution layer에서부터 style과 content를 reconstruct하는 방법론을 제안했다는 것이다. 이 방법은 앞에서 언급한 <a href="http://arxiv.org/abs/1412.0035">Understanding deep image representations by inverting them</a> 논문 처럼 현재 feature map에서 원래 이미지를 최대한 복원하는 content reconstruction과, 아래 논문 등에서 제안되어왔던 texture 분석과 생성 등을 복원하는 texture reconstruction을 결합한 것이다. 참고로 이 work들은 맨 처음 논문을 제외하면 neural network 기반 work은 아니다 (첫 번째 논문은 이 논문을 작성한 연구팀이 이 논문을 arXiv에 올리기 3개월 전에 arXiv에 올린 다른 논문이다). 자세한건 뒤에서 더 다루도록하자.</p>


<ul>
<li><a href="http://arxiv.org/abs/1505.07376">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. &ldquo;Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks.&rdquo; arXiv preprint arXiv:1505.07376 (2015).</a></li>
<li><a href="http://www.cns.nyu.edu/heegerlab/content/publications/Heeger-siggraph95.pdf">Heeger, David J., and James R. Bergen. &ldquo;Pyramid-based texture analysis/synthesis.&rdquo; Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. ACM, 1995.</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=363108">Portilla, Javier, and Eero P. Simoncelli. &ldquo;A parametric texture model based on joint statistics of complex wavelet coefficients.&rdquo; International Journal of Computer Vision 40.1 (2000): 49-70.</a></li>
</ul>


<p>이 논문에서 제안하는 두 가지 reconstruction 방법을 CNN의 각 layer에 대해 적용해보면 다음과 같은 결과를 얻을 수 있다. 참고로 이 논문에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a>를 선택했다. 이 네트워크는 총 16개의 convolution layer와 3개의 fully connected layer로 이루어져있다. 이 네트워크에 대한 설명은 method 부분에서 더 자세히 다루도록 하겠다.</p>




<p><img id="92-reconst-img" src="http://SanghyukChun.github.io/images/post/92-5.png" width="600"></p>




<p>위 그림은 CNN 하나에서 서로 다른 두 가지 방법으로 각각 style과 content를 layer 별로 reconstruction한 결과이다. 하나의 같은 CNN에 대해 두 가지 다른 reconstruction을 진행한 것인데, 위쪽 그림은 고흐의 &#60;별이 빛나는 밤&#62;의 style을 layer 별로 reconstruction한 것이고, 아래 그림은 튀빙겐에서 찍은 사진의 content를 layer 별로 reconstruction한 것이다.</p>


<p>먼저 style reconstruction에서 알 수 있는 것은 layer가 얕을수록 원래 content 정보는 거의 무시하고 'texture'를 복원한다는 것이다. 반면 깊은 layer로 가게 될수록 점점 원래 content 정보가 포함이 되는 것을 볼 수 있다. 이런 현상이 발생하는 이유는 이 논문에서는 style을 같은 layer에 있는 feature map들 간의 correlation으로 정의하기 때문이다. 이를 구체적으로 어떻게 수학적으로 정의하였는지는 뒤에서 좀 더 자세하게 살펴보도록 하자. Style을 correlation으로 생각하기 때문에, 가장 style이 복원이 잘되는 얕은 layer에서는 원본 content가 거의 무시되고 correlation을 가장 좋게하는 style만 나오는 것이고, 깊은 layer로 갈수록 style이 제대로 복원이 되지 않을 것이므로 원본 content의 정보가 증가해 correlation이 작아지는 결과를 얻게 되는 것이다.</p>


<p>다음으로 content reconstruction을 보자. 이 그림을 통해 낮은 level의 layer는 거의 완벽하게 원본 이미지를 보존하고 있고 layer가 깊어질수록 원본 이미지의 정보는 조금씩 소실되지만, 가장 중요한 high-level content는 거의 유지가 되는 것을 볼 수 있다.</p>


<p>이 논문은 같은 CNN이라고 할지라도 content와 style에 대한 representation이 분리가 되어있다는 것을 중요하게 언급하고 있다. 그렇기 때문에 같은 network을 사용하여 서로 다른 이미지에서 서로 다른 content와 style을 reconstruction해서 그 둘을 섞는 것이 가능한 것이다. 이것이 중요한 이유는 실제로 reconstruction을 하는 과정은 임의의 image를 input으로 삼고, image를 parameter로 하여 목표하는 style과 content에 대한 loss를 minimize하는 optimization 과정이기 때문이다. 이 두 가지 다른 optimization process를 오직 하나의 network만 사용하여 진행할 수 있기 때문에 \(A\)(혹은 튀빙겐에서 찍은 사진)이라는 input의 content를 가지면서 \(B\)(혹은 고흐의 &#60;별이 빛나는 밤&#62;)이라는 input의 style을 가지도록하는 방향으로 input 이미지의 gradient를 구할 수 있는 것이다. 수식으로 나타내보자. input image를 \(x\)라고 해보자. 우리 목표는 \(x\)와 \(A\) 간의 content가 얼마나 다른지 표현하는 loss function \(\mathcal L_{content} (x,A)\)와 \(x\)와 \(B\) 간의 style이 얼마나 다른지 표현하는 loss function \(\mathcal L_{style (x,A)}\)를 minimize하는 \(x\)를 찾는 것이다. 따라서 우리가 풀고 싶은 optimization problem은 다음과 같다.</p>


<p>\[x = \arg\max_x \alpha\mathcal L_{content} (x,A) + \beta\mathcal L_{style} (x,B)\]</p>


<p>이런 식으로 식을 쓸 수 있을 것이다 (\(\alpha\)와 \(\beta\)는 적당한 상수라고 하자). 이런 optimization을 푸는 가장 간단한 방법으로 \(x\)에 대한 gradient를 구하고 gradient descent optimization을 하는 것인데, 두 loss를 같은 network에 대해 design할 수 있기 때문에 gradient가 간단해지는 것이다. 그러면 이제 구체적으로 어떻게 각각의 loss가 정의되었는지 살펴보자.</p>




<h5>Methods</h5>


<p>이 paper에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a> 네트워크를 사용한다. 이 네트워크는 옥스포드의 VGG(Visual Geometry Group)에서 만든 네트워크로, <a href="http://SanghyukChun.github.io/88">Batch Normalization</a>이 적용되기 이전 inception network (혹은 GoogleNet) 등에 비해 꽤 우수한 성능을 보이는 네트워크이다. 아래 논문을 통해 발표하였다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. &ldquo;Very deep convolutional networks for large-scale image recognition.&rdquo; arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>


<p>자세한 method를 설명하기에 앞서 VGG 19 네트워크 자체에 대해 다뤄야하는데, 이 네트워크는 총 16개의 convolution layer, 5개의 pooling layer, 3개의 fully connected layer로 구성되어있다. 이 논문은 제공된 16개의 convolution layer에서 생성되는 feature map을 사용해 style loss와 content loss를 계산한다. 이 네트워크는 다음과 같은 형태로 구성되어있다.</p>


<p><img class="center" src="/images/post/92-6.png" width="400"></p>

<p>이 논문에서 사용한 VGG 19는 E에 해당하며, conv 2개 - pooling - conv 2개 - pooling - conv 4개 ... 이런 식으로 구성되어있다. 각각의 conv layer들은 pooling layer를 기준으로, 순서대로 conv 1_1, conv 1_2, conv 2_1, conv 2_2, conv 3_1, conv 3_2, conv 3_3, ... conv 5_4 라는 이름을 가지고 있다. 즉, conv 5_1 이면 4번쨰 pooling layer 바로 다음 conv layer를 말하는 것이다.</p>


<p>이 논문에서는 fully connected layer는 사용하지 않고, 16개의 conv layer와 5개의 pooling layer만 사용하는데, image reconstruction에 있어서는 max pooling보다는 average pooling을 고르는 것이 그림이 조금 더 자연스럽고 좋아보이는 결과로 나오기 때문에 max pooling 대신 average pooling을 사용하였다고 한다.</p>




<p>그럼 먼저 비교적 간단한 content loss 부터 살펴보도록하자. 이 논문은 feature map을 \(F^l \in \mathcal R^{N_l \times M_l}\)으로 정의하였다. 이때 \(N_l\)은 \(l\) 번째 레이어의 filter 개수이고, \(M_l\)은 각각의 filter의 가로와 세로를 곱한 값이며, 즉 각 filter들의 output 개수이다. 또한 \(F^l_{ij}\)는 \(i\) 번째 필터의 \(j\) 번째 output을 의미하게 된다. 이제 우리가 비교하려는 두 가지 이미지를 각각 \(p\)와 \(x\)라 하고, 각각의 \(l\) 번째 layer의 feature representation을 \(P^l, F^l\)로 정의하자. 이렇게 정의하였을 때, \(l\) 번째 layer의 content loss는 다음과 같이 간단하게 정의된다.</p>


<p>\[ \mathcal L_{content} (p, x, l) = \frac{1}{2} \sum_{ij} \big( F^l_{ij} - P^l_{ij} \big)^2. \]</p>


<p>즉, \(p\)와 \(x\)에 대해 각각 feature map \(P^l, F^l\)을 계산하고, 이 둘의 차의 Frobenius norm (\(\| P^l - F^l \|_F\))을 loss로 선택한 것이다. 이 error를 각각의 layer에 대해 따로 정의하게 된다. 이제 튀빙겐에서 찍은 사진 \(p\)의 \(l\) 번째 layer의 representation을 사용해 image reconstruction을 한다고 가정해보자. 이때 \(l\) 번째 layer에서 복원한 이미지를 \(x^l\)라고 하자. 앞서 정의한 loss를 minimize하는 \(x^l\)를 찾아야하므로, 우리는 다음과 같은 식을 얻는다.</p>


<p>\[x^l = \arg\max_x \mathcal L_{content} (p, x, l). \]</p>


<p>이 식을 풀기 위한 가장 간단한 방법은 \(x^l\)을 random image로 initialize하고 \(\frac{\mathcal L_{content} (p, x, l)}{x}\)를 게산해 gradient descent method를 사용하는 것이다. Loss를 layer의 각각의 activation으로 미분한 결과는 다음과 같다.</p>


<p>\[ \frac{\partial \mathcal L_{content} (p, x, l)}{\partial F^l_{ij}} = (F^l_{ij} - P^l_{ij})_{ij} \mbox{ if } F^l_{ij} > 0, \mbox{ otherwise, } 0.\]</p>


<p>이 값을 사용하면 전체 gradient를 back-propagation 알고리즘을 사용해 간단하게 계산할 수 있게 된다. <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 아래 부분에서 복원한 5개의 이미지는 각각 conv 1_1, conv 2_2, conv 3_1, conv 4_1, conv 5_1에서 loss를 계산하여 복원한 것이다.</p>




<p>다음으로, style에 대한 loss를 정의해보자. 이 논문에서 style이라는 것은 같은 layer의 서로 다른 filter들끼리의 correlation으로 정의한다. 즉, filter가 \(N_l\)개 있으므로 이것들의 correlation은 \(G^l \in \mathcal R^{N_l \times N_l}\)이 될 것이다. 이때, correlation을 계산하기 위하여 각각의 filter의 expectation 값을 사용하여 correlation matrix를 계산한다고 한다. 즉, \(l\)번째 layer에서 필터가 100개 있고, 각 필터별로 output이 400개 있다면, 각각의 100개의 필터마다 400개의 output들을 평균내어 값을 100개 뽑아내고, 그 100개의 값들의 correlation을 계산했다는 것이다. 이렇게 계산한 matrix를 Gram matrix라고 하며 \(G^l_{ij}\)라고 적으며 다음과 같이 계산할 수 있다.</p>


<p>\[ G^l_{ij} = \sum_{k} F^l_{ik} F^l_{kj}.\]</p>


<p>두 개의 image \(a\)와 \(x\) 간의 style이 얼마나 다른지를 나타내는 style loss \(\mathcal L_{style}\)은 \(G^l_{ij}\)를 사용하여 다음과 같이 정의된다.</p>


<p>\[ \mathcal L_{style} (a,x) = \sum_{l=0}^L w_l E_l \]</p>


<p>\(L\)은 loss에 영향을 주는 layer 개수, \(w_l\)은 전부 더해서 1이 되는 weight이고, \(E_l\)은 layer \(l\)의 style loss contribution이다. 이 값은 다음과 같이 정의된다.</p>


<p>\[ E_l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} \big( G^l_{ij} - A^l_{ij} \big)^2. \]</p>


<p>역시 마찬가지로, \(p\), 혹은 고흐의 &#60;별이 빛나는 밤&#62;의 layer 별 style reconstruction 역시 이 \(\mathcal L_{style} (a,x)\)를 minimize하는 \(x\)를 찾는 것으로 풀 수 있으며 이 문제는 back-propagation algorithm으로 풀 수 있다.</p>


<p>\[ \frac{\partial \mathcal E_l}{\partial F^l_{ij}} = \frac{1}{N_l^2 M_l^2} \sum_{i,j} \big( \big(F^l)^\top \big( G^l_{ij} - A^l_{ij} \big)\big)_{ji} \mbox{ if } F^l_{ij} > 0 \mbox{ otherwise } 0. \]</p>


<p>다시 한 번 <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 윗 부분에서 복원한 이미지를 살펴보면, 순서대로 loss 계산을 위해 conv 1_1만 사용하여 복원한 그림, conv 1_1, conv 2_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1, conv 4_1을 선택한 그림 ... 이런 식으로 선택하여 복원을 한 그림이다. 이때 '선택' 한다는 것의 개념은 선택한 layer의 \(w_l\)의 값을 0이 아닌 같은 값으로 두고 나머지는 전부 0으로 설정하는 것이다. 예를 들어 c 그림은 4개만 영향을 주므로 conv 1_1, conv 2_1, conv 3_1, conv 4_1만 \(w_l = 0.25\)이고 나머지는 0이다.</p>


<p>이제 마지막으로 이 두 가지 loss를 한 번에 optimization하는 과정만 남았다. \(\alpha\)와 \(\beta\)는 content와 style 중 어느 쪽에 더 초점을 둘 것인지 조정하는 파라미터로, 보통 \(\alpha/\beta\)으로 \(10^{-3}\)이나 \(10^{-4}\) 정도를 고른다고 한다.</p>


<p>\[\mathcal L_{total} (p,a,x) = \alpha \mathcal L_{content} (p, x) + \beta \mathcal L_{style} (a,x)\]</p>


<p></p>

<p>논문에서는 style에 얼마나 많은 layer를 고려하는지에 따라, 그리고 \(\alpha/\beta\)의 값을 조정함에 따라 다음과 같이 결과가 달라진다고 report하고 있다.</p>


<p><img src="/images/post/92-7.png" width="600"></p>

<p>x 축이 \(\alpha/\beta\), y축은 순서대로 앞에서처럼 conv 1_1, conv 2_1, conv 3_1, conv 4_1, conv 5_1을 선택한 것이다 (A: 1_1, B: 1_1, 2_1, C: 1_1, 2_1, 3_1, ...) 이 값들을 어떻게 조정하느냐에 따라 style과 content의 적당한 trade-off를 조정할 수 있다. Layer를 더 많이 사용할수록, 그리고 \(\alpha/\beta\) 값이 작아질수록 content보다는 style에 더 치중된 결과가 나오게 된다. 그리고 당연히 layer를 더 적게 사용하거나 \(\alpha/\beta\)의 값을 키울수록 그 반대의 결과가 나오게 된다.</p>


<p><img src="/images/post/92-2.png" width="600"></p>

<p>위 그림은 앞에서 언급한 <a href="https://github.com/jcjohnson/neural-style">'neural style'</a>을 사용해 만든 그림이다. 원본 content 이미지로 브래드 피트의 사진을 넣고, style 이미지로 피카소의 &#60;자화상&#62;을 넣은 다음, \(\alpha/\beta\) 값을 조정하면서 값이 변하는 것을 관측한 것이다.</p>


<p>맨 처음 글을 시작하며 보았던 그림에서는, content representation은 conv 4_2의 것만을 사용하고, style representation은 conv 1_1, 2_1, 3_1, 4_1, 5_1 에 각각 \(w_l = 1/5\), 나머지는 \(w_l=0\)으로 하여 사용했다. 또한 B,C,D 그림은 \(\alpha/\beta = 10^{-3}\), E,D 그림은 \(\alpha/\beta = 10^{-4}\)를 사용하였다고 한다.</p>




<h5>Comments</h5>


<ul>
<li>개인적인 생각으로는, 이 논문의 결과는 고흐나 뭉크 등의 &lsquo;스타일'이 분명한 인상주의, 표현주의, 야수파 화풍의 화가들의 그림을 더 잘 generate할 것으로 생각된다. 나중에 사람들이 실험해본 결과도 그렇고, 대체로 고흐 등의 경우 스타일이 특색이 뚜렷해서 그러한지 꽤 그럴싸한 결과가 나오는 반면, 피카소 등으로 대변되는 입체파 처럼 '스타일'을 넘어서는 그 무언가가 존재하는 경우 기대만큼 좋은 결과로 이어지는 것 같지는 않다. 원래 이 논문에서 제안하는 알고리즘의 목적 자체가 그림의 texture를 learning하여 content는 유지한 상태로 texture만 변경시키는 것이므로, 내용 자체가 변화하는 입체파 등의 독특한 그림을 제대로 따라하는 것은 불가능하기 떄문에 그런 것으로 보인다. (+ 글을 쓰면서 개인적으로 궁금해진게, 캐리커쳐는 어떻게 반응할지 궁금해졌다. 나중에 public하게 공개된 코드를 사용해서 실험해봐야겠다)</li>
<li>왜 method에서 전체 conv layer를 사용하는 것이 아니라 일부만 사용하는 것인지 다소 아리송하다. 또한 왜 style reconstruction을 위해 conv 1_1, 2_1, &hellip; 5_1 의 정보만 사용했고, content는 왜 conv 4_2를 사용하였는지 역시 의아하다. 아마 제일 잘 되는 것을 골랐을텐데, 왜 그것들이 제일 잘되는 것일까 궁금해진다.</li>
<li>CNN에 대한 이해가 충분히 있어야 쉽게 읽을 수 있는 논문이었다. 추가로 VGG network에 대한 이해도도 있으면 도움이 되는 것 같다. 맨 처음 논문을 읽을 때는 이런 것들에 대해 감이 좀 약해서 읽어도 이해하기가 어려웠는데, CNN 공부를 다시 끝내고 다른 선행 연구들을 적당히 이해한 채로 다시 읽어보니 쉽게 이해할 수 있었다.</li>
</ul>


<h5>Summary of A Neural Algorithm of Artistic Style</h5>


<ul>
<li>CNN의 conv layer가 feature map이라는 것에서부터 착안하여, feature map에서 style과 content를 reconstruct하는 optimization problem을 제안하였다.</li>
<li>하나의 CNN에서 content와 style representation이 separable하므로 style과 content를 한 번에 update하는 알고리즘을 만들 수 있다.</li>
<li>Content loss는 두 이미지 각각의 feature matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 4_2 만 사용하였다.</li>
<li>Style loss는 두 이미지 각각의 Gram matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 1_1, 2_1, 3_1, 4_1, 5_1 만 사용하였다.</li>
<li>이때 style loss가 Gram matrix가 되는 이유는 style을 한 레이어 안에 있는 filter들의 correlation으로 정의했기 때문이다. 이때 correlation 계산은 각각의 filter들의 expectation 값들을 사용한다.</li>
<li>VGG 19 네트워크를 사용했으며, FC layer는 제거하고 max pooling 대신 avg pooling을 사용하였다.</li>
<li>Content loss와 Style loss의 비율을 조정하여 style과 content 중에서 어느 것에 집중할지 선택할 수 있다. 논문에서는 0.001 정도를 사용하였다.</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1508.06576">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. &ldquo;A Neural Algorithm of Artistic Style.&rdquo; arXiv preprint arXiv:1508.06576 (2015).</a></li>
<li><a href="https://github.com/jcjohnson/neural-style">&lsquo;neural style&rsquo;</a></li>
<li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. &ldquo;Very deep convolutional networks for large-scale image recognition.&rdquo; arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Basic Principles in Deep Neural Networks]]></title>
    <link href="http://SanghyukChun.github.io/54/"/>
    <updated>2015-09-29T17:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/54</id>
    <content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 포스트는 2014년 6월 16일 카이스트에서 당시 Yoshua Bengio 교수님 연구실에서 포닥 과정을 밟고 계신 장민석 박사님의 The Basic Principles in Deep Neural Networks 라는 이름의 세미나를 요약한 내용이다. 내용은 주로 Deep learnining을 supervised learning, unsupervised learning의 관점에서 각각 바라보면서 어떤 컨셉들이고, 어떤 연구들이 진행이 되어있는지 훑어보는 정도의 간단한 내용이었다.</p>


<p>어쩌다보니 1년 넘게 포스팅을 못하다가 이제와서 포스트를 등록하게 되었는데, 이 글을 제대로 정리할 정도로 여유가 없기도 했고, 내가 이 내용을 이해할 수 있을 정도의 내공이 없기 때문이기도 했다. 지금은 어느 정도 여유가 생기기도 했고, 내가 내용을 대략이나마 이해하고 있기 때문에 일 년 전 내용이기는 하지만, 다시 한 번 내용을 정리해서 올려본다.</p>


<p>이 세미나는 크게 네 가지 파트로 나뉘어진다. 먼저 Deep learning이 무엇인지 간단한 introduction part, supervised deep learning, unsupervised deep learning, 마지막으로 아직 연구가 진행 중인 advanced topic이다. 이 글에서는 introduction과 supervised learning part에 대해서 주로 다루고, 나머지 부분에 대해서는 간단하게 훑고 지나가기만 하도록 하겠다.</p>




<h5>Part 1 - Introduction</h5>


<p>Deep Learning이 무엇인지 알기 전에 먼저 Machine Learning이 무엇인지 알 필요가 있다. Machine Learning에 대해 보다 깊게 알고 싶다면 내가 아직 계속 작성 중인 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning Study</a> 글들을 읽어보아도 좋고, 다른 좋은 글들을 참고해도 좋을 것 같다. 이 세미나에서는 Machine Learning이 주 주제가 아니기 때문에, 머신러닝이라는 것을 '데이터를 통해 모델을 learning하고 learning한 모델을 사용해 주어진 query에 대답하는 것'이라고 정의하였다. 결국 내가 <a href="http://SanghyukChun.github.io/57">예전 글</a>에서 아래 그림에서 정의했던 것과 크게 차이는 없다.</p>


<p><img class="center" src="/images/post/57-2.png" width="500"></p>

<p>Deep Learning에 대해 설명하기 전에 먼저 perception이라는 문제에 대해 살펴보자. Perception이란 우리말로 하면 '인지' 정도로 해석할 수 있다. 이 문제는 주어진 정보에 대해 내가 필요한 어떤 특정 정보를 inference 하는 것이다. 예를 들어서 이 포스트의 글자가 무엇인지 읽어들이는 문제는 간단한 문제이지만, 이 문장들을 바탕으로 어떤 의미를 가지고 있는지 inference하는 것은 어려운 문제이다. 또 다른 예로는 vision 데이터를 하나 주고 주어진 vision data가 어떤 object인지 classification하는 문제도 perceptron이다. 이 문제는 사람에게는 아주 간단한 문제이지만 컴퓨터에게는 엄청나게 어려운 문제이다. Deep learning은 사람이 perception하는 방식을 모방하여, 사람만큼 perception을 해보자는 취지로 만들어진 model이라고 생각할 수 있다. 사람은 뇌의 neuron과 synapse 등으로 대표되는 일어나는 일렬의 화학적, 전기적 신호 전달 과정을 통해 perception을 하게 된다. 이것을 수학적 모델로 표현하고, 그것을 조금 deep하게 만든 것이 deep learning이다. (Neural Network와 이에 대한 intuition을 조금 더 자세히 알고 싶다면 내가 쓴 <a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a>과, <a href="http://SanghyukChun.github.io/75">Deep Learning 1</a>을 읽어보기를 권한다.)</p>


<p>Deep Learning이라는 분야는 최근 10년 동안 엄청나게 hot해진 분야이다. 그런데 사실 deep learning이라는 분야, 혹은 neural network는 사실 4-50년도 넘은 엄청 오래된 분야이다.</p>


<ul>
<li>1958 Rosenblatt proposed perceptrons</li>
<li>1980 Neocognitron (Fukushima, 1980)</li>
<li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
<li>1985 Boltzmann machines (Ackley et al., 1985)</li>
<li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
<li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
<li>1993 Sparse coding (Field, 1993)</li>
</ul>


<p>즉 우리가 지금 쓰고 있는 Neural network의 기본적인 연구는 이미 90년대 이전에 다 끝나있었다. (심지어 1995년에 Machine Learning에 어마어마한 연구 결과를 남긴 Vapnik과 Jackel이 10년 뒤인 2005년에 아무도 Neural net을 쓰고 있지 않을 것이라고 내기를 했을 정도라고 한다) 근데 갑자기 Deep learning이 왜 이렇게 hot해졌을까? 이는 내가 <a href="http://SanghyukChun.github.io/75">Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN</a> 글에서 자세히 다뤘으므로 이 글에서는 간단하게 결론만 말하도록 하겠다. 그 이유는 Deep Learning이 ImageNet에서 기존 결과를 거의 박살을 냈기 때문이다. Deep Learning이 나오기 전에는 error가 0.27 ~ 0.30정도, 그러니까 27%에서 30% 정도였다고 한다. 기존 computer vision 연구자들은 이 정도 결과가 거의 한계치라고 여겨지고 있었는데 갑자기 2012년에 Deep convolutional neural network를 하는 팀이 0.153으로 거의 2배 가까운 성능 향상을 보여주었다고 한다. 그리고 그 다음 해에는 상위 20개 팀에서 2개 팀 빼고 전부 Deep learning을 써서 Classification을 했는데, 최고 성능은 또 0.117로 개선되었다고 한다. 그 결과 computer vision을 비롯한 기존 학계에서 엄청나게 주목을 받게 되었다고 한다.</p>


<p>Deep learning에 주목하는 것은 학계만이 아니다. Deep learning을 기본 기술로 사용하는 스타트업도 엄청나게 늘어나고 있고, (<a href="http://techcrunch.com/2014/01/26/google-deepmind/">Deep learning 기술 회사인 Deep mind M 이상에 인수</a>, <a href="http://techneedle.com/archives/15662">Captcha 퍼즐 암호 99.8% 성공률로 해석</a>, <a href="http://www.technologyreview.com/news/525586/facebook-creates-software-that-matches-faces-almost-as-well-as-you-do/">사람의 얼굴 인식 능력을 상회하는 소프트웨어 개발</a>) 심지어 현재 구글이나 애플 등에서 음성 인식에 쓰는 알고리듬도 deep learning이다. 이렇듯 deep learning은 학계에서만 관심을 가지는 분야가 아니라 실제 산업에서도 아주 빠르게 적용되고 사용되고 있는 분야이기 때문에 더더욱 주목할만하다. 보통 학계에서 연구한 결과가 실제 산업에서 적용되기까지의 시간이 분야마다 조금씩 다른데, deep learning은 오히려 산업에서 먼저 개발하고 학계에 발표할 정도로 학계와 산업이 함께 집중하고 있는 분야이다.</p>


<h5>What is 'Deep' Learning?</h5>


<p>기존 machine learning 문제를 푸는 방법은 크게 3단계로 구분 할 수 있다.</p>


<ol>
<li>Feature Engineering: 주어진 데이터를 사용해 machine learning tool에서 사용할 수 있는 feature를 뽑아내는 과정. 이 과정은 machine learning이 아니며, domain knowledge와 engineer의 knowhow가 강하게 drive하는 과정</li>
<li>Learning: 1에서 주어진 feature 데이터를 사용해 machine learning model을 train하는 과정</li>
<li>Inference: 2에서 학습한 model을 사용해 새로운 데이터를 inference하는 과정</li>
</ol>


<p>첫 번째 단계는 machine learning은 아니지만, 실제 최종 performance에 큰 영향을 미치는 과정이다. 예를 들어 우리가 SVM을 사용해 image를 학습한다고 가정해보자. 100만 화소짜리 이미지를 사용해 learning을 해야하는 경우, 1번 과정이 없으면 우리는 엄청나게 high dimensional data를 사용해 SVM을 풀어야하지만, 이렇게 높은 dimension의 데이터를 사용하게 되면 <a href="http://SanghyukChun.github.io/59#59-4-cd">curse of dimensionality</a> 등의 문제로 인해 나쁜 performance를 얻게 될 확률이 크다. 그 밖에도 특정 pattern의 noise가 계속 등장하고 그 noise로 인해 outlier가 많이 생기는 등의 상황도 생길 수 있다. 특히 여러 multi media 데이터를 처리하기 위해서는 이 feature engineering이 중요한 문제이며, 단순히 feature를 잘 고르는 것 만으로도 어려운 모델을 사용하지 않고 가장 간단한 linear 모델만으로도 문제가 해결되는 경우도 많이 있다 (예: 지문 인식). 때문에 이 과정은 절대 무시할 수 없는 과정이지만 domain knowledge에 너무 크게 dependency가 있고, general purpose machine learning과 분리가 된다는 문제가 존재한다. 그러나 여기에서 중요한 점은, domain knowledge를 반영하는 것이 그렇지 않은 것에 비해 훨씬 더 우수한 결과를 낸다는 점이다.</p>


<p>그렇기 때문에 우리는 feature 역시 machine learning technique를 사용해 learning해보자는 idea를 제안할 수 있다. 예를 들어 앞에서 제안한 image같은 경우, PCA 등의 dimensionality reduction technique들을 사용한다면 더 낮은 차원의 데이터로 문제를 해결할 수 있다.</p>


<p>1-a. Feature Engineering: domain knowledge를 반영하여 representation learning의 input으로 사용할 feature를 생성하는 과정
1-b. Feature/Representation Learning: 1-a의 결과를 사용해 PCA 등의 unsupervised feature learning을 뽑아내는 과정
2. Learning: 생략
3. Inference: 생략</p>

<p>이 경우 feature extraction에서도 general machine learning 방법론을 적용할 수 있다는 장점이 존재하지만, 여전히 representation learning은 domain knowledge를 반영하지 못하기 때문에 domain knowledge와 general machine learning 간의 간극이 발생한다. 다시 말해서, PCA는 Image data의 특성을 살릴 수 없는 learning 모델이기 때문에, 결국 domain knowledge를 반영하기 위한 새로운 feature engineering 과정이 필요하다는 의미이다.</p>


<p>Deep learning은 이런 문제를 해결하기 위하여 아래와 같은 모델을 제안한다.</p>


<ol>
<li>Jointly learning everything: 한 번에 모델 하나로 feature engineering, representation learning, model learning까지 끝내는 과정</li>
<li>Inference</li>
</ol>


<p>우리가 domain knowledge를 반영하여 모델을 하나 설계한 다음, 나머지 feature engineering이나 representation learning 등의 과정을 한 번에 짬뽕해서 해결하자는 것이다. 이것이 가능한 이유는 deep learning 모델의 특성 때문이라고 할 수 있다. Neural network 모델은 아래 그림처럼 layer가 쌓여있는 형태로 구성이 되어있는데, 마치 각각의 layer를 feature extraction 과정으로 바꿔서 생각할 수 있다. 위로 올라갈수록 점점 우수한 feature를 뽑아내게 되고, 맨 마지막 layer에서 linear classifier를 learning하는 과정처럼 생각할 수 있는 것이다.</p>


<p><img src="/images/post/74-3.png" width="600"></p>

<p>또한 중요한 것은, PCA 등의 unsupervised feature extraction을 결합한 경우에는 feature extraction이 unsupervised learning이기 때문에 데이터나 최종 output loss function에 영향을 받지 못하는데 반해서, deep learning model은 모든 것들을 하나의 loss function으로 한 번에 handle하기 때문에 모든 것들을 jointly learning한다고 표현할 수 있는 것이다. 이를 가장 잘 표현하는 말이 Yoshua Bengio 교수의 "Let the data decides"라고 할 수 있다. 어떤 모델을 써야 좋은 feature를 뽑을 수 있을까에 신경쓰지말고, 처음부터 deep learning 모델처럼 좋은 모델을 사용해 데이터에서부터 좋은 결과를 낼 수 있도록 데이터가 알아서 하도록 하라는 취지의 말인데, 개인적으로 이 얘기는 특히 CNN 모델에 잘 맞는 얘기라고 생각한다. <a href="http://SanghyukChun.github.io/75#75-cnn">예전 글</a>에서도 다뤘듯이 CNN은 모델은 vision 데이터의 특성을 최대한 활용하여 feature map을 만들어내는 것이 목적이다보니, convolution과 polling layer는 최대한 feature를 만들어 내는 과정으로 쓰이고, fully connected layer에서 해당 feature를 사용하여 classification을 하는 형태가 된다. 그렇기 때문에 한 번에 preprocessing 혹은 feature engineering part와 learning하는 part가 합쳐진 형태가 되는 것이 아닐까 추측해본다.</p>


<p>그러나 단순히 deep learning을 사용하면 feature extraction과 결합된 형태로 모델을 learning할 수 있다는 이유로 아무도 쓰지 않던 deep learning을 많이 쓰기 시작한 것은 아니다. Deep learning을 많이 연구하게 된 원인으로 박사님은 두 가지 이유를 꼽았는데, 하나는 서로 다른 분야라고 생각하면서 연구되었던 PCA, Neural PCA, Probabilistic PCA, Autoencoder, Belief Network, Restricted Boltzmann Machine 등의 분야가 사실은 서로 각자의 특수한 케이스이거나 혹은 다른 표현형이라는 것이 알려지면서 결국 한 분야로 수렴하였다는 것과, 또 하나는 예전에는 알려지지 않았던 것들이 이제는 많이 알려져서 예전에는 어렵게 접근했던 것들을 이제는 쉽게 learning할 수 있다고 한다. 그 중에서 특히 non-convex optimization에 대해 많은 기술들이 연구되어서 non convex optimization이기 때문에 optimization이 불가능하다고 두려워 할 필요가 없다는 것이 가장 큰 이유라고 한다. 또한 inference와 training사이의 interaction에 대해서도 더 많이 이해하고 있다는 점도 꼽을 수 있으며, GPU 등의 하드웨어 발전으로 인해 예전보다 computation power가 exponential하게 증가한 것도 그 중의 한 원인이라고 한다.</p>


<p><img src="/images/post/54-4.png" width="600"></p>

<p>위의 그림은 각 종과 현재 개발된 NN들의 뉴런 개수를 비교한 것이다 (y 축 뉴런 개수의 스케일은 log scale이다). 맨 처음 DBN이 나올 때만 해도 편충보다 뉴런이 조금 많고 거머리보다 10배 적었다. 2012년 ImageNet에서 우수한 성과를 거둔 AlexNet의 neuron 개수는 개미보다 조금 많고 벌에 비해서 한참 적다. 그리고 2014년 기준으로 가장 큰 AdamNet의 뉴런 개수는 아직도 개구리의 뉴런 개수보다 적다. 그렇기 때문에 앞으로 사람이 가지고 있는 뉴런의 개수만큼 뉴런을 가지는 NN 모델이 개발되려면 많은 시간이 남았다고 전망하고 있다. 뉴런의 개수가 많을수록 좋은 모델이 된다는 것은 이미 알고 있지만 뉴런의 개수를 단순하게 많이 늘릴 수 없는 이유는 learning time이 엄청나게 오래걸리기 때문이다. 개인적으로는 이 부분이 아직까지도 deep learning이 발전할 여지가 많이 있다는 것을 의미한다고 생각한다. 왜냐하면 뉴런의 개수를 늘리는 일은 computation cost가 엄청나게 많이 드는 일이고, parameter 역시 exponential하게 증가하기 때문에 overfitting issue를 handle하는 것이 점점 더 중요해지기 때문이다. 더 이상의 regularization은 없다고 생각할 수도 있지만, 최근에 나온 <a href="http://SanghyukChun.github.io/88">Batch Normalization</a> work이 dropout 등의 기존 성과를 뛰어넘는 좋은 performance를 내고 있는 것을 보면, 충분히 더 좋은 접근 방법이 나올 수 있을 것이라고 믿는다.</p>




<h5>Part 2 - Supervised Neural Network</h5>


<p>Deep learning도 machine learning의 일종이기 때문에 supervised/unsupervised/reinforcement learning의 세 가지 접근 방법으로 바라보는 것이 가능하다. 이 part에서는 neural network로 supervised learning, 특히 classification을 어떻게 푸는지에 대해서 주로 다뤘었다. 세미나에서는 multilayer perceptron과 그것의 learning, regularization, 그리고 기타 등등에 대해 다뤘었지만, 이 글에서는 learning algorithm인 back-propagation에 대한 설명은 <a href="http://SanghyukChun.github.io/74#backprop">예전에 쓴 글의 링크</a>로 대체하고, 주로 어떻게 MLP를 regularization할 수 있는지 등에 대해서 다룰 것이다.</p>


<p>예전에도 설명했듯 neural network는 back-propagation이라는 알고리즘을 사용해 model parameter를 찾는다. 이 알고리즘은 any cost function이 주어졌을 때, 그 문제를 풀기 위한 gradient descent method를 chain rule을 사용해 간단하게 바꾼 알고리즘이라고 할 수 있다. 이 알고리즘을 사용하게 되면 모든 노드의 derivative를 전부 계산할 필요가 없고, 대신 매우 적은 양의 계산으로 마치 전체의 gradient를 계산한 것과 같은 효과를 얻을 수 있기 때문에 NN update는 거의 이 방법을 사용한다. 또한 모든 data에 대한 gradient를 계산하여 완벽한 gradient를 찾는 대신, batch라는 개념을 도입해 stochastic gradient descent method를 사용해 문제를 해결한다. 원래 문제가 Convex가 아니기 때문에 제대로 된 gradient descent와 SGD가 서로 다른 곳으로 수렴하기는 하지만, 이 점은 크게 중요한 이슈는 아니라고 한다.</p>


<p>여기에서 질문이 하나 나왔었다. 이렇게 할 수 있는 이유는 neural network에서 chain rule을 적용할 수 있기 때문인데 혹시 그렇다뎐 Hessian을 계산할 수는 없을까라는 질문이 나왔다. 왜냐하면 second derivative method가 gradient method보다 훨씬 좋다는 것은 이미 잘 알려진 사실이기에, 만약 Hessian을 efficient하게 계산할 수 있다면 learning 속도를 크게 향상시킬 수 있을 것으로 기대할 수 있기 때문이다. 그러나 아직까지는 실제 NN에서 Hessian 등의 2nd derivative를 계산하는 것은 Hessian Matrix를 계산해야할 뿐 아니라, 그것의 inverse까지 계산해야하므로 매우 expensive하다고 하고, 구현도 복잡하다고 한다. 그렇기 때문에 대신 Hessian matrix를 직접 구하지 않고 그것의 inverse를 estimate하는 방법들이 있지만, 그러나 여전히 큰 NN에는 부적합하기 때문에 보통 Gradient를 사용한다고 한다 (여기에서 언급된 Hessian matrix의 inverse를 estimate하는 방법을 Hessian-Free optimization이라고 부른다).</p>




<h5>Regularization</h5>


<p>Regularization을 Bayesian 관점에서 바라본다면 좋은 prior를 제안하는 것과 같다. 예를 들어 '모델이 이렇게 복잡할리 없으니 모델의 complexity를 penalty term으로 추가해야겠다' 라는 regularization method도 model complexity에 대한 prior를 반영한 것이라 할 수 있다. Neural Network에서도 마찬가지로 여러 prior를 바탕으로 다양한 regularization 방법들이 존재한다.</p>


<h6>Weight Decay</h6>


<ul>
    <li>Prior: Weight의 값이 너무 크지 않을 것이다 (이를 model이 sharp하지 않고 smooth하다 라고 표현한다).</li>
    <li>Approach: 다음과 같은 형태로 optimization objective를 바꾼다.</li>
    <p>\[\min_w E(w) + \lambda w^\top w.\]</p>
    <li>Gradient update rule은 다음과 같이 바뀐다</li>
    <p>\[w(t+1) = w(t) (1 - 2 \eta \lambda) - \eta \nabla E (w(t)). \]</p>
</ul>




<h6>Smoothness and Noise Injection</h6>


<ul>
    <li><p>Prior: Smoothness, \(f(x)\)와 \(f(x+\varepsilon)\)은 거의 비슷할 것이다.</p></li>
    <li><p>Approach: noise에 대한 change를 줄이는 것은 곧, \(\min \sum_i | \frac{\partial f(x_i)}{\partial x} |^2\) 과 같다.</p></li>
    <li>따라서 위 식을 optimization function에 추가하여 문제를 풀게 되는데, Bishop 책에 따르면, 이 regularization term을 넣고 optimization하는 것은, random Gaussian noise를 input에 추가하여 learning하는 것과 equivalent하다는 것이 알려져 있으므로, input data에 random Gaussian noise를 섞는 것으로 대체할 수 있다.</li>
</ul>




<h6>Dropout (Hinton 2012)</h6>


<ul>
    <li>Prior: 하나의 classifier를 learning하는 것 보다, 여러 개의 classifier를 learning하고 이를 ensemble하여 classification하는 것이 더 좋다.</li>
    <li>Approach: 하나의 neural network 모델에서 여러 개의 모델을 learning할 수 있도록, NN의 node를 random하게 지운다. 이 경우 lower bound optimization과 같은 효과를 내기 때문에 조금 더 general한 model을 학습하는 것이 가능하며, dropout을 선택하는 것과 그렇지 않은 것은 약 10~20%의 성능 차이를 보인다. Drop하는 node는 보통 50%를 선택한다 (이 값도 제일 좋은 값을 learning해보려고 시도해봤는데 모두 0.5로 converge하였다고 한다).</li>
</ul>




<h5 id="54-common-recipe">Common Recipe for DNN.</h5>


<p>세미나에서 박사님은 앞에서 살펴본 regularization과 optimization method들을 바탕으로 아래와 같은 common recipe를 제안하였다.</p>


<ol>
<li>Rectifier나 Maxout을 사용해라 (이 방법은 dropout처럼 값이 음수인 뉴런은 drop하고, 양수인 뉴런만 존재한다고 생각하면 계속 다른 NN을 사용하는 것처럼 생각할 수 있다. 이런 접근 방식을 사용하게 되면 differentiable하지는 않지만 sub-gradient를 사용하여 문제를 풀 수 있다고 한다).</li>
<li>Preprocess data and choose features carefully (각 데이터 domain에 맞는 preprocessing을 취하자)

<ul>
<li>Image: Whitening, Raw, SIFT, HoG?</li>
<li>Speech: Raw? Spectrum?</li>
<li>Text: Characters? words? tree?
*General: z-Normalization?</li>
</ul>
</li>
<li>Dropout이나 다른 regularization method들을 사용하자.</li>
<li>데이터가 적은 경우라면 Unsuperviesd Pretraining (Hinton et al 2006) 을 사용해보자. 그러나 데이터가 많으면 오히려 나쁜 결과를 내게 되므로 쓰지 말자.</li>
<li>Carefully search for hyperparameters (Random search, Bayesian optimization + Greedy search는 hyperparameter가 exponential하게 많으므로 불가능하다.)</li>
<li>Often, deeper the better (이미지 &ndash; 레이어 7개 이상, 스피치 &ndash; 12개, 14개,&hellip;, 그 이외에는 hyper parameter라고 한다. 박사님은 2개부터 시작한다고 한다. + 참고로 지금 이미지에서는 VGG등의 최신 논문들은 19개까지 쌓기도 하고, Google의 Inception의 경우 어마어마하게 깊다.)</li>
<li>Build an ensemble of neural networks</li>
<li>Use GPU. (좋은 파워와 메인보드가 필요하고, 쿨링과 전기세를 조심하라고 조언해주셨다)</li>
</ol>


<p>그러나 중요한 점은, 아무도 vanilla MLP (모든 layer가 fully connected layer인 NN)는 사용하지 않는다는 사실이다. 그 대신 domain knowledge가 많이 반영된 CNN이나 RNN등의 모델을 선택하여 사용한다. 최근 work들을 보면 Image등의 static한 데이터는 보통 CNN을 사용하고, sequencial data는 거의 RNN을 사용한다. CNN과 RNN에 대한 설명은 생략하도록 하겠다.</p>


<p>박사님은 deep learning의 좋은 점으로, 모델을 만들 때 부터 domain knowledge를 적용해서 CNN이나 RNN 등의 새로운 모델을 만들 수 있다는 점을 꼽았다. 만약 SVM 등의 기존 방법들을 사용한다면 모델 자체에서 그런 아이디어를 적용하지 어렵지만, deep learning은 그러기에 용이하다는 것이다. 예를 들어 Convolutional Neural Network의 Translation, Rotation, Temporal, Frequency invariance 등을 꼽을 수 있을 것이다. 이렇게 만든 CNN은, convolution layer와 pooling layer를 엄청나게 deep하게 쌓아 좋은 feature를 뽑아내고 마지막에 그것에 fully connected layer를 붙여서 linear classifier를 learning하는 방식으로 'deep' CNN을 구성할 수 있다. 반면 RNN은 time을 길게 가져가는 방식으로 'deep' RNN을 구성할 수 있는데, 이렇듯 'deep'하게 만드는 방식도 모델 특성을 따라가기 때문에 domain knowledge가 잘 반영된 모델이 deep해졌을 때도 잘 동작할 수 있는 것 같다고 생각한다.</p>




<h5>나머지 part들</h5>


<p>Unsupervised Learning 파트는 RBM, DNN 그리고 NADE에 대해 설명하고 마지막에 manifold leraning과 denoising autoencoder를 다루는 것으로 끝났다.</p>


<p>Advanced Topics 파트는 Deep Reinforcement Learning에 대해 다루고 (<a href="http://SanghyukChun.github.io/90">Atari 논문</a>을 다뤘다), 당시에 막 연구가 되고 있던 NLP 연구에 대해 잠시 언급했다 (1년이 지난 지금은 NLP 쪽으로 많은 연구가 진행되었다). 그 밖에 optimization 관점에서, local minima라고 여겼던 부분이 사실 local minima가 아니라 flat한 부분이었다는 관측결과가 많이 나오고 있다는 말과, 적당하게 2nd order method를 섞으면 성능이 크게 향상된다는 말까지 언급하였다 (그러나 아직까지 2nd order optimization을 많이 사용하는 것 같지는 않다). 마지막으로 deep learning의 최대 약점으로 꼽히던 이론적인 분석 결과에 대해 많은 연구가 진행되고 있다는 얘기까지 잠깐 언급하고 세미나가 마무리 되었다.</p>




<h5>정리</h5>


<p>이 글은 내가 거의 1년 3개월 전에 들었던 세미나를 바탕으로 쓰여진 글이다. 박사님이 해주신 얘기도 많이 섞여있고 내 개인적인 의견도 많이 섞여있지만, 맨 처음 deep learning을 접할 때 큰 도움이 되었던 세미나인 만큼, 공유할 수 있으면 좋을 것 같아 정리해서 올려본다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle competition - Poker rule induction]]></title>
    <link href="http://SanghyukChun.github.io/87/"/>
    <updated>2015-09-26T15:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/87</id>
    <content type="html"><![CDATA[<p>이 글은 2015년 봄 내가 조교를 했었던 KAIST 빅데이터 분석개론 수업에서 중간 프로젝트로 나왔었던 Kaggle Competition을 내가 개인적으로 진행해보면서 느꼈던 점들을 시간 순서에 맞춰 적은 일종의 개발기이다. <a href="https://github.com/SanghyukChun/kaggle-poker_rule">깃허브 레포지토리</a>도 있으니 코드가 궁금하다면 이 레포지토리를 확인하면 될 것 같다. 깃허브 기준으로 이 프로젝트는 2015년 3월 20일부터 4월 19일까지 대략 한 달간 진행하였다.</p>


<p>이 competition을 위하여 4가지 정도의 아이디어를 냈었다. 먼저 normal KNN을 최대한 튜닝해보는 것, 문제가 'rule'이라는 것에 초점을 맞추어 rule을 정의하고 그것을 learning하는 방법, 세 번째로 card set에 대한 확률모델을 정의하여 likelihood를 maximization하는 방법, 마지막으로 기존 KNN의 input을 적당하게 바꾸어 KNN으로 처리하는 방법이었다. 이 중 실제로 submission까지 이어진 아이디어는 처음과 마지막 아이디어인데, 첫 번째는 0.67575, 두 번째에는 0.96908을 달성하였다. 이 글에서는 각각의 아이디어를 생각하게 된 계기와 왜 실패했고, 왜 성공했는지에 대해 정리해보도록하겠다.</p>


<p>이 competition을 시작하게 된 계기는, 내가 조교를 맡고 있던 수업에서 프로젝트로 <a href="https://www.kaggle.com">Kaggle competition</a>의 poker rule induction 문제를 풀기로 했기 때문이었다. (<a href="https://www.kaggle.com/c/poker-rule-induction">competition 링크</a>). 문제 자체가 간단하고, 마음만 먹으면 99% 이상을 찍는 단순한 알고리즘을 만드는 것이 어렵지 않은 문제였기 때문에, 나름의 조건으로 poker rule이 아니라 그 어떤 카드 게임에도 general하게 learning 결과를 적용할 수 있는 framework을 만들자는 것과, 또 하나는 deep learning을 사용하지 않고, 학생들이 이미 알고있는 기초적인 framework에 재미있을 법한 아이디어들을 섞어보기로 하였다. 그렇기 때문에 시작은 가장 간단하고 직관적인 KNN부터 적용해보는 것으로 시작해보았다. training set을 8:2로 나눠서 validation 실험을 진행해본 결과, 그냥 raw data를 사용하고 그냥 KNN을 사용하게 되면, 결과가 50% 정도 밖에 나오지 않았다. 여기에서 내가 해볼 수 있는 가장 간단한 개선은 k의 개수를 조절하는 것과 metric을 바꾸는 것, 그리고 input data를 바꾸는 것이다.</p>


<h5>첫 번째 아이디어 KNN</h5>


<p>K를 이리저리 조절해보아도 크게 차이가 나지는 않았다. 잘 선택하니 55% 언저리도 나오기는 했지만, K와는 다른 문제가 있어보였다. 그 문제를 해결하기 위해 먼저 input data를 5차원 데이터로 바꿔보았다. 즉, 지금은 5개의 카드 각각에 대해 rank와 suit를 따로 표현하여 10차원 데이터로 표현이 되지만, rank와 suit를 합쳐보는 것이다. 그리고 knn을 해보니 결과는 50% 미만. 왜 이럴까 생각을 해보니 당연히 1~52까지 카드가 배치가 될텐데, 이 숫자는 인덱스를 의미하지 진짜 '거리'를 의미하지 않기 때문에 문제가 발생한다는 것을 알 수 있었다. 따라서 input data는 real number여서는 안되고, 52차원짜리 binary로 개선해야겠다는 생각을 하게 되었다. 즉, 원래 10차원 real value data가 이제 52차원 binary data로 바뀌게 되었다. 같은 카드가 두 번 나오지 않기 때문에 반드시 binary임이 보장된다. 그러나 52차원은 너무 크기 때문에 PCA를 사용하여 차원을 조금 더 낮은 차원으로 보내보기도 하였다. 정리하자면 인풋을 binary로 정의하고, distance는 cosine으로 바꿔보고 PCA에 사용할 low dimension을 잘 선택하고 K를 잘 조절해본 결과 8:2 세팅에서 68.233%까지 성능이 향상되는 것을 볼 수 있었다. 2015년 3월 20일 새벽 2시 반, 리소스 문제로 인해 아직 test data를 서버에 제출하지는 못하였다. 아무래도 코드를 개선해야할 것 같다.</p>


<h5>두 번째 아이디어 'rule' learning</h5>


<p>그러나 이런 식의 방법을 사용해 KNN의 성능을 개선시킬 수는 있지만, 이는 근본적이 해결책이 되지 못한다. 문제를 해결하기 위해서는 조금 더 문제를 잘 정의하고, 좋은 알고리즘을 찾아야만 한다. 즉, 문제를 어떻게 모델링하느냐에 대한 이슈가 아직 해결되지 않은 것이다. 우리는 무엇을 찾아야하는가? 나는 여기에서 한 가지 생각을 했다. 결국 우리가 찾고 싶은 것은 Poker rule이다. 즉, 만약 우리가 rule에 대한 전체 domain을 정의할 수 있고, 각각의 rule에 대한 performance measure를 정의할 수 있다면, 가장 좋은 rule을 찾는 알고리즘을 디자인 할 수 있지 않을까? 생각이 여기까지 진행되니 바로 자연스럽게 다음 질문이 나오게 되었다. 'rule'은 어떻게 정의해야할까? 처음에는 이렇게 생각했다. 결국 카드게임은 카드들을 비교하여 좋은 '조합'을 가진 사람이 이기는 게임이다. 따라서 나는 5장의 카드들로 이루어진 모든 pair 조합만큼의 dimension을 가지고 (즉, \(5 \choose 2\) = 10개) 두 카드를 비교하는 rule의 개수가 \(n\)개라고 했을 때 각각의 piar들에게 n개 중의 하나에 대응시키게 되면, 우리는 10차원 real value vector로 rule을 표시할 수 있지 않을까? 그런데 문제가 있었다. 앞에서 본 것 처럼, rule의 개수를 \(n\)개 라고 한다고 해서, 1번째 rule과 100번째 rule이 어떤 우열관계가 있는게 아니다. 따라서 결국 binary로 만들어야할 것 같다. 그래서 어떻게 두 카드 사이의 rule을 binary로 만들 수 있을지 가만 생각해보니, 결국 rule이라는 것은 현재 이 카드가 다른 카드 어떤 것과 대응되는지 되지 않는지가 아닌가라는 생각이 들었다. 예를 들어 우리가 52개의 카드를 가지고 있을 때, 같은 숫자를 가진다는 rule은, 총 52 * 52개의 모든 카드 조합 중에서 정확하게 4*13 = 52개의 조합들을 의미하는 것이 아닐까. 다시 말해서, (1, 1), (1, 14), (1, 27), (1,40), (2,2), ... 이런 식으로 정의가 된다고 생각할 수 있는 것이다. 그렇게 생각하게 되면 총 52*52 = 2704개의 rule이 필요하게 되더라.</p>


<p>이렇게 문제를 단순화시키고나니 문제의 목적은 'rule'을 찾는 것이며, rule은 다음과 같은 binary operation으로 정의할 수 있었다. 먼저 우리는 임의의 두 개의 카드 pair에 대해 총 52 * 52 = 2704개의 rule을 가진다. 왜냐하면 1부터 52까지 범위를 가지는 숫자 두 개를 연결하는 방법이 52*52개 만큼 있기 때문이다. 그리고 그 중 k개의 rule을 뽑아내고, k-1개의 binary operation을 사용해 각 점수에 대한 rule을 구해낸다. 예를 들어 카드의 값을 1부터 52까지 대응시켰을 때, 1 pair rule을 이 operation으로 나타내보면, (A 카드의 값이 1이고, B 카드의 값이 1인 rule) or (A 카드의 값이 2이고, B 카드의 값이 2인 rule) or .... 이 될 것이다. 이 경우 k는 13이고, k-1개의 operation들은 모두 or이다. 마찬가지로 2에 대해서 rule을 구하고, 계속해서 9까지 rule을 구한다.</p>


<p>우리가 모든 card set을 가지고 있다면 위의 문제를 direct하게 풀면 문제를 완벽하게 풀 수 있지만, 그렇지 않기 때문에 overfitting이 일어나게 된다. 따라서 나는 이 문제를 어떻게 더 generalize시킬 수 있느냐를 고민해보았다. 이 문제는 Rule의 총 개수를 2704개 보다 훨씬 더 적은 양으로 mapping할 수 있다면 비교적 쉽게 풀 수 있다. 나머지는 전부 training data에서 optimization으로 해결할 수 있는 문제들이니까.</p>


<p>그러나 이윽고 나는 다른 문제에 부딪히게 되었다. Rule이 이것보다 훨씬 많다는 것을 깨달았기 때문이다. Rule은 and operation으로만 이어지는 것이 아니라 or operation으로도 이어질 뿐 아니라 연산 순서 역시 중요했었다. 예를 들어 Rule = (Rule 1 and Rule 2 and Rule 3) or Rule 4 or (Rule 5 and Rule 6) 같은 지저분한 rule도 있을 수 있었기 때문이다. 즉, 내가 문제를 너무 단순하게 봤었다. 이렇게 문제를 생각하게 되면 rule에 대한 강력한 assumption이 없는 이상 더 이상의 generalization은 불가능하고, 직접적으로 rule을 learning하는 것이 어렵다는 결론에 도달하였다.</p>


<h5>세 번째 아이디어 grapical probability model</h5>


<p>다음으로 내가 생각했던 것은, 확률 모델로 문제를 디자인해보자는 것이었다. 주어진 데이터를 \(X\)라고 한다면 \(p(y|X)\)가 제일 큰 \(y\)를 고르면 되도록 말이다. 여러가지 확률 모델들이 있지만 (예를 들어 naive baysian도 확률모델이다) 내가 생각했던 아이디어는 각각의 \(y\)마다 확률 모델을 만들고, \(X\)가 \(y\)인지 아닌지 binary로 판단하게 하는 방식이었다. 이렇게 생각한 이유는, 실제로 rule이 중복해서 나올 수 있기 때문이다. 예를 들어 Triple은 two pair이기도 하고, four card는 triple이면서 two pair이기도 하다. 이렇게 모델을 정의하고 모든 \(y\)에 대해 지금 \(X\)를 대입한 후, 특정 threshold를 넘는 \(y\) 중에서 가장 큰 숫자를 고르게 하는 것이다.</p>


<p>이 아이디어를 실행하기 위해 가장 중요한 것은 어떤 probability model을 선택하느냐는 것이었다. 내가 처음 생각한 아이디어는 RBM이었지만, RBM은 주어진 데이터에 대한 joint probability만 표현할 수 있지, 어떤 데이터가 그것에 속하지 않는지 learning하는 것이 어려웠다. 이 문제가 근본적으로 기존 방법들로 접근하기 어려운 이유는 sample bias가 너무 심하기 때문이다. 즉, 0점 짜리가 거의 대부분에 속하고 그 다음으로 1점, 2점... 순서로 가기 때문에 각 sample들이 uniform하게 분포하지 않고, 그 때문에 일반적인 방법으로 접근하게 되었을 때 과도하게 0에 치중된 모델이 나오게된다는 점이었다. 나는 이 때문에 각각의 점수별로 모델을 따로 가져가고, 대신 binary로 모델을 풀고 bias를 최대한 해결할 수 있는 방법을 생각해보려 했었다.</p>


<p>그러나 conditional probabilty를 이런 방식으로 leanring할 수 있는 모델이 (학생들이 알 수 있거나 생각할 수 있을법한) 모델 중에서는 도저히 떠오르지 않아서 이 방법도 선택하지 않게 되었다.</p>


<h5>마지막 아이디어 결국 다시 KNN</h5>


<p>그래서 결국 다시 KNN으로 돌아가게 되었다. 학생 중 하나가 나에게 sorting을 하는 방법에 대해서 물어봤었고, suit를 차라리 없애는 것이 훨씬 결과가 잘 나온다는 얘기를 들었기에 그렇게 한 번 진행해보았다. 그런데 결과가 너무 잘 나오는게 아닌가 (0.96908) 심지어 1-NN을 선택했고, 내가 한 것이라고는 suit를 무시하고 rank만 5개 고르고 5개를 sorting한 것을 넣은 것에 불과했는데. 도저히 이해가 안되서 하루쯤 생각해보니 이게 왜 잘 동작하는지 알 수 있었다. KNN 세팅에서 binary로 바꾸고 하는 과정을 거치지 않았을 때 50%의 성공을 거뒀던 것에 비해 너무 말도 안되는 성능 향상이었기 때문이다. 게다가 1-NN이 아니라 2-NN이나 3-NN의 퍼포먼스가 형편없다는 것도 이해할 수 없었다.</p>


<p>실제 poker rule에서 suit와 카드 순서에 영향을 받는 룰은 Flush 와 straight, 그리고 Royal flush 뿐이지만, 이 녀석들은 거의 나타나지 않는 녀석들이었다. 이렇게 input 데이터를 sorting하게 되면 전체 경우수는 오직 ( 13 choose 5 - 13 ) = 1274 개 뿐이다. 13을 뺸 이유는 같은 rank가 같은 경우는 오직 4개 뿐이기 때문이다. 그런데 training 데이터의 개수는 25,000개 넘기 때문에 엄청 높은 확률로 rank만 고려했을 때 training 데이터와 정확하게 같은 training 데이터가 존재하게 되는 것이다. 이 말은 다시 말하면 왜 K=1 일때만 제대로 동작하는지를 증명하는 말이기도 하다. 즉, 아주 높은 확률로 항상 '정확하게 같은' 데이터 셋을 training데이터에서 고를 수 있으니 당연히 K=1을 선택해야만 제대로 결과가 동작하게 되는 것이다.</p>


<p>나는 여기까지만 시도하고 그만두었다. 이때부터 엄청 바빠지기도 했고, 내가 처음 걸었던 조건처럼 poker에 대한 가정을 최소화한 상태로 학생들도 알 수 있는 간단한 아이디어로 이 문제를 해결하는 것이 어렵다고 느꼈었기 때문이다. 그래도 최종 결과를 96% 정도 달성하였으니 이 정도면 나름 나쁘지 않은 결과가 아닐까 생각한다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (20) Reinforcement Learning]]></title>
    <link href="http://SanghyukChun.github.io/76/"/>
    <updated>2015-09-21T15:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/76</id>
    <content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="http://SanghyukChun.github.io/57">첫 글</a>에서 Machine Learning은 크게 세 가지로 구분된다는 얘기를 했었지만, 지금까지 다뤘던 주제들은 모두 supervised learning이거나 unsupervised learning이었다. Reinforcement learning은 그 둘과는 구분되는 명백히 다른 task이지만, machine learning에서 그만큼 대중적인 분야는 아니다. 아직까지 reinforcement learning을 사용한 적절한 application이 많이 제안된 것도 아니라서 practical하게 많이 사용지도 않는다. 그러나 reinforcement learning을 사용하면 supervised, unsupervised learning와는 전혀 다른 방법으로 문제를 접근하는 것이 가능하다. 최근 deep learning과 reinforcement learning을 결합한 재미있는 연구주제들도 나오고 있는 만큼 (<a href="http://SanghyukChun.github.io/90">Atari 리뷰</a>, <a href="http://SanghyukChun.github.io/91">Visual Attention 리뷰</a>), 앞으로 더 재미있는 방향으로 연구될 수 있는 주제가 아닐까 생각한다.</p>


<p>이 글은 Andrew Ng. 교수가 Stanford에서 강의하는 CS229 Machine Learning 수업의 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">lecture note</a>를 바탕으로 쓰여졌다. 이 글이 조금 부족하다고 느끼는 경우에는 해당 reference를 읽어보면 큰 도움이 될 것으로 생각된다.</p>




<h5>Reinforcement Learning: Problem Definition</h5>


<p>Supervised learning은 주어진 데이터의 label을 mapping하는 function을 찾는 문제이다. 이 경우 알고리즘은 얼마나 label을 정확하게 분류하느냐 혹은 정해진 loss function을 minimize시킬 수 있느냐에만 초점을 맞추어 모델을 learning하게 된다. 분명 supervised learning은 상당히 많은 application들에 응용될 수 있는 방법이다. 하지만 모든 문제들이 이런 방식으로 해결할 수 있는 것은 아니다. 예를 들어 2족 보행을 하는 알고리즘을 디자인한다고 생각해보자. 우리가 알고 싶은 것은 어떻게 로봇의 관절들을 움직여야 로봇이 넘어지지 않고 잘 걸을 수 있을까이다. 이 경우 우리는 관절의 움직임을 control하는 function을 learning해야한다. 이 문제를 머신러닝으로 풀기 위해서는 어떻게 문제를 정의해야할까? <a href="http://SanghyukChun.github.io/57">첫 글</a>에서 머신러닝 문제는 (1) 데이터 (2) output (3) target function (4) loss를 minimize하는 algorithm이 필요하다고 언급했었다. 먼저 데이터는 현재 관절들의 상태(각도, 위치 등)와 주변 환경(흙 위인지 아스팔트 위인지 앞에 벽이 있는지 등등)을 데이터라고 정의하자. 우리는 지금 보행을 learning하는 알고리즘을 찾고 있으므로 원하는 output은 지금 상태 다음의 관절 상태가 될 것이다. 즉, [f: 현재 관절 상태, 환경 -> 다음 관절 상태]라는 target function까지 정의할 수 있다. Loss는 특정 시간 이후 얼마나 많이 걸었는지 등으로 판단할 수 있을 것이다.</p>


<p>그렇다면 이 문제는 supervised learning이나 unsupervised learning으로 풀 수 있을까? 데이터만 무한하게 있다면 가능할지도 모르지만 현실적으로는 그럴 수 없다는 것을 알 수 있다. 왜냐하면 (data, output)의 조합이 너무 많기 때문에 의미있는 learning을 할 수 있을 정도로 많은 데이터를 모을 수 없기 때문이다. 즉, 어떤 action이 'correct' action인가 판단하는 것이 불가능하다. 대신에 이렇게 생각해보면 어떨까? 우리가 알고 싶은 것은 관절 상황과 환경이 주어졌을 때 로봇이 어떻게 action을 취해야하는가라는 policy이다. 매 action을 주어진 policy를 통해 시행하고 나면 다음 state가 정의된다. 만약 성공적으로 걸었다면 +1 점을 주고 넘어졌다면 -1점을 주는 방식을 통해 매 action의 reward를 정의할 수 있을 것이다. 그렇다면 static한 데이터 셋에서 거의 무한하게 많이 필요한 (data, output)를 사용해 learning하는 방법 대신에, 직접 매 순간 action을 실행해 reward를 받으면서 최종적으로 맨 마지막에 모든 reward의 합이 가장 좋게 만드는 policy를 learning하는 것이다.</p>


<p>이런 식으로 reinforcement learning을 high level로 설명할 수 있다. 그렇다면 RL을 조금 더 formal하게 정의해보자.</p>




<h5 id="76-mdp">Markov Decision Process (MDP): Problem definition</h5>


<p>앞에서 설명한 방식대로 RL을 정의하면 RL problem은 정말 여러가지 형태로 정의할 수 있지만, 보통 RL문제를 푼다고 하면 Markov Decision Process (MDP)를 의미한다. MDP는 \( (S, A, \{P_{sa}\}, \gamma, R ) \) 이라는 것들의 튜플이다. 각각에 대해 알아보도록 하자.</p>




<ul>
    <li><p>\(S\) - state들의 set을 의미한다. 앞에서 예를 든 2족 보행 로봇의 경우 모든 가능한 관절의 상태와 환경 등이 state가 된다. 참고로 state와 다음에 기술한 action의 개수가 유한하다면 \(|S| < \inf, |A| < \inf\), 주어진 MDP를 finite MDP라고 부른다.</p></li>
    <li><p>\(A\) - action들의 set을 의미한다. 2족 보행 로봇의 경우 어떻게 관절을 control할 것인가를 의미한다.</p></li>
    <li><p>\(P_{sa}: (s_t, a_t) \to s_{a_t}\) - State의 transition probability로, 특정 state에서 특정 action을 취했을 떄 다음 state는 어떤 state가 될지에 대한 확률 값이다.</p></li>
    <li><p>\(R: S \times A \to \mathbb R\) - 주어진 state에 action을 수행했을 때 얻게 되는 reward를 function으로 표현한 것이다. Reinforcement learning의 목표는 시간이 \(T\)만큼 흘렀을 때 최종적으로 얻게 되는 모든 reward들의 합을 (정확하게는 그것의 expectation을) maximization하는 policy를 learning하는 것이다.</p></li>
    <li><p>\(\gamma \in [0,1) \) - 앞에서 설명한 reward의 discount factor로, 시간이 지날수록 reward의 가치를 떨어뜨리고, 처음 받은 reward의 가치를 더 키워주는 역할을 한다. 즉, time \(t\)에서 얻은 reward를 \(r_t\)라고 했을 때, 전체 reward \(R_{tot}\)는 \(\sum_{t=0}^T \gamma^t r_t\)가 된다.</p></li>
</ul>




<p>MDP의 dynamics는 다음과 같은 식이다. 먼저 initial state \(s_0\)에서 어떤 초기 action \(a_0\)를 수행하게 된다. 이 행동으로 인하여 주어진 probability \(P_{s_0 a_0}\)에 따라 다음 state \(s_1\)이 확률적으로 결정된다. 그리고 그 결과로 reward \(R(s_0, a_0)\)를 얻게 된다. 이를 다시 \(s_1\)에 대해 반복하면서 state가 terminal state에 도달할 때 까지 이 과정을 반복하게 된다. 이때, MDP의 Markov property 때문에 다음 step의 reward와 transition probability는 오직 지금 state와 지금 action에 의해서만 결정된다.</p>


<p>\[s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} \ldots.\]</p>


<p>이때, 앞에서 설명한 바와 같이 매 action을 취할 때 마다 reward가 결정된다. 이때 최대한 빠르게 좋은 reward를 받을수록 좋기 때문에 나중에 얻은 reward보다 일찍 얻은 reward의 값이 같더라도, 일찍 얻은 reward가 더 valuable하다고 가정한다. 이것을 우리는 discount factor를 통해 조절하게 되며, 그 결과 우리가 maximization하고 싶은 최종 reward는 discount factor \(\gamma\)에 의해 다음과 같이 결정된다. (참고로 이 값을 sum of discounted reward라고 부른다.)</p>


<p>\[R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots.\]</p>


<p>하지만 아무리 같은 state와 action으로 시작했다고 하더라도 이 과정은 전부 \(P_{sa}\)에 의해 확률적으로 결정되는 값이기 때문에, 실제로 maximization하기 위한 target은 그 값의 expectation으로 주어진다.</p>


<p>\[\mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots ].\]</p>


<p>우리가 위 expectation을 maximization하기 위해 learning해야하는 것이 바로 policy \(\pi: S \to A\) 이다. Policy는 state에서 action으로 mapping되는 function이다 (즉, \(a_t = \pi (s_t)\).  앞에서 설명했던 transition probability는 현재 state와 action을 다음 state와 mapping해주는 function이었고, policy는 지금 state에서 내가 어떤 action을 취해야하는지 mapping해주는 function인 것이다. 즉, policy가 어떻게 변화하느냐에 따라 최종 reward가 크게 바뀌게 된다.</p>




<h5 id="76-bellman">Value function and Bellman Equation</h5>


<p>그럼 어떻게 reward를 maximize하는 policy를 learning할 수 있을까? 그것을 설명하기에 앞서, 먼저 policy \(\pi\)의 value function \(V^\pi (s)\)이라는 것을 정의해보자. 이때 앞으로 reward \(R(s,a)\)는 state에 의해서만 결정된다고 가정하고, notation을 \(R(s)\)로 바꾸도록 하겠다. 만약 action과 state 둘 다에 의해 reward가 결정되는 경우는 앞으로 설명하게 될 value function \(V\)가 아니라 <a href="76-qfunction">action-value function \(Q\)</a>라는 것을 정의하고 그것에 대한 Bellman Equation을 구해 아래와 같은 방식을 그대로 적용하는 것이 가능하다.</p>


<p>olicy \(\pi\)의 value function \(V^\pi (s)\)은 다음과 같이 정의된다.</p>


<p>\[ V^\pi (s) = \mathbb E[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \ldots | s_0 = s, \pi ]. \]</p>


<p>이 값은 즉, 간단하게 이야기 하여 주어진 state \(s\)를 initial state로 두고 action을 policy \(\pi\)를 사용하여 고르게 되었을 때 우리가 얻게되는 reward의 expectation 값이 된다. 이렇게 정의했을 경우 fixed policy \(\pi\)에 대해 value function은 다음과 같은 관계식을 가진다. 증명은 크게 어렵지 않으므로 생략하겠다.</p>


<p>\[ V^\pi (s) = R(s) + \gamma \sum_{s^\prime \in S} P_{s \pi(s)} (s^\prime) V^\pi (s^\prime). \]</p>


<p>이 관계식을 Bellman Equation이라고 부르며, 이 관계식을 통해 우리는 \(V^\pi (s)\)과 다음과 같은 두 가지 성분으로 표현된다는 것을 알 수 있다. 첫째로 immediate reward \(R(s, \pi(s))\)이다. 이 값은 우리가 처음 state \(s\)에서 바로 얻을 수 있는 reward를 의미한다. 다음으로 future reward의 expectation이다. 이 값에 discount factor를 곱하고 immediate reward를 더하게 되면 우리가 원하는 \(V^\pi (s)\)를 구하는 것이 가능하다. 이때, future reward term은 사실 \( \mathbb E_{s^\prime \sim P_{s \pi(s)}} [V^\pi (s^\prime)] \) 으로 표현할 수 있는데, 다시 말해 future reward term은 처음 state \(s\)에서 policy \(\pi\)로 정해진 다음 state \(s^\prime\)의 distribution에 대한 sum of discounted reward의 expectation 값과 같다는 것을 알 수 있다. 그러므로 두 번째 term은 MDP의 한 step이 지나고 난 이후에 발생하는 모든 sum of discounted reward들의 expectation이라는 것을 알 수 있는 것이다.</p>


<p>Bellman Equation을 사용하면 finite MDP에 대해 value function \(V^\pi (s)\)를 효율적으로 계산할 수 있다. 만약 finite MDP에 대해 문제를 풀고 있다고 가정해보자. 그렇다면 주어진 모든 state \(s\)에 대해 \(V^\pi (s)\)의 Bellman Equation을 적는 것이 가능한데, 이렇게 되면 우리는 \(|S|\)개의 linear equation과 \(|S|\)개의 variable들 (이 경우는 각 state에 대한 \(V^\pi (s)\)들)이 존재하기 때문에 간단한 연립방적식으로 value function의 값을 찾는 것이 가능하다.</p>


<p>하지만 우리가 처음부터 원했던 것은 optimal policy \(\pi^*\)이지, 주어진 \(\pi\)에 대한 expectation of sum of discounted reward가 아니다. 하지만 이 optimal policy 역시 Bellman Equation을 사용하면 계산할 수 있다. 이것을 어떻게 하는지 설명하기 전에 앞서, 먼저 optimal value function \(V^* (s)\)를 다음과 같이 정의해보자.</p>


<p>\[ V^*(s) = \max_\pi V^\pi (s). \]</p>


<p>즉, optimal value function은 모든 policy \(\pi\)에 대한 value function \(V^\pi (s)\) 중에서 가장 reward를 maximize시키는 policy를 통해 얻게 된 reward가 된다. Optimal value function 역시 Bellman Equation을 증명할 수 있는데, 그 식은 다음과 같다.</p>


<p></p>

<p>\[ V^* (s) = R(s) + \max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>


<p></p>

<p>앞의 term은 위와 동일하고, 두 번째 term은 모든 expected future sum of discounted reward를 action \(a\)에 대해 maximize한 결과이다. 즉, 모든 action 중에서 reward를 가장 maximize하는 action을 선택하였을 때 얻게되는 reward의 값이다. 그런데 그런 action을 고르는 방법이 바로 optimal policy \(\pi^*\)이므로, optimal policy는 다음과 같이 구할 수 있다.</p>


<p>\[ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>


<p>모든 state \(s\)와 모든 policy \(\pi\)에 대해 우리는 다음과 같은 관계식을 얻을 수 있다.</p>


<p>\[V^*(s) = V^{\pi^*}(s) \geq V^\pi (s).\]</p>


<p>첫번째 관계식은 optimal policy \(\pi\)에 대한 value function \(V^{\pi^*}(s)\)와 optimal value function \(V^*(s)\)가 모든 state \(s\)에 대해 equivalent하다는 것을 보여준다. 이 내용이 중요한 이유는, 초기 state가 무엇인지와 관계없이 항상 같은 optimal policy \(\pi^*\)를 사용해 optimal value function을 구할 수 있다는 의미가 되기 때문이다. 즉, 만약 optimal policy를 구할 수 있는 algorithm이 있다면 그 알고리즘의 initial state로 어느 state를 고르더라도 우리는 항상 같은 policy를 얻게된다는 사실을 암시한다. 그리고 두 번째 equation은 모든 policy \(\pi\)에 대한 value function보다 \(V^{\pi^*}(s)\)의 값이 더 크거나 같다는 것을 의미한다. 만약 optimal policy를 구하는 algorithm이 value function을 monotonically increase 시키는 방향으로 learning한다고 했을때, update되는 값의 upper bound가 존재하므로 항상 converge하게 된다는 것을 알 수 있다.</p>


<p>Finite MDP의 optimal policy를 구하는 대표적인 두 알고리즘으로는 value iteration과 policy iteration이라는 알고리즘이 존재한다. 두 알고리즘은 이름에서 알 수 있듯 모두 iterative algorithm이며, 위에서 언급한 intuition이 그대로 적용되는 알고리즘들이다. 즉, initial state에 invariant하며 iteration 동안 value function이 monotonically increase한다. 그리고 그 값이 converge하게 되면 우리가 원하는 optimal policy를 구할 수 있다.</p>




<h5>Value Iteration</h5>


<p>Value iteration 알고리즘은 다음과 같다.</p>


<ol>
    <li><p>Initialize \(V(s) = 0, ~\mbox{ for all } s\).</p></li>
    <li>Repeat until converge</li>
    <p>\[V(s) = R(s) + \max_{a\in A} \gamma \sum_{a^\prime} P_{sa} (s^\prime) V(s^\prime), ~\mbox{ for all } s. \]</p>
</ol>


<p>이 알고리즘은 앞서 설명했던 Bellman Equation에서 optimal value function의 관계식을 iterative하게 반복하면서 찾아나가는 알고리즘이다. 이 알고리즘의 두 번째 state는 synchronous update와 asynchronous update 총 두 가지 방법으로 접근이 가능하다. 먼저 synchronous update는 모든 \(s\)에 대해 value function \(V(s)\) 값 을 계산하고 그 값들을 한 번에 update하는 방법이고, asynchronous update는 한 state \(s\)에 대해 \(V(s)\)를 구하고 바로 update를 하는 방법이다. 쉽게 생각하면 asynchronous update는 stochastic gradient descent같은 방법이라 생각하면 된다. 이 두 가지 방법 모두 finite하고 polynomial time 안에 \(V\)가 optimal value function \(V^*\)으로 수렴한다는 것을 증명할 수 있다. 이렇게 구해진 \(V^*\)를 사용하면 앞에서 구했던 다음 관계식을 통해 optimal policy를 구할 수 있다.</p>


<p>\[ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>




<h5>Policy Iteration</h5>


<p>이번에는 policy iteration 알고리즘에 대해 살펴보자. 알고리즘은 다음과 같다.</p>


<ol>
    <li><p>Initialize \(\pi\) randomly</p></li>
    <li>Repeat until converge</li>
    <ul>
        <li><p>Let \(V = V^\pi\).</p></li>
        <li><p>For each state \(s\), let \(\pi(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \)</p></li>
    </ul>
</ol>


<p>Value iteration이 value function의 값을 update하는 알고리즘이라면 policy iteration은 policy를 udpate하는 iterative algorithm이다. 두 알고리즘 모두 Bellman equation을 통해 얻어지는 알고리즘이다. Policy iteration에서 \(\pi(s)\)를 업데이트하는 방식을 주어진 value function \(V\)에 대해 greedy한 policy update rule이라고 부른다. Policy iteration 역시 polynomial time 안에 optimal policy로 수렴하게 된다.</p>


<p>Value iteration과 policy iteration은 모두 MDP를 해결하는 알고리즘이며, 둘 중 무엇이 더 좋냐를 비교할 수는 없다. 그러나 일반적으로 small MDP에 대해서는 policy iteration이 빠른 시간 안에 효과적으로 수렴하고, large MDP의 경우에는 policy iteration에서 greedy policy rule update가 비효율적일 수 있기 때문에 value iteration으로 문제를 푸는 것이 computationally 좀 더 efficient하다고 한다.</p>


<p>다만 value iteration과 policy iteration은 이론적으로 optimal value function을 계산할 수 있도록 보장하는 알고리즘이기는 하지만, 실제 세상의 large MDP에 적용하기에는 모든 state와 action에 대한 경우 수를 계산하는 이런 알고리즘들은 다소 비효율적이다. 대신 다른 방법으로 value function을 update할 수 있는 알고리즘을 제안하기도 하는데, 예를 들면 지난 번에 리뷰했던 <a href="http://SanghyukChun.github.io/90">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a> 논문을 예로 들 수 있을 것 같다.</p>




<h5 id="76-qfunction">Action Value Function</h5>


<p>Reward가 state, action 둘 다에 의해 결정될 경우, value function \(V^\pi (s)\)가 아니라 action value function \(Q^\pi (s,a)\)를 사용하여야한다. Q function은 다음과 같이 정의할 수 있다.</p>


<p>\[ Q^\pi (s,a) = \mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots | s_0 = s, a_0 = a, \pi ]. \]</p>


<p>이 값을 사용하게 되면 initial state와 action에 대해 앞에서 value function에 대해 계산했던 것들을 그대로 반복할 수 있다. 먼저 \(Q^* (s,a) = \max_\pi Q^\pi (s,a)\)이고, optimal action value function의 Bellman Equation은 다음과 같이 주어진다.</p>


<p>\[ Q^* (s,a) = R(s,a) + \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) \max_{a\in A} Q^* (s^\prime, a). \]</p>


<p>남은 부분은 value function으로 진행했던 내용과 동일하게 진행하면 된다.</p>




<h5>Learning Model for MDP</h5>


<p>지금까지 앞에서 살펴봤던 내용은 모두 MDP의 state transition probability와 reward function이 전부 주어진 상태라고 가정하고 문제를 푸는 방법이었다. 하지만 실제로는 transition probability와 reward가 직접적으로 알려져있지않고, 실제 action을 수행하기 전까지 알 수 없는 것들이 훨씬 많다. 이 경우 data를 통해 transition probability와 reward function을 estimate해야한다. 이 경우 각각의 state에 대해 모든 action을 반복적으로 수행하면서 probability의 approximation 값을 구하고, reward 역시 같은 방법으로 계산해야한다.</p>




<h5>정리</h5>


<p>이 글에서는 reinforcement learning의 가장 기본적인 모델인 MDP에 대해 다루었다. MDP는 state, action, reward function, transition matrix와 discount factor로 구성된 튜플이며, optimal policy를 구하기 위해서 value function이라는 개념을 도입하고, 이 optimal value function을 계산할 수 있다면 optimal policy를 구할 수 있다. Optimal value function을 update하기 위해서 Bellman Equation이라는 관계식을 사용해 value iteration과 policy iteration이라는 알고리즘까지 살펴보았다. 이 경우 모든 reward와 transition matrix는 이미 알려져있다고 가정하였고, 만약 모르는 경우 finite MDP에서는 실제 estimation을 통해 model을 leanring해야한다는 얘기까지 하였다. 실제로 MDP 문제를 다루게 될 경우 이것보다 훨씬 복잡한 문제를 다뤄야할 일이 많지만, 근본적으로는 value iteration 등을 사용하여 optimal value function 혹은 optimal action value function의 값을 구해 optimal policy를 구한다는 기본적인 아이디어는 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 21일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">Reinforcement Learning Lecture Note &ndash; Stanford CS229 Lecture Note by Andrew Ng</a></li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 &ndash; RBM, DNN, CNN</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
