<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-Learning | README]]></title>
  <link href="http://SanghyukChun.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2014-08-02T18:40:04+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[2014 ICML 후기]]></title>
    <link href="http://SanghyukChun.github.io/55/"/>
    <updated>2014-06-26T16:16:00+09:00</updated>
    <id>http://SanghyukChun.github.io/55</id>
    <content type="html"><![CDATA[<p>이번에 <a href="icml.cc/2014/" target="new">ICML 2014</a>를 다녀왔다. 내 첫 해외 학회이기도 했고, 처음으로 Machine Learning과 관련된 연구를 하는 사람들의 talk을 듣고, 그 사람들이 직접 하는 일들을 많이 볼 수 있어서 개인적으로 많이 고무된 상태로 학회에 참여했었다. 첫 학회를 다녀온 기념으로 학회에서 내가 느낀 점들을 간단하게 정리해보고자 한다. 대충 보자면 아래 리스트 정도가 될 것 같다.</p>


<ul>
<li>항상 열정을 가지고 있고 열심인 사람들</li>
<li>어떤 식으로 연구를 해야겠다라는 생각</li>
<li>Machine Learning이라는 학문의 방대함</li>
<li>Neural Network와 Deep Learnig의 강세</li>
<li>Real Industry와 Machine Learning</li>
</ul>


<p>먼저 사람들. 개인적으로 내 자신에게 살짝 아쉬운 점이라면 학회에서 만난 사람들에게 먼저 다가가서 이야기를 걸거나 할 베짱이 많이 없었다는 것이다. 사실 내가 그렇게 사람들에게 먼저 다가가고 얘기를 나누기에는 내가 알고 있는 지식이 많이 부족함에도 원인이 있기는 하지만 그래도 다들 같은 분야에 관심을 가지고 (세부 관심사는 조금씩 다를 수 있지만) 나와 비슷한 입장을 가진 사람들도 있을 수 있었을텐데 내가 조금 적극적이지 못했던 부분이 있다. 학기 중에 윤준보교수님 수업에서 학회를 가면 그 사람들과 이야기를 많이 나누어보라는 얘기를 해주셨는데 막상 내게 그런 기회가 생기니 할 수 있는 말이 많이 없더라. 조금 더 정진해서 그런 대화에 두려움이 없을 정도의 지식을 쌓아야할텐데. 그런 개인적인 아쉬움은 잠시 넘겨서 생각을 해보자면, ICML에서 만난 사람들은 정말 열정있는 사람들이었다. 고작 15분에서 20분짜리 talk조차 이해하지 못하고 허덕이고 있는 나와는 다르게, 정말 발표자의 talk을 이해하고 그들과 communication하면서 질문하는 모습이 멋져보였다. 1시간 가까이 되는 invited talk이나 key note talk에서도 많은 부분을 놓치지 않으려 노력하는 모습도 멋졌다고 생각한다. 나도 그런 멋지다고 생각한 모습에 한 단계 더 다가가야할텐데.</p>


<p>그리고 그 다음은 역시 연구였다. 내가 대학원생이 되었고, 연구가 나에게 가장 중요한 비중을 차지하게 된 이상, 어떤 연구를 할 것이며 어떻게 연구를 할 것이며.. 혹은 왜 연구를 해야하는 것이냐 등의 물음은 나에게 굉장히 중요한 물음이다. 약간 어느 정도는 간접적으로 그 질문들에 대한 답을 얻을 수 있었던 것 같은데, 어떤 공명감으로 연구를 한다는 느낌보다는 이 문제를 풀어야하겠다는 그런 근본적인 호기심? 같은게 영향을 미치는게 아닌가 싶다. 사실 명확하게 이거다! 싶은 느낌은 잘 들지 않았지만 앞서 말했듯이 다들 열정적으로 임하고 질문 하나하나가 날카롭게 들어가는 모습을 보면서 이런 학회에 publish를 하는 사람들은 어떤 생각으로 다른 사람들의 talk을 듣는지 조금이나마 간접적으로라도 체험할 수 있지 않았나 싶다. 아무튼 진짜 열정적으로 해야한다. 그게 진짜 큰 것 같다.</p>


<p>또 학회에서 놀라웠던 점이라면 Machine Learning이라는 분야 자체가 생각보다도 훨씬 더 방대했다는 점이다. 전체 Track이 6개가 parallel 하게 돌아가면서 전체 다 합쳐서 거의 300개 가까이 되는 talk이 진행이 됐으니까.. (<a href="http://icml.cc/2014/index/article/12.htm" target="new">스케쥴</a>) 단순히 accept된 paper만 많은 것이 아니라 각 track의 주제 또한 너무나도 다양하였다. Networks and Graph-Based Learning, Reinforcement Learning, Bayesian Optimization and Gaussian Processes, Supervised Learning, Neural Networks and Deep Learning, Graphical Models, Bandits, Monte Carlo, Statistical Methods, Structured Prediction, Deep Learning and Vision, Matrix Completion and Graphs, Learning Theory, Clustering and Nonparametrics, Active Learning, Optimization, Large-Scale Learning, Latent Variable Models, Online Learning and Planning, Clustering, Metric Learning and Feature Selection, Optimization, Neural Language and Speech, Graphical Models and Approximate Inference, Online Learning, Monte Carlo and Approximate Inference, Method-Of-Moments and Spectral Methods, Boosting and Ensemble Methods, Matrix Factorization, Nonparametric Bayes, Manifolds, Kernel Methods, Unsupervised Learning and Detection, Crowd-Sourcing, Manifolds and Graphs, Regularization and Lasso, Nearest-Neighbors and Large-Scale Learning, Topic Models, Sparsity, Neural Theory and Spectral Methods, Features and Feature Selection, Time Series and Sequences.... 와 진짜 많다. 물론 이 전체를 또 잘 묶으면 더 줄어들 수 있겠지만 그래도 일단 각각의 Track들이 서로 다른 주제를 가지고 이렇게 많이 있다는 사실이 놀라웠다. 글쎄, 그래도 굳이 크게 나누자면, (1) Learning for Graphical Model (2) Traditional Machine Learning Problems (Bayesian, Supervised Learning...), (3) Optimization (4) Monte Carlo (5) Unsupervised Learning (Clustering, Metric Learning...) (6) Neural Network (7) Others 정도가 아닐까. 모르겠다 너무 많고 내가 모르는 분야가 너무 많아서. 아무튼 정말 Machine Learning이 어마어마하게 큰 분야라는 것을 다시 한 번 느끼게 되었다. 나는 저 많은 Track 중에서 어느 분야에 기여를 할 수 있을까?</p>


<p>꼭 그런건 아니었지만, 전반적으로 Deep learning 과 관련된 talk들. 심지어 'Deep' 이라는 이름이 들어가기만 해도 컨퍼런스 룸이 터질듯한 것을 볼 수 있었다. 정말 요즘 이게 핫하긴 핫하다. 근데 난 이상하게 정말 Deep learning이 싫은데.. 이유를 잘 모르겠다. 가장 practical하게 powerful해서 그렇겠지? 중국에서 해서 그런지는 모르겠지만 Deep learning세션은 중국인들이 바글바글 몰려서 진짜 산만했었다. 그만큼 가장 핫하다는 뜻이고, 중국인들이 이런 실용적인 것들에 무지 관심이 많다는 것을 느꼈다. 이론쪽보다는 확실히 그런 practical 한 세션에 중국인들이 압도적으로 많았다. Deep learning 관련 시스템 쪽도 사람 엄청 많았고.. 진짜 그야말로 Deep Learning의 시대라고 봐도 무방할 정도. 대단하더라.</p>


<p>마찬가지 맥락에서, 굉장히 많은 기업들이 ICML을 찾았다. 구글, Facebook, 아마존, MS, 야후 같은 글로벌 기업은 물론이고 바이두, 알리바바 같은 중국 기업들도 엄청 많았다. 그만큼 머신러닝을 전공한 사람들의 힘이 필요하다는, 그런 사람들에 대한 수요가 확실하구나.. 라는 그런 생각이 들더라. 기업 연구소 특히 MS나 구글 연구소 등에서도 많은 논문들이 나오는걸 보고, 저런 연구소에서 일하는 것도 생각보다는 나쁘지 않을 수도 있다는 그런 생각도 들고 그랬다.</p>


<p>ICML에서 여러모로 많은 자극을 받았다. 재미도 있었고. 내년 ICML은 내가 intivation 되서 갔으면 좋겠다! 나도 좋은 논문을 쓸 수 있었으면.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Basic Principles in Deep Neural Networks]]></title>
    <link href="http://SanghyukChun.github.io/54/"/>
    <updated>2014-06-17T19:23:00+09:00</updated>
    <id>http://SanghyukChun.github.io/54</id>
    <content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 포스트는 2014년 6월 16일 카이스트에서 유명한 Yoshua Bengio 교수님 연구실에서 포스트 닥터 과정을 밟고 계신 장민석 박사님의 The Basic Principles in Deep Neural Networks 라는 이름의 semiar를 요약한 내용이다. 내용은 주로 Deep learnining을 supervised learning, unsupervised learning의 관점에서 각각 바라보면서 어떤 컨셉들이고, 어떤 연구들이 진행이 되어있는지 훑어보는 정도의 간단한 내용이었다.</p>


<h5>Introduction</h5>


<p>Deep learning은 Machine Learning의 분야 중 하나이다. 그렇다면 Machine Learning은 무엇일까? 뭐 이거에 대한 얘기는 워낙 많이 했었고, 조만간 시간이 되면 내가 생각하고 있는 머신러닝에 대해 다시 포스팅을 할 예정이니 생략하도록 하겠다. 일단 이 세미나에서는 간단하게 머신러닝은 모델이 데이터에서 러닝하고 러닝한 데이터를 가지고 들어온 쿼리에 대해 answer하는 것이 머신러닝의 전부다! 라고 얘기해주셨다.</p>


<p>그리고 perception이라는 것에 대해서도 설명해주셨는데, 우리말로 하면 '인지' 정도 되겠다. 조금 더 구체적으로 얘기하면 내가 지금 인지하고 있는 물체에 대한 정보를 '안지' 하는 것이다. 즉, 내가 지금 읽고 있는 이 글이 어떤 철자를 가지고 있고 어떤 의미를 가지고 있는가에 대해 판단하는 것 아니면 사진 안에 있는 무언가가 고양이인지 개인지 확인하는 것, '인지'라는 단어가 한국어인지 일본어인지 판단하는 것들이 perception의 한 예라 할 수 있겠다. 우리는, 즉 사람은 이런 과정을 매우 손쉽게 하지만, 실제로 이것은 굉장히 어려운 작업이다. Deep learning이란 결국 사람이 perception을 어떻게 하는가를 기반으로 learning을 해보자라는 컨셉이다. 즉, Human learning에서 motive를 얻는 것인데, 사실 사람이 이런 인지를 한다는 것은 뇌에 의해서 일어나는 현상이며 사실은 물리적으로 매우 간단한 뉴런이라는 것들이 복합적으로 작용하면서 일어나는 것이다. 그렇기 때문에 사람의 뇌를 모방하여 매우 간단한 뉴런이라는 것들을 조합하여 인지 능력을 향상시키는 것이 Neural Network이고, 그것들을 또 폭발적으로 많이 조합한 것이 Deep learning이라 할 수 있겠다.</p>


<p>근데 사실 NN은 거의 4-50년 가까이된 무지하게 오래된 분야임 왜 이제와서 핫한가? 에 대한 질문이 있을 수 있다. history를 보면 더 명확해지는데, 아래의 리스트를 한 번 보자.</p>


<ul>
<li>1958 Rosenblatt proposed perceptrons</li>
<li>1980 Neocognitron (Fukushima, 1980)</li>
<li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
<li>1985 Boltzmann machines (Ackley et al., 1985)</li>
<li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
<li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
<li>1993 Sparse coding (Field, 1993)</li>
</ul>


<p>즉 우리가 지금 쓰고 있는 Neural network의 기본적인 연구는 이미 90년대 이전에 다 끝나있었다. 심지어 1995년에 Machine Learning에 어마어마한 연구 결과를 남긴 Vapnik과 Jackel이 10년 뒤인 2005년에 아무도 Neural net을 쓰고 있지 않을 것이라고 내기를 했을 정도니까.. 근데 갑자기 Deep learning이 정말 갑자기 빵! 하고 히트를 쳐버렸다. 그 배경을 보자면, ImageNet이라는 것이 있는데, 그냥 쉽게 생각하면 사진을 주고 어떤 물체인지 인식을 하는 일종의 race 비슷한 것이다. 그런데 그 이전까지 모든 최고의 image recognition 연구하는 사람들이 이뤄놓은 결과가 0.27 ~ 0.30정도 였는데 (아마 27%에서 30% 정도만 틀린다는 의미일 것이다) 갑자기 2012년에 Deep convolutional neural network를 하는 팀이 0.153으로 거의 2배 가까운 성능 향상을 보여주었다. 그리고 그 다음 해에는 상위 20개 팀에서 2개 팀 빼고 전부 Deep learning을 써서 Classification을 했다고 한다. 최고 성능은 또 0.117로 엄청나게 개선되고.</p>


<p>그리고 Deep learning을 기본 기술로 사용하는 스타트업도 엄청나게 늘어나고 있고, (<a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="new">Deep learning 기술 회사인 Deep mind M 이상에 인수</a>, <a href="http://techneedle.com/archives/15662" target="new">Captcha 퍼즐 암호 99.8% 성공률로 해석</a>, <a href="http://www.technologyreview.com/news/525586/facebook-creates-software-that-matches-faces-almost-as-well-as-you-do/" target="new">사람의 얼굴 인식 능력을 상회하는 소프트웨어 개발</a>) 심지어 현재 구글이나 애플 등에서 음성 인식에 쓰는 알고리듬이 deep learning이라고 하니, 단순히 학계에서만 붐인 것이 아니라 산업에서도 엄청나게 핫하다고 할 수 있다.</p>


<p>아니 근데 그래서 Deep learning이 뭘까? 뭐가 Deep하다는 걸까? 기존 Machine Learning이랑은 무엇이 다를까? 등의 질문이 들 수밖에 없다.</p>


<p>기존 머신러닝은 크게 3단계로 이뤄진다. Feature engineering, Learning, Inference. 그렇기 때문에 맨 처음 Feature extraction에서 Data의 Domain knowledge과 general machine learning이 분리가 된다. 즉, 내가 image를 이용해 machine learning을 한다고 해서 Learning 알고리듬이나 Inference에 영향을 주지는 않는다. 그러나 실제로는 inference가 learning에도 영향을 미치고, domain knowledge에도 영향을 미치는 등, 이 세가지는 분리가 되어있지 않다. 그래서 초기 Deep learning은 1a Feature engineering 1b Feature/Representation learning 2. Learning 3. Inference. 이렇게 데이터에서 나온 learnign결과를 접목하여 성능을 개선시키려는 노력에서 부터 시작되었다고 한다. 현재 쓰이는 실제 딥러닝은 1. Jointly learn everything 2. Inference 단 두가지 단계로, 데이터 자체에서 모든 것을 처리하는 것이라고 한다. 아래 그림을 보면 조금 이해가 빠를 것 같다.</p>


<p><img src="/images/post/54-1.png" width="500">
<img src="/images/post/54-2.png" width="500">
<img src="/images/post/54-3.png" width="500"></p>

<p>세미나 중에 박사님께서 Yoshua Bengio교수님 얘기 중에서 상당히 인상적이라고 했었던 얘기라면서 해줬던 얘기가 "Let the data decides" 였다. 즉, 우리가 데이터에서 피쳐 뽑는데, 좋은 모델을 찾는데에 시간을 쓰지 말고, 처음부터 좋은 머신러닝 모델을 가져와서 데이터에서부터 좋은 결과를 만들어낼 수 있도록 데이터가 알아서 하게 해라.. 뭐 이런 얘기라고 한다.</p>


<p>아무도 쓰지 않던 Deep learning 혹은 Neural network를 갑자기 사람들이 많이 쓰는 이유는 결국 사람들이 따로따로 연구하던 PCA, Neural PCA, Probabilistic PCA, Autoencoder, Belief Network, Restricted Boltzmann Machine이 서로 각자의 특수한 케이스이거나 혹은 다른 표현형이라는 것이 알려지면서 결국 한 분야로 수렴하기도 하였고, 이제는 많은 것들이 알려져서 이전에는 learning하기 어려웠던 것들을 쉽게 learning할 수 있다고 한다. 특히 이제 더 이상 non-convex optimization에 대해 두랴워 할 필요가 없다는 것이 가장 큰 이유라고 한다. 또한 inference와 training사이의 interaction에 대해서도 더 많이 이해하고 있으며, 예전보다 computation power가 exponential하게 증가했으며 특히 GPU를 사용하게 되면 정말 계산량이 폭발적으로 늘어난다고 한다.</p>


<p><img src="/images/post/54-4.png" width="600"></p>

<p>위의 그림은 각 종과 현재 개발된 NN들의 뉴런 개수를 비교한 것이데 스케일은 log scale이다. 맨 처음 DBN이 나올 때만 해도 편충보다 뉴런이 조금 많고 거머리보다 10배 적었는데, 나중에 AlexNet에서는 개미보다 조금 많고 벌에 비해서 한참 적다. 그리고 여전히 우리는 개구리의 뉴런 개수보다 적은 양의 뉴런만을 가지고 있다. 사람이 가지고 있는 뉴런의 개수만큼 발전을 하려면 아직까지 갈 길이 멀다는 얘기이다. 단순히 뉴런의 개수를 늘릴 수 없는 이유는 역시 시간이 오래걸리기 때문이다. 아직까지도 발전할 여지는 많이 있는 학문이라는 뜻이려나.</p>


<h5>Supervised Neural Network</h5>


<p>이 부분에서는 neural network로 supervised learning, 특히 classification을 어떻게 푸는지에 대해 얘기가 되며, Multilayer perceptron과 그것의 learning, regularization, 그리고 기타 등등에 대해 다르게 될 것이다.</p>


<p>Supervised learning이란 무엇인가. 데이터가 \(D = { (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) }\) 으로 주어졌을 때, 어떤 assumption \(y_n = f^* (x_n) \)을 찾는 것이 supervised learning이라 할 수 있다. 이때, 주어진 데이터는 완벽한 데이터가 아니라 다소 noise가 끼어있다고 가정한다. 또한 이것을 D에 포함되지 않은 데이터에 대해 잘 설명할 수 있는 assumption을 찾는 것이 목표인데 이유는 모든 데이터가 noisy하다고 생각하기 때문이다.</p>


<p>Probabilistic view로 보면 Underlying distribution X and Y|X이 존재한다고 했을 때, 데이터는 이 distribution에서부터 만들어지는 것이라 생각할 수 있다. 이때 우리가 찾고자하는 assumption은 \(p(y|x\)가 된다. 마찬가지로 D에 속하지 않은 데이터에 대해서 잘 설명할 수 있는 distribution을 찾는 것이 목적이다.</p>


<p>근데 '잘' 설명한다는 것은 무엇일까. 즉, Evaluation and Generalization은 어떻게 하는 것일까. 이건 원래 함수를 f라고 하고 추측한 함수를 f* 라고 했을 때 아래 그림과 같은 방법으로 추측하게 된다. 그런데 이때 이것은 확률적으로 바라봤을 때 KL divergence와 동일하기 때문에 그 아래 그림과 같이 설명할 수도 있다. 보통은 MC approximation (샘플을 막 뽑아서 성능 특정)을 사용해서 우리가 가지고 있는 샘플 중 일부분을 추출해 제외하고 f*를 찾아서 그 에러를 줄이는 방식으로 테스트를 한다.</p>


<p><img src="/images/post/54-5.png" width="500"></p>

<p>근데 왜 D가 아닌 D test에 대해 솔루션을 찾을까? 이유는 우리가 가진 resource가 finite하기 때문이다. 만약 시간이 무한하면 우리가 취하고 있는 몬테카를로가 언젠가 진짜 확률로 수렴하겠지만, 우리가 가진 자원이 유한하기 때문에 반드시 샘플 에러가 발생하게 된다. 또한 실제로 이것이 굉장히 큰 문제이기 때문에 우리가 못 봤던 테스트 셋으로 테스트를 하여 성능을 측정하는 것이다.</p>


<p></p>

<p>자 뭐 이것의 대표적인 예로는 linear regression혹은 ridge, lasso 등이 있다고 설명을 해줬지만 사실 부연 설명에 불과하므로 패스. 그렇다면 우리가 흔히 말하는 MLP, multilayer perceptron도 이것으로 설명이 가능한데, 아래와 같다.</p>


<p><img src="/images/post/54-6.png" width="500"></p>

<p>맨 아래 식은 일종의 cost function에 대한 optimization이다. 그리고 우리는 이런 optimization을 하기 위한 알고리듬으로 gradient descent algorithm을 가지고 있는데, 이 방법의 문제는 모든 데이터에 대해 코스트를 계산하여 step을 진행하기 때문에 너무 느리다는 것이다. 따라서 우리는 수 많은 데이터들 중에서 하나만 골라서 업데이트를 하는 Stochastic Gradient Descent를 사용한다. 하지만 이 경우 Convex가 아니라면 둘이 서로 다른 곳으로 수렴을 하게 된다는 문제점이 발생하지만, 사실 크게 상관없다고 한다.</p>


<p>그리고 neural network에서 쓰는 Gradient descent method는 backpropagation이 있다. <a href="http://SanghyukChun.github.io/42" taget="new">이전</a>에 설명했던 컨셉이니 생략하고, 중요한 점만 얘기하면, 이 방법을 쓰면 모든 노드의 derivative를 전부 계산할 필요가 없다는 것이다. 대신 우리는 매우 적은 양의 계산으로 마치 전체의 gradient를 계산한 것과 같은 효과를 얻을 수 있다.</p>


<p>여기에서 질문이 하나 나왔는데, 사실 이렇게 할 수 있는 이유는 chain rule을 적용할 수 있기 때문인데 (아까 건 링크 참고) 혹시 그렇다뎐 Hessian을 계산할 수는 없을까라는 질문이 나왔다. 결론적으로 말하자면, 실제 NN에서 Hessian 등의 2nd derivative를 계산하는 것은 매우 expensive하다는 것이며, 구현도 복잡하다고 한다. 대신 H를 직접 구하지 않고 그것의 역수를 계산하는 것들이 있긴 있다고 한다. 그러나 여전히 큰 NN에는 부적합하기 때문에 보통 Gradient를 사용한다고 한다.</p>


<p>Regularization &ndash; MAP.
Prob perspective: find a model M by ML (argmax p(D|M) ), MAP(argmax p(M|D) )
ML은 bayes rule! 둘을 쉽게 바꿀 수 있다.
prior distribution이 전체 model의 prob를 scope을 줄여주는 역할을 한다!
Regularization의 Omega term이 prior 역할을 하는 것.
NN의 Reg. (1) Weight Decay.
Prior: 모델은 너무 sharp하지 않다! 모델은 굉장히 smooth 할 것이다. theta_j ~ N(0, (M lambda)^-1 )
theta = argmin( loss function + lambda sum theta<sup>2</sup> )
(2) Smoothness and Noise Injection.
Input space의 굉장히 작은 부분만 관심이 있으니깐 그 부분에만 smooth하게 만들자.
instance 주변에도 비슷한 값이어야 한다. f(x) <del>~ f(x+e) &lt;=> min sum | d f(x<sup>n</sup>) / dx | ^2
&lt;=> 근데 이걸 계산하는건 계산할 때 마다 white Gaussian noise를 넣어서 계산하는 것과 동일한 결과를 낸다 1995 Bishop
(3) Ensemble learning and dropout
computation이 빨라졌으니까, 한 50개 learning하고 그걸 가지고 ensemble learning하자!
예전에는 트레이닝이 잘 안되서 잘 안했던건데, 이제는 트레이닝이 잘 되니까 그게 잘 된다
아이디어: NN을 learning할 때 마다 서로 다른 preprocessing을 하고나서 모아서 averaging 하자 object recognition은 사람보다 성능이 좋음 헐 ㅋㅋ
(4) dropout.
하나만 배우면서 exponentially many classifiers를 하자! 힌튼 2012
매번 feedforward를 할 때마다 random하게 node 를  drop하자.
수학적으로 얘기하면, lower bound를 optimization을 한다. 하고 안하고는 10</del>20% gain을 늘린다.
실제로 쓰기에는 너무 variation이 크다. 그래서 실제 classification에 좋은 결과가 안나올 수 있음. 그래서 activation을 절반으로 까는 방식으로!
결과적으로 0.5를 드랍, 0.5를 까는 것이 general하게 좋은 것으로 보인다.
다른 식으로 값을 러닝해보자는 아이디어로 해봤더니 값이 전부 0.5로 컨벌지 하더라 헐 ㅋㅋㅋㅋㅋ</p>

<p>Common Recipe for DNN.
1. User a piecewise linear hidden unit. (Rectifier, Maxout) => max{0,x} h({x1m,, xp}) = max {x1, ,,,,, xp). differentiable하지 않은데? sub gradient랑 비슷하고, 아니면 dropout이랑 비슷하게 생각하면 됨. 계속 NN을 다른 것을 쓴다고 보면 됨. 0이 되었던 애들은 뉴런이 없다고 보고.. 뭐 결국 sub gradient라고 하네요
2. Preprocess data and choose features carefully. Image: Whitening, Raw, SIFT, HoG, Speech: Raw? Spectrum?, Text: Characters? words? tree?, General: z-Normalization?
3. User Dropout and other regularization methods
4. Unsuperviesd Pretraining (Hinton et al 2006) &ndash;> a lot a unlabeled samples. label 데이터 많으면 안해도 된다네요
5. Carefully search for hyperparameters (Random search, Bayesian optimization..) => greed search 하면 exponential하게 많아서 안됨 ㅠㅠ
6. Often, deeper the better (이미지 &ndash; 레이어 7개 이상, 스피치 &ndash; 12개, 14개… 막 올라감, 그 이외에는 hyper parameter. 보통 난 2개 부터 시작한다)
7. Build and ensemble of neural networks
8. Use GPU. 파워랑 보드 좋은거 사야하고 쿨링 잘하고 전기세 조심하고..</p>

<p>Hessian의 값이 너무 크고, 그러니까 계산하기도 힘들고 저장도 하기 힘들고 그걸 inverse 하는건 더 힘들고!!
가장 큰 단점은 stochastic method가 없다 한 업데이트가 너무 expensive하다 ㅠㅠ
그러니깐 2nd order가 좋은건 알고 있고, 진짜 이론적으로 좋지만, in practice, 실제로는 model이 너무 크기 때문에 힘들다 으엉 ㅠㅠ</p>

<p>Domain Knowledge?
Data Preprocessing and feature extraction… Contrast normalization, Relative coordinate, Bag-of-Words representation.
NN의 좋은 점. 예를 들어서 SVM은 튜닝할 여지가 거의 없지만, NN은 처음부터 domain knowledge를 적용해서 model을 만들 수 있음.
ex. Convolutional Neural Network: Translation, Rotation, Temporal, Frequency invariance
Convolution and Pooling
Convolution: filter 하나만 가지고 전부 적용하면 된다
Pooling: local에서 계산한 convolution의 max를 취하고,… 뭐 그래서 구하는 듯
Convolution layer
1. Contrast Normalization
2. Convolution
3. Pooling
4. NonLinearity
Deep CNN: conv layer를 무지하게 쌓고 마지막에 full connection을 이어준다.</p>

<p>Recursive Neural Network (RNN)
Text: Compositional Structure. Tree 형태가 된다.
근데 문장에 따라 tree가 달라져 ㅠㅠ
아이디어: 같은 NN을 모든 tree의 edge에 apply를 한다 ( input NP, VP => S )
sentence &ndash;> parser로 쪼갬 &ndash;> filter를 NN으로 갈아낌</p>

<p>Unsupervised learning
Label이 없다!
DNN을 써서 visualization.</p>

<p>Feature extraction. => without domain knowledge &ndash;> learned feature!!
Generative Model.
Underlying diet. X~Pr(D)</p>

<p>Classification, Missing value reconstruction, Denoising, Structured Output Prediction, Outlier Detection.
Ultimately it comes down to learning a diet of x.</p>

<p>Density/Distribution Estimation.
Latent Variable Models p(x). Define a parametric form of joint dist. Derive a learning rule for theta.
RBM. Markov random field 의 특정 케이스.
Joint distribution, Marginal distribution.. Learning rule Maximum Likelihood.
Belief Network. Joint distribution은 구할 수 있음. Marginal distribution… 못구함. MLE로 learning. (중요하지 않음)
NADE. no latent variable h. Joint dist = Marginal dist. condition들을 NN으로 모델링하자. ML. (중요하지 않음)
Intractability!!!!
Boltzmann Machine. 다 어려움.
RBM. Marginal Probability, Posterior Prob를 구할 수 있지만 normalization constant와 conditional prob가 어려움.
Belief net norm만 없음
NADE nomr x margi x posterior x. 근데 conditional prob가 잘 안됨. 그리고 퍼포먼스가 안좋다.</p>

<p>GM &ndash; learning to infer
All we want is to infer the conditional diet of unknown variables.
Approximate Inference in a Graphical Model. p(x_mis|x_obs) ~~ Q(x_mis|x_obs)
Methods: Loopy belief propagation, variational inference/message-passing
Q<sup>k</sup>(x_mis|x_obs) = f(Q<sup>k-1</sup> (x_mis|x_obs) ). => chain 처럼 이을 수 있음</p>

<p>NADE-k. => graphical 하게 시작한 RBM같은 애들은 structure가 고정되는데 얘는 이런 모델을 만들고 싶으면 그냥 만들면 된다
Lesson!!!
Do not maximize log-likelihood but minimize actual cost!
&lt;=> Don’t do what model tells you to do but do what you aim to do.
… popular science journalists don’t care about GM ㅠㅠ</p>

<p>Manifold learning &ndash; semi supervised learning
GM이 뜨는 이유는 Pre training은 왜 하는가..? 그냥 하는게 아니다
Classification이라는 hot한 supervised learning의 task에서도 unlabled data가 performance를 향상시킨다.
Manifold learning &ndash; New representation
1. phi should reflect changes along the manifold 2. phi should not change for orthogonal noise
문제. representation을 어떻게 learnig할 것이며 어떻게 transform할 것이냐
Solution. Denosing Autoencoder.
manifold…</p>

<p>Semi-supervised learning in action (1) Layer-wise Pretraining
manifold를 어떻게 러닝할 것이냐 data를 스트레칭 아웃. 원래 데이터가 구석에 있었는데 representation을 진행할 때 마다 다른empty space를 무시하게..
각각 pretraining하는 데이터는 label이 있거나 없거나 상관없다. unsupervised learning으로 끝나버린다. hyperparameter의 개수와 sensitivity가 크게 줄어든다.
visualizaion에 쓴다. manifold embedding and visualization.
non linear visualization은 non parametic. 새 data가 들어오면 새로 learning해야 함.
하지만 autoencoder는 parametric model이라 그냥 feed forward만 하면 된다.</p>

<p>What other applications for unsupervised DNN?
Deep Reinforcement learning.. NN의 input을 state 로 하여 새로운 function을 define
NLP.. Natural Language is a huge set of variable length sequences of high dim vectors.
Hugh set. 데이터가 어마어마하지만 실제 set도 커버 못함. 거기에 input까지 variable length ㅠㅠ sequence도 high dim vector 으앙
시멘틱을 보존하면서 encoding&hellip;</p>

<p>random function은 high dimension local optima가 e로 bound가 된다고 하네요. 많아도 다 비슷한 값을 가진다. 근데 뭐가 문제? flat한 지점이 무지하게 많은겨 (saddle point)
Saddle point를 벗어나는 방법은? 2nd order method! … 오히려 2nd order를 쓰면 그 지점들에 멈추는 경우가 많음.
Saddle-Free Newton Method (<a href="http://arxiv.org/pdf/1406.2572.pdf">http://arxiv.org/pdf/1406.2572.pdf</a>)</p>

<p>Deep Neural Network를 보면 Theory가 거의 없다.. 사실 많이 되어있다. 특히 circuit theory, dynamics.. 등으로 할 수 있음
근데 그 어떤 것도 Rectifier에 대해 분석한게 없다 최근 좀 되고 있음 (<a href="http://arxiv.org/pdf/1402.1869.pdf">http://arxiv.org/pdf/1402.1869.pdf</a>)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week4 & 5 - Applications]]></title>
    <link href="http://SanghyukChun.github.io/43/"/>
    <updated>2014-04-05T21:38:00+09:00</updated>
    <id>http://SanghyukChun.github.io/43</id>
    <content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 4주차와 5주차 강의를 요약한 글이다. 이 강의에서는 이론적인 형태의 무언가를 배운다기보다는 real application의 예로 Neural Network를 사용해 word prediction (NLP에서 쓰이는) 그리고 object recognition를 하는 방법에 대해 다룬다. 내가 관심이 있는 부분은 실제 이 NN을 어떻게 적용하냐라기 보다는 NN이란 무엇이며 어떤 이론적 배경이 있는 것이며 그 알고리듬에 더 관심이 있기 때문에 대부분이 engineerning issue인 이번 두 강의들은 대략적으로 어떤 내용에 대해서 다루는지만 아주 간략하게 기술하였다.</p>


<h5>Learning feature vectors for words</h5>


<p>Speech recoginition에서 가장 중요한 문제 중 하나는 바로 제대로 인식되지 않은 단어가 무엇일지 추측하는 것이다. 아무리 기술이 좋아지더라도 speech를 완전하게 복원하는 것은 불가능하다. 살제 사람들도 완전하게 speech를 인식하는 것은 아니고 때에 따라 적당히 추측해서 이해하는 것처럼, 이런 문제가 음성 인식에서 많이 중요한 문제로 대두되고 있다. 나도 seri를 사용하다보면 음성인식률이 생각보다 좋지않아서 실망하고는 하는데, 이런 문제점을 해결하기 위해 다양한 방법들이 제시되고 있다. 이 강의는 이런 것들을 개선시키기 위해 neural network를 도입했었던 연구들에 대해 다룬다. 참고로 여기에서 사용하는 모든 NN들은 softmax function neuron을 사용하고 error function은 cross entropy error를 사용한다. 이유는 sigmoid function을 사용했을 때 error가 rmse라면 제대로 우리가 원하는 방향으로 학습하기가 어렵기 때문이다. 왜냐하면 sigmoid function은 양 쪽 끝 부분이 거의 평평하기 때문에 만약 우리가 정 반대쪽 방향에서 rmse의 gradient 방향을 취하게 된다면 거의 변화가 없다고 판단할 수도 있기 때문이다. 따라서 이런 방법을 개선하기 위해서 cross entropy error를 사용하였고, 이 방법을 사용하기 위하여 sigmoid를 softmax로 바꿔서 probability distribution으로 만들어준 것이다. 아무튼 이 강의에서 설명하는 방법들은 이런 방법들에 기반해서 NN을 만든다. 아무튼 우리가 처음에 풀려고 했었던 단어를 추측하는 고전적인 방법 중에 <a href="http://en.wikipedia.org/wiki/N-gram" target="new">N-gram</a>문제라는 것이 있는데, n개의 단어 배열들을 학습하여 임의의 n-1개의 단어가 주어졌을 때 그 다음 단어가 무엇일지 예측하는 문제이다. 실제로 자연언어처리에서 많이 사용하는 기법 중 하나인데, 보통 trigram을 많이 사용한다 (3개의 단어 시퀀스를 학습) 그런데 이 경우 우리가 모든 단어를 학습할 수도 없고, 우리가 관측하지 못한 단어배열이라고 해서 세상에 존재하지 않는 단어 배열이라 확신할 수가 없기 때문에 당연히 성능 역시 좋지 않을 것이라고 예측할 수 있을 것이다. 대신 input을 앞의 2개의 단어를 취하고 그 output을 세 번째 단어로 하는 neural network를 학습할 수도 있을 것이다. 하지만 이렇게 할 경우 output의 양이 너무 많아지므로 대신 3번째 단어의 후보군들의 집합을 같이 넣어서 결과를 얻는 방식을 취할 수도 있을 것이다. 더 성능을 높이기 위해서 엄청 긴 word seqeunce를 통채로 학습하고, 임의의 단어의 앞의 n개 단어 뒤의 m개 단어를 보고 추측하는 것이 가능할 것이다. 이 경우, 지금 단어가 random인지 제대로 된 단어인지 일부러 섞어서 learning을 하게 되면 output을 binary로 받는 것이 가능해져서 엄청나게 빠른 test time을 가질 수가 있게 된다는 장점이 있다.</p>


<h5>Object recognition with neural nets</h5>


<p>우리가 물체를 인식하는 것을 매우 자연스러운 일로 생각하지만 실제로 이것을 구현하는 것은 절대로 쉬운 일이 아니다. 우리는 물체가 살짝 가려져 있어도 구분이 가능하고, 또한 해당 물체를 유동적으로 인식한다. 무슨 말인가하면, 인식 알고리듬을 디자인하는데 있어서 문제점은 (1) 물체가 다른 물체 혹은 주변 환경에 의해서 가려진 상태일 때 (2) 밝기와 조명에 따른 해당 물체의 색 변화 (pixel 정보가 변한다) (3) 같은 클래스에 속해도 조금씩 다른 모습 - 예를 들어서 손글씨 숫자는 비록 같은 숫자이나 전부 필체가 달라서 variation이 있다. (4) 물체가 정의되는 것은 모양이 아니라 다른 방법으로 결정되는 경우도 많다 - 예를 들어 의자는 모습으로 구분하는 것이 아니라 그 사용처가 어디인가에 따라 분류해야 구분할 수 있을 것이다.</p>


<p>그 밖에도 결국 우리가 사용할 수 있는 정보는 image 정보이고, 디지털 input image는 단순한 pixel map이다. 예를 들어서 1024 by 720 pixel의 사진이라고 한다면 총 737280개의 pixel이 input이 될 것이다. 각 pixel은 RGB정보를 가지고 있으므로 값은 (0,0,0) ~ (255,255,255) 사이의 값으로 정해질 것이다. 색상이 중요하지 않은 경우에는 흑백 사진으로 바꾸어 단순히 밝기로만 판단하기도 한다. 아무튼 그렇기 때문에 약간의 물체의 이동도 성능을 크게 바꿔버릴 수 있다. 단순히 옆으로 이동한 것 뿐 아니라 회전된 정보나 뒤집힌 정보는 우리가 인식 알고리듬을 작성하는데에 매우 어려운 부분으로 작용한다. 따라서 우리는 모든 정보를 normalization시켜야할 필요가 있으며 align해야 할 필요가 있다. 이런 처리 없이는 올바른 정보를 학습하기가 매우 어려워질 것이다.</p>


<p>이 강의에서는 이런 문제점들을 가지고 있는 object recognition을 neural network로 접근한다. 이런 경우 determistic method보다 훨씬 더 좋은 성능을 낼 수 있으리라는 것은 자명할 것이다. Input은 pixel map이고, output은 어떤 object인지 알려주는 label 혹은 class가 될 것이다. 이런 Hidden layer가 포함된 neural network를 backpropagation 방법을 사용하여 learning한다. 실제로 많은 image recoginition에서 neural network를 사용하고 있으며, 앞에서 설명했었던 여러 문제점들을 해결해주는 경우가 많다. 또한 아마 이런 목적을 가지고 설계된 알고리듬 중에서는 neural network가 가장 성능이 좋을 것이다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week3 - Backpropagation]]></title>
    <link href="http://SanghyukChun.github.io/42/"/>
    <updated>2014-03-26T00:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/42</id>
    <content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 3주차 강의를 요약한 글이다. 이 렉쳐에서는 Perceptron의 한계를 극복하기 위해 도입된 multi-layer feed forward network를 learning하는 algorithm인 backpropagation algorithm에 대해서 다룬다.</p>


<h5>Learning the weights of a linear neuron</h5>


<p><a href="http://SanghyukChun.github.io/40#what-perceptrons-cant-do">Lecture 2의 마지막</a>에서 다뤘던 perceptron의 가장 큰 문제점은 문제가 조금만 복잡해지거나, linear하게 표현되지 않는 문제는 올바른 결과로 수렴할 수 없다는 것이었다. 예를 들어서 엄청나게 간단한 로직인 xor은 perceptron으로 learning될 수 없다. 이런 문제를 해결하기 위해 <a class="red tip" title="이 렉쳐에서는 multi-layer perceptron이라고 지칭한다.">multi-layer feed forwad neural network</a>의 필요성이 대두된다. 일단 우리는 기존의 perceptron algorithm으로는 해당 문제를 해결할 수 없다는 것을 알고있으므로, 무언가 다른 알고리듬을 한 번 고안해보도록 하자. 가장 간단하게 생각할 수 있는 알고리듬은 어떤 error function을 정의하고 그 error를 minimize시키는 network를 learning하는 것일 것이다. 그렇다면 어떤 error function을 minimize해야할까? 간단하게 neural network의 output을 계산해서 expected value (target value)와 actual output value의 차이를 error로 정의하면 어떨까? 즉, neural network의 output이 우리가 원하는 output과 가장 근접한 weight를 learning하는 방법을 취하는 것이다. 근데 여기에서 문제가 하나 생기게 되는데, 바로 이 error function이 weight에 대해서 convex하지가 않다는 것이다. 당연히 관련되는 weight set도 엄청나게 많고, network자체가 convex한 form이 아니기 때문이다. 즉, 실제 좋은 performance를 내는 weight들의 여러 set을 취해 그 중간 값을 취해 얻은 weight가 좋은 weight가 아닐 수도 있다는 것이다. perceptron algorithm의 가장 attractive한 점은 algorithm이 convex하기 때문에 언제나 같은 값으로 converge한다는 점인데, 이런 방식으로는 그런 convergence가 보장이 안되는 것이다.</p>


<p>일단, 어쨌거나 perceptron이 적용이 안되는 상황이니깐, 위에서 정의한 error minimization의 측면에서 문제를 접근해보도록하자. 가장 간단한 예시인 Linear neuron에 대해서 살펴보자. 일단 y를 neuron을 통해서 얻어진 estimated output, w를 weight vector, x를 input vector라고 하면 다음과 같은 식을 세울 수 있다.</p>


<p>$$ y = \sum_i w_i x_i = \mathbf w^\top \mathbf x $$</p>


<p></p>

<p>이 상황에서 input vector x의 real output을 t (target output)이라고 해보자. 이런 상황에서 가장 간단한 error는 actual output과 desired output의 squared difference이다. 즉, 이를 수식으로 나타내면 \(error = \sqrt{t-y}\)로 표현할 수 있을 것이다. 우리의 목표는 이런 상황에서 weight를 iterative method를 사용하여 구하고 싶은 것이다. Iterative method라는 것은 어떤 특정한 반복적인 알고리듬을 사용하여 (예를 들어 gradient descent나 perceptron처럼) 계속 값을 update시켜나가면서 가장 적절한 것으로 보이는 값을 찾아내는 방법이다. 즉, \(w_{t+1} = f(w_t) \)로 표현이 가능하다. \(w_t\)는 t번 째 loop에서 w의 값이고, f는 w를 update하는 rule이다. 그렇다면 여기에서 잠시 궁금한 점이 생길 수 있다. 만약 우리가 target vector를 알고 있다면, 왜 문제를 analytically하게 해결하지 않을까? 즉, 우리가 이미 x와 y를 안다면 이를 가장 최적화시키는 w를 계산으로 단 한 번에 구할 수 있을 것인데, 왜 하필 iterative method를 사용하여 계속 값을 update하는 것일까? 훨씬 비효율적이지 않을까? 이 질문에 대한 알고리듬 관점에서 바라봤을 때의 답을 간략하게 말해주자면, 그런 형태의 analytic solution은 반드시 문제가 linear해야하고 또 squared error measure에 대해서만 working하기 때문인 것이 하나, 그리고 Iterative method가 조금 비효율적으로 보일지는 몰라도 더 복잡한 네트워크에 대해서 generalize하기가 더 간단힌 이유 하나를 들 수 있을 것이다.</p>


<p>이런 iterative method는 맨 처음 모든 weight를 random하게 guess하고 <a class="red tip" title="조건은 바뀔 수 있다. 예를 들어 input 3개를 보고 update하는 것도 가능하다. 뒤에서 조금 더 자세히 다루도록 하겠다.">매 input마다</a> 적절하게 weight를 update시킨다. 이 방법은 weight가 어떤 특정한 value로 converge할 때까지 계속된다. 그렇다면 이런 방법의 예를 하나 들어보자. 이 강의에서는 다음과 같은 function을 정의한다. \(price = x_{fish} w_{fish} + x_{chip} w_{chip} + x_{ketchup} w_{ketchup}\) 즉, 내가 식당에서 <a class="red tip" title="fish and chips라고 하는 요리.. 생선튀김이랑 감자튀김 같이 먹는거랑 똑같다">생선과 칩과 케첩</a>을 먹었을 때 내가 지불해야하는 금액을 내가 먹은 양 (x), 그리고 각 item들의 가격 (w)으로 나타낸 것이다. 내가 알고 있는 값은 input x (내가 시킨 양) 그리고 계산서를 통해 얻은 값이다. 하지만 나는 w를 모르며, 이 w를 찾는 것이 목적이다. 그렇다면 처음에는 random하게 w를 guess할 수 있을 것이다. 이때, (120, 50, 100)이 true weight라고 해보자. 즉, 현재 input이 2,5,3일 때 price는 850일 것이다. 현재 우리는 weight에 대한 정보가 없으므로 모두 50이라고 가정하면 내가 estimate한 price는 500이고, error의 값은 350이 된다. 이때, \(\triangle w_i = \epsilon x_i (t-y)\)라는 learning rule이 있다고 해보자. (이 learning rule은 delta-rule이라는 규칙으로, 바로 다음 단락에서 자세히 다루도록 하겠다.) 이 수식을 적용하면 다음 weight는 70, 100, 80이 되고 error는 30으로 줄어들게 된다 (esitimated price = 880, true = 850) 이런 식으로 각 iteration마다 error의 값을 줄여나가면서 true weight를 찾는 것이 iterative method의 작동원리인 것이다.</p>


<p>그렇다면 이런 방법에서 가장 중요한 개념은 아마 learning rule일 것이다. 이 렉쳐에서는 'Delta Rule'이라는 rule을 소개하고 있다. 이 방법은 일종의 Gradient Descent method인데, single layer neural network에서 주로 사용하는 방법이라고 한다. 자세한 설명은 <a href="http://en.wikipedia.org/wiki/Delta_rule" target="new">wiki</a>를 참고. 그렇다면 왜 delta-rule은 \(\triangle w_i = \epsilon x_i (t-y)\) 의 꼴을 띄고 있는 것일까? 증명은 간단하다. error를 squared residuals summation error로 정의하고 차근차근 수식을 전개하면 해당 꼴을 얻을 수 있다. wiki에도 언급이 되어 있으므로 설명이 미진하다면 wiki를 참고하면 될 것 같다. 먼저 \(E = \sum_j \frac 1 2 (t_j - y_j)^2\)이라하자. (notation은 wiki의 notation을 사용하겠다.) 이 error는 convex function이고 domain도 convex하므로 gradient descent method를 사용하면 error의 global minimum값을 반드시 찾을 수 있다. 따라서 만약 우리가 "weight space"에 대해서 이 error를 최소화하게 된다면 매 순간 minimize하기 위해 내려가는 방향 즉, 이 함수의 gradient 값은 \(\frac {\partial E} {\partial w_{ji}}\)이 될 것이다. 이때, 이 gradient descent는 error를 줄이기 위해서 필요한 weight들의 change이고, 방향은 반대이므로 \(\triangle w_{ij} = - \epsilon \frac \partial E \partial w_{ji}\)라고 할 수 있는 것이다. 그리고 뒤의 미분항을 간단하게 chain rule을 사용하여 정리하면 이전의 식은 결국 다음과 같은 수식으로 표현이 가능하다.</p>


<p>$$ \triangle w_{ij} = \epsilon (t_j - y_j)x_i $$</p>


<p>wiki에서는 active function의 미분항까지 들어가게 되는데, 이 경우는 일단 생략하였다.</p>


<p>이제 update rule을 만들었으니 필연적으로 생기는 question들을 점검해보자. (1) 이 알고리듬은 반드시 global한 값으로 converge하는가? - convex optimization이기 때문에 global truth로 converge하긴한다. 적절한 step size가 필요한데 이것은 이론적으로 구할 수 있으므로 큰 상관이 없다. (2) converge rate는 얼마나 될 것인가? - gradient descent method들이 대부분 그러하듯 많이 느릴 것이다. 이를 개선하기 위해 steepest descent method를 적용하는 등의 방법이 있는 것으로 보인다. 마지막으로 perceptron과 비교해보자. perceptron은 'error가 발생해야만' update가 일어났으며, error는 binary error였기 때문에 update가 일어나지 않을 수도 있었다. 하지만 지금은 error가 real function이므로 error는 거의 항상 non zero value가 되고 update도 지속적으로 일어난다. 또한 perceptron이 아무런 parameter tuning이 없던 것과 비교해 (margin은 일단 예외로 하자) learning rate를 골라야하는 귀찮은 문제가 하나 생기게 되었다.</p>


<h5>The error surface for a linear neuron</h5>


<p>이 소강의는 거의 언급할 내용이 없다. 앞에서 이미 이 문제가 convex임을 밝혔으며, 또한 weight space라는 concept역시 이미 언급했다. 언급되고 있는 문제는 거의 gradient descent method의 문제점들이다. 특히 convergence rate가 느린 경우, zig-zag하게 수렴하는 경우는 어떻게 해야할 것인가? 등에 대한 question만 던지는 강의이기 때문에 과감하게 생략하도록 하겠다.</p>


<h5>Learning the weights of a logistic output neuron</h5>


<p>delta rule을 logistic neuron에 대해 적용하는 것인데, 결론만 얘기하면</p>


<p>$$ \triangle w_{ij} = \epsilon (t_j - y_j) y_i (1-y_i) x_i $$</p>


<p>의 꼴이 된다. 즉, 앞에서 언급했던 activate function의 미분값인 \(y_i (1-y_i)\)가 포함되는 형태라는 것만 알아두면 된다. 다만, 이 경우에 binary threshold neuron이 아니라 logistic neuron을 쓰는 이유는 binary threshold neuron은 error가 항상 0아니면 1이기 때문에 gradient descent method를 사용할 수 없기 때문이다. 이제 간단한 배경지식을 갖추었으니 이번 렉쳐의 메인인 backpropagation으로 넘어가보자.</p>


<h5>The backpropagation algorithm</h5>


<p>자, 사실 앞에서 이런저런 얘기를 주절주절 했던 이유는 바로 backpropagation algorithm에 대해 설명하기 위해서였다. 이 algorithm은 당연히 iterative method이며, logistic neuron에 대해서 delta-rule (gradient descent method)를 적용하여 최적의 weight를 계산해낸다. 이 알고리듬은 hidden layer가 존재하는 neural network를 learning하기 위해 사용이 되는 알고리듬이며, <a class="red tip" title="backpropagation은 역전파, 즉 반대 쪽으로 영향을 미친다는 뜻이다, 이 경우는 결과를 통해 weight를 학습하기 때문에 역전파라고 부른다">이름에서 알 수 있듯</a> network의 output value에서부터 역으로 weight를 learning하게 된다. 왜 우리는 hidden layer가 존재하는 neural network를 learning해야할까? 이런 방법을 쓰지 않으면 network가 항상 linear하기 때문에 real problem을 풀 수가 없기 때문이다. 그리고 또한, hidden layer를 사용한다는 의미는 우리가 임의의 feature를 정하고, 각 feature들의 weight가 얼마나 되는지 학습을 한다는 의미와 같다. 무슨 얘기이냐하면, 만약 엄청나게 dimension이 큰 input이 있을 때 (예 - 해상도 높은 사진) 실제 algorithm을 돌릴 때 모든 input을 사용해 learning하는 것은 거의 의미가 없고 (특히 high dimension, samll input인 경우는 overfitting issue가 크게 작용한다.) 해당 알고리듬에 대입해서 실행시킬 feature를 뽑아내는 과정을 필요로 하는 경우가 많다. 그런데 대부분의 경우 우리는 이런 feature를 heuristic하게 찾는다. 즉, 사진에서 눈, 코, 입을 feature로 삼아야한다고 우리의 heuristic으로 결정하고, masking을 손으로 하고 그 결과를 알고리듬에 대입하는 것이다. 그런데 hidden layer를 사용하게 되면 그런 불필요한 행동을 줄일 수 있다. 만약 hidden unit각각이 머리카락, 눈, 입술, 코, 귀 등등을 의미하고 있다면 적절한 weight를 learning함으로써 feature에 대한 weight를 결정할 수 있고, 우리가 일일이 손으로 하던 것들을 자동화시킬 수 있는 것이다. 이렇기 때문에 hidden layer가 포함된 neural network가 powerful하고 meaningful하다. 그리고 backpropagation을 사용하는 이유는 그것이 가장 효율적이고 빠른 학습 방법 중 하나이기 때문이다.</p>


<p>Backpropagation이 아닌 다른 예를 하나 생각해보자. 예를 들어서 output을 사용하지 않고 initial weight를 주고 weight를 조금씩 변화시키면서 적절한 값을 찾을 수도 있을 것이다. (Learning using perturbations) 즉, 원하는 target value를 고정해두고 해당 value에 가장 가깝도록 weight를 하나하나 강제로 조정하면서 전체 weight를 찾아가는 다소 reinforcement learning과 비슷한 방법으로 접근하는 것이 가능할 수도 있다. 그러나 이런 방법은 큰 문제가 있다. 먼저 weight가 많아질수록 찾아야하는 값이 많아지고 computation time이 엄청나게 빠르게 증가할 것이다. 또한 이런 방법은 weight에 대해 network가 convex하다면 의미가 있을 수 있지만 당연히 hidden layer가 포함된 network는 convex하지 않다. 결국 이 방법은 우리가 상상도 하지 못할 만큼 많은 양의 computation time을 필요로 하는 좋지 못한 방법인 것이다. 심지어 아주 적은 수의 neuron만 있더라도 바로 뒤에서 설명하게 될 backpropagation이 더 성능이 우수하기 때문에 이런 방법 자체를 사용하지 않는 것이다.</p>


<p>그렇다면 이제 backpropagation algorithm에 대해 discribe해보자. backpropagation의 기본 아이디어는 우리가 hidden unit들 그 자체에 대해서 알 필요가 하나도 없고 (알 수도 없을 뿐더러), 대신 hidden unit들로 인해 생성되는 error change를 관측하는 것이 더 낫다는 것이다. 즉, hidden unit 그 자체의 activity를 learning하는 것이 아니라, hidden unit들로 인해서 생겨나는 error derivatives를 사용하자는 것이다. 이 방법은 ouput layer에서 아래 layer로 정보를 backpropagation하여 (역으로 보내어) lower layer에서 그 값을 기준으로 다시 weight를 update시킨다. input pattern은 hidden layer에 전달이 되고, 다시 hidden layer가 output layer로 전달을 시키므로 (hidden layer가 하나일 때) 이런 방법으로 현재 weight에 대한 expected value와 estimated value 사이의 error를 구할 수 있고 이것을 최소화 하는 방향으로 weight를 learning하는 것이다. weight를 learning할 때는 앞에서 우리가 이미 살펴보았던 delta-rule을 사용하여 output layer에서의 각 neuron들의 error를 사용해 weight들을 update한다.</p>


<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network" target="new">링크</a>)</p>


<p><img src="/images/post/42-1.png" width="600"></p>

<p>Backpropagation은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>


<h6>Phase 1: Propagation</h6>


<ol>
    <li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 'forward' propagation이라 한다.)</li>
    <li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 'back' propagation이라 한다.)</li>
</ol>


<h6>Phase 2: Weight update</h6>


<ol>
    <li>Delta rule을 사용해 weight를 update한다. update rule은 다음과 같다. (delta rule for logistic neuron)<br>
        \( \triangle w_{ij} = \epsilon (t_j - y_j) y_i (1-y_i) x_i \)</li>
</ol>


<p>위의 과정은 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -> hidden k, hidden k -> hidden k-1, ... hidden 2 -> hidden 1, hidden 1 -> input의 과정을 거치면서 계속 weight가 update되는 것이다. 그리고 이 cycle자체가 converge했다고 판단될 때 까지 계속 반복된다.</p>


<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 '역으로' 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>


<h5>How to use the derivatives computed by the backpropagation algorithm</h5>


<p>Overfitting과 Optimization issue가 나오는데, Online, batch update 중 무엇을 고르느냐, 어떻게 overfitting을 줄이냐 등등, 이미 예전에 많이 다뤘거나 앞으로 다시 다뤄질 주제들이라 판단되어 생략하도록 하겠다.</p>


<p>다만, backpropagation에 대해 중요한 언급이 빠져있어서 첨언을 하자면, backpropagation 은 항상 global optimum으로 converge하지 않기 때문에 언제나 local minimum으로 converge할 가능성이 존재한다. 이는 특히 hidden layer가 많아지면, 혹은 네트워크가 deep해지면 deep해질 수록 더 심해진다. 따라서 initial value를 어떻게 설정하느냐가 매우 민감하다. initial value에 따라 수렴하는 방향이 달라질 수 있기 때문인데, 나중에 배울 Deep belif network에서는 initial value를 미리 pre-training하는 방법으로 이를 극복해낸다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coursera Neural Networks for Machine Learning Week2 - Perceptron]]></title>
    <link href="http://SanghyukChun.github.io/40/"/>
    <updated>2014-03-21T07:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/40</id>
    <content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 글은 Geoffrey Hinton 교수가 2012년 Coursera에서 강의 한 <a href="https://class.coursera.org/neuralnets-2012-001/lecture" target="new">Neural Networks for Machine Learning</a> 2주차 강의를 요약한 글이다. 첫 주 강의에서 Neural Network란 무엇이며 어떤 종류의 Neural Network들이 있는지 등에 대해 간략하게 다뤘다면, 이 강의에서는 가장 오래된 Neural Network 중 하나인 Perceptron을 설명하는 내용이 주가 된다.</p>


<h5>An overview of the main types of neural network architecture</h5>


<p><a href="http://SanghyukChun.github.io/39" target="new">이전 글</a>에서 Neuron들에는 어떤 종류가 있을 수 있는가 다뤘었다. 대충 linear neuron, linear threshold neuron, binary neuron, binary threshold neuron, sigmod neuron 등이 있었다. 그렇다면 neuron들로 구성된 neural network에는 어떤 type들로 구분되는가도 간략하게 알아보도록 해보자.</p>


<p>일단 가장 간단한 형태의 network로 Feed-forward neural network가 존재한다. 가장 일반적으로 쓰이고 실제 어플리케이션에 적용되는 neural network들도 대부분이 feed-forward라고 한다. 이 네트워크는 상당히 간단한 구조인데, 첫 번째 layer는 input이며 가장 마지막 layer는 output이다. 그리고 중간의 input과 output으로 관찰되지 않는 영역을 "hidden" layer라고 하는데, 당연히 visuable하지 않으므로 (우리가 직접 관측하는 영역이 아니므로) hidden이라고 불리는 것이다. 만약 hidden layer가 하나보다 많이 존재한다면 이 network는 "deep" neural network라고 불린다.</p>


<p><img src="/images/post/40-1.png" width="350"></p>

<p>위의 그림이 Feed-forward neural network의 간단한 예시이다. (이 그림은 Hinton 교수의 slide에서 가져왔다.)</p>


<p>이보다 조금 더 복잡한 network로는 Recurrent network라는 것이 존재한다. "Recurrent"라는 이름이 붙은 이유는 graph에 cycle이 존재하기 때문인데, 이 말인 즉슨, 이 network에서는 arrow를 계속 따라가다보면 어느 순간 같은 장소를 계속 돌고 있을 수도 있다는 의미이다. 당연히 일반적인 방법으로 이것을 학습하는 것은 매우 복잡한 일이고 어려운 일이다. 그럼에도 일단 이 네트워크는 가장 "biologically" 현실적인 네트워크라고 한다.</p>


<p><img src="/images/post/40-2.png" width="200"></p>

<p>위와 같이 directed cycle이 존재하는 경우 recurrent network라고 하는데, 이 방법을 사용해서 sequential data를 modeling할 수 있다고 한다. 그런 행위가 가능한 근본적인 이유는 이 방법 자체가 일종의 시간 축으로 very deep한 network로 치환이 가능하기 때문이다. 그림으로 보면 아래와 같은 형태가 된다.</p>


<p><img src="/images/post/40-3.png" width="250"></p>

<p>자 다시 위의 그림을 보면서 차근차근 설명하자면, 위의 그림은 매 시간마다 하나의 hidden layer를 가지는 네트워크이며, 각 hidden layer는 그 다음 hidden layer에 무언가 information을 주는 형태이다. 즉, 자기 자신이 자기 자신에게 정보를 주는 cycle이 존재하는 형태이며, 매 시간마다 input과 output이 존재한다고 생각할 수 있다. 이런 이유로 recurrent network를 이런 형태의 network로 치환하여 생각할 수 있는 것이다. 당연히 실제로 학습하기는 무지하게 어렵지만, 실제 이런 network가 계속 연구가 되고 있으며 2011년 Ilya Sutskever의 연구에서 이런 형태의 network를 사용해 wikipedia의 단어들을 학습해 자동으로 sentence를 generate하는 모듈을 만들어서 실행시킨 결과, 다음과 같은 문장을 얻었다고 한다.</p>


<p>In 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.</p>


<p>물론, 완전한 형태의 영어는 아니지만, 매 순간 단 하나의 단어만을 generate한 결과임에도 불구하고 엄청나게 뛰어난 성능을 보이고 있음을 알 수 있다. 일반적으로 이런 sentance generate을 위한 모델은 무지무지 복잡하고 여러 단어를 동시에 학습하거나 생성하거나 하는 등의 과정을 거치는데 이 논문에서는 오직 단어를 하나씩만 생성했음에도 꽤 그럴듯한 영어가 나왔다는 점이 고무적이라는 것이다.</p>


<p>마지막으로 Symmetrically connected network가 있다. 이 network는 recurrent network의 special한 case라고 보아도 무방한데, 간단히 말하자면 이전의 neural network들은 모두 directed graph였지만, 이 symmetrically connected network는 undirected graph이다. 즉, 각 layer간에 symmetric한 edge, 다시 말하자면 양 방향으로 서로 같은 weight를 가지게 된다는 의미이다. 이런 network는 energy function이라는 것을 도입하면 recurrent network보다 훨씬 분석하기가 용이하며, performance도 powerful하다. 만약 hidden unit이 없다면 Hopfield network라고 부르며, hidden layer가 존재하면 Boltzmann machine 이라 부르는데, 이 녀석은 나중에 언젠가 다루게 될 Deep network에서 이 Boltzmann machine을 restrict시킨 형태인 Restricted Boltzmann Machine (RBM)을 설명할 때 다시 한 번 자세하게 다룰 예정이다. (Coursera lecture로 따지면 거의 맨 끝 즈음이다.)</p>


<h5>Perceptrons: The first generation of neural networks</h5>


<p>자, 어쨌거나 2주차 강의의 핵심은 바로 perceptron이다. 이 녀석은 가장 오래된 neural network 중 하나이며, 특정 상황에서는 정말 outperform한 결과를 보여주지만 그 한계가 분명한 알고리듬이다. 1690년대 Frank Rosenblatt에 의해 제안된 알고리듬으로 Artificial neural network을 태동하게 한 알고리듬이지만, 그 한계가 너무나 명백하여 한 동안 neural network 연구 자체가 이뤄지지 않게 한 원인이 되기도 한다. 1969년 Minsky가 perceptron이 linear가 아니면 아무것도 할 수 없다는 것을 증명했는데 (단적인 예로, xor조차 학습하지 못한다) 당시 multi layer perceptron에도 이 방식이 적용될 것이라 다소 과도한 추측을 하는 바람에 neural network 연구 자체가 한 동안 메일 스트림이 아니었다. 아무튼, perceptron은 엄청 간단한 feed-forward network의 일종이다. 무지무지 간단하게 그림 하나로 표현하면 아래와 같다 (그림은 google image에서 찾은 그림..)</p>


<p><img src="/images/post/40-4.png" width="600"></p>

<p>하나하나 간단하게 설명해보자. 일단 input layer가 있다. 맨 아래 \(x_o\)는 \(x_n\)의 오타로 추정된다. 맨 위의 1은 bias를 위한 term이다. 이전 글에서 bias에 대해 설명한 것을 기억하는지? input과 weight를 linear combination 형태로 정리하고 나서 거기에 상수 항으로 더해지는 값이 bias이다. 즉, input과 상관없이 늘 더해지는 값으로, \(b = 1 \times w_o\) 라고 봐도 무방한 것이다. 아무튼, 지금은 간단하게 input layer에서 원래 input vector x와 bias term 1을 weight vector와 곱한 형태인 \(z = \sum_i w_i x_i\)를 계산했다고 간단하게 생각해보자. perceptron의 decision rule은 간단한데, 방금 계산한 값이 어떤 threshold를 넘으면 값을 activate, 넘지 못하면 값을 deactive 시키는 것이다. 간단하게 얘기하면 perceptron에서는 binary threshold neuron을 사용하는 것이다. 이 threshold를 결정하는 것은? 바로 bias가 그 역할을 하게 된다. 그러므로 이 알고리듬에서 "learning"하는 것은 weight와 bias가 될 것이다. 음.. 뭔가 간단하게 bias는 무시하고 weight만 학습하는 방법은 없을까? 앞에서 bias를 weight로 간단하게 치환한 방법을 사용하면 이렇게 문제를 간단하게 만드는 것이 가능해진다. 원래 input vector에 value 1을 추가하여 마치 input vector가 하나 더 있고, 그 component에 대한 weight가 존재하는 것처럼 trick을 쓰는 것이 가능해진다. 따라서 bias도 weight와 같은 방법으로 자연스럽게 learning할 수 있게 되고, 더 이상 threshold에 대해 고민할 필요가 없어진다!</p>


<p>perceptron이 weight를 학습하는 방법도 매우 간단하다. input vector가 들어왔을 때, 현재 weight로 맞는 값이 나온다면 weight는 update되지 않는다. (\(w_{t+1} = w_t\)) 만약 1이 나와야하는데 0이 나온다면 weight vector에 input vector를 더해준다. (\(w_{t+1} = w_t + v\)) 만약 0이 나와야하는데 1이 나온다면 weight vector에서 input vector를 더해주는 방식으로 weight를 update한다. (\(w_{t+1} = w_t - v\))</p>


<p>조금 더 잘 describe해보자면, input x에 대해서 output(label) y는 다음과 같은 수식으로 표현된다 -아래 수식에서는 편의를 위해 y = {-1,1} 이라고 하자-</p>


<p>$$ y = sign( \sum_{i=0}^n w_i x_i ) \hskip 1em where, x_0 = 1 \hskip 0.3em and \hskip 0.3em w_0 = -b$$</p>


<p>즉, label y는 vector w와 x의 inner product로 나타낼 수 있으며 이 때 bias b는 \(x_0 = 1\), \(w_0 = -b\)라는 형태로 간단한 weight vector와 input vector의 linear combination으로 표현할 수 있게 되는 것이다. 이 때 update rule은 다음과 같다</p>


<p>$$ w_{t+1} = w_t + y_n x_n, \hskip 1em when \hskip 0.3em misclassified $$</p>


<p>misclassified가 발생했을 때만 update가 일어나며, update rule은 원래 y와 x를 곱해서 원래 vector에 더해주는 것이다. 즉, 1이 나와야하는데 -1이 나왔다면 w에 +x를 취해주고, -1이 나와야하는데 1이 나왔다면 w에 -x를 취해주는 것이다. 그리고 step을 진행시키면서 (t가 점점 증가하면서) misclassified point가 발견될 때 마다 이 알고리듬을 반복한다. 이렇게 설명하면 조금 더 깔끔하게 수식적으로 설명이 가능해진다.</p>


<h5>A geometrical view of perceptrons</h5>


<p>위와 같은 update rule이 선택되는 이유는 무엇인가? 왜 하필이면 input vector를 합해야할까? 이런 질문들은 모두 geometric하게 해석할 수 있다. feature가 n개일 때, input vector와 weight vector는 some n-dimensional vectors이므로, 이 vector들이 존재하는 vector space를 정의하는 것이 가능해지기 때문이다. 여기에서는 weight space라는 새로운 형태의 space를 정의해서 perceptron을 해석할 것이다. 따라서 원래 수식과 대조하여 생각해보면 우리가 궁극적으로 찾고자하는 truth weight vector는 올바른 answer에 대한 어떤 hyperplane일 것이라는 것도 충분히 추측할 수 있다. 무슨 소리냐하면, input vector와 weight vector의 inner product의 sign이 y를 결정한다는 의미는, 곧 그 내각이 90도보다 크냐 작으냐로 생각할 수 있고 (물론 n-dimensional vector에서는 각도 개념이 정의하기 나름이지만) 아마도 대부분의 input vector들에 대해서 올바른 label을 가지게 하는 어떤 hyperplane이 우리가 찾고자하는 궁극적인 weight vector들이라는 것이다. 그림으로 설명해보자.</p>


<p><img src="/images/post/40-5.png" width="600"></p>

<p>위의 그림에서 correct answer가 1이라면 input vector와 weight vector의 inner product를 구했을 때 올바른 값이 나오기 위해서는 당연히 초록색 vector이어야한다는 사실을 알 수 있을 것이다. 이유는 위에서 언급했듯 사이각이 90도 보다 작은 두 벡터의 inner product는 언제나 0보다 크기 때문이다. 따라서 input vector에 orthogonal한 plane을 그리고, 그 plane을 기준으로 weight vector가 올바른 곳에 존재하는지 그렇지 않은지 간단하게 알 수 있을 것이다. 다음에는 correct answer가 0인 경우 (-1인 경우)를 살펴보자. 이 경우에는 두 벡터의 사이 각이 90도보다 커야하므로, input vector에 orthogonal한 plane의 반대 부분이 올바른 weight vector의 위치가 됨을 알 수 있다. 그렇다면 올바르지 않은 (misclassified된) weight vector를 올바른 영역으로 옮기기 위해서 어떤 행동을 취할 수 있을까? 조금만 생각해보면 정말 간단한 vector sum으로 hyperplane의 반대쪽으로 보낼 수 있다는 것을 알 수 있다. 왼쪽 상황에서는 빨간 벡터를 초록 벡터로 만들기 위해서 간단하게 빨간 벡터에 파란 벡터를 대해주면 되고 (\(w_{t+1} = w_t + v\)) 오른쪽 경우는 빼주면 된다 (\(w_{t+1} = w_t - v\)). 이런 이유로 벡터를 더하고 빼는 것 만으로 weight가 '개선'되었다고 할 수 있는 것이다. 만약 weight들이 올바르게 learning되었다면 우리는 아래와 같은 결과를 얻게 될 것이다.</p>


<p><img src="/images/post/40-6.png" width="300"></p>

<p>즉, 올바른 weight는 서로 다른 input vector들이 모두 well-classified되게하는 어떤 vector임을 알 수 있다. Training을 하면서, 우리가 찾아내는 값은 바로 가장 올바른 weight vector를 찾는 것이며, 위의 그림에서 볼 수 있듯 우리는 space 위에 여러 hyperplane을 그릴 수 있고, 이를 이용하여 good weight들이 위치하는 hypercone을 그릴 수 있다. 재미있는 점은, 이 cone위의 vector는 convex하다는 것이다 (그 어떤 벡터 두 개를 골라도 그 중간에 존재하는 모든 벡터들이 cone안에 존재한다) 즉, 우리가 만약 이 문제를 convex하게 해결한다면 항상 우리는 global optimum값을 찾을 수 있게 되는 것이다.</p>


<h5>Why the learning works</h5>


<p>위에서 geometric view로 perceptron을 서술하였으니, 이번에는 도대체 왜 이런 알고리듬이 작동하는지 알아보도록 해보자. 사실 엄밀한 수학적 증명이 강의에 나오지 않기 때문에 복잡한 증명은 생략하고, 간단하게 그림으로 설명해보도록 하겠다. 일단 아래 그림을 보면서 진행해보도록하자.</p>


<p><img src="/images/post/40-7.png" width="500"></p>

<p>아래 그림의 상황은 current weight vector와 any feasible한 weight vector 사이의 거리 \(d_a^2+d_b^2\)을 고려해보도록 하자. 만약 이런 상황에서 perceptron이 misclassified된다면, learning 알고리듬이 current vector를 조금 더 feasible한 weight vecotr에 가까워지도록 움직여줄 것이다. 하지만 문제가 생기는데, 거의 plane에 근접하게 있는 point를 생각해보자. 이 그림에서는 노란색 점이 그것이다. 이 점은 분명 조금 더 "feasible vector"에 가깝게 움직여질 필요성이 있지만, 노란색 점은 이미 feasible region 위에 위치하기 때문에 아무리 알고리듬이 running하더라도 절대로 feasible point 근처로 옮겨지지 않는 것이다. 이런 문제점을 해결하기 위해서 'margin'이라는 컨셉이 도입된다.</p>


<p><img src="/images/post/40-8.png" width="300"></p>

<p>위의 그림에는 margin이라는 것이 표현되어 있는데, 이 margin은 feasible한 weight vector를 조금 더 strict하게 정해주는 역할이다. 즉, feasible region을 plane에서 margin 보다 더 멀리 떨어진 위치로 정의하고, 이 region안에 존재하는 vector를  "generously feasible"한 weight vecotr로 정의하는 것이다. 즉, 이제는 노란색 vector가 margin보다 더 조금 떨어져 있기 때문에 더 이상 "feasible"한 vector가 아니므로 perceptron algorithm을 사용하여 이 벡터를 옮기는 것이 가능해지는 것이다.</p>


<p>이런 가정하에, 이 알고리듬이 converge한다는 것이 증명가능하다고 하는데, 구체적인 증명과정은 강의에 설명되어있지는 않고, 간단한 아이디어만 서술되어있다. 그 아이디어는 크게 세 개인데, perceptron이 feasible region에 존재하지 않는 weight vector를 update하고, update마다 missclassified vector와 feasible vector사이의 distnace가 감소되는 방향으로 update가 될 것이다. 또한 이 거리는 매 번 최소한 input vector의 lenght의 제곱근만큼은 감소한다는 것이다. 따라서 유한한 숫자의 iteration안에 weight vector가 반드시 feasible region안에 위치하게 된다는 것이다. 물론 이 모든 것은 그러한 feasible region이 존재하는 경우에만 동작하는 것은 당연할 것이다.</p>


<h5 id="what-perceptrons-cant-do">What perceptrons can't do</h5>


<p>하지만 perceptron은 너무나도 명확한 한계점이 존재한다. input vector가 binary이기 때문에 모든 input을 binary feature로 바꾸어야한다는 점도 문제이지만, 가장 큰 문제는 linearly separable하지 않은 dataset들은 learning할 수가 없다는 것이다. 엄청나게 간단한 예를 살펴보도록하자. xor은 binary 연산의 가장 기본적인 연산 중 하나이다. 두 값이 같으면 0, 다르면 1을 return하는 것인데, 이를 2차원 평면에 포함하면 아래와 같은 상황이 되어버린다.</p>


<p><img src="/images/post/40-9.png" width="300"></p>

<p>초록색 label이 된 점들이 output이 0인 점들, 빨간색 점들은 ouput이 1인 점들이다. 당연하게도, 이 점들을 구분할 수 있는 '단 하나의' plane은 존재하지 않는다. 단순히 이 결과만 보더라도 perceptron이 얼마나 제한적인 상황에 대해서만 동작하는지 분명하게 알 수 있다. 또한 perceptron의 decision making은 summation으로 이루어지기 때문에, 만약 n 차원 벡터의 패턴이 아래와 같으면 구분이 불가능한 것이다</p>


<p><img src="/images/post/40-10.png" width="300"></p>

<p>pattern A는 점들의 set이 1, 1, 2로 존재해야하고, pattern B는 2, 2로 존재해야하는데 둘 다 합이 4이기 때문에 perceptron으로는 이를 구분하는 것이 불가능하다.</p>


<p>이렇듯 perceptron은 그 한계가 너무나 명확하다. 그러나 이는 single layer perceptron에 한정된 문제이지 neural network 전체의 문제는 아니다. 이를 해결하는 방법은 생각보다 간단한데, 바로 hidden unit을 learning하는 것이다. multiple hidden layer는 neural network가 더 이상 linear하지 않고 non-linear하게 해주는 역할을 하는데, non-linear해지기 때문에 learning하기가 힘들어지지만, 만약 learning이 가능하다면 그 만큼 powerful해지는 것이다. 그렇다면 이런 net을 learning하는 것은 가능할까? 결론부터 얘기하자면 엄청나게 어렵다. 때문에 이에 대한 연구가 활발히 이루어지고 있으며 꽤 성공적인 결과들이 존재한다. 또한 hidden layer의 weights를 learning하는 것은 feature를 learning하는 것과 같아지기 때문에 더 이상 feature에 대한 문제도 없어지고, 여러모로 hidden unit을 learning하면 그 한계를 깰 수 있는 network가 될 수 있는 것이다.</p>




<h5>Coursera Neural Networks for Machine Learning</h5>


<p>다른 요약글들 보기 (<a href="http://SanghyukChun.github.io/blog/categories/cousera-nn/">카테고리로 이동</a>)</p>


<ul>
    <li>Lecture 1: <a href="http://SanghyukChun.github.io/39">Introduction</a></li>
    <li>Lecture 2: <a href="http://SanghyukChun.github.io/40">The Perceptron learning procedure</a></li>
    <li>Lecture 3: <a href="http://SanghyukChun.github.io/42">The backpropagation learning proccedure</a></li>
    <li>Lecture 4: <a href="http://SanghyukChun.github.io/43">Learning feature vectors for words</a></li>
    <li>Lecture 5: <a href="http://SanghyukChun.github.io/43">Object recognition with neural nets</a></li>
    <li>Lecture 6: Optimization: How to make the learning go faster</li>
    <li>Lecture 7: Recurrent neural networks</li>
    <li>Lecture 8: More recurrent neural networks</li>
    <li>Lecture 9: Ways to make neural networks generalize better</li>
    <li>Lecture 10: Combining multiple neural networks to improve generalization</li>
    <li>Lecture 11: Hopfield nets and Boltzmann machines</li>
    <li>Lecture 12: Restricted Boltzmann machines (RBMs)</li>
    <li>Lecture 13: Stacking RBMs to make Deep Belief Nets</li>
    <li>Lecture 14: Deep neural nets with generative pre-training</li>
    <li>Lecture 15: Modeling hierarchical structure with neural nets</li>
    <li>Lecture 16: Recent applications of deep neural nets</li>
</ul>

]]></content>
  </entry>
  
</feed>
