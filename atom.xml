<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[README]]></title>
  <link href="http://SanghyukChun.github.io/atom.xml" rel="self"/>
  <link href="http://SanghyukChun.github.io/"/>
  <updated>2015-10-14T00:56:24+09:00</updated>
  <id>http://SanghyukChun.github.io/</id>
  <author>
    <name><![CDATA[Sanghyuk Chun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[고흐의 그림을 따라그리는 Neural Network, A Neural Algorithm of Artistic Style (2015)]]></title>
    <link href="http://SanghyukChun.github.io/92/"/>
    <updated>2015-10-14T00:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/92</id>
		<content type="html"><![CDATA[<p>얼마 전, <a href="http://arxiv.org/abs/1508.06576 ">&#8216;A Neural Algorithm of Artistic Style &#8217;</a> 이라는 이름의 충격적인 논문이 arXiv에 업로드되었다. 기술 전문 잡지나 신문이 아닌 스브스뉴스 같은 일반적인 기사를 보도하는 매체에서도 보도가 되었을 정도로 요즘 꽤 이슈가 되고 있는 논문이다.</p>


<ul>
<li><a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: &ldquo;이건 &lsquo;반 고흐&#8217;의 그림이 아닌 &#8216;컴퓨터&#8217;의 그림입니다.&rdquo;</a></li>
<li><a href="http://www.dailymail.co.uk/sciencetech/article-3214634/The-algorithm-learn-copy-artist-Neural-network-recreate-snaps-style-Van-Gogh-Picasso.html">DailyMail: &ldquo;The algorithm that can learn to copy ANY artist: Neural network can recreate your snaps in the style of Van Gogh or Picasso&rdquo;</a></li>
<li><a href="http://www.theguardian.com/technology/2015/sep/02/computer-algorithm-recreates-van-gogh-painting-picasso">The Guardian: &ldquo;Computer algorithm recreates Van Gogh painting in one hour&rdquo;</a></li>
</ul>


<p>자고로 백문이 불여일견이라, 이게 도대체 무슨 contribution이 있길래 사람들의 이목이 쏠리고 있는지 논문에 첨부되어있는 그림을 먼저 보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/92-1.jpg" width="600"></p>

<p>이 그림들은 모두 사람이 그린 것이 아니라 neural network를 사용하여 generate한 것이다. 이 논문은 제목 그대로, &#8216;artistic style&#8217;을 learning하는 neural network algorithm을 제안한다. 여기에서 artistic style이라는 것을 어떻게 정의하였는지는 나중에 조금 더 자세히 살펴보도록하자. 위 그림은 이 논문에서 제안한 알고리즘을 사용하여 report한 결과이다. 원본이 되는 A가 독일의 튀빙겐이라는 곳에서 찍은 &#8216;사진&#8217;이다. B부터 F는 유명한 거장들의 그림 &#8216;style&#8217;과 A의 &#8216;content&#8217;를 가지는 그림을 generate한 결과이다. 순서대로 B는 J.M.W. 터너의 &#60;미노타우르스 호의 난파&#62;, C는 그 유명한 빈센트 반 고흐의 &#60;별이 빛나는 밤&#62;을, D는 뭉크의 &#60;절규&#62;, E는 피카소의 &#60;앉아 있는 나체의 여성&#62;, F는 칸딘스키의 &#60;구성 VII&#62;이다. 놀랍게도 알고리즘을 통해 얻은 그림은 원본 사진의 content는 거의 그대로 보존하면서, 동시에 다른 그림의 style을 특징을 잘 살려서 가지고 있다.</p>


<p>이 짧은 논문이 사람들에게 얼마나 큰 충격을 주었는지는 길게 적지 않아도 알 수 있을 것이라고 생각한다. 논문이 나오고 얼마 지나지 않아 <a href="https://github.com/jcjohnson">jcjohson</a> 이라는 github user가 torch 기반으로 만든 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>이라는 프로젝트를 github에 공개하였다. 논문이 정말 좋은 결과를 낸 것인지 사람들이 이런 저런 사진과 그림들을 사용해 실험해본 결과, 논문에서 이야기하는 것 처럼 실제로 아무 사진을 적당히 골라서 적당한 그림을 넣어주면 사진의 내용은 보존한 채로 질감만 바꿔서 출력해주는 것을 알 수 있었다. 아래 그림은 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>을 사용해 금문교 사진과 여러 예술가들의 그림을 사용해 generate한 결과이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/92-3.png" width="600"></p>

<p>이 논문이 나온게 9월 말이었는데, 벌써 한국 개발팀에서 스마트폰 app까지 개발했을 정도로 관심이 뜨겁다. (<a href="http://news.sbs.co.kr/news/endPage.do?news_id=N1003203149">스브스뉴스: 이건 &#8216;반 고흐&#8217;의 그림이 아닌 &#8216;컴퓨터&#8217;의 그림입니다.</a>)</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/92-4.jpg" width="300"></p>

<p>꽤나 흥미로운 논문인 만큼, 어떤 아이디어를 사용했고, 어떤 방법론을 사용했는지까지 한 번 차근차근 살펴보도록 하자.</p>




<h5>Content &#38; Style Reconstruction using CNN</h5>




<p>Deep learning이 지금처럼 급부상하게 된 배경에는 (비전 분야를 중심으로 한) <a class="red tip" title="Convolutional Neural Network">CNN</a>의 엄청난 힘이 있었다. CNN이 비전에서 월등한 성능을 내는 이유를 여러가지로 설명할 수 있겠지만, 일반적으로는 CNN은 각각의 layer가 &#8216;feature&#8217;의 의미를 지니기 때문이라고 설명한다. 각각의 layer가 feature를 생성해내고, 이 feature들이 hierarchy하게 쌓이면서 더 높은 layer로 갈수록 더 좋은 feature를 만들어낸다는 것이다. CNN은 이 feature를 hard-coding하여 뽑아내는대신, 데이터에서부터 &#8216;가장 좋은&#8217; 최종 feature를 만들도록 학습시키기 때문에 아주 좋은 feature를 사용해 perceptron 등의 간단한 classifier로 높은 performance를 얻게 되는 것이다 (CNN에 익숙하지 않다면 <a href="http://SanghyukChun.github.io/75/#75-cnn">CNN에 대해 설명했었던 이전 글</a>을 참고하면 좋을 것 같다). 그렇기 때문에 각각의 convolution layer의 output은 흔히 feature map으로 표현이 된다.</p>


<p>주어진 이미지에서 feature를 뽑아내는 것은 CNN을 통하여 지금까지 항상 하던 일이었다. 그렇다면 반대로 할 수도 있지 않을까? 즉, CNN의 중간 feature map을 사용하여 원래 이미지를 복원하는 작업을 하는 것이다. 이렇게 feature map에서부터 이미지를 reconstruction 할 수만 있다면, deep CNN에서 layer를 지면서 어떤 재미있는 일들이 벌어지고 있는지 사람이 직접 눈으로 확인할 수 있을 것이다. 이런 visualization에 대한 motivation 때문에 그 동안 CNN의 convolution layer에서 원래 이미지를 reconstruction하는 작업들은 꾸준하게 제안되어 왔다. 그 중 가장 유명한 work으로 다음과 같은 work이 있다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1412.0035">Mahendran, Aravindh, and Andrea Vedaldi. &ldquo;Understanding deep image representations by inverting them.&rdquo; arXiv preprint arXiv:1412.0035 (2014).</a></li>
</ul>


<p>이 논문은 주어진 feature map에서 image를 복원하는 방법을 제안한다. 단순히 이미지를 복원하는 것이 아니라, 이미지를 특정 목적에 맞게 변형하는 work도 진행되어왔다. 대표적인 예가 아래 논문과 Google DeepMind의 <a href="http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html">Deep Dream</a>이다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1412.1897">Nguyen, Anh, Jason Yosinski, and Jeff Clune. &ldquo;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.&rdquo; arXiv preprint arXiv:1412.1897 (2014).</a></li>
</ul>


<p>이 논문은 deep CNN이 거의 100% 확률로 오답을 발생시키도록 이미지를 조작한 work이다. 예를 들어 주어진 이미지가 &#8216;새&#8217; 라는 label을 가지고 있다고 판별했을 때, &#8216;문어&#8217;라는 label을 100% 로 가지도록 이미지를 조작하는 것이다. 사람이 봤을 때는 여전히 &#8216;새&#8217; 사진이지만, CNN은 &#8216;문어&#8217;라고 판별해버리는 것이다.</p>


<p>이렇듯, 다양한 목적으로 CNN이 이미 주어져 있을 때, 특정 목적에 따라 이미지를 update하는 방법론들은 이미 예전부터 연구가 계속 진행되어왔다. 위에 링크한 3개의 work은 꽤 흥미로운 주제들이기 때문에 나중에 또 따로 포스팅할 수 있도록 하겠다. 이 논문의 contribution은 각 convolution layer에서부터 style과 content를 reconstruct하는 방법론을 제안했다는 것이다. 이 방법은 앞에서 언급한 <a href="http://arxiv.org/abs/1412.0035">Understanding deep image representations by inverting them</a> 논문 처럼 현재 feature map에서 원래 이미지를 최대한 복원하는 content reconstruction과, 아래 논문 등에서 제안되어왔던 (neural network 기반 work은 아니다) texture 분석과 생성 등을 복원하는 texture reconstruction을 결합한 것인데, 자세한건 뒤에서 더 다루도록하자.</p>


<ul>
<li><a href="http://www.cns.nyu.edu/heegerlab/content/publications/Heeger-siggraph95.pdf">Heeger, David J., and James R. Bergen. &ldquo;Pyramid-based texture analysis/synthesis.&rdquo; Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. ACM, 1995.</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=363108">Portilla, Javier, and Eero P. Simoncelli. &ldquo;A parametric texture model based on joint statistics of complex wavelet coefficients.&rdquo; International Journal of Computer Vision 40.1 (2000): 49-70.</a></li>
</ul>


<p>이 논문에서 제안하는 두 가지 reconstruction 방법을 CNN의 각 layer에 대해 적용해보면 다음과 같은 결과를 얻을 수 있다. 참고로 이 논문에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a>를 선택했다. 이 네트워크는 총 16개의 convolution layer와 3개의 fully connected layer로 이루어져있다. 이 네트워크에 대한 설명은 method 부분에서 더 자세히 다루도록 하겠다.</p>




<p><img id="92-reconst-img" src="http://SanghyukChun.github.io/images/post/92-5.png" width="600"></p>




<p>위 그림은 CNN 하나에서 서로 다른 두 가지 방법으로 각각 style과 content를 layer 별로 reconstruction한 결과이다. 하나의 같은 CNN에 대해 두 가지 다른 reconstruction을 진행한 것인데, 위쪽 그림은 고흐의 &#60;별이 빛나는 밤&#62;의 style을 layer 별로 reconstruction한 것이고, 아래 그림은 튀빙겐에서 찍은 사진의 content를 layer 별로 reconstruction한 것이다.</p>


<p>먼저 style reconstruction에서 알 수 있는 것은 layer가 얕을수록 원래 content 정보는 거의 무시하고 &#8216;texture&#8217;를 복원한다는 것이다. 반면 깊은 layer로 가게 될수록 점점 원래 content 정보가 포함이 되는 것을 볼 수 있다. 이런 현상이 발생하는 이유는 이 논문에서는 style을 같은 layer에 있는 feature map들 간의 correlation으로 정의하기 때문이다. 이를 구체적으로 어떻게 수학적으로 정의하였는지는 뒤에서 좀 더 자세하게 살펴보도록 하자. Style을 correlation으로 생각하기 때문에, 가장 style이 복원이 잘되는 얕은 layer에서는 원본 content가 거의 무시되고 correlation을 가장 좋게하는 style만 나오는 것이고, 깊은 layer로 갈수록 style이 제대로 복원이 되지 않을 것이므로 원본 content의 정보가 증가해 correlation이 작아지는 결과를 얻게 되는 것이다.</p>


<p>다음으로 content reconstruction을 보자. 이 그림을 통해 낮은 level의 layer는 거의 완벽하게 원본 이미지를 보존하고 있고 layer가 깊어질수록 원본 이미지의 정보는 조금씩 소실되지만, 가장 중요한 high-level content는 거의 유지가 되는 것을 볼 수 있다.</p>


<p>이 논문은 같은 CNN이라고 할지라도 content와 style에 대한 representation이 분리가 되어있다는 것을 중요하게 언급하고 있다. 그렇기 때문에 같은 network을 사용하여 서로 다른 이미지에서 서로 다른 content와 style을 reconstruction해서 그 둘을 섞는 것이 가능한 것이다. 이것이 중요한 이유는 실제로 reconstruction을 하는 과정은 임의의 image를 input으로 삼고, image를 parameter로 하여 목표하는 style과 content에 대한 loss를 minimize하는 optimization 과정이기 때문이다. 이 두 가지 다른 optimization process를 오직 하나의 network만 사용하여 진행할 수 있기 때문에 \(A\)(혹은 튀빙겐에서 찍은 사진)이라는 input의 content를 가지면서 \(B\)(혹은 고흐의 &#60;별이 빛나는 밤&#62;)이라는 input의 style을 가지도록하는 방향으로 input 이미지의 gradient를 구할 수 있는 것이다. 수식으로 나타내보자. input image를 \(x\)라고 해보자. 우리 목표는 \(x\)와 \(A\) 간의 content가 얼마나 다른지 표현하는 loss function \(\mathcal L_{content} (x,A)\)와 \(x\)와 \(B\) 간의 style이 얼마나 다른지 표현하는 loss function \(\mathcal L_{style (x,A)}\)를 minimize하는 \(x\)를 찾는 것이다. 따라서 우리가 풀고 싶은 optimization problem은 다음과 같다.</p>


<p>\[x = \arg\max_x \alpha\mathcal L_{content} (x,A) + \beta\mathcal L_{style} (x,B)\]</p>


<p>이런 식으로 식을 쓸 수 있을 것이다 (\(\alpha\)와 \(\beta\)는 적당한 상수라고 하자). 이런 optimization을 푸는 가장 간단한 방법으로 \(x\)에 대한 gradient를 구하고 gradient descent optimization을 하는 것인데, 두 loss를 같은 network에 대해 design할 수 있기 때문에 gradient가 간단해지는 것이다. 그러면 이제 구체적으로 어떻게 각각의 loss가 정의되었는지 살펴보자.</p>




<h5>Methods</h5>


<p>이 paper에서는 CNN 모델로 <a href="http://arxiv.org/abs/1409.1556">VGG 19</a> 네트워크를 사용한다. 이 네트워크는 옥스포드의 VGG(Visual Geometry Group)에서 만든 네트워크로, <a href="http://SanghyukChun.github.io/88">Batch Normalization</a>이 적용되기 이전 inception network (혹은 GoogleNet) 등에 비해 꽤 우수한 성능을 보이는 네트워크이다. 아래 논문을 통해 발표하였다.</p>


<ul>
<li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. &ldquo;Very deep convolutional networks for large-scale image recognition.&rdquo; arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>


<p>자세한 method를 설명하기에 앞서 VGG 19 네트워크 자체에 대해 다뤄야하는데, 이 네트워크는 총 16개의 convolution layer, 5개의 pooling layer, 3개의 fully connected layer로 구성되어있다. 이 논문은 제공된 16개의 convolution layer에서 생성되는 feature map을 사용해 style loss와 content loss를 계산한다. 이 네트워크는 다음과 같은 형태로 구성되어있다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/92-6.png" width="400"></p>

<p>이 논문에서 사용한 VGG 19는 E에 해당하며, conv 2개 - pooling - conv 2개 - pooling - conv 4개 &#8230; 이런 식으로 구성되어있다. 각각의 conv layer들은 pooling layer를 기준으로, 순서대로 conv 1_1, conv 1_2, conv 2_1, conv 2_2, conv 3_1, conv 3_2, conv 3_3, &#8230; conv 5_4 라는 이름을 가지고 있다. 즉, conv 5_1 이면 4번쨰 pooling layer 바로 다음 conv layer를 말하는 것이다.</p>


<p>이 논문에서는 fully connected layer는 사용하지 않고, 16개의 conv layer와 5개의 pooling layer만 사용하는데, image reconstruction에 있어서는 max pooling보다는 average pooling을 고르는 것이 그림이 조금 더 자연스럽고 좋아보이는 결과로 나오기 때문에 max pooling 대신 average pooling을 사용하였다고 한다.</p>




<p>그럼 먼저 비교적 간단한 content loss 부터 살펴보도록하자. 이 논문은 feature map을 \(F^l \in \mathcal R^{N_l \times M_l}\)으로 정의하였다. 이때 \(N_l\)은 \(l\) 번째 레이어의 filter 개수이고, \(M_l\)은 각각의 filter의 가로와 세로를 곱한 값이며, 즉 각 filter들의 output 개수이다. 또한 \(F^l_{ij}\)는 \(i\) 번째 필터의 \(j\) 번째 output을 의미하게 된다. 이제 우리가 비교하려는 두 가지 이미지를 각각 \(p\)와 \(x\)라 하고, 각각의 \(l\) 번째 layer의 feature representation을 \(P^l, F^l\)로 정의하자. 이렇게 정의하였을 때, \(l\) 번째 layer의 content loss는 다음과 같이 간단하게 정의된다.</p>


<p>\[ \mathcal L_{content} (p, x, l) = \frac{1}{2} \sum_{ij} \big( F^l_{ij} - P^l_{ij} \big)^2. \]</p>


<p>즉, \(p\)와 \(x\)에 대해 각각 feature map \(P^l, F^l\)을 계산하고, 이 둘의 차의 Frobenius norm (\(\| P^l - F^l \|_F\))을 loss로 선택한 것이다. 이 error를 각각의 layer에 대해 따로 정의하게 된다. 이제 튀빙겐에서 찍은 사진 \(p\)의 \(l\) 번째 layer의 representation을 사용해 image reconstruction을 한다고 가정해보자. 이때 \(l\) 번째 layer에서 복원한 이미지를 \(x^l\)라고 하자. 앞서 정의한 loss를 minimize하는 \(x^l\)를 찾아야하므로, 우리는 다음과 같은 식을 얻는다.</p>


<p>\[x^l = \arg\max_x \mathcal L_{content} (p, x, l). \]</p>


<p>이 식을 풀기 위한 가장 간단한 방법은 \(x^l\)을 random image로 initialize하고 \(\frac{\mathcal L_{content} (p, x, l)}{x}\)를 게산해 gradient descent method를 사용하는 것이다. Loss를 layer의 각각의 activation으로 미분한 결과는 다음과 같다.</p>


<p>\[ \frac{\partial \mathcal L_{content} (p, x, l)}{\partial F^l_{ij}} = (F^l_{ij} - P^l_{ij})_{ij} \mbox{ if } F^l_{ij} > 0, \mbox{ otherwise, } 0.\]</p>


<p>이 값을 사용하면 전체 gradient를 back-propagation 알고리즘을 사용해 간단하게 계산할 수 있게 된다. <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 아래 부분에서 복원한 5개의 이미지는 각각 conv 1_1, conv 2_2, conv 3_1, conv 4_1, conv 5_1에서 loss를 계산하여 복원한 것이다.</p>




<p>다음으로, style에 대한 loss를 정의해보자. 이 논문에서 style이라는 것은 같은 layer의 서로 다른 filter들끼리의 correlation으로 정의한다. 즉, filter가 \(N_l\)개 있으므로 이것들의 correlation은 \(G^l \in \mathcal R^{N_l \times N_l}\)이 될 것이다. 이때, correlation을 계산하기 위하여 각각의 filter의 expectation 값을 사용하여 correlation matrix를 계산한다고 한다. 즉, \(l\)번째 layer에서 필터가 100개 있고, 각 필터별로 output이 400개 있다면, 각각의 100개의 필터마다 400개의 output들을 평균내어 값을 100개 뽑아내고, 그 100개의 값들의 correlation을 계산했다는 것이다. 이렇게 계산한 matrix를 Gram matrix라고 하며 \(G^l_{ij}\)라고 적으며 다음과 같이 계산할 수 있다.</p>


<p>\[ G^l_{ij} = \sum_{k} F^l_{ik} F^l_{kj}.\]</p>


<p>두 개의 image \(a\)와 \(x\) 간의 style이 얼마나 다른지를 나타내는 style loss \(\mathcal L_{style}\)은 \(G^l_{ij}\)를 사용하여 다음과 같이 정의된다.</p>


<p>\[ \mathcal L_{style} (a,x) = \sum_{l=0}^L w_l E_l \]</p>


<p>\(L\)은 loss에 영향을 주는 layer 개수, \(w_l\)은 전부 더해서 1이 되는 weight이고, \(E_l\)은 layer \(l\)의 style loss contribution이다. 이 값은 다음과 같이 정의된다.</p>


<p>\[ E_l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} \big( G^l_{ij} - A^l_{ij} \big)^2. \]</p>


<p>역시 마찬가지로, \(p\), 혹은 고흐의 &#60;별이 빛나는 밤&#62;의 layer 별 style reconstruction 역시 이 \(\mathcal L_{style} (a,x)\)를 minimize하는 \(x\)를 찾는 것으로 풀 수 있으며 이 문제는 back-propagation algorithm으로 풀 수 있다.</p>


<p>\[ \frac{\partial \mathcal E_l}{\partial F^l_{ij}} = \frac{1}{N_l^2 M_l^2} \sum_{i,j} \big( \big(F^l)^\top \big( G^l_{ij} - A^l_{ij} \big)\big)_{ji} \mbox{ if } F^l_{ij} > 0 \mbox{ otherwise } 0. \]</p>


<p>다시 한 번 <a href="#92-reconst-img">앞서 봤던 reconstruction 그림</a>의 윗 부분에서 복원한 이미지를 살펴보면, 순서대로 loss 계산을 위해 conv 1_1만 사용하여 복원한 그림, conv 1_1, conv 2_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1을 사용한 그림, conv 1_1, conv 2_1, conv 3_1, conv 4_1을 선택한 그림 &#8230; 이런 식으로 선택하여 복원을 한 그림이다. 이때 &#8216;선택&#8217; 한다는 것의 개념은 선택한 layer의 \(w_l\)의 값을 0이 아닌 같은 값으로 두고 나머지는 전부 0으로 설정하는 것이다. 예를 들어 c 그림은 4개만 영향을 주므로 conv 1_1, conv 2_1, conv 3_1, conv 4_1만 \(w_l = 0.25\)이고 나머지는 0이다.</p>


<p>이제 마지막으로 이 두 가지 loss를 한 번에 optimization하는 과정만 남았다. \(\alpha\)와 \(\beta\)는 content와 style 중 어느 쪽에 더 초점을 둘 것인지 조정하는 파라미터로, 보통 \(\alpha/\beta\)으로 \(10^{-3}\)이나 \(10^{-4}\) 정도를 고른다고 한다.</p>


<p>\[\mathcal L_{total} (p,a,x) = \alpha \mathcal L_{content} (p, x) + \beta \mathcal L_{style} (a,x)\]</p>


<p></p>

<p>논문에서는 style에 얼마나 많은 layer를 고려하는지에 따라, 그리고 \(\alpha/\beta\)의 값을 조정함에 따라 다음과 같이 결과가 달라진다고 report하고 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/92-7.png" width="600"></p>

<p>x 축이 \(\alpha/\beta\), y축은 순서대로 앞에서처럼 conv 1_1, conv 2_1, conv 3_1, conv 4_1, conv 5_1을 선택한 것이다 (A: 1_1, B: 1_1, 2_1, C: 1_1, 2_1, 3_1, &#8230;) 이 값들을 어떻게 조정하느냐에 따라 style과 content의 적당한 trade-off를 조정할 수 있다. Layer를 더 많이 사용할수록, 그리고 \(\alpha/\beta\) 값이 작아질수록 content보다는 style에 더 치중된 결과가 나오게 된다. 그리고 당연히 layer를 더 적게 사용하거나 \(\alpha/\beta\)의 값을 키울수록 그 반대의 결과가 나오게 된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/92-2.png" width="600"></p>

<p>위 그림은 앞에서 언급한 <a href="https://github.com/jcjohnson/neural-style">&#8216;neural style&#8217;</a>을 사용해 만든 그림이다. 원본 content 이미지로 브래드 피트의 사진을 넣고, style 이미지로 피카소의 &#60;자화상&#62;을 넣은 다음, \(\alpha/\beta\) 값을 조정하면서 값이 변하는 것을 관측한 것이다.</p>


<p>맨 처음 글을 시작하며 보았던 그림에서는, content representation은 conv 4_2의 것만을 사용하고, style representation은 conv 1_1, 2_1, 3_1, 4_1, 5_1 에 각각 \(w_l = 1/5\), 나머지는 \(w_l=0\)으로 하여 사용했다. 또한 B,C,D 그림은 \(\alpha/\beta = 10^{-3}\), E,D 그림은 \(\alpha/\beta = 10^{-4}\)를 사용하였다고 한다.</p>




<h5>Comments</h5>


<ul>
<li>개인적인 생각으로는, 이 논문의 결과는 고흐나 뭉크 등의 &lsquo;스타일&#8217;이 분명한 인상주의, 표현주의, 야수파 화풍의 화가들의 그림을 더 잘 generate할 것으로 생각된다. 나중에 사람들이 실험해본 결과도 그렇고, 대체로 고흐 등의 경우 스타일이 특색이 뚜렷해서 그러한지 꽤 그럴싸한 결과가 나오는 반면, 피카소 등으로 대변되는 입체파 처럼 &#8216;스타일&#8217;을 넘어서는 그 무언가가 존재하는 경우 기대만큼 좋은 결과로 이어지는 것 같지는 않다. 원래 이 논문에서 제안하는 알고리즘의 목적 자체가 그림의 texture를 learning하여 content는 유지한 상태로 texture만 변경시키는 것이므로, 내용 자체가 변화하는 입체파 등의 독특한 그림을 제대로 따라하는 것은 불가능하기 떄문에 그런 것으로 보인다. (+ 글을 쓰면서 개인적으로 궁금해진게, 캐리커쳐는 어떻게 반응할지 궁금해졌다. 나중에 public하게 공개된 코드를 사용해서 실험해봐야겠다)</li>
<li>왜 method에서 전체 conv layer를 사용하는 것이 아니라 일부만 사용하는 것인지 다소 아리송하다. 또한 왜 style reconstruction을 위해 conv 1_1, 2_1, &hellip; 5_1 의 정보만 사용했고, content는 왜 conv 4_2를 사용하였는지 역시 의아하다. 아마 제일 잘 되는 것을 골랐을텐데, 왜 그것들이 제일 잘되는 것일까 궁금해진다.</li>
<li>CNN에 대한 이해가 충분히 있어야 쉽게 읽을 수 있는 논문이었다. 추가로 VGG network에 대한 이해도도 있으면 도움이 되는 것 같다. 맨 처음 논문을 읽을 때는 이런 것들에 대해 감이 좀 약해서 읽어도 이해하기가 어려웠는데, CNN 공부를 다시 끝내고 다른 선행 연구들을 적당히 이해한 채로 다시 읽어보니 쉽게 이해할 수 있었다.</li>
</ul>


<h5>Summary of A Neural Algorithm of Artistic Style</h5>


<ul>
<li>CNN의 conv layer가 feature map이라는 것에서부터 착안하여, feature map에서 style과 content를 reconstruct하는 optimization problem을 제안하였다.</li>
<li>하나의 CNN에서 content와 style representation이 separable하므로 style과 content를 한 번에 update하는 알고리즘을 만들 수 있다.</li>
<li>Content loss는 두 이미지 각각의 feature matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 4_2 만 사용하였다.</li>
<li>Style loss는 두 이미지 각각의 Gram matrix의 차의 frobenius norm으로 표현이 된다. 최종 결과를 위해서는 conv 1_1, 2_1, 3_1, 4_1, 5_1 만 사용하였다.</li>
<li>이때 style loss가 Gram matrix가 되는 이유는 style을 한 레이어 안에 있는 filter들의 correlation으로 정의했기 때문이다. 이때 correlation 계산은 각각의 filter들의 expectation 값들을 사용한다.</li>
<li>VGG 19 네트워크를 사용했으며, FC layer는 제거하고 max pooling 대신 avg pooling을 사용하였다.</li>
<li>Content loss와 Style loss의 비율을 조정하여 style과 content 중에서 어느 것에 집중할지 선택할 수 있다. 논문에서는 0.001 정도를 사용하였다.</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1508.06576">Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. &ldquo;A Neural Algorithm of Artistic Style.&rdquo; arXiv preprint arXiv:1508.06576 (2015).</a></li>
<li><a href="https://github.com/jcjohnson/neural-style">&lsquo;neural style&rsquo;</a></li>
<li><a href="http://arxiv.org/abs/1409.1556">Simonyan, Karen, and Andrew Zisserman. &ldquo;Very deep convolutional networks for large-scale image recognition.&rdquo; arXiv preprint arXiv:1409.1556 (2014).</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Basic Principles in Deep Neural Networks]]></title>
    <link href="http://SanghyukChun.github.io/54/"/>
    <updated>2015-09-29T17:45:00+09:00</updated>
    <id>http://SanghyukChun.github.io/54</id>
		<content type="html"><![CDATA[<h5>들어가기 전에</h5>


<p>이 포스트는 2014년 6월 16일 카이스트에서 당시 Yoshua Bengio 교수님 연구실에서 포닥 과정을 밟고 계신 장민석 박사님의 The Basic Principles in Deep Neural Networks 라는 이름의 세미나를 요약한 내용이다. 내용은 주로 Deep learnining을 supervised learning, unsupervised learning의 관점에서 각각 바라보면서 어떤 컨셉들이고, 어떤 연구들이 진행이 되어있는지 훑어보는 정도의 간단한 내용이었다.</p>


<p>어쩌다보니 1년 넘게 포스팅을 못하다가 이제와서 포스트를 등록하게 되었는데, 이 글을 제대로 정리할 정도로 여유가 없기도 했고, 내가 이 내용을 이해할 수 있을 정도의 내공이 없기 때문이기도 했다. 지금은 어느 정도 여유가 생기기도 했고, 내가 내용을 대략이나마 이해하고 있기 때문에 일 년 전 내용이기는 하지만, 다시 한 번 내용을 정리해서 올려본다.</p>


<p>이 세미나는 크게 네 가지 파트로 나뉘어진다. 먼저 Deep learning이 무엇인지 간단한 introduction part, supervised deep learning, unsupervised deep learning, 마지막으로 아직 연구가 진행 중인 advanced topic이다. 이 글에서는 introduction과 supervised learning part에 대해서 주로 다루고, 나머지 부분에 대해서는 간단하게 훑고 지나가기만 하도록 하겠다.</p>




<h5>Part 1 - Introduction</h5>


<p>Deep Learning이 무엇인지 알기 전에 먼저 Machine Learning이 무엇인지 알 필요가 있다. Machine Learning에 대해 보다 깊게 알고 싶다면 내가 아직 계속 작성 중인 <a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning Study</a> 글들을 읽어보아도 좋고, 다른 좋은 글들을 참고해도 좋을 것 같다. 이 세미나에서는 Machine Learning이 주 주제가 아니기 때문에, 머신러닝이라는 것을 &#8216;데이터를 통해 모델을 learning하고 learning한 모델을 사용해 주어진 query에 대답하는 것&#8217;이라고 정의하였다. 결국 내가 <a href="http://SanghyukChun.github.io/57">예전 글</a>에서 아래 그림에서 정의했던 것과 크게 차이는 없다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/57-2.png" width="500"></p>

<p>Deep Learning에 대해 설명하기 전에 먼저 perception이라는 문제에 대해 살펴보자. Perception이란 우리말로 하면 &#8216;인지&#8217; 정도로 해석할 수 있다. 이 문제는 주어진 정보에 대해 내가 필요한 어떤 특정 정보를 inference 하는 것이다. 예를 들어서 이 포스트의 글자가 무엇인지 읽어들이는 문제는 간단한 문제이지만, 이 문장들을 바탕으로 어떤 의미를 가지고 있는지 inference하는 것은 어려운 문제이다. 또 다른 예로는 vision 데이터를 하나 주고 주어진 vision data가 어떤 object인지 classification하는 문제도 perceptron이다. 이 문제는 사람에게는 아주 간단한 문제이지만 컴퓨터에게는 엄청나게 어려운 문제이다. Deep learning은 사람이 perception하는 방식을 모방하여, 사람만큼 perception을 해보자는 취지로 만들어진 model이라고 생각할 수 있다. 사람은 뇌의 neuron과 synapse 등으로 대표되는 일어나는 일렬의 화학적, 전기적 신호 전달 과정을 통해 perception을 하게 된다. 이것을 수학적 모델로 표현하고, 그것을 조금 deep하게 만든 것이 deep learning이다. (Neural Network와 이에 대한 intuition을 조금 더 자세히 알고 싶다면 내가 쓴 <a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a>과, <a href="http://SanghyukChun.github.io/75">Deep Learning 1</a>을 읽어보기를 권한다.)</p>


<p>Deep Learning이라는 분야는 최근 10년 동안 엄청나게 hot해진 분야이다. 그런데 사실 deep learning이라는 분야, 혹은 neural network는 사실 4-50년도 넘은 엄청 오래된 분야이다.</p>


<ul>
<li>1958 Rosenblatt proposed perceptrons</li>
<li>1980 Neocognitron (Fukushima, 1980)</li>
<li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
<li>1985 Boltzmann machines (Ackley et al., 1985)</li>
<li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
<li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
<li>1993 Sparse coding (Field, 1993)</li>
</ul>


<p>즉 우리가 지금 쓰고 있는 Neural network의 기본적인 연구는 이미 90년대 이전에 다 끝나있었다. (심지어 1995년에 Machine Learning에 어마어마한 연구 결과를 남긴 Vapnik과 Jackel이 10년 뒤인 2005년에 아무도 Neural net을 쓰고 있지 않을 것이라고 내기를 했을 정도라고 한다) 근데 갑자기 Deep learning이 왜 이렇게 hot해졌을까? 이는 내가 <a href="http://SanghyukChun.github.io/75">Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN</a> 글에서 자세히 다뤘으므로 이 글에서는 간단하게 결론만 말하도록 하겠다. 그 이유는 Deep Learning이 ImageNet에서 기존 결과를 거의 박살을 냈기 때문이다. Deep Learning이 나오기 전에는 error가 0.27 ~ 0.30정도, 그러니까 27%에서 30% 정도였다고 한다. 기존 computer vision 연구자들은 이 정도 결과가 거의 한계치라고 여겨지고 있었는데 갑자기 2012년에 Deep convolutional neural network를 하는 팀이 0.153으로 거의 2배 가까운 성능 향상을 보여주었다고 한다. 그리고 그 다음 해에는 상위 20개 팀에서 2개 팀 빼고 전부 Deep learning을 써서 Classification을 했는데, 최고 성능은 또 0.117로 개선되었다고 한다. 그 결과 computer vision을 비롯한 기존 학계에서 엄청나게 주목을 받게 되었다고 한다.</p>


<p>Deep learning에 주목하는 것은 학계만이 아니다. Deep learning을 기본 기술로 사용하는 스타트업도 엄청나게 늘어나고 있고, (<a href="http://techcrunch.com/2014/01/26/google-deepmind/">Deep learning 기술 회사인 Deep mind M 이상에 인수</a>, <a href="http://techneedle.com/archives/15662">Captcha 퍼즐 암호 99.8% 성공률로 해석</a>, <a href="http://www.technologyreview.com/news/525586/facebook-creates-software-that-matches-faces-almost-as-well-as-you-do/">사람의 얼굴 인식 능력을 상회하는 소프트웨어 개발</a>) 심지어 현재 구글이나 애플 등에서 음성 인식에 쓰는 알고리듬도 deep learning이다. 이렇듯 deep learning은 학계에서만 관심을 가지는 분야가 아니라 실제 산업에서도 아주 빠르게 적용되고 사용되고 있는 분야이기 때문에 더더욱 주목할만하다. 보통 학계에서 연구한 결과가 실제 산업에서 적용되기까지의 시간이 분야마다 조금씩 다른데, deep learning은 오히려 산업에서 먼저 개발하고 학계에 발표할 정도로 학계와 산업이 함께 집중하고 있는 분야이다.</p>


<h5>What is &#8216;Deep&#8217; Learning?</h5>


<p>기존 machine learning 문제를 푸는 방법은 크게 3단계로 구분 할 수 있다.</p>


<ol>
<li>Feature Engineering: 주어진 데이터를 사용해 machine learning tool에서 사용할 수 있는 feature를 뽑아내는 과정. 이 과정은 machine learning이 아니며, domain knowledge와 engineer의 knowhow가 강하게 drive하는 과정</li>
<li>Learning: 1에서 주어진 feature 데이터를 사용해 machine learning model을 train하는 과정</li>
<li>Inference: 2에서 학습한 model을 사용해 새로운 데이터를 inference하는 과정</li>
</ol>


<p>첫 번째 단계는 machine learning은 아니지만, 실제 최종 performance에 큰 영향을 미치는 과정이다. 예를 들어 우리가 SVM을 사용해 image를 학습한다고 가정해보자. 100만 화소짜리 이미지를 사용해 learning을 해야하는 경우, 1번 과정이 없으면 우리는 엄청나게 high dimensional data를 사용해 SVM을 풀어야하지만, 이렇게 높은 dimension의 데이터를 사용하게 되면 <a href="http://SanghyukChun.github.io/59#59-4-cd">curse of dimensionality</a> 등의 문제로 인해 나쁜 performance를 얻게 될 확률이 크다. 그 밖에도 특정 pattern의 noise가 계속 등장하고 그 noise로 인해 outlier가 많이 생기는 등의 상황도 생길 수 있다. 특히 여러 multi media 데이터를 처리하기 위해서는 이 feature engineering이 중요한 문제이며, 단순히 feature를 잘 고르는 것 만으로도 어려운 모델을 사용하지 않고 가장 간단한 linear 모델만으로도 문제가 해결되는 경우도 많이 있다 (예: 지문 인식). 때문에 이 과정은 절대 무시할 수 없는 과정이지만 domain knowledge에 너무 크게 dependency가 있고, general purpose machine learning과 분리가 된다는 문제가 존재한다. 그러나 여기에서 중요한 점은, domain knowledge를 반영하는 것이 그렇지 않은 것에 비해 훨씬 더 우수한 결과를 낸다는 점이다.</p>


<p>그렇기 때문에 우리는 feature 역시 machine learning technique를 사용해 learning해보자는 idea를 제안할 수 있다. 예를 들어 앞에서 제안한 image같은 경우, PCA 등의 dimensionality reduction technique들을 사용한다면 더 낮은 차원의 데이터로 문제를 해결할 수 있다.</p>


<p>1-a. Feature Engineering: domain knowledge를 반영하여 representation learning의 input으로 사용할 feature를 생성하는 과정
1-b. Feature/Representation Learning: 1-a의 결과를 사용해 PCA 등의 unsupervised feature learning을 뽑아내는 과정
2. Learning: 생략
3. Inference: 생략</p>

<p>이 경우 feature extraction에서도 general machine learning 방법론을 적용할 수 있다는 장점이 존재하지만, 여전히 representation learning은 domain knowledge를 반영하지 못하기 때문에 domain knowledge와 general machine learning 간의 간극이 발생한다. 다시 말해서, PCA는 Image data의 특성을 살릴 수 없는 learning 모델이기 때문에, 결국 domain knowledge를 반영하기 위한 새로운 feature engineering 과정이 필요하다는 의미이다.</p>


<p>Deep learning은 이런 문제를 해결하기 위하여 아래와 같은 모델을 제안한다.</p>


<ol>
<li>Jointly learning everything: 한 번에 모델 하나로 feature engineering, representation learning, model learning까지 끝내는 과정</li>
<li>Inference</li>
</ol>


<p>우리가 domain knowledge를 반영하여 모델을 하나 설계한 다음, 나머지 feature engineering이나 representation learning 등의 과정을 한 번에 짬뽕해서 해결하자는 것이다. 이것이 가능한 이유는 deep learning 모델의 특성 때문이라고 할 수 있다. Neural network 모델은 아래 그림처럼 layer가 쌓여있는 형태로 구성이 되어있는데, 마치 각각의 layer를 feature extraction 과정으로 바꿔서 생각할 수 있다. 위로 올라갈수록 점점 우수한 feature를 뽑아내게 되고, 맨 마지막 layer에서 linear classifier를 learning하는 과정처럼 생각할 수 있는 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-3.png" width="600"></p>

<p>또한 중요한 것은, PCA 등의 unsupervised feature extraction을 결합한 경우에는 feature extraction이 unsupervised learning이기 때문에 데이터나 최종 output loss function에 영향을 받지 못하는데 반해서, deep learning model은 모든 것들을 하나의 loss function으로 한 번에 handle하기 때문에 모든 것들을 jointly learning한다고 표현할 수 있는 것이다. 이를 가장 잘 표현하는 말이 Yoshua Bengio 교수의 &#8220;Let the data decides&#8221;라고 할 수 있다. 어떤 모델을 써야 좋은 feature를 뽑을 수 있을까에 신경쓰지말고, 처음부터 deep learning 모델처럼 좋은 모델을 사용해 데이터에서부터 좋은 결과를 낼 수 있도록 데이터가 알아서 하도록 하라는 취지의 말인데, 개인적으로 이 얘기는 특히 CNN 모델에 잘 맞는 얘기라고 생각한다. <a href="http://SanghyukChun.github.io/75#75-cnn">예전 글</a>에서도 다뤘듯이 CNN은 모델은 vision 데이터의 특성을 최대한 활용하여 feature map을 만들어내는 것이 목적이다보니, convolution과 polling layer는 최대한 feature를 만들어 내는 과정으로 쓰이고, fully connected layer에서 해당 feature를 사용하여 classification을 하는 형태가 된다. 그렇기 때문에 한 번에 preprocessing 혹은 feature engineering part와 learning하는 part가 합쳐진 형태가 되는 것이 아닐까 추측해본다.</p>


<p>그러나 단순히 deep learning을 사용하면 feature extraction과 결합된 형태로 모델을 learning할 수 있다는 이유로 아무도 쓰지 않던 deep learning을 많이 쓰기 시작한 것은 아니다. Deep learning을 많이 연구하게 된 원인으로 박사님은 두 가지 이유를 꼽았는데, 하나는 서로 다른 분야라고 생각하면서 연구되었던 PCA, Neural PCA, Probabilistic PCA, Autoencoder, Belief Network, Restricted Boltzmann Machine 등의 분야가 사실은 서로 각자의 특수한 케이스이거나 혹은 다른 표현형이라는 것이 알려지면서 결국 한 분야로 수렴하였다는 것과, 또 하나는 예전에는 알려지지 않았던 것들이 이제는 많이 알려져서 예전에는 어렵게 접근했던 것들을 이제는 쉽게 learning할 수 있다고 한다. 그 중에서 특히 non-convex optimization에 대해 많은 기술들이 연구되어서 non convex optimization이기 때문에 optimization이 불가능하다고 두려워 할 필요가 없다는 것이 가장 큰 이유라고 한다. 또한 inference와 training사이의 interaction에 대해서도 더 많이 이해하고 있다는 점도 꼽을 수 있으며, GPU 등의 하드웨어 발전으로 인해 예전보다 computation power가 exponential하게 증가한 것도 그 중의 한 원인이라고 한다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/54-4.png" width="600"></p>

<p>위의 그림은 각 종과 현재 개발된 NN들의 뉴런 개수를 비교한 것이다 (y 축 뉴런 개수의 스케일은 log scale이다). 맨 처음 DBN이 나올 때만 해도 편충보다 뉴런이 조금 많고 거머리보다 10배 적었다. 2012년 ImageNet에서 우수한 성과를 거둔 AlexNet의 neuron 개수는 개미보다 조금 많고 벌에 비해서 한참 적다. 그리고 2014년 기준으로 가장 큰 AdamNet의 뉴런 개수는 아직도 개구리의 뉴런 개수보다 적다. 그렇기 때문에 앞으로 사람이 가지고 있는 뉴런의 개수만큼 뉴런을 가지는 NN 모델이 개발되려면 많은 시간이 남았다고 전망하고 있다. 뉴런의 개수가 많을수록 좋은 모델이 된다는 것은 이미 알고 있지만 뉴런의 개수를 단순하게 많이 늘릴 수 없는 이유는 learning time이 엄청나게 오래걸리기 때문이다. 개인적으로는 이 부분이 아직까지도 deep learning이 발전할 여지가 많이 있다는 것을 의미한다고 생각한다. 왜냐하면 뉴런의 개수를 늘리는 일은 computation cost가 엄청나게 많이 드는 일이고, parameter 역시 exponential하게 증가하기 때문에 overfitting issue를 handle하는 것이 점점 더 중요해지기 때문이다. 더 이상의 regularization은 없다고 생각할 수도 있지만, 최근에 나온 <a href="http://SanghyukChun.github.io/88">Batch Normalization</a> work이 dropout 등의 기존 성과를 뛰어넘는 좋은 performance를 내고 있는 것을 보면, 충분히 더 좋은 접근 방법이 나올 수 있을 것이라고 믿는다.</p>




<h5>Part 2 - Supervised Neural Network</h5>


<p>Deep learning도 machine learning의 일종이기 때문에 supervised/unsupervised/reinforcement learning의 세 가지 접근 방법으로 바라보는 것이 가능하다. 이 part에서는 neural network로 supervised learning, 특히 classification을 어떻게 푸는지에 대해서 주로 다뤘었다. 세미나에서는 multilayer perceptron과 그것의 learning, regularization, 그리고 기타 등등에 대해 다뤘었지만, 이 글에서는 learning algorithm인 back-propagation에 대한 설명은 <a href="http://SanghyukChun.github.io/74#backprop">예전에 쓴 글의 링크</a>로 대체하고, 주로 어떻게 MLP를 regularization할 수 있는지 등에 대해서 다룰 것이다.</p>


<p>예전에도 설명했듯 neural network는 back-propagation이라는 알고리즘을 사용해 model parameter를 찾는다. 이 알고리즘은 any cost function이 주어졌을 때, 그 문제를 풀기 위한 gradient descent method를 chain rule을 사용해 간단하게 바꾼 알고리즘이라고 할 수 있다. 이 알고리즘을 사용하게 되면 모든 노드의 derivative를 전부 계산할 필요가 없고, 대신 매우 적은 양의 계산으로 마치 전체의 gradient를 계산한 것과 같은 효과를 얻을 수 있기 때문에 NN update는 거의 이 방법을 사용한다. 또한 모든 data에 대한 gradient를 계산하여 완벽한 gradient를 찾는 대신, batch라는 개념을 도입해 stochastic gradient descent method를 사용해 문제를 해결한다. 원래 문제가 Convex가 아니기 때문에 제대로 된 gradient descent와 SGD가 서로 다른 곳으로 수렴하기는 하지만, 이 점은 크게 중요한 이슈는 아니라고 한다.</p>


<p>여기에서 질문이 하나 나왔었다. 이렇게 할 수 있는 이유는 neural network에서 chain rule을 적용할 수 있기 때문인데 혹시 그렇다뎐 Hessian을 계산할 수는 없을까라는 질문이 나왔다. 왜냐하면 second derivative method가 gradient method보다 훨씬 좋다는 것은 이미 잘 알려진 사실이기에, 만약 Hessian을 efficient하게 계산할 수 있다면 learning 속도를 크게 향상시킬 수 있을 것으로 기대할 수 있기 때문이다. 그러나 아직까지는 실제 NN에서 Hessian 등의 2nd derivative를 계산하는 것은 Hessian Matrix를 계산해야할 뿐 아니라, 그것의 inverse까지 계산해야하므로 매우 expensive하다고 하고, 구현도 복잡하다고 한다. 그렇기 때문에 대신 Hessian matrix를 직접 구하지 않고 그것의 inverse를 estimate하는 방법들이 있지만, 그러나 여전히 큰 NN에는 부적합하기 때문에 보통 Gradient를 사용한다고 한다 (여기에서 언급된 Hessian matrix의 inverse를 estimate하는 방법을 Hessian-Free optimization이라고 부른다).</p>




<h5>Regularization</h5>


<p>Regularization을 Bayesian 관점에서 바라본다면 좋은 prior를 제안하는 것과 같다. 예를 들어 &#8216;모델이 이렇게 복잡할리 없으니 모델의 complexity를 penalty term으로 추가해야겠다&#8217; 라는 regularization method도 model complexity에 대한 prior를 반영한 것이라 할 수 있다. Neural Network에서도 마찬가지로 여러 prior를 바탕으로 다양한 regularization 방법들이 존재한다.</p>


<h6>Weight Decay</h6>


<ul>
    <li>Prior: Weight의 값이 너무 크지 않을 것이다 (이를 model이 sharp하지 않고 smooth하다 라고 표현한다).</li>
    <li>Approach: 다음과 같은 형태로 optimization objective를 바꾼다.</li>
    <p>\[\min_w E(w) + \lambda w^\top w.\]</p>
    <li>Gradient update rule은 다음과 같이 바뀐다</li>
    <p>\[w(t+1) = w(t) (1 - 2 \eta \lambda) - \eta \nabla E (w(t)). \]</p>
</ul>




<h6>Smoothness and Noise Injection</h6>


<ul>
    <li><p>Prior: Smoothness, \(f(x)\)와 \(f(x+\varepsilon)\)은 거의 비슷할 것이다.</p></li>
    <li><p>Approach: noise에 대한 change를 줄이는 것은 곧, \(\min \sum_i | \frac{\partial f(x_i)}{\partial x} |^2\) 과 같다.</p></li>
    <li>따라서 위 식을 optimization function에 추가하여 문제를 풀게 되는데, Bishop 책에 따르면, 이 regularization term을 넣고 optimization하는 것은, random Gaussian noise를 input에 추가하여 learning하는 것과 equivalent하다는 것이 알려져 있으므로, input data에 random Gaussian noise를 섞는 것으로 대체할 수 있다.</li>
</ul>




<h6>Dropout (Hinton 2012)</h6>


<ul>
    <li>Prior: 하나의 classifier를 learning하는 것 보다, 여러 개의 classifier를 learning하고 이를 ensemble하여 classification하는 것이 더 좋다.</li>
    <li>Approach: 하나의 neural network 모델에서 여러 개의 모델을 learning할 수 있도록, NN의 node를 random하게 지운다. 이 경우 lower bound optimization과 같은 효과를 내기 때문에 조금 더 general한 model을 학습하는 것이 가능하며, dropout을 선택하는 것과 그렇지 않은 것은 약 10~20%의 성능 차이를 보인다. Drop하는 node는 보통 50%를 선택한다 (이 값도 제일 좋은 값을 learning해보려고 시도해봤는데 모두 0.5로 converge하였다고 한다).</li>
</ul>




<h5 id="54-common-recipe">Common Recipe for DNN.</h5>


<p>세미나에서 박사님은 앞에서 살펴본 regularization과 optimization method들을 바탕으로 아래와 같은 common recipe를 제안하였다.</p>


<ol>
<li>Rectifier나 Maxout을 사용해라 (이 방법은 dropout처럼 값이 음수인 뉴런은 drop하고, 양수인 뉴런만 존재한다고 생각하면 계속 다른 NN을 사용하는 것처럼 생각할 수 있다. 이런 접근 방식을 사용하게 되면 differentiable하지는 않지만 sub-gradient를 사용하여 문제를 풀 수 있다고 한다).</li>
<li>Preprocess data and choose features carefully (각 데이터 domain에 맞는 preprocessing을 취하자)

<ul>
<li>Image: Whitening, Raw, SIFT, HoG?</li>
<li>Speech: Raw? Spectrum?</li>
<li>Text: Characters? words? tree?
*General: z-Normalization?</li>
</ul>
</li>
<li>Dropout이나 다른 regularization method들을 사용하자.</li>
<li>데이터가 적은 경우라면 Unsuperviesd Pretraining (Hinton et al 2006) 을 사용해보자. 그러나 데이터가 많으면 오히려 나쁜 결과를 내게 되므로 쓰지 말자.</li>
<li>Carefully search for hyperparameters (Random search, Bayesian optimization + Greedy search는 hyperparameter가 exponential하게 많으므로 불가능하다.)</li>
<li>Often, deeper the better (이미지 &ndash; 레이어 7개 이상, 스피치 &ndash; 12개, 14개,&hellip;, 그 이외에는 hyper parameter라고 한다. 박사님은 2개부터 시작한다고 한다. + 참고로 지금 이미지에서는 VGG등의 최신 논문들은 19개까지 쌓기도 하고, Google의 Inception의 경우 어마어마하게 깊다.)</li>
<li>Build an ensemble of neural networks</li>
<li>Use GPU. (좋은 파워와 메인보드가 필요하고, 쿨링과 전기세를 조심하라고 조언해주셨다)</li>
</ol>


<p>그러나 중요한 점은, 아무도 vanilla MLP (모든 layer가 fully connected layer인 NN)는 사용하지 않는다는 사실이다. 그 대신 domain knowledge가 많이 반영된 CNN이나 RNN등의 모델을 선택하여 사용한다. 최근 work들을 보면 Image등의 static한 데이터는 보통 CNN을 사용하고, sequencial data는 거의 RNN을 사용한다. CNN과 RNN에 대한 설명은 생략하도록 하겠다.</p>


<p>박사님은 deep learning의 좋은 점으로, 모델을 만들 때 부터 domain knowledge를 적용해서 CNN이나 RNN 등의 새로운 모델을 만들 수 있다는 점을 꼽았다. 만약 SVM 등의 기존 방법들을 사용한다면 모델 자체에서 그런 아이디어를 적용하지 어렵지만, deep learning은 그러기에 용이하다는 것이다. 예를 들어 Convolutional Neural Network의 Translation, Rotation, Temporal, Frequency invariance 등을 꼽을 수 있을 것이다. 이렇게 만든 CNN은, convolution layer와 pooling layer를 엄청나게 deep하게 쌓아 좋은 feature를 뽑아내고 마지막에 그것에 fully connected layer를 붙여서 linear classifier를 learning하는 방식으로 &#8216;deep&#8217; CNN을 구성할 수 있다. 반면 RNN은 time을 길게 가져가는 방식으로 &#8216;deep&#8217; RNN을 구성할 수 있는데, 이렇듯 &#8216;deep&#8217;하게 만드는 방식도 모델 특성을 따라가기 때문에 domain knowledge가 잘 반영된 모델이 deep해졌을 때도 잘 동작할 수 있는 것 같다고 생각한다.</p>




<h5>나머지 part들</h5>


<p>Unsupervised Learning 파트는 RBM, DNN 그리고 NADE에 대해 설명하고 마지막에 manifold leraning과 denoising autoencoder를 다루는 것으로 끝났다.</p>


<p>Advanced Topics 파트는 Deep Reinforcement Learning에 대해 다루고 (<a href="http://SanghyukChun.github.io/90">Atari 논문</a>을 다뤘다), 당시에 막 연구가 되고 있던 NLP 연구에 대해 잠시 언급했다 (1년이 지난 지금은 NLP 쪽으로 많은 연구가 진행되었다). 그 밖에 optimization 관점에서, local minima라고 여겼던 부분이 사실 local minima가 아니라 flat한 부분이었다는 관측결과가 많이 나오고 있다는 말과, 적당하게 2nd order method를 섞으면 성능이 크게 향상된다는 말까지 언급하였다 (그러나 아직까지 2nd order optimization을 많이 사용하는 것 같지는 않다). 마지막으로 deep learning의 최대 약점으로 꼽히던 이론적인 분석 결과에 대해 많은 연구가 진행되고 있다는 얘기까지 잠깐 언급하고 세미나가 마무리 되었다.</p>




<h5>정리</h5>


<p>이 글은 내가 거의 1년 3개월 전에 들었던 세미나를 바탕으로 쓰여진 글이다. 박사님이 해주신 얘기도 많이 섞여있고 내 개인적인 의견도 많이 섞여있지만, 맨 처음 deep learning을 접할 때 큰 도움이 되었던 세미나인 만큼, 공유할 수 있으면 좋을 것 같아 정리해서 올려본다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle competition - Poker rule induction]]></title>
    <link href="http://SanghyukChun.github.io/87/"/>
    <updated>2015-09-26T15:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/87</id>
		<content type="html"><![CDATA[<p>이 글은 2015년 봄 내가 조교를 했었던 KAIST 빅데이터 분석개론 수업에서 중간 프로젝트로 나왔었던 Kaggle Competition을 내가 개인적으로 진행해보면서 느꼈던 점들을 시간 순서에 맞춰 적은 일종의 개발기이다. <a href="https://github.com/SanghyukChun/kaggle-poker_rule">깃허브 레포지토리</a>도 있으니 코드가 궁금하다면 이 레포지토리를 확인하면 될 것 같다. 깃허브 기준으로 이 프로젝트는 2015년 3월 20일부터 4월 19일까지 대략 한 달간 진행하였다.</p>


<p>이 competition을 위하여 4가지 정도의 아이디어를 냈었다. 먼저 normal KNN을 최대한 튜닝해보는 것, 문제가 &#8216;rule&#8217;이라는 것에 초점을 맞추어 rule을 정의하고 그것을 learning하는 방법, 세 번째로 card set에 대한 확률모델을 정의하여 likelihood를 maximization하는 방법, 마지막으로 기존 KNN의 input을 적당하게 바꾸어 KNN으로 처리하는 방법이었다. 이 중 실제로 submission까지 이어진 아이디어는 처음과 마지막 아이디어인데, 첫 번째는 0.67575, 두 번째에는 0.96908을 달성하였다. 이 글에서는 각각의 아이디어를 생각하게 된 계기와 왜 실패했고, 왜 성공했는지에 대해 정리해보도록하겠다.</p>


<p>이 competition을 시작하게 된 계기는, 내가 조교를 맡고 있던 수업에서 프로젝트로 <a href="https://www.kaggle.com">Kaggle competition</a>의 poker rule induction 문제를 풀기로 했기 때문이었다. (<a href="https://www.kaggle.com/c/poker-rule-induction">competition 링크</a>). 문제 자체가 간단하고, 마음만 먹으면 99% 이상을 찍는 단순한 알고리즘을 만드는 것이 어렵지 않은 문제였기 때문에, 나름의 조건으로 poker rule이 아니라 그 어떤 카드 게임에도 general하게 learning 결과를 적용할 수 있는 framework을 만들자는 것과, 또 하나는 deep learning을 사용하지 않고, 학생들이 이미 알고있는 기초적인 framework에 재미있을 법한 아이디어들을 섞어보기로 하였다. 그렇기 때문에 시작은 가장 간단하고 직관적인 KNN부터 적용해보는 것으로 시작해보았다. training set을 8:2로 나눠서 validation 실험을 진행해본 결과, 그냥 raw data를 사용하고 그냥 KNN을 사용하게 되면, 결과가 50% 정도 밖에 나오지 않았다. 여기에서 내가 해볼 수 있는 가장 간단한 개선은 k의 개수를 조절하는 것과 metric을 바꾸는 것, 그리고 input data를 바꾸는 것이다.</p>


<h5>첫 번째 아이디어 KNN</h5>


<p>K를 이리저리 조절해보아도 크게 차이가 나지는 않았다. 잘 선택하니 55% 언저리도 나오기는 했지만, K와는 다른 문제가 있어보였다. 그 문제를 해결하기 위해 먼저 input data를 5차원 데이터로 바꿔보았다. 즉, 지금은 5개의 카드 각각에 대해 rank와 suit를 따로 표현하여 10차원 데이터로 표현이 되지만, rank와 suit를 합쳐보는 것이다. 그리고 knn을 해보니 결과는 50% 미만. 왜 이럴까 생각을 해보니 당연히 1~52까지 카드가 배치가 될텐데, 이 숫자는 인덱스를 의미하지 진짜 &#8216;거리&#8217;를 의미하지 않기 때문에 문제가 발생한다는 것을 알 수 있었다. 따라서 input data는 real number여서는 안되고, 52차원짜리 binary로 개선해야겠다는 생각을 하게 되었다. 즉, 원래 10차원 real value data가 이제 52차원 binary data로 바뀌게 되었다. 같은 카드가 두 번 나오지 않기 때문에 반드시 binary임이 보장된다. 그러나 52차원은 너무 크기 때문에 PCA를 사용하여 차원을 조금 더 낮은 차원으로 보내보기도 하였다. 정리하자면 인풋을 binary로 정의하고, distance는 cosine으로 바꿔보고 PCA에 사용할 low dimension을 잘 선택하고 K를 잘 조절해본 결과 8:2 세팅에서 68.233%까지 성능이 향상되는 것을 볼 수 있었다. 2015년 3월 20일 새벽 2시 반, 리소스 문제로 인해 아직 test data를 서버에 제출하지는 못하였다. 아무래도 코드를 개선해야할 것 같다.</p>


<h5>두 번째 아이디어 &#8216;rule&#8217; learning</h5>


<p>그러나 이런 식의 방법을 사용해 KNN의 성능을 개선시킬 수는 있지만, 이는 근본적이 해결책이 되지 못한다. 문제를 해결하기 위해서는 조금 더 문제를 잘 정의하고, 좋은 알고리즘을 찾아야만 한다. 즉, 문제를 어떻게 모델링하느냐에 대한 이슈가 아직 해결되지 않은 것이다. 우리는 무엇을 찾아야하는가? 나는 여기에서 한 가지 생각을 했다. 결국 우리가 찾고 싶은 것은 Poker rule이다. 즉, 만약 우리가 rule에 대한 전체 domain을 정의할 수 있고, 각각의 rule에 대한 performance measure를 정의할 수 있다면, 가장 좋은 rule을 찾는 알고리즘을 디자인 할 수 있지 않을까? 생각이 여기까지 진행되니 바로 자연스럽게 다음 질문이 나오게 되었다. &#8216;rule&#8217;은 어떻게 정의해야할까? 처음에는 이렇게 생각했다. 결국 카드게임은 카드들을 비교하여 좋은 &#8216;조합&#8217;을 가진 사람이 이기는 게임이다. 따라서 나는 5장의 카드들로 이루어진 모든 pair 조합만큼의 dimension을 가지고 (즉, \(5 \choose 2\) = 10개) 두 카드를 비교하는 rule의 개수가 \(n\)개라고 했을 때 각각의 piar들에게 n개 중의 하나에 대응시키게 되면, 우리는 10차원 real value vector로 rule을 표시할 수 있지 않을까? 그런데 문제가 있었다. 앞에서 본 것 처럼, rule의 개수를 \(n\)개 라고 한다고 해서, 1번째 rule과 100번째 rule이 어떤 우열관계가 있는게 아니다. 따라서 결국 binary로 만들어야할 것 같다. 그래서 어떻게 두 카드 사이의 rule을 binary로 만들 수 있을지 가만 생각해보니, 결국 rule이라는 것은 현재 이 카드가 다른 카드 어떤 것과 대응되는지 되지 않는지가 아닌가라는 생각이 들었다. 예를 들어 우리가 52개의 카드를 가지고 있을 때, 같은 숫자를 가진다는 rule은, 총 52 * 52개의 모든 카드 조합 중에서 정확하게 4*13 = 52개의 조합들을 의미하는 것이 아닐까. 다시 말해서, (1, 1), (1, 14), (1, 27), (1,40), (2,2), &#8230; 이런 식으로 정의가 된다고 생각할 수 있는 것이다. 그렇게 생각하게 되면 총 52*52 = 2704개의 rule이 필요하게 되더라.</p>


<p>이렇게 문제를 단순화시키고나니 문제의 목적은 &#8216;rule&#8217;을 찾는 것이며, rule은 다음과 같은 binary operation으로 정의할 수 있었다. 먼저 우리는 임의의 두 개의 카드 pair에 대해 총 52 * 52 = 2704개의 rule을 가진다. 왜냐하면 1부터 52까지 범위를 가지는 숫자 두 개를 연결하는 방법이 52*52개 만큼 있기 때문이다. 그리고 그 중 k개의 rule을 뽑아내고, k-1개의 binary operation을 사용해 각 점수에 대한 rule을 구해낸다. 예를 들어 카드의 값을 1부터 52까지 대응시켰을 때, 1 pair rule을 이 operation으로 나타내보면, (A 카드의 값이 1이고, B 카드의 값이 1인 rule) or (A 카드의 값이 2이고, B 카드의 값이 2인 rule) or &#8230;. 이 될 것이다. 이 경우 k는 13이고, k-1개의 operation들은 모두 or이다. 마찬가지로 2에 대해서 rule을 구하고, 계속해서 9까지 rule을 구한다.</p>


<p>우리가 모든 card set을 가지고 있다면 위의 문제를 direct하게 풀면 문제를 완벽하게 풀 수 있지만, 그렇지 않기 때문에 overfitting이 일어나게 된다. 따라서 나는 이 문제를 어떻게 더 generalize시킬 수 있느냐를 고민해보았다. 이 문제는 Rule의 총 개수를 2704개 보다 훨씬 더 적은 양으로 mapping할 수 있다면 비교적 쉽게 풀 수 있다. 나머지는 전부 training data에서 optimization으로 해결할 수 있는 문제들이니까.</p>


<p>그러나 이윽고 나는 다른 문제에 부딪히게 되었다. Rule이 이것보다 훨씬 많다는 것을 깨달았기 때문이다. Rule은 and operation으로만 이어지는 것이 아니라 or operation으로도 이어질 뿐 아니라 연산 순서 역시 중요했었다. 예를 들어 Rule = (Rule 1 and Rule 2 and Rule 3) or Rule 4 or (Rule 5 and Rule 6) 같은 지저분한 rule도 있을 수 있었기 때문이다. 즉, 내가 문제를 너무 단순하게 봤었다. 이렇게 문제를 생각하게 되면 rule에 대한 강력한 assumption이 없는 이상 더 이상의 generalization은 불가능하고, 직접적으로 rule을 learning하는 것이 어렵다는 결론에 도달하였다.</p>


<h5>세 번째 아이디어 grapical probability model</h5>


<p>다음으로 내가 생각했던 것은, 확률 모델로 문제를 디자인해보자는 것이었다. 주어진 데이터를 \(X\)라고 한다면 \(p(y|X)\)가 제일 큰 \(y\)를 고르면 되도록 말이다. 여러가지 확률 모델들이 있지만 (예를 들어 naive baysian도 확률모델이다) 내가 생각했던 아이디어는 각각의 \(y\)마다 확률 모델을 만들고, \(X\)가 \(y\)인지 아닌지 binary로 판단하게 하는 방식이었다. 이렇게 생각한 이유는, 실제로 rule이 중복해서 나올 수 있기 때문이다. 예를 들어 Triple은 two pair이기도 하고, four card는 triple이면서 two pair이기도 하다. 이렇게 모델을 정의하고 모든 \(y\)에 대해 지금 \(X\)를 대입한 후, 특정 threshold를 넘는 \(y\) 중에서 가장 큰 숫자를 고르게 하는 것이다.</p>


<p>이 아이디어를 실행하기 위해 가장 중요한 것은 어떤 probability model을 선택하느냐는 것이었다. 내가 처음 생각한 아이디어는 RBM이었지만, RBM은 주어진 데이터에 대한 joint probability만 표현할 수 있지, 어떤 데이터가 그것에 속하지 않는지 learning하는 것이 어려웠다. 이 문제가 근본적으로 기존 방법들로 접근하기 어려운 이유는 sample bias가 너무 심하기 때문이다. 즉, 0점 짜리가 거의 대부분에 속하고 그 다음으로 1점, 2점&#8230; 순서로 가기 때문에 각 sample들이 uniform하게 분포하지 않고, 그 때문에 일반적인 방법으로 접근하게 되었을 때 과도하게 0에 치중된 모델이 나오게된다는 점이었다. 나는 이 때문에 각각의 점수별로 모델을 따로 가져가고, 대신 binary로 모델을 풀고 bias를 최대한 해결할 수 있는 방법을 생각해보려 했었다.</p>


<p>그러나 conditional probabilty를 이런 방식으로 leanring할 수 있는 모델이 (학생들이 알 수 있거나 생각할 수 있을법한) 모델 중에서는 도저히 떠오르지 않아서 이 방법도 선택하지 않게 되었다.</p>


<h5>마지막 아이디어 결국 다시 KNN</h5>


<p>그래서 결국 다시 KNN으로 돌아가게 되었다. 학생 중 하나가 나에게 sorting을 하는 방법에 대해서 물어봤었고, suit를 차라리 없애는 것이 훨씬 결과가 잘 나온다는 얘기를 들었기에 그렇게 한 번 진행해보았다. 그런데 결과가 너무 잘 나오는게 아닌가 (0.96908) 심지어 1-NN을 선택했고, 내가 한 것이라고는 suit를 무시하고 rank만 5개 고르고 5개를 sorting한 것을 넣은 것에 불과했는데. 도저히 이해가 안되서 하루쯤 생각해보니 이게 왜 잘 동작하는지 알 수 있었다. KNN 세팅에서 binary로 바꾸고 하는 과정을 거치지 않았을 때 50%의 성공을 거뒀던 것에 비해 너무 말도 안되는 성능 향상이었기 때문이다. 게다가 1-NN이 아니라 2-NN이나 3-NN의 퍼포먼스가 형편없다는 것도 이해할 수 없었다.</p>


<p>실제 poker rule에서 suit와 카드 순서에 영향을 받는 룰은 Flush 와 straight, 그리고 Royal flush 뿐이지만, 이 녀석들은 거의 나타나지 않는 녀석들이었다. 이렇게 input 데이터를 sorting하게 되면 전체 경우수는 오직 ( 13 choose 5 - 13 ) = 1274 개 뿐이다. 13을 뺸 이유는 같은 rank가 같은 경우는 오직 4개 뿐이기 때문이다. 그런데 training 데이터의 개수는 25,000개 넘기 때문에 엄청 높은 확률로 rank만 고려했을 때 training 데이터와 정확하게 같은 training 데이터가 존재하게 되는 것이다. 이 말은 다시 말하면 왜 K=1 일때만 제대로 동작하는지를 증명하는 말이기도 하다. 즉, 아주 높은 확률로 항상 &#8216;정확하게 같은&#8217; 데이터 셋을 training데이터에서 고를 수 있으니 당연히 K=1을 선택해야만 제대로 결과가 동작하게 되는 것이다.</p>


<p>나는 여기까지만 시도하고 그만두었다. 이때부터 엄청 바빠지기도 했고, 내가 처음 걸었던 조건처럼 poker에 대한 가정을 최소화한 상태로 학생들도 알 수 있는 간단한 아이디어로 이 문제를 해결하는 것이 어렵다고 느꼈었기 때문이다. 그래도 최종 결과를 96% 정도 달성하였으니 이 정도면 나름 나쁘지 않은 결과가 아닐까 생각한다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (20) Reinforcement Learning]]></title>
    <link href="http://SanghyukChun.github.io/76/"/>
    <updated>2015-09-21T15:33:00+09:00</updated>
    <id>http://SanghyukChun.github.io/76</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="http://SanghyukChun.github.io/57">첫 글</a>에서 Machine Learning은 크게 세 가지로 구분된다는 얘기를 했었지만, 지금까지 다뤘던 주제들은 모두 supervised learning이거나 unsupervised learning이었다. Reinforcement learning은 그 둘과는 구분되는 명백히 다른 task이지만, machine learning에서 그만큼 대중적인 분야는 아니다. 아직까지 reinforcement learning을 사용한 적절한 application이 많이 제안된 것도 아니라서 practical하게 많이 사용지도 않는다. 그러나 reinforcement learning을 사용하면 supervised, unsupervised learning와는 전혀 다른 방법으로 문제를 접근하는 것이 가능하다. 최근 deep learning과 reinforcement learning을 결합한 재미있는 연구주제들도 나오고 있는 만큼 (<a href="http://SanghyukChun.github.io/90">Atari 리뷰</a>, <a href="http://SanghyukChun.github.io/91">Visual Attention 리뷰</a>), 앞으로 더 재미있는 방향으로 연구될 수 있는 주제가 아닐까 생각한다.</p>


<p>이 글은 Andrew Ng. 교수가 Stanford에서 강의하는 CS229 Machine Learning 수업의 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">lecture note</a>를 바탕으로 쓰여졌다. 이 글이 조금 부족하다고 느끼는 경우에는 해당 reference를 읽어보면 큰 도움이 될 것으로 생각된다.</p>




<h5>Reinforcement Learning: Problem Definition</h5>


<p>Supervised learning은 주어진 데이터의 label을 mapping하는 function을 찾는 문제이다. 이 경우 알고리즘은 얼마나 label을 정확하게 분류하느냐 혹은 정해진 loss function을 minimize시킬 수 있느냐에만 초점을 맞추어 모델을 learning하게 된다. 분명 supervised learning은 상당히 많은 application들에 응용될 수 있는 방법이다. 하지만 모든 문제들이 이런 방식으로 해결할 수 있는 것은 아니다. 예를 들어 2족 보행을 하는 알고리즘을 디자인한다고 생각해보자. 우리가 알고 싶은 것은 어떻게 로봇의 관절들을 움직여야 로봇이 넘어지지 않고 잘 걸을 수 있을까이다. 이 경우 우리는 관절의 움직임을 control하는 function을 learning해야한다. 이 문제를 머신러닝으로 풀기 위해서는 어떻게 문제를 정의해야할까? <a href="http://SanghyukChun.github.io/57">첫 글</a>에서 머신러닝 문제는 (1) 데이터 (2) output (3) target function (4) loss를 minimize하는 algorithm이 필요하다고 언급했었다. 먼저 데이터는 현재 관절들의 상태(각도, 위치 등)와 주변 환경(흙 위인지 아스팔트 위인지 앞에 벽이 있는지 등등)을 데이터라고 정의하자. 우리는 지금 보행을 learning하는 알고리즘을 찾고 있으므로 원하는 output은 지금 상태 다음의 관절 상태가 될 것이다. 즉, [f: 현재 관절 상태, 환경 -> 다음 관절 상태]라는 target function까지 정의할 수 있다. Loss는 특정 시간 이후 얼마나 많이 걸었는지 등으로 판단할 수 있을 것이다.</p>


<p>그렇다면 이 문제는 supervised learning이나 unsupervised learning으로 풀 수 있을까? 데이터만 무한하게 있다면 가능할지도 모르지만 현실적으로는 그럴 수 없다는 것을 알 수 있다. 왜냐하면 (data, output)의 조합이 너무 많기 때문에 의미있는 learning을 할 수 있을 정도로 많은 데이터를 모을 수 없기 때문이다. 즉, 어떤 action이 &#8216;correct&#8217; action인가 판단하는 것이 불가능하다. 대신에 이렇게 생각해보면 어떨까? 우리가 알고 싶은 것은 관절 상황과 환경이 주어졌을 때 로봇이 어떻게 action을 취해야하는가라는 policy이다. 매 action을 주어진 policy를 통해 시행하고 나면 다음 state가 정의된다. 만약 성공적으로 걸었다면 +1 점을 주고 넘어졌다면 -1점을 주는 방식을 통해 매 action의 reward를 정의할 수 있을 것이다. 그렇다면 static한 데이터 셋에서 거의 무한하게 많이 필요한 (data, output)를 사용해 learning하는 방법 대신에, 직접 매 순간 action을 실행해 reward를 받으면서 최종적으로 맨 마지막에 모든 reward의 합이 가장 좋게 만드는 policy를 learning하는 것이다.</p>


<p>이런 식으로 reinforcement learning을 high level로 설명할 수 있다. 그렇다면 RL을 조금 더 formal하게 정의해보자.</p>




<h5 id="76-mdp">Markov Decision Process (MDP): Problem definition</h5>


<p>앞에서 설명한 방식대로 RL을 정의하면 RL problem은 정말 여러가지 형태로 정의할 수 있지만, 보통 RL문제를 푼다고 하면 Markov Decision Process (MDP)를 의미한다. MDP는 \( (S, A, \{P_{sa}\}, \gamma, R ) \) 이라는 것들의 튜플이다. 각각에 대해 알아보도록 하자.</p>




<ul>
    <li><p>\(S\) - state들의 set을 의미한다. 앞에서 예를 든 2족 보행 로봇의 경우 모든 가능한 관절의 상태와 환경 등이 state가 된다. 참고로 state와 다음에 기술한 action의 개수가 유한하다면 \(|S| < \inf, |A| < \inf\), 주어진 MDP를 finite MDP라고 부른다.</p></li>
    <li><p>\(A\) - action들의 set을 의미한다. 2족 보행 로봇의 경우 어떻게 관절을 control할 것인가를 의미한다.</p></li>
    <li><p>\(P_{sa}: (s_t, a_t) \to s_{a_t}\) - State의 transition probability로, 특정 state에서 특정 action을 취했을 떄 다음 state는 어떤 state가 될지에 대한 확률 값이다.</p></li>
    <li><p>\(R: S \times A \to \mathbb R\) - 주어진 state에 action을 수행했을 때 얻게 되는 reward를 function으로 표현한 것이다. Reinforcement learning의 목표는 시간이 \(T\)만큼 흘렀을 때 최종적으로 얻게 되는 모든 reward들의 합을 (정확하게는 그것의 expectation을) maximization하는 policy를 learning하는 것이다.</p></li>
    <li><p>\(\gamma \in [0,1) \) - 앞에서 설명한 reward의 discount factor로, 시간이 지날수록 reward의 가치를 떨어뜨리고, 처음 받은 reward의 가치를 더 키워주는 역할을 한다. 즉, time \(t\)에서 얻은 reward를 \(r_t\)라고 했을 때, 전체 reward \(R_{tot}\)는 \(\sum_{t=0}^T \gamma^t r_t\)가 된다.</p></li>
</ul>




<p>MDP의 dynamics는 다음과 같은 식이다. 먼저 initial state \(s_0\)에서 어떤 초기 action \(a_0\)를 수행하게 된다. 이 행동으로 인하여 주어진 probability \(P_{s_0 a_0}\)에 따라 다음 state \(s_1\)이 확률적으로 결정된다. 그리고 그 결과로 reward \(R(s_0, a_0)\)를 얻게 된다. 이를 다시 \(s_1\)에 대해 반복하면서 state가 terminal state에 도달할 때 까지 이 과정을 반복하게 된다. 이때, MDP의 Markov property 때문에 다음 step의 reward와 transition probability는 오직 지금 state와 지금 action에 의해서만 결정된다.</p>


<p>\[s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} \ldots.\]</p>


<p>이때, 앞에서 설명한 바와 같이 매 action을 취할 때 마다 reward가 결정된다. 이때 최대한 빠르게 좋은 reward를 받을수록 좋기 때문에 나중에 얻은 reward보다 일찍 얻은 reward의 값이 같더라도, 일찍 얻은 reward가 더 valuable하다고 가정한다. 이것을 우리는 discount factor를 통해 조절하게 되며, 그 결과 우리가 maximization하고 싶은 최종 reward는 discount factor \(\gamma\)에 의해 다음과 같이 결정된다. (참고로 이 값을 sum of discounted reward라고 부른다.)</p>


<p>\[R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots.\]</p>


<p>하지만 아무리 같은 state와 action으로 시작했다고 하더라도 이 과정은 전부 \(P_{sa}\)에 의해 확률적으로 결정되는 값이기 때문에, 실제로 maximization하기 위한 target은 그 값의 expectation으로 주어진다.</p>


<p>\[\mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots ].\]</p>


<p>우리가 위 expectation을 maximization하기 위해 learning해야하는 것이 바로 policy \(\pi: S \to A\) 이다. Policy는 state에서 action으로 mapping되는 function이다 (즉, \(a_t = \pi (s_t)\).  앞에서 설명했던 transition probability는 현재 state와 action을 다음 state와 mapping해주는 function이었고, policy는 지금 state에서 내가 어떤 action을 취해야하는지 mapping해주는 function인 것이다. 즉, policy가 어떻게 변화하느냐에 따라 최종 reward가 크게 바뀌게 된다.</p>




<h5 id="76-bellman">Value function and Bellman Equation</h5>


<p>그럼 어떻게 reward를 maximize하는 policy를 learning할 수 있을까? 그것을 설명하기에 앞서, 먼저 policy \(\pi\)의 value function \(V^\pi (s)\)이라는 것을 정의해보자. 이때 앞으로 reward \(R(s,a)\)는 state에 의해서만 결정된다고 가정하고, notation을 \(R(s)\)로 바꾸도록 하겠다. 만약 action과 state 둘 다에 의해 reward가 결정되는 경우는 앞으로 설명하게 될 value function \(V\)가 아니라 <a href="76-qfunction">action-value function \(Q\)</a>라는 것을 정의하고 그것에 대한 Bellman Equation을 구해 아래와 같은 방식을 그대로 적용하는 것이 가능하다.</p>


<p>olicy \(\pi\)의 value function \(V^\pi (s)\)은 다음과 같이 정의된다.</p>


<p>\[ V^\pi (s) = \mathbb E[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \ldots | s_0 = s, \pi ]. \]</p>


<p>이 값은 즉, 간단하게 이야기 하여 주어진 state \(s\)를 initial state로 두고 action을 policy \(\pi\)를 사용하여 고르게 되었을 때 우리가 얻게되는 reward의 expectation 값이 된다. 이렇게 정의했을 경우 fixed policy \(\pi\)에 대해 value function은 다음과 같은 관계식을 가진다. 증명은 크게 어렵지 않으므로 생략하겠다.</p>


<p>\[ V^\pi (s) = R(s) + \gamma \sum_{s^\prime \in S} P_{s \pi(s)} (s^\prime) V^\pi (s^\prime). \]</p>


<p>이 관계식을 Bellman Equation이라고 부르며, 이 관계식을 통해 우리는 \(V^\pi (s)\)과 다음과 같은 두 가지 성분으로 표현된다는 것을 알 수 있다. 첫째로 immediate reward \(R(s, \pi(s))\)이다. 이 값은 우리가 처음 state \(s\)에서 바로 얻을 수 있는 reward를 의미한다. 다음으로 future reward의 expectation이다. 이 값에 discount factor를 곱하고 immediate reward를 더하게 되면 우리가 원하는 \(V^\pi (s)\)를 구하는 것이 가능하다. 이때, future reward term은 사실 \( \mathbb E_{s^\prime \sim P_{s \pi(s)}} [V^\pi (s^\prime)] \) 으로 표현할 수 있는데, 다시 말해 future reward term은 처음 state \(s\)에서 policy \(\pi\)로 정해진 다음 state \(s^\prime\)의 distribution에 대한 sum of discounted reward의 expectation 값과 같다는 것을 알 수 있다. 그러므로 두 번째 term은 MDP의 한 step이 지나고 난 이후에 발생하는 모든 sum of discounted reward들의 expectation이라는 것을 알 수 있는 것이다.</p>


<p>Bellman Equation을 사용하면 finite MDP에 대해 value function \(V^\pi (s)\)를 효율적으로 계산할 수 있다. 만약 finite MDP에 대해 문제를 풀고 있다고 가정해보자. 그렇다면 주어진 모든 state \(s\)에 대해 \(V^\pi (s)\)의 Bellman Equation을 적는 것이 가능한데, 이렇게 되면 우리는 \(|S|\)개의 linear equation과 \(|S|\)개의 variable들 (이 경우는 각 state에 대한 \(V^\pi (s)\)들)이 존재하기 때문에 간단한 연립방적식으로 value function의 값을 찾는 것이 가능하다.</p>


<p>하지만 우리가 처음부터 원했던 것은 optimal policy \(\pi^*\)이지, 주어진 \(\pi\)에 대한 expectation of sum of discounted reward가 아니다. 하지만 이 optimal policy 역시 Bellman Equation을 사용하면 계산할 수 있다. 이것을 어떻게 하는지 설명하기 전에 앞서, 먼저 optimal value function \(V^* (s)\)를 다음과 같이 정의해보자.</p>


<p>\[ V^*(s) = \max_\pi V^\pi (s). \]</p>


<p>즉, optimal value function은 모든 policy \(\pi\)에 대한 value function \(V^\pi (s)\) 중에서 가장 reward를 maximize시키는 policy를 통해 얻게 된 reward가 된다. Optimal value function 역시 Bellman Equation을 증명할 수 있는데, 그 식은 다음과 같다.</p>


<p></p>

<p>\[ V^* (s) = R(s) + \max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>


<p></p>

<p>앞의 term은 위와 동일하고, 두 번째 term은 모든 expected future sum of discounted reward를 action \(a\)에 대해 maximize한 결과이다. 즉, 모든 action 중에서 reward를 가장 maximize하는 action을 선택하였을 때 얻게되는 reward의 값이다. 그런데 그런 action을 고르는 방법이 바로 optimal policy \(\pi^*\)이므로, optimal policy는 다음과 같이 구할 수 있다.</p>


<p>\[ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>


<p>모든 state \(s\)와 모든 policy \(\pi\)에 대해 우리는 다음과 같은 관계식을 얻을 수 있다.</p>


<p>\[V^*(s) = V^{\pi^*}(s) \geq V^\pi (s).\]</p>


<p>첫번째 관계식은 optimal policy \(\pi\)에 대한 value function \(V^{\pi^*}(s)\)와 optimal value function \(V^*(s)\)가 모든 state \(s\)에 대해 equivalent하다는 것을 보여준다. 이 내용이 중요한 이유는, 초기 state가 무엇인지와 관계없이 항상 같은 optimal policy \(\pi^*\)를 사용해 optimal value function을 구할 수 있다는 의미가 되기 때문이다. 즉, 만약 optimal policy를 구할 수 있는 algorithm이 있다면 그 알고리즘의 initial state로 어느 state를 고르더라도 우리는 항상 같은 policy를 얻게된다는 사실을 암시한다. 그리고 두 번째 equation은 모든 policy \(\pi\)에 대한 value function보다 \(V^{\pi^*}(s)\)의 값이 더 크거나 같다는 것을 의미한다. 만약 optimal policy를 구하는 algorithm이 value function을 monotonically increase 시키는 방향으로 learning한다고 했을때, update되는 값의 upper bound가 존재하므로 항상 converge하게 된다는 것을 알 수 있다.</p>


<p>Finite MDP의 optimal policy를 구하는 대표적인 두 알고리즘으로는 value iteration과 policy iteration이라는 알고리즘이 존재한다. 두 알고리즘은 이름에서 알 수 있듯 모두 iterative algorithm이며, 위에서 언급한 intuition이 그대로 적용되는 알고리즘들이다. 즉, initial state에 invariant하며 iteration 동안 value function이 monotonically increase한다. 그리고 그 값이 converge하게 되면 우리가 원하는 optimal policy를 구할 수 있다.</p>




<h5>Value Iteration</h5>


<p>Value iteration 알고리즘은 다음과 같다.</p>


<ol>
    <li><p>Initialize \(V(s) = 0, ~\mbox{ for all } s\).</p></li>
    <li>Repeat until converge</li>
    <p>\[V(s) = R(s) + \max_{a\in A} \gamma \sum_{a^\prime} P_{sa} (s^\prime) V(s^\prime), ~\mbox{ for all } s. \]</p>
</ol>


<p>이 알고리즘은 앞서 설명했던 Bellman Equation에서 optimal value function의 관계식을 iterative하게 반복하면서 찾아나가는 알고리즘이다. 이 알고리즘의 두 번째 state는 synchronous update와 asynchronous update 총 두 가지 방법으로 접근이 가능하다. 먼저 synchronous update는 모든 \(s\)에 대해 value function \(V(s)\) 값 을 계산하고 그 값들을 한 번에 update하는 방법이고, asynchronous update는 한 state \(s\)에 대해 \(V(s)\)를 구하고 바로 update를 하는 방법이다. 쉽게 생각하면 asynchronous update는 stochastic gradient descent같은 방법이라 생각하면 된다. 이 두 가지 방법 모두 finite하고 polynomial time 안에 \(V\)가 optimal value function \(V^*\)으로 수렴한다는 것을 증명할 수 있다. 이렇게 구해진 \(V^*\)를 사용하면 앞에서 구했던 다음 관계식을 통해 optimal policy를 구할 수 있다.</p>


<p>\[ \pi^*(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \]</p>




<h5>Policy Iteration</h5>


<p>이번에는 policy iteration 알고리즘에 대해 살펴보자. 알고리즘은 다음과 같다.</p>


<ol>
    <li><p>Initialize \(\pi\) randomly</p></li>
    <li>Repeat until converge</li>
    <ul>
        <li><p>Let \(V = V^\pi\).</p></li>
        <li><p>For each state \(s\), let \(\pi(s) = \arg\max_{a\in A} \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) V^* (s^\prime). \)</p></li>
    </ul>
</ol>


<p>Value iteration이 value function의 값을 update하는 알고리즘이라면 policy iteration은 policy를 udpate하는 iterative algorithm이다. 두 알고리즘 모두 Bellman equation을 통해 얻어지는 알고리즘이다. Policy iteration에서 \(\pi(s)\)를 업데이트하는 방식을 주어진 value function \(V\)에 대해 greedy한 policy update rule이라고 부른다. Policy iteration 역시 polynomial time 안에 optimal policy로 수렴하게 된다.</p>


<p>Value iteration과 policy iteration은 모두 MDP를 해결하는 알고리즘이며, 둘 중 무엇이 더 좋냐를 비교할 수는 없다. 그러나 일반적으로 small MDP에 대해서는 policy iteration이 빠른 시간 안에 효과적으로 수렴하고, large MDP의 경우에는 policy iteration에서 greedy policy rule update가 비효율적일 수 있기 때문에 value iteration으로 문제를 푸는 것이 computationally 좀 더 efficient하다고 한다.</p>


<p>다만 value iteration과 policy iteration은 이론적으로 optimal value function을 계산할 수 있도록 보장하는 알고리즘이기는 하지만, 실제 세상의 large MDP에 적용하기에는 모든 state와 action에 대한 경우 수를 계산하는 이런 알고리즘들은 다소 비효율적이다. 대신 다른 방법으로 value function을 update할 수 있는 알고리즘을 제안하기도 하는데, 예를 들면 지난 번에 리뷰했던 <a href="http://SanghyukChun.github.io/90">Playing Atari With Deep Reinforcement Learning (NIPS 2013)</a> 논문을 예로 들 수 있을 것 같다.</p>




<h5 id="76-qfunction">Action Value Function</h5>


<p>Reward가 state, action 둘 다에 의해 결정될 경우, value function \(V^\pi (s)\)가 아니라 action value function \(Q^\pi (s,a)\)를 사용하여야한다. Q function은 다음과 같이 정의할 수 있다.</p>


<p>\[ Q^\pi (s,a) = \mathbb E[ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \ldots | s_0 = s, a_0 = a, \pi ]. \]</p>


<p>이 값을 사용하게 되면 initial state와 action에 대해 앞에서 value function에 대해 계산했던 것들을 그대로 반복할 수 있다. 먼저 \(Q^* (s,a) = \max_\pi Q^\pi (s,a)\)이고, optimal action value function의 Bellman Equation은 다음과 같이 주어진다.</p>


<p>\[ Q^* (s,a) = R(s,a) + \gamma \sum_{s^\prime \in S} P_{sa} (s^\prime) \max_{a\in A} Q^* (s^\prime, a). \]</p>


<p>남은 부분은 value function으로 진행했던 내용과 동일하게 진행하면 된다.</p>




<h5>Learning Model for MDP</h5>


<p>지금까지 앞에서 살펴봤던 내용은 모두 MDP의 state transition probability와 reward function이 전부 주어진 상태라고 가정하고 문제를 푸는 방법이었다. 하지만 실제로는 transition probability와 reward가 직접적으로 알려져있지않고, 실제 action을 수행하기 전까지 알 수 없는 것들이 훨씬 많다. 이 경우 data를 통해 transition probability와 reward function을 estimate해야한다. 이 경우 각각의 state에 대해 모든 action을 반복적으로 수행하면서 probability의 approximation 값을 구하고, reward 역시 같은 방법으로 계산해야한다.</p>




<h5>정리</h5>


<p>이 글에서는 reinforcement learning의 가장 기본적인 모델인 MDP에 대해 다루었다. MDP는 state, action, reward function, transition matrix와 discount factor로 구성된 튜플이며, optimal policy를 구하기 위해서 value function이라는 개념을 도입하고, 이 optimal value function을 계산할 수 있다면 optimal policy를 구할 수 있다. Optimal value function을 update하기 위해서 Bellman Equation이라는 관계식을 사용해 value iteration과 policy iteration이라는 알고리즘까지 살펴보았다. 이 경우 모든 reward와 transition matrix는 이미 알려져있다고 가정하였고, 만약 모르는 경우 finite MDP에서는 실제 estimation을 통해 model을 leanring해야한다는 얘기까지 하였다. 실제로 MDP 문제를 다루게 될 경우 이것보다 훨씬 복잡한 문제를 다뤄야할 일이 많지만, 근본적으로는 value iteration 등을 사용하여 optimal value function 혹은 optimal action value function의 값을 구해 optimal policy를 구한다는 기본적인 아이디어는 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 21일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">Reinforcement Learning Lecture Note &ndash; Stanford CS229 Lecture Note by Andrew Ng</a></li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 &ndash; RBM, DNN, CNN</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (19) Deep Learning - RBM, DBN, CNN]]></title>
    <link href="http://SanghyukChun.github.io/75/"/>
    <updated>2015-09-21T00:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/75</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="http://SanghyukChun.github.io/74">이전 글</a>에서 기본적인 neural network에 대한 introduction과, feed-forward network를 푸는 <a href="http://SanghyukChun.github.io/74#backprop">backpropagtion 알고리즘</a>과 optimization을 하기 위해 기본적으로 사용되는 <a href="http://SanghyukChun.github.io/74#sgd">stochastic gradient descent</a>에 대해 다루었다. 이 글에서는 deep learning이란 것은 정확히 무엇이며, 왜 deep learning이 최근 크게 급부상하게 되었는지에 대해 시간 순으로 다룰 것이다. 이 글에서는 unsupervised learning의 한 방법인 Redtrited Boltzmann Machine (RBM)과 그것을 사용한 Deep Belief Network (DBN)에 대해 다룰 것이다. 또 다른 unsupervised learning 방법 중 하나인 (denoising) auto-encoder 역시 다루고자하였으나, 이 내용까지 전부 다루기에는 내용이 너무 길어져서 이 글에서는 생략하였다. 주의할 점은, 2007년 2008년에 막 deep learning이라는 이름으로 나왔던 연구들인 unsupervised pretraining 방법들은 현재 거의 쓰이지 않는 연구방법들이라는 것이다. 때문에 지금까지도 사용되는 조금 더 practical한 방법들인 neural network regularization (예를 들어서 ReLU, Dropout 등), optimization method (momentum, adagrad 등)에 대해서는 이 이후 한 번의 posting을 더 하여 다루도록 하겠다. 추가로, 오래된 work임에도 불구하고 아직도 computer vision 쪽에서 엄청나게 많이 사용하는 Convolutional Neural Network (CNN) 에 대해서도 다루도록 하겠다.</p>




<h5>Deep Neural Network</h5>


<p>Deep learning이라는 것은 사실 deep neural network를 의미하는 것이다. Neural network에 대해서는 <a href="http://SanghyukChun.github.io/74">이전 글</a>에서 설명하였고, 그럼 deep이란 무엇인가하면, 다른게 아니라 feed-forward network에서 가운데 hidden layer가 1개 보다 많으면 &#8216;deep&#8217;하다고 말하는 것이다. 요즘은 layer를 무조건 1개보다는 많이 쌓기 때문에 요즘 나오는 neural network 연구는 모두 deep learning 연구라고 생각하면 된다.</p>


<p>그런데 사실 deep learning은 전혀 새로운 연구분야가 아니고 이미 몇 십년 전에 기본적인 연구가 끝난 분야이다.</p>


<ul>
<li>1958 Rosenblatt proposed perceptrons</li>
<li>1980 Neocognitron (Fukushima, 1980)</li>
<li>1982 Hopfield network, SOM (Kohonen, 1982), Neural PCA (Oja, 1982)</li>
<li>1985 Boltzmann machines (Ackley et al., 1985)</li>
<li>1986 Multilayer perceptrons and backpropagation (Rumelhart et al., 1986) 1988 RBF networks (Broomhead&amp;Lowe, 1988)</li>
<li>1989 Autoencoders (Baldi&amp;Hornik, 1989), Convolutional network (LeCun, 1989) 1992 Sigmoid belief network (Neal, 1992)</li>
<li>1993 Sparse coding (Field, 1993)</li>
</ul>


<p>이렇듯 이미 가장 중요한 기초적인 연구는 예전에 다 끝났다. 지난 글에서 설명한 backpropagation 알고리즘은 이미 1986년 나온 알고리즘이고, 1989년에 나온 convolutional network가 요즘도 vision 분야에서 늘 사용하는 그 CNN이다. 그런데 정작 deep learning은 2000년도 중반이 지나고나서야 주목을 받기 시작했다. 왜 그랬을까?</p>




<h5>Why Deep Learning?</h5>


<p>Deep learning이 예전에 &#8216;사기꾼&#8217; 취급을 받았던 이유는 크게 세 가지 이유가 있었다. 먼저 &#8216;deep&#8217; learning에 대한 이론적인 결과가 전무했다는 점 (network가 deep 해지면 문제가 더 이상 convex해지지 않는데, 이 상태에 대해 좋은 convergence는 어디이며 어떤게 좋은 initialization인가 등에 대한 연구가 전무하다. 즉, learning하면 overfitting이 너무 심하게 일어난다), 둘째로 이론적으로 연구가 많이 진행되어있는 &#8216;deep&#8217; 하지 않은 network (perceptron이라고 한다) 는 xor도 learning할 수 없는 한계가 존재한다는 점 (linear classifier라 그렇다). 마지막으로 computation cost가 무시무시해서 그 당시 컴퓨터로는 도저히 처리할 엄두조차 낼 수 없었다는 점이다.</p>


<p>그렇다면 지금은 무엇이 바뀌었길래 deep learning이 핫해진걸까? 가장 크게 차이 나는 점은 예전과는 다르게 overfitting을 handle할 수 있는 좋은 연구가 많이 나오게 되었다. 처음 2007, 2008년에 등장했던 unsupervised pre-training method들 (이 글에서 다룰 내용들), 2010년도쯤 들어서서 나오기 시작한 수많은 regularization method들 (dropout, ReLU 등). 그리고 과거보다 하드웨어 스펙이 압도적으로 뛰어난데다가, GPU parallelization에 대한 이해도가 높아지게 되면서 에전과는 비교도 할 수 없을정도로 많은 computation power를 사용할 수 있게 된 것이다. 현재까지 알려진바로는 network가 deep할 수록 그 최종적인 성능이 좋아지며, optimization을 많이 하면 할 수록 그 성능이 좋아지기 때문에, computation power를 더 많이 사용할 수 있다면 그만큼 더 좋은 learning을 할 수 있다는 것을 의미하기 때문에 하드웨어의 발전 역시 중요한 요소이다.</p>


<p>그리고 무엇보다 무시할 수 없는 것은, deep learning 기반의 approach들이 다른 방법론들을 압도하는 분야들이 있다는 것이다. 대표적인 분야가 바로 computer vison이다. 우리가 잘 알고 있는 <a href="http://yann.lecun.com/exdb/mnist/">MNSIT</a> 데이터셋은 물론이고, <a href="http://www.image-net.org/">ImageNet</a>과 그것 중에서 10개의 class만 떼어내서 만들어낸 데이터셋인 <a href="http://www.cs.toronto.edu/~kriz/cifar.html">Cifar-10</a> 대해서도 가장 잘 하고 있는건 역시 neural network이다. 아래 표들을 살펴보자 (<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">출처</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-1.png" width="600">
<img src="http://SanghyukChun.github.io/images/post/75-2.png" width="600"></p>

<p>여기서 잠시 MNIST가 28 by 28 짜리 handwrite digit을 모아놓은 데이터셋이라는 것은 많이 언급했던 내용이니 넘어가고, ImageNet competition에 대해 잠깐 언급하고 넘어가보도록 하자. <a href="http://www.image-net.org/">ImageNet</a>이라는 것은 사실 데이터셋의 이름이 아니라 매년 새로운 task가 주어지는 competition이다. 보통 실험에 사용하는건 <a href="http://image-net.org/challenges/LSVRC/2012/index">2012년 데이터셋 (ILSVRC 2012)</a>인데, training 데이터가 1000개 class에 데이터 개수는 거의 128만개 가까이 되는 엄청나게 큰 데이터 셋이다. Test data는 공개되지 않았고, 대신 validation set으로 공개된 데이터는 총 5만개짜리 데이터이다. 전체 데이터 사이즈는 거의 150GB가까이 된다. 이때 데이터를 많이 사용하는 이유는, 이때 task가 iamge classification이고, 많은 논문들이 이 때의 데이터를 기준으로 실험하기 때문인듯 하다. 예를 들어서 <a href="http://image-net.org/challenges/LSVRC/2015/">ILSVRC 2015</a>에는 더 이상 classification task가 존재하지 않고, object detection이나 object localization등의 task만 주어져있는 상태이다.</p>


<p>현재 ImageNet dataset (혹은 ILSVRC 2012)에서 state-of-art classification performance를 보이는 work은 <a href="http://SanghyukChun.github.io/88">지난 번에 review</a>했던 <a href="http://arxiv.org/abs/1502.03167">Batch Normalization</a> 논문인데, classification error는 20.1%이고, 1000개의 class 중에서 확률이 가장 높다고 판단한 top 5개 중에서 우리가 원하는 정답이 있을 확률인 top-5 error는 4.9%에 달한다. ImageNet dataset을 보면 1000개의 데이터가 전부 독립적인 것이 아니라 어느 정도 비슷한 데이터도 섞여있는 만큼, top-5 error가 5% 이하라는 것은 진짜 어마어마한 수준이라고 할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-8.png" width="600"></p>

<p>이렇듯 deep learning은 computer vision 쪽에서 압도적인 성능을 보이고 있을 뿐 아니라, 최근에는 language model, NLP, machine translation 등의 다양한 분야에서도 좋은 결과를 내고 있다. 무엇보다 deep learning 쪽 분야는 Google, MS, Yahoo 심지어는 Apple과 삼성 등에서도 투자를 많이 하고 있고 실제로 엄청나게 많은 연구들이 행해지고 그 연구들이 나오자마자 거의 바로 산업에 적용될 정도로 practical하게 많이 쓰이고 있는 분야가 되었다. 그렇기 때문에 아마도 당분간은 머신러닝 분야에서 deep learning의 강세는 이어질 것으로 보인다.</p>


<p>이 글의 남은 부분의 첫 번째 부분에서는 NIPS 2006에 발표된 Bengio 교수 연구팀의 <a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Greedy layer-wise training of deep networks</a> 연구와 NIPS 2007에 발표된 Hinton 교수 연구팀의 <a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">A fast learning algorithm for deep belief nets</a> 두 논문을 통해 제안되었던 unsupervised pretraining method 들에 대해서 다룰 것이다. 이 부분은 더 이상 practical usage로 사용되지는 않지만, deep learning의 거의 첫 번째 연구결과라고 해도 좋을 정도로 의미있는 연구결과들이므로 한 번쯤 알아둘 필요가 있다고 생각한다.</p>


<p>그리고 나머지 부분에서는 정말 오래된 연구결과이지만 아직까지도 쓰이고 있는 Convolutional Neural Network (CNN)에 대해 다룰 것이다. 이 결과는 앞서 ImageNet에서 가장 좋은 결과를 내고 있다는 Batch Normalization 에서도 기본 골격으로 사용하고 있는 vision 쪽에서는 가장 기초가 되는 엄청나게 중요한 개념이므로 마찬가지로 이 글에서 다루도록 하겠다. 만약 practical한 목적으로 이 글을 읽고 있다면 아래 unsupervised pretraining 섹션은 건너뛰고 바로 <a href="#75-cnn">CNN</a> 섹션부터 읽더라도 크게 상관없다.</p>




<h5>Problems to solve for deep learning</h5>


<p>Deep learning이 흥하기까지 수 많은 연구결과들이 있었지만, 지금처럼 deep learning이 hot하게 되기까지는 앞에서 말했던 것처럼 regularization method들이나 initialization method들, 그리고 overfitting을 최대한 피할 수 있는 optimization mehtod 등이 많이 제안되면서부터라고 할 수 있다. 이 연구들이 공통적으로 고민하고 있는 것은 <a href="http://SanghyukChun.github.io/59">overfitting</a>이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/59-1.png" width="500"></p>

<p>Overfitting은 주어진 데이터의 양에 비해 모델의 complexity가 높으면 발생하게 된다. 안타깝게도 neural network가 깊어질수록 model의 complexity는 exponential하게 증가하게 된다. 그렇기 때문에 거의 무한한 표현형을 learning할 수 있는 deep network가 좋다는 것을 다들 알고 있음에도 불구하고, 너무나 overfitting이 심하게 발생하기 때문에 neural network 연구가 멈추게 된 것이다. 하지만 2007~8년 즈음하여 overfitting을 막기 위하여 새로운 initialziation을 제안하는 work이 나오게 되는데 그 work이 바로 앞에서 설명 했던 NIPS에 발표되었던 두 work이다.</p>


<p>먼저 Restricted Boltzmann Machine (RBM) 에 대해 설명해보자.</p>


<p></p>

<h5>Restricted Boltzmann Machine (RBM): Introduction</h5>


<p>이 섹션은 상당히 수식이 많으며, 너무 복잡한 수식은 생략한 채 넘어가기 때문에 다소 설명이 모자랄 수 있다. 조금 더 관심이 있는 사람들을 위하여 아래의 참고자료들을 추천한다. 난이도 순서대로 당장 필요한 정도에 따라 읽으면 좋을 것 같은 순서대로 배치하였다. 내가 생각했을 때 알고리즘에 대한 심층적인 이론적 설명이 많은 순서대로 나열하였으니 처음부터 천천히 읽어보면 좋을 것 같다. 특히 마지막 참고자료는 상당히 이론적인 내용들을 굉장히 차근차근 어렵지 않게 담고 있으므로, RBM을 제대로 공부하고 싶다면 꼭 읽어보면 좋을 것 같다.</p>


<p></p>

<ul>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Hinton, Geoffrey. &ldquo;A practical guide to training restricted Boltzmann machines.&rdquo; Momentum 9.1 (2010): 926.</a></li>
<li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">Bengio, Yoshua. &ldquo;Learning deep architectures for AI.&rdquo; Foundations and trends® in Machine Learning 2.1 (2009): 1-127.</a> (20쪽 부터)</li>
<li><a href="http://image.diku.dk/igel/paper/AItRBM-proof.pdf">Fischer, Asja, and Christian Igel. &ldquo;An introduction to restricted Boltzmann machines.&rdquo; Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Springer Berlin Heidelberg, 2012. 14-36.</a></li>
</ul>


<p>RBM은 graphical probabilistic model의 일종으로, undirected graph로 표현되는 모델이다. Probability는 energy function의 형태로 표현이 되는데, 원래 RBM이라는 모델 자체가 Ising model이라는 물리 분야에서 많이 사용되는 모델의 일종이기 때문에 그 형식을 그대로 본 따온 것으로 보인다. RBM의 기본적인 형태는 다음과 같다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-3.png" width="200"></p>

<p>이 모델은 complete undirected bipartite graph을 띄고 있다. 이때 각각의 biparition은 visual unit들과 hidden unit들로 이루어져있으며 이 경우는 모든 unit들이 binary인 경우에 대해서만 다룬다. 따라서 visual layer와 hidden layer는 서로 internal edge가 존재하지 않고, layer들끼리 undirected fully connected된 형태를 띄고 있다. 이 모델은 graphical probabilistic model이기 때문에 각각의 visual node \(v\)들과 hidden node \(h\)들은 random variable을 의미하게 되며, 이 모델은 \(v,h\)의 joint probability를 표현하는 모델이 된다. 이 모델은 joint probability를 아래와 같은 energy function form으로 표현한다.</p>


<p>\[p(v,h) = \frac{e^{-E(v,h)}}{Z}, \mbox{ where } E(v,h) := -\sum_i a_i v_i - \sum_j b_j h_j - \sum_i \sum_j v_i w_{ij} h_j \mbox{ and } Z = \sum_{v,h} e^{-E(x,h)} \]</p>


<p>RBM의 parameter는 bais term \(a_i, b_j\)와 weight term \(w_{ij}\)로, 이 값들이 변화함에 따라 joint probability가 변하게 된다. RBM은 주어진 데이터들을 가장 잘 설명하는, 즉 \(p(v)\)의 값이 가장 커지도록 하는 parameter를 learning하게된다. 보통 이 값은 log likelihood로 다음과 같이 표현된다.</p>


<p>\[\theta = \arg\max_\theta \log \mathcal L (v) = \arg\max_\theta \sum_{v \in V} \log P(v). \]</p>


<p>이때 likelihood \(P(v)\)는 \(P(v) = \sum_h P(v,h) = \frac{1}{Z} \sum_h e^{-E(v,h)}\)로 계산할 수 있다. 이 log-likelihood 값을 maximize하는 문제는 non-convex문제이기 때문에 global optimum을 찾는 것은 불가능하고, 대신 stochastic gradient descent를 사용하여 local optimum을 계산하게 된다. SGD를 사용해 update를 하기로 하였으니, 각각의 sample \(v\)에 대한 gradient \(\frac{\partial\log p(v)}{\partial\theta}\)를 계산해보자.</p>


<p>\[ \log p(v) = -\log \frac{1}{Z}\sum_h e^{-E(v,h)} = \ln \sum_h e^{-E(v,h)} - \ln \sum_{v,h} e^{-E(v,h)}\]</p>


<p>\[ \frac{\partial\log p(v)}{\partial\theta} = \frac{\partial\ln \sum_h e^{-E(v,h)}}{\partial\theta} - \frac{\partial\ln \sum_{v,h} e^{-E(v,h)}}{\partial\theta}\]</p>


<p>\[ = -\frac{1}{\sum_h e^{-E(v,h)} } \sum_h e^{-E(v,h)} \frac{\partial E(v,h)}{\partial \theta} + \frac{1}{\sum_{v,h} e^{-E(v,h)} } \sum_{v,h} e^{-E(v,h)} \frac{\partial E(v,h)}{\partial \theta}\]</p>


<p>\[ = -\sum_h p(h|v) \frac{\partial E(v,h)}{\partial \theta} + \sum_{v,h} p(v,h) \frac{\partial E(v,h)}{\partial \theta}.\]</p>


<p>이때, \(p(h|v)\)는 \(p(h|v) = \frac{p(v,h)}{p(v)} = \frac{ e^{-E(v,h)} }{\sum_h e^{-E(v,h)}}\) 로부터 유도되는 값이다. 즉, 우리가 optimization하고 싶은 gradient는 \(\frac{\partial E(v,h)}{\partial \theta}\)의 값의 \(p(h|v)\)와 \(p(v,h)\)에 대한 expectation 값이 된다. 예를 들어 \(w_{ij}\)의 경우 \(\frac{\partial E(v,h)}{\partial w_{ij}} = v_i h_j\)이므로 \(v_i h_j\)의 \(p(h|v)\)와 \(p(v,h)\)에 대한 expectation을 구하게 된다면 우리가 목표하는 gradient를 얻는 것이 가능하다.</p>


<p>\[ \frac{\partial\log p(v)}{\partial\theta} = \sum_h p(h|v) v_i h_j - \sum_{v,h} p(v,h) v_i h_j\]</p>


<p>\[ = \sum_h p(h|v) v_i h_j -  \sum_{v,h} p(v,h) v_i h_j = p(h_j=1|v) v_j - \sum_v p(v) p(h_j=1|v)v_j\]</p>


<p>라는 결과를 얻을 수 있다. (지금은 i와 j가 고정된 상황이므로 \(\sum_h\)를 하게 되면 \(h_j\)의 값이 0이거나 1인 경우 둘 밖에 없고, 0인 경우는 \(v_i h_j\)가 0이므로 위와 같은 식을 얻을 수 있다). 이때, \(p(h_j = 1|v)\)는 아래와 같이 간단하게 계산할 수 있다.</p>


<p>\[p(h_j = c|v) = \frac{1}{Z} exp(-\sum_i a_i v_i - \sum_{\ell\neq j} b_\ell h_\ell - b_j * c - \sum_i \sum_{\ell\neq j} v_i w_{i\ell} h_\ell - \sum_i v_i w_{ij} * c)\]</p>


<p>\[p(h_j = 1|v) = \frac{p(h_j = 1|v)}{p(h_j = 1|v) + p(h_j = 0|v)} = \frac{1}{1 + exp(-b_j-\sum_i v_i w_{ij})} = \sigma(b_j+\sum_i v_i w_{ij}).\]</p>


<p>즉, conditional probability는 sigmoid function이 된다. 마찬가지로 \(p(v_i = 1 | h) = \sigma(a_i+\sum_j h_j w_{ij})\)로 계산할 수 있다. 그렇기 때문에 우리가 주어진 데이터 \(v_i\)도 알고 있고, \(p(h_j=1|v)\) 역시 sigmoid로 계산할 수 있기 때문에, log likelihood의 weight에 대한 gradient값인 \(\sum_h p(h|v) v_i h_j -  \sum_{v,h} p(v,h) v_i h_j\)의 앞부분은 간단하게 계산할 수 있다.</p>


<p>그러나 문제가 되는 부분은 뒷 부분이다. 안타깝게도 이 경우는 모든 \(v,h\)의 조합에 대해 값을 모두 계산해야하기 때문에 이 값을 정확하게 계산하기 위해 필요한 computational complexity는 exponential이 된다. 그런데 이 값이 정확하게 우리가 구하고 싶은 마지막 final 값도 아니고, 겨우 중간 단계의 한 번의 gradient를 계산하기 위해 필요한 step에 불과한데 iteration 안에 exponential complexity를 가지는 step이 있는건 큰 문제가 된다. 그렇기 때문에 이 RBM문제를 해결하기 위해 도입되는 알고리즘이 Contrastive Divergence라는 gradient approximation 알고리즘이다.</p>




<h5>Restricted Boltzmann Machine (RBM): Contrastive Divergence</h5>


<p>Contrastive Divergence 알고리즘을 한 마디로 요약하면: \(p(v,h)\)를 계산하는 MCMC (Gibbs Sampling)의 step을 converge할 때 까지 돌리는 것이 아니라, 한 번만 돌려서 \(p(v,h)\)를 approximate하고, 그 값을 사용하여 \(\sum_{v,h} p(v,h) v_i h_j\)을 계산해 gradient의 approximation 값을 구한다.</p>


<p>MCMC는 원하는 stationary distribution을 가지는 MC를 design하여 목표로하는 distribution을 만들어내는 알고리즘 family를 일컫는다. 이 내용도 꽤나 방대한 내용이므로, 필요하다면 나중에 추가로 포스팅을 하도록 하겠다. Gibbs Sampling은 MCMC 알고리즘 family 중 하나로, 여러 random variable들의 joint probability를 계산하기 위한 알고리즘이다. 사실 내용은 엄청 간단한데, 한 variable을 제외한 나머지 r.v.를 fix하고 나머지 fixed된 r.v.가 주어졌다고 가정하고 conditional probability를 구해 현재 r.v.를 update하는 것을 모든 variable들에 대해 distribution이 converge할 때까지 반복하는 것이다. 이 과정을 엄청나게 많이 반복해서 stationary distribution에 converge했을 정도로 많이 iteration을 돌리게 되면, 우리는 iteration을 돌리면서 얻어내는 sequence들로부터 r.v.들의 joint probability로부터 sample하는 것과 같은 확률로 sample들을 얻을 수 있다.</p>


<p>따라서 이 알고리즘을 사용하면 앞에서 exponential complexity가 문제가 되었던 \(p(v,h)\)를 계산하는 것이 가능하다. 그런데 문제는 보통 MCMC가 converge할 때 까지 걸리는 시간이 결코 적지 않다는 것이다. 이론적으로 polynomial complexity를 보장할 수는 있지만, 실제 leanring time이 너무 길어져서 practical하게 쓰기 어렵다. 앞에서 설명한 것 처럼 이 distribution이 한 번의 gradient update만을 위해 사용되는 RBM에서는 그 시간을 모두 사용하기에는 너무 비효율적이다.</p>


<p>그래서 RBM은 Gibbs sampleing을 끝까지 돌리는 대신 이런 생각을 하게 된다. &#8216;어차피 정확하게 converge한 distribution이나, 중간에 멈춘 distribution이나 대략의 방향성은 공유할 것이다. 그렇기 때문에 완벽한 gradient 대신 Gibbs sampling을 중간에 멈추고 그 approximation 값을 update에 사용하자.&#8217; 이 아이디어가 바로 Contrastive Divergence의 전부라고 할 수 있다. Contrastive Divergence는 전체 RBM update 과정 중에서 이 Gibbs sampling을 한 번만 돌리는 부분을 일컫는 말이며, Hinton이 처음 제안한 이후 나중에 이 알고리즘이 충분한 시간이 흐른 후에 전체 log likelihood의 local optimum으로 converge한다는 이론적 결과까지 증명된다.</p>


<p>Contrastive Divergence를 도입한 RBM update 알고리즘은 다음과 같다. (notation이 조금 다를 수 있다)</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-4.png" width="600"></p>

<p>이 과정을 계속 반복하면 우리가 원래 원했던 hidden node와 visible node들의 joint probability를 표현하는 RBM을 learning할 수 있게 된다. RBM이 이렇게 간단하게 learning되는 이유는 restricted라는 조건이 있기 때문이다. 즉, 같은 layer들끼리는 connection이 없기 때문에 \(p(h|v) = \prod_j p(h_j|v)\)로 간단하게 표현되기 때문에 leanring이 간단해지는 것이다. 그렇기 때문에 restricted 되지 않은 general boltzmann machine은 RBM 처럼 마냥 간단하게 update되지 않는다.</p>


<h5>Deep Beilf Network (DBN)</h5>


<p>DBN은 \(\ell\) 개의 layer를 가진 joint distribution을 표현하는 graphical model이다. 참고로 앞에서 RBM은 1-layer 모델이었다. DBN의 확률 모델은 다음과 같은 식으로 표현된다. 이때 \(h^k\)는 k번째 layer의 hidden variable들을 표현하는 notation이다.</p>


<p>\[P(x, h^1, \ldots, h^\ell) = \bigg( \prod_{k=1}^{\ell-2} P(h^k|h^{k-1}) \bigg) P(h^{\ell-1},h^{\ell})\]</p>


<p>이때 data \(x\)는 \(h^0\)이고, 각각의 \(P(h^k|h^{k-1})\)는 RBM에서 visible unit이 given된 conditional probability로 표현되고, joint probability \(P(h^{\ell-1},h^{\ell})\)는 RBM의 joint probability로 given된다. 이 모델은 아래와 같은 알고리즘으로 learning할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-5.png" width="600"></p>

<p>즉, 이 모델은 RBM을 맨 아래 data layer부터 차근차근 stack으로 쌓아가면서 전체 parameter를 update하는 모델이다. 이 모델을 그림으로 표현하면 아래와 같은 그림이 된다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-6.png" width="200"></p>

<p>마지막 layer는 joint probability를 의미하고, 나머지 layer들은 모두 conditional probability로 표현된다. 참고로 전체를 jointly하게 표현하는 모델을 Deep Boltzmann Machine (DBM) 이라고 하는데, 이 모델의 경우 RBM update를 하는 알고리즘과 비슷한 알고리즘으로 전체 모델을 update하게 된다. 그러나 이 논문이 발표될 당시에는 DBN이 훨씬 간단하고 computational cost가 적기 때문에 DBN이라는 모델을 제안한 것으로 보인다.</p>


<p></p>

<p>이 모델이 의미있는 이유는 joint probability를 잘 표현하는 좋은 graphical model이어서가 아니라, 이 모델로 deep network를 pre-training하고 backpropagation 알고리즘을 돌렸더니 overfitting 문제가 크게 일어나지 않고 MNIST 등에서 좋은 성과를 거뒀기 때문이다. 즉, parameter initialization을 DBN의 joint probability를 maximize하는 (layer-wise로 \(\ell\)개의 RBM을 learning하는) 방식으로 하고 나서, 그렇게 구해진 parameter들로 deep network를 initialization하고 fine-tuning (backpropation) 을 했을 때, 항상 그 정도 size의 deep network에서 발생하던 overfitting issue가 사라지고 성능이 우수한 classifier를 얻을 수 있었기 때문이다.</p>


<p>DBN으로 unsupervised pre-training한 deep network 모델을 사용했을 때 MNIST 데이터 셋에서 그 동안 다른 모델들로 거뒀던 성능들보다 훨씬 우수한 결과를 얻을 수 있었고, 그때부터 deep learning이라는 것이 큰 주목을 받기 시작했다. 그러나 지금은 데이터가 충분히 많을 경우 이런 방식으로 weight를 initialization하는 것 보다 random initialization의 성능이 훨씬 우수하다는 것이 알려져있기 때문에 practical한 목적으로는 거의 사용하지 않는다.</p>




<h5 id="75-cnn">Convolutional Neural Network (CNN): Introduction</h5>


<p>DBN이 지금은 practical한 목적으로 거의 사용되지 않는 것과는 대조적으로, 1989년에 제안된 이 모델은 아직까지도 많이 쓰이는 deep network 모델이다. 특히 computer vision에 특화된 이 네트워크는 인간의 시신경 구조를 모방하여 인간이 vision 정보를 처리하는 것을 흉내낸 모형이다.</p>


<p>DBN은 overfitting issue를 initialization으로 해결하였지만, CNN은 overfitting issue를 모델 complexity를 줄이는 것으로 해결한다. CNN은 convolution layer와 pooling layer라는 두 개의 핵심 구조를 가지고 있는데, 이 구조들이 model parameter 개수를 효율적으로 줄여주어 결론적으로 전체 model complexity가 감소하는 효과를 얻게 된다.</p>


<p></p>

<h5>Convolutional Neural Network (CNN): Convolution Layer</h5>


<p>먼저 convolution layer에 대해 설명해보자. Convolution layer를 설명하기 전에 먼저 convolution operation에 대해 알아보자. Convolution이란 signal processing 분야에서 아주 많이 사용하는 operation으로, 다음과 같이 표현된다.</p>


<p>\[s(t) = (x * w)(t) = \int x(a)w(t-1) da.\]</p>


<p>예를 들어 이 operation은 주어진 데이터 \(x\)에 filter \(w\)를 사용해 데이터를 처리할 때 사용된다. 이 operation을 적용한 간단한 예를 보자. (<a href="http://www.sfu.ca/~truax/conv.html">출처</a>)</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-7.jpg" width="400"></p>

<p>이렇듯 convolution은 어떤 filter를 사용하여 주어진 image의 적절한 feature를 뽑아내기 위해 사용했던 operation이다. 이때 \(s(t)\)를 데이터 \(x\)의 feature map이라고 부른다. Deep learning이 널리 사용되기 이전에는 다른 머신러닝 framework에 이미지를 input으로 넣고 처리하기 위해서는 먼저 filter를 고르고 그 filter로 image를 convolution하는 preprocessing을 거쳐서 적절한 feature map을 얻어낸 이후에 그것을 machine learning framework의 input으로 넣어 돌리는 방식을 사용했었다. 그렇기 때문에 이런 feature engineering이 전체 performance에 큰 영향을 미치는 경우가 많았다. 어떤 filter를 선택할 것이며, 얼마나 많은 filter를 고를 것인지 등의 영역은 feature engineering의 영역이고, 이론적인 영역이 아니기 때문에 machine learning 분야에서는 큰 관심을 두는 분야는 아니었다. 데이터는 잘 처리되었다고 가정하고 그 데이터를 사용해 어떤 좋은 알고리즘을 개발하느냐가 그 동안 머신러닝 framework들의 아이디어였다면, CNN의 핵심 아이디어는 preprocessing이 실제 performance에 크게 영향을 미치니까, 아예 이 preprocessing을 가장 잘해주는, 가장 좋은 feature map을 뽑아주는 convolution filter를 learning하는 모델을 만들어버리자는 것이다.</p>


<p>최대한 작은 complexity를 가지면서 우수한 filter를 표현하기 위한 CNN의 핵심 아이디어는 다음 세 가지이다: sparse interactions (혹은 sparse weight라고도 한다), parameter sharing (혹은 tied wieght라고도 한다), equivariant representations. 즉, CNN은 layer와 layer간에 모든 connection을 연결하는 대신 일부만 연결하고 (sparse weight), 그리고 그 weight들을 각각 다른 random variable로 취급하여 따로 update하는 대신 특정 weight group들은 weight 값이 항상 같도록 parameter를 share한다 (parameter sharing). 그리고 앞의 아이디어를 잘 활용하여 shift 등의 transform에 대해서 equivariant한 (자세한 내용은 밑에서 설명한다) representation을 learning하도록 모델을 구성한다.</p>


<p>Sparse weight를 사용하게 되면 모든 가능한 connection을 사용하는 것 보다 훨씬 적은 표현형을 learning하게 된다는 단점이 있지만, 반대로 model의 complexity가 낮아진다는 장점이 존재한다. CNN은 vision과 관련된 task를 수행하도록 design된 network라는 것은 이미 언급한바 있다. 이런 vision 데이터를 처리하는 task를 하게 될 경우에는 주어진 input의 dimension에 비해 실제 필요한 feature의 dimension은 극히 적다는 domain knowledge를 우리는 이미 가지고 있다. 즉 input인 이미지의 경우 픽셀 값이 적으면 몇 백에서 많으면 몇 백만에 이를 정도로 dimension이 엄청나게 높지만, 우리가 필요한 &#8216;feature&#8217;는 그 중에서도 극히 일부 영역, 이를테면 edge detection 등의 그에 비해 훨씬 적은 dimension으로 표현 가능하기 때문에 최대한 parameter를 줄여서 더 효율적인 feature map을 뽑아내기 위하여 weight를 sparse하게 사용한다. 이를 그림으로 표현하면 아래와 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-9.png" width="600"></p>

<p>이 그림은 같은 output \(s_3\)에 영향을 주는 edge들과 input node를 표현한 그림이다. 왼쪽은 가능한 connection이 전부 있는 것이 아니라 그 일부만 존재하고, \(s_3\)에 영향을 주는 input이 \(x_2, x_3, x_4\) 뿐이지만 오른쪽은 모든 가능한 connection이 있어서 model parameter의 개수가 크게 차이나고 모든 input이 \(s_3\)에 영향을 주는 것을 알 수 있다. 실제 image 데이터를 처리하기에는 왼쪽 모델이 조금 더 나은데, 그 이유는 한 feature를 결정하기 위해서 모든 image 정보가 필요한 것이 아니라, image의 일부분만 필요하기 때문이다. 예를 들어 내가 face segmentation, 즉 얼굴 사진에서 눈 코 입 등을 찾아내는 task를 수행한다고 하면, 주어진 사진에서 &#8216;눈&#8217;이 어디인지 표현하기 위해서 모든 이미지가 다 필요한 것이 아니라 눈 주변의 local한 데이터만 필요할 것이라고 유추할 수 있다. 오른쪽 그림은 필요하지 않은 배경까지 모두 고려하여 눈에 대한 정보를 찾는 셈이고, 왼쪽 그림은 local한 정보만을 주고 눈에 대한 정보를 처리하게 하는 것이다. 따라서 vision task를 처리하기에는 적절한 sparse weight가 더 효율적인 모델이라는 것을 알 수 있다. 때문에 CNN의 convolution layer는 hidden node 하나가 image의 local한 patch와 연결되어있는 형태로 되어있다. 예를 들어 한 hidden node 마다 image의 3 by 3 patch만을 연결하는 방식이다. 그림으로 표현하면 아래와 같은 식이다. (출처: <a href="http://www.codeproject.com/Articles/523074/Online-handwriting-recognition-using-multi-convolu">Code project - Online handwriting recognition using multi convolution neural networks</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-11.png" width="600"></p>

<p>여기에서 subsampling은 일단 나중에 설명하도록 하고 (subsampling part가 pooling layer에 해당한다) 가장 왼쪽의 image data의 일부분에 해당하는 patch만 다음 hidden layer의 한 unit에 연결하는 것이다. 이런 식으로 네트워크를 만들게 되면, patch size에 따라 다음 feature map의 size가 결정될 것이다. 예를 들어 100 by 100 이미지에서 5 by 5 patch를 사용해 convolution layer를 구축할 경우, 이 layer의 feature map은 96 by 96이 될 것이다.</p>


<p>CNN은 이런 sparse weight에 parameter sharing을 또 더하여 vision task에 최적화된 network를 learning하게 된다. Parameter를 share하게 되면 그러지 않는 것과 비교하여 보다 적은 parameter만을 가지게 되므로 model의 complexity가 줄어드는 효과가 있을 뿐 아니라, 각각의 patch마다 따로 필터를 learning하는 대신, 모든 patch에 동일한 필터를 적용하도록 강제하는 효과가 있다. CNN은 아래 그림과 같이 각각의 hidden node들이 같은 location에 대해 같은 weight를 가지도록 설정하여 모든 hidden node들이 각각 다른 patch에 대해 같은 filter를 처리하는 것과 같은 형태로 모델을 디자인한다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-8.png" width="200"></p>

<p>위 그림에서 같은 색으로 칠해진 edge는 서로 같은 weight를 가진다. 위 그림에서 볼 수 있듯, CNN은 fully connected layer를 가지지 않고, 그 sparse한 weight들에서도 서로 weight를 공유하도록 설정되어있다. 그렇지 않은 네트워크와 비교해보면 다음과 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-10.png" width="600"></p>

<p>각각의 그림에서 검은색으로 연결된 edge들은 서로 같은 parameter를 가진다. 즉, 왼쪽은 한 번에 5개의 edge가 같은 weight를 가지지만, 오른쪽은 하나의 parameter로 한 개의 edge만 표현할 수 있다. 이렇게 표현하게 되면 convolution layer operation이 간단한 matrix multiplication으로 주어지게 되어 gradient를 계산하기 한 층 더 수월해진다는 장점도 존재한다. 자세한 내용은 algorithm 쪽에서 다루도록 하자.</p>




<p>마지막으로 equivalent representations는 위와 같은 sparse weight와 tied weight를 어떤 특정한 형태로 효율적으로 배치하게 되었을 때, 주어진 input의 변화에 대해 output이 변화하는 방식이 equivariant해지는 현상을 의미한다 (equivalent가 아니다). 예를 들어 function \(f\)가 function \(g\)와 equivariant하다는 의미는, \(f(g(x)) = g(f(x))\)인 경우를 말한다. 이미지 처리를 예로 들면 \(g\)는 임의의 linear transform이라고 할 수 있다. Shift, rotate, scale등의 image에 대한 transform들이 그것인데, 우리는 같은 이미지가 돌아가거나 움직이거나 살짝 scale되더라도 그 이미지가 어떤 이미지인지 잘 판별할 수 있지만, 컴퓨터에게는 그런 transform이 픽셀 값이 완전히 바뀌는 결과를 낳기 때문에 어떤 정보인지 판별하기 어려운 것이다. 그런데 만약 우리가 어떤 transform \(g\)에 대해 equivariant representation을 만들어내는 network \(f\)를 만들 수 있다면, input이 shift되거나 rotate되더라도 항상 적절한 representation을 가지도록 할 수 있을 것이다. (실제 CNN은 shift에만 equivariant하다.)</p>


<p>즉, 앞에서 shared parameter가 각각의 patch에 대해 같은 filter를 처리하는 것 처럼 설정하였기 때문에, 만약 image가 shift되더라도 feature map의 형태가 크게 뒤틀리는 것이 아니라, feature map도 image와 함께 shift되는 형태를 보이게 될 것이다.</p>


<p>그런데 실제로는 한 image에 한 개의 filter가 아니라 여러 개의 filter가 필요할 수도 있다. 앞에서 설명한 convolution layer는 한 개의 convolution filter를 표현할 뿐이지만, 실제로는 이런 convolution filter가 한 개가 아니라 여러 개 만든 다음 그 값들을 concate하여 feature map을 표현해야할 수도 있다. 그렇기 때문에 실제로 CNN model은 한 개의 convolution layer가 아니라 아래와 같이 여러 개의 convolution layer가 결합된 꼴을 하고 있다. 참고로 공식적으로는 각각의 layer 혹은 filter를 kernel이라 하고, 그 kernel들이 모여있는 것을 한 layer로 부른다. (<a href="http://masters.donntu.org/2012/fknt/umiarov/diss/indexe.htm#p4">출처</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-12.png" width="600"></p>

<h5>Convolutional Neural Network (CNN): Pooling Layer</h5>


<p>Convolution Layer만 여러 개 연결하여 deep network를 구성하는 것도 가능하지만, 실제로는 더 dimension이 낮은 feature map을 얻기 위하여 subsampling이라는 것을 하게 된다. 앞에서 예로 들었던 것처럼 100 by 100 이미지에 5 by 5 convoltion patch size를 가지는 convolution layer를 연결할 경우 feature map의 size는 96 by 96이 되는데, 사실 이 96 by 96 feature map은 서로 매우 highly correlated 되어있는 값이 것이다. 특히 서로 이웃해있을수록 겹치는 영역이 많기 때문에 거의 비슷한 값을 가질 것이라고 예상할 수 있다. 아래 그림을 보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/75-11.png" width="600"></p>

<p>이미 앞에서 나왔던 그림이지만 설명을 위하여 다시 가져왔다. Pooling layer는 convolution layer의 feature map을 조금 더 줄여주는 역할을 한다. 전체 feature map을 그대로 들고가는 대신, 예를 들어 96 by 96 image feature map을 2 by 2 patch들로 쪼개는 것이다. 이렇게 할 경우 총 48 by 48 개의 output이 생기게 될텐데, subsampling이라는 것은 각각의 2 by 2 patch는 max, average 등의 operation을 행하는 것을 의미한다. 보통 max operation을 사용하고, 이 경우 간단하게 max pooling을 사용한다 라고 이야기 한다. 가끔 average pooling을 사용하는 경우도 있지만 보통 classification을 위한 모델들은 max pooling을 사용하니 참고하면 좋을 것 같다.</p>


<p>CNN은 이렇게 convolution layer와 pooling layer가 결합된 형태로 deep 하게 구성이 된다. 개인적으로 아래 그림이 CNN의 convolution layer와 max pooling layer를 잘 표현하는 그림이라고 생각한다. (<a href="http://inspirehep.net/record/1252539/plots">출처</a>)</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-13.png" width="300"></p>

<h5>Convolutional Neural Network (CNN): Backpropagation</h5>


<p>CNN의 기본 model은 알았으니 이제 이 network의 parameter를 어떻게 learning해야할지 알아보자. 기본적인 update algorithm은 <a href="http://SanghyukChun.github.io/74">이전 글</a>에서 설명했던 <a href="http://SanghyukChun.github.io/74#backprop">backpropagation algorithm</a>을 사용한다.</p>


<p>먼저 간단한 max pooling layer 부터 살펴보자. Pooling layer는 아래 p by q size의 patch 중에서 max 값을 선택하는 layer이다. 때문에 이를 수식으로 표현해보면 다음과 같이 쓸 수 있다. \((x,y)\)는 pooling layer feature map의 x,y좌표를 나타내고, (\(h_l\)은 l번째 layer의 hidden variable들)</p>


<p>\[h_{l+1} (x, y) = max_{a-p\leq a\leq a+p, b-q\leq b\leq b+q}(h_l (x+a, y+b))\]</p>


<p>Parameter는 없으므로 \(\frac{\partial h_{l+1} }{\partial h_{l}}\)만 계산하면 된다. 이 경우 주어진 \((x,y)\)가 만약 max pooling을 통해 선택된 값이라면 값을 그대로 passing하고, 만약 선택되지 않은 값이라면 0을 할당하면 된다.</p>


<p>Convolution layer는 operation이 꽤 복잡한데, 먼저 아래 그림을 보자.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/75-14.png" width="400"></p>

<p>이때 l+1 번째 layer 중에서 i 번째 convolution filter의 x,y 좌표 값은 아래와 같이 표현된다. 아래 conv layer의 kernel은 m 개, 위 conv layer의 kernel은 n개 라고 해보자.</p>


<p>\[h_{l+1}(i, x,y) = \sum_{j=1}^m \sum_{a=1}^p \sum_{b=1}^q h_l (j, x+a, y+b) * w(i,n;a,b)\]</p>


<p>값이 좀 많이 복잡하긴 한데, 미분 값을 계산해보면, parameter의 gradient는 \(\frac{E}{\partial w} = \sum_x \sum_y \frac{E}{h_{l+1} } (x,y) h_l (x,y)\)와 같이 바로 전 layer의 pixel값에 대해 gradient 값을 곱한 것을 전부 더한 형태로 구할 수 있고, \(\frac{E}{\partial h_l} \)은 weight w로 이전 layer의 gradient를 convolution한 것들을 전부 더한 것과 같은 결과를 얻게 된다.</p>


<p>CNN의 모든 operation들은 단순 연산이 많고 branch가 없기 때문에 core가 많고, 모든 core가 하나의 operation pointer를 공유하는 GPU를 사용해 효율적으로 parallization하기 좋다. 보통 CNN은 <a href="caffe.berkeleyvision.org">caffe</a>라는 C++ library를 사용해 learning하기 때문에 위에서 언급한 알고리즘을 실제로 구현할 일은 많지 않을 것 같다.</p>




<h5>정리</h5>


<p>Deep learning은 neural network의 layer를 deep 하게 쌓은 것에 지나지 않지만, 아무것도 하지 않고 layer를 깊게 쌓기만하면 overfitting이 너무 강하게 발생하여 제대로 된 결과를 얻을 수 없다. 이 글에서는 두 가지 overfitting을 피하는 방법을 설명하였다. 첫 번째 DBN은 주어진 network를 DBN이라는 RBM이 stack으로 쌓여있는 graphical probabilistic model로 표현한다. 그리고 주어진 데이터에 대해 likelihood를 maximize하는 parameter를 찾아서 그 값을 initial point로 사용해 gradient descent를 실행한다. 이때 RBM의 gradient 값을 정확히 구하는 것이 힘들기 때문에 Gibbs sampling의 iteration을 converge할때까지 돌리는 대신 한 번만 돌리는 Contrastive Divergence 알고리즘이 제안된다. DBN은 RBM을 layer wise greedy update rule을 통해 parameter를 update하게 된다.</p>


<p>두 번째로 설명한 CNN은 sparse weight, tied weight, equivariant representation이라는 세 가지 아이디어를 기반으로 모델의 complexity는 최소화하면서 vision에 최적화되어있는 형태의 모델이다. Parameter update는 backpropagation으로 하게 되는데, 보통 구현되어있는 툴을 사용하게 되므로 세부 update rule을 직접 구현할 일은 많지 않을 것 같다.</p>


<p>이 밖에 regularization이나 optimization method들과 같이 deep learning과 관련된 중요한 개념들 역시 추후 다른 포스트를 통해 소개할 수 있도록 하겠다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 21일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
<li><a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Bengio, Yoshua, et al. &ldquo;Greedy layer-wise training of deep networks.&rdquo; Advances in neural information processing systems 19 (2007): 153.</a></li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. &ldquo;A fast learning algorithm for deep belief nets.&rdquo; Neural computation 18.7 (2006): 1527-1554.</a></li>
<li><a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">Classification datasets results</a></li>
<li><a href="http://deeplearning.net/tutorial/rbm.html">DeepLearning.net &ndash; Restricted Boltzmann Machines (RBM) Tutorial</a></li>
<li><a href="http://deeplearning.net/tutorial/lenet.html">DeepLearning.net &ndash; Convolutional Neural Network (LeNet) Tutorial</a></li>
<li><a href="http://image.diku.dk/igel/paper/AItRBM-proof.pdf">Fischer, Asja, and Christian Igel. &ldquo;An introduction to restricted Boltzmann machines.&rdquo; Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Springer Berlin Heidelberg, 2012. 14-36.</a></li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Hinton, Geoffrey. &ldquo;A practical guide to training restricted Boltzmann machines.&rdquo; Momentum 9.1 (2010): 926.</a></li>
<li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">Bengio, Yoshua. &ldquo;Learning deep architectures for AI.&rdquo; Foundations and trends® in Machine Learning 2.1 (2009): 1-127.</a></li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 &ndash; RBM, DNN, CNN</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Models of Visual Attention (NIPS 2014)]]></title>
    <link href="http://SanghyukChun.github.io/91/"/>
    <updated>2015-09-19T13:14:00+09:00</updated>
    <id>http://SanghyukChun.github.io/91</id>
		<content type="html"><![CDATA[<p>이번에 리뷰할 논문은 Google DeepMind가 NIPS 2014에 발표한 Recurrent Neural Networ와 Reinforcement Learning을 결합한 <a href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a> 이라는 논문이다. <a href="http://SanghyukChun.github.io/90">지난 글</a>에서 리뷰했던 <a href="http://arxiv.org/abs/1312.5602">Playing Atari With Deep Reinforcement Learning</a>과 같은 연구팀에서 진행한 연구인듯하다. Atari 논문에서는 전통적인 RL 문제인 &#8216;게임&#8217;을 풀기 위하여 CNN으로 action-value function을 모델링하고 value iteration을 대체하는 새로운 action-value function learning 모델과 알고리즘을 제안했다면, 이 논문은 기존 RL 문제라기보다는 오히려 좀 더 클래식한 classification 문제라고 할 수 있는 image recognition 문제에 RNN 구조와 RL 구조를 결합하여 reward maximization optimization problem을 푸는 모델과 알고리즘을 제안한다.</p>




<h5>Motivation</h5>


<p>CNN 기반의 image classification은 이미 인간이 할 수 있는 수준에 거의 근접하였다. 그러나 CNN을 사용한 기존 접근 방법은 input size가 fix되어있어야하고, pixel size가 엄청나게 크면 그만큼 computation cost가 그대로 늘어난다는 단점이 존재한다. 하지만 실제 사람이 물체를 인식하거나 할 때를 생각해보면, <a class="red tip" title="주의, 집중 등의 뜻이 있다.">&#8216;attention&#8217;</a>이 존재한다는 것을 알 수 있다. 즉, 배경을 포함한 모든 정보를 사용하여 물체를 인식하는 것이 아니라 자신이 focus하고 있는 일부분과 그 주변 부의 정보들을 &#8216;훑어보면서&#8217; 훑어본 sequence들을 복합적으로 종합하여 결론을 내린다는 것을 알 수 있다. 만약 이런 방식으로 &#8216;focusing&#8217;을 하는 모델을 만들 수 있다면 지금 보고 있는 화면의 일부 만을 사용하므로 더 적은 &#8216;bandwidth&#8217;의 데이터를 저장해도 되고, 정보를 처리하기 위해 좀 더 적은 양의 pixel이 필요할 것이다. 그렇기 때문에 단순 pixel map을 파악하는 것 보다 이런 &#8216;atenttion&#8217;을 고려한 훨씬 더 human-like한 모델을 설계한다면 기존 CNN의 단점을 해결하는 데에 도움이 될 수 있을 것이라는 것이 이 논문의 motivation이다. 이 논문은 visual scence의 attention-based processing을 attention을 어떻게 취할 것인지를 action으로 생각하여 일종의 control problem으로 모델링하여 문제를 해결한다.</p>


<p>이 논문은 기존 CNN기반 approach들처럼 각 time stamp에 대해 전체 이미지를 한 번에 처리하거나 혹은 이미지 박싱을 하는 대신에 모델이 attend해야할 다음 location을 과거 정보와 현재 reward를 기반으로 선택하는 모델을 제안한다. 이 모델은 기존 CNN 모델과는 다르게 image의 크기가 바뀌더라도 computation이나 memory가 그 크기에 linear하게 증가하지 않고 모델에 의해 조절 가능하다는 특징이 있다. </p>




<h5>The Recurrent Attention Model (RAM)</h5>


<p>구체적인 모델을 정의하기 위하여 먼저 attention problem을 정의해보자. 이 논문은 attention problem을 visual 환경과 interact하는 목표지향적인 agent가 행하는 sequential decision process로 정의한다. 각 time stamp하다 agent는 bandwidth-limited sensor만을 사용해 environment를 observe하게 된다. 즉, agent는 한 번에 전체 environmnet를 감지하지 않고, 매 time stamp마다 local한 정보 만을 감지한다. 대신 agent는 sensor를 어떻게 사용할 것인지, 다시 말해서 sensor의 다음 location을 선택하는 action을 취할 수 있다. 마치 사람이 시선을 쭉 움직이면서 visual scence을 훑어보는 것처럼 말이다. 만약 reward를 image classification과 관련되도록 정의한다면 이런 attention 문제는 한 번에 센서가 볼 수 있는 정보가 한정되어있고, action을 어떻게 취하느냐에 따라 결과가 (reward가) 크게 달라지기 때문에 state별로 reward를 maximize하는 action을 취하는 policy를 learning하는 reinforcement learning 문제로 생각할 수 있다.</p>


<p>이제 모델을 조금 더 구체적으로 정의해보자.</p>




<ul>
    <li><p>\(x_t\): agent가 time \(t\)에 관측한 environment (전체 image의 일부분)</p></li>
    <li><p>\(\ell_t\): agent가 time \(t\)에 focus하고 있는 region의 좌표 값, 실제 agent는 \(\ell_t\)의 주변을 관측한다. 이 값은 논문에서 sensor control의 action으로 사용된다.</p></li>
    <li><p>\(a_t\): agent의 time \(t\)에서의 environment action. Classification의 경우는 \(a_t\)가 classification을 하는 decision을 내리는 용도로 사용된다. 즉, MNIST data로 실험하는 경우 가능한 \(a_t\)의 경우 수는 [0-9]이며, 각각 0부터 9까지의 숫자를 나타내게 된다.</p></li>
    <li><p>\(r_t\): agent가 maximize하고자하는 목표 값이다. Image classification은 time \(t\)에서 정확한 classification을 했으면 reward가 1, 아니라면 reward가 0이 되도록 설정하였다고 한다.</p></li>
    <li><p>\(h_t\): time \(t\)에서 agent의 state를 &#8216;hidden&#8217; state로 표현한 것으로, 원래 state는 \(s_{1:t} = x_1, \ell_1, a_1, \ldots, x_{t-1}, \ell_{t-1}, a_{t-1}, x_t\)로 표현되지만, 만약 \(h_t\)를 이 모든 state들을 &#8216;summarize&#8217;하는 것과 같이 모델링 할 수 있다면, 전체 state를 보는 대신, summerized internal state인 \(h_t\)로 state 표현을 대신할 수 있다.</p></li>
</ul>




<p>위와 같은 모델을 설계하기 위하여 이 논문에서는 다음과 같은 RNN 형태의 neural netork model은 제안하고 있다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/91-1.png" width="200"></p>

<p>Agent에게는 매 시간마다 전체 image의 일부분 정보인 \(x_t\)와 바로 전 state를 표현하는 \(h_{t-1}\)이 input으로 들어온다. 이 정보들을 사용하여 agent는 sensor를 어떻게 움직일 것인지 결정하는 (다음으로 살펴볼 위치 정보를 결정하는) \(\ell_t\)와 주어진 task를 수행하는 action (이 경우는 image classification이므로 \(a_t\) 그 자체가 label 정보를 담은 action이 된다) \(a_t\)라는 action을 취하게 된다. 이 모델을 시간에 대해 unfold한 것이 논문에 나와있는 Figure 1.c이다. </p>


<p><img src="http://SanghyukChun.github.io/images/post/91-2.PNG" width="600"></p>

<p>이때 \(f_g, f_\ell, f_a\)는 각각 input data에 대한 정보를 처리하는 네트워크 (glimpse network \(f_g\)), 위치 정보를 결정하는 네트워크 (location network \(f_\ell\)), 그리고 action의 값을 결정하는 네트워크를 (action network \(f_a\)) 의미한다. 각각의 네트워크에 대해 하나하나 살펴보도록 하자.</p>


<p>먼저 gimpse network \(f_g\)는 주어진 input image \(x_t\)와, 그 중 일부의 위치정보 \(\ell_t\) 만을 받아서 원래 image의 일부분만 &#8216;attention&#8217; 하여 적절한 feature를 뽑아내는 네트워크이다. Glimpse라는 말은 한국어로 &#8216;언뜻 보다&#8217; 라는 의미를 가지고 있는데, 다시 말해 주어진 이미지를 살짝 훑어보고 그 정보를 잘 정리하여 주어진 RNN core network가 정보를 잘 처리할 수 있도록 만들어주는 역할을 한다. 이 네트워크는 아래 그림과 같이 구성되어있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/91-4.PNG" width="600"></p>

<p>이 네트워크는 glimpse sensor라는 것의 output과 \(\ell_{t-1}\)의 정보를 concate하는 역할을 한다. 여기에서 중요한 것은 glimpse sensor라는 것인데, 이 센서는 마치 사람의 &#8216;망막처럼&#8217; (retina-like) 정보를 처리하는 역할을 한다. 즉, 이 센서를 사용해 전체 이미지에서 좁은 영역에 해당하는 정보를 뽑아내는 역할을 하는 것이다. 이 센서는 아래와 같은 구조를 띄고 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/91-3.PNG" width="600"></p>

<p>Glimpse sensor는 주어진 이미지 \(x_t\)의 한 위치 \(\ell_{t-1}\)을 받아서 해당 위치에서 특정 거리 \(d_1\)만큼 떨어진 이미지를 추출한다. 그리고 나서 그것보다 더 넓은 범위인 \(d_2\)만큼 떨어진 이미지를 추출하고, 다시 그것보다 큰 \(d_3\)만큼 떨어진 이미지를 추출하는 과정을 \(k\)번 반복하여 \(k\)개의 patch를 만든다. 이렇게 하는 이유는 사람의 망막이 중심부에 가까울수록 데이터의 해상도를 높게 받아들이고 중심부에서 멀어질수록 이미지가 흐려지도록 처리하기 때문이다. Sensor에서 이 값들을 생성하고 나면 \(\rho(x_t, \ell_{t-1})\) 이라는 transform을 처리하게 되는데, image classification 실험을 위해서 사용한 transform은 모든 사진을 같은 크기로 resize한 다음 concate하는 transform이라고 한다. 이렇게 될 경우 중심부에 가까울수록 정보량이 많아지고 정확해지지만 멀어질수록 해상도가 낮은 정보를 받게 될 것이다.</p>


<p>모든 glimpse network의 lyaer들은 기본적인 inner product layer를 사용한다 (\(Linear(x) = Wx + b\)). 그리고 neuron으로는 ReLU unit (\(ReLU(x) = \max(x,0))\)을 사용한다. 즉, </p>


<p>\[h_g = ReLU(Linear(\rho(x,\ell))), h_\ell = ReLU(Linear(\ell)) \]</p>


<p>그리고 glimpse network의 output \(g\)는 \(g = ReLU(Linear(h_g) + Linear(h_\ell)\)로 정의한다. Glimpse network 말고도 location network와 core network도 거의 같은 방식으로 정의하게 되는데, 각각 \(f_\ell (h) = Linear(h)\), \(h_t = f_h(h_{t-1}) = ReLU(Linear(h_{t-1}) + Linear(g_t) )\)로 정의한다. 이때, core network는 state vecotr \(h\)의 dimension이 256인 LSTM을 사용한다. 마지막으로 action network \(f_a (h) = exp(Linear(h))/Z\), 즉 linear softmax classifier로 정의한다. 그 이외 설정은 모두 앞에서 설명한 것과 같다.</p>




<h5>Training</h5>


<p>실험에 대해 알아보기 전에, 이 network를 어떻게 learning할 수 있는지 잠시 살펴보도록하자. 이 네트워크에서 우리가 learning해야할 parameter는 glimpse network, core network 그리 action network의 parameter인 \(\theta_g, \theta_h, \theta_a\)이다. Optimization을 하기 위한 target function은 total reward를 maximize하는 함수로 설정할 것이다. 조금 더 formal한 설명을 위하여 interaction sequences \(s_{1:N}\)과, 그것의 모든 가능한 state들의 distribution \(p(s_{1:T}; \theta)\)을 introduce해보자. 이렇게 정의할 경우 우리는 아래와 같은 target function의 \(p(s_{1:T}; \theta)\)에 대한 expectation을 maximize하는 문제로 reward maximization problem을 formal하게 정의할 수 있다.</p>


<p>\[J(\theta) = \mathbb E_{p(s_{1:T};\theta)} \bigg[ \sum_{t=1}^T r_t \bigg] = \mathbb E_{p(s_{1:T};\theta)} \big[R\big]. \]</p>


<p>그러나 이 함수 \(J(\theta)\)를 maximize하는 것은 trivial한 일이 아닌데, 다행스럽게도 이미 예전에 다른 work에서 이 \(J(\theta)\)의 gradient의 sample approximation이 아래와 같이 유도된다는 것을 보였다고 한다.</p>


<p>\[\nabla J(\theta) = \sum_{t=1}^T \mathbb E_{p(s_{1:T};\theta)} \big[ \nabla_\theta \log \phi (u_t ~|~ s_{1:t};\theta) R \big] \simeq \frac{1}{M} \sum_{i=1}^M \sum_{t=1}^T \nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta) R^i .  \]</p>


<p>위의 관계식에서의 \(\nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta)\)은 RNN의 gradient를 계산해야하는 것으로 간단하게 구할 수 있다. 다만 이 관계식이 unbiased estimation of gradient를 제공하기는 하지만, variance가 너무 높다는 단점이 있다고 한다. 그래서 이 논문에서는 아래와 같은 form으로 gradient를 estimation하여 variance의 값을 줄이도록 하였다고 한다.</p>


<p>\[ \frac{1}{M} \sum_{i=1}^M \sum_{t=1}^T \nabla_\theta \log \phi (u_t^i ~|~ s_{1:t}^i;\theta) (R_t^i - b_t), \mbox{ where } R_t^i = \sum_{t^prime=1}^T r_{t^\prime}^i. \]</p>




<h5>Experiments</h5>


<p>이 논문에서는 MNIST에 대해 실험을 진행했다. 실험은 우리가 보통 사용하는 centered digit, non-centered digit은 물론이고, <a title="흐트러트리다, 어지럽히다라는 뜻이 있다" class="red tip">cluttered</a> non-centered digit에 대한 실험도 진행했다. 마지막 실험은 MNIST digit에 random하게 8 by 8 subpatch를 더하여 데이터를 조금 더 &#8216;지저분하게&#8217; 만들어서 실험을 진행했다. 비교군은 MNIST의 state-of-art인 모델들이 아니라, 가장 간단한 2 layer fully connect neural network를 사용하였다. 아마 state-of-art 모델들은 워낙 성능이 뛰어나서 아직 극복이 안되는 모양이다. 실험 결과는 아래와 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/91-5.png" width="600">
<img src="http://SanghyukChun.github.io/images/post/91-6.png" width="600"></p>

<p>결과가 outperform하다고 할 수는 없지만, 간단한 2-layer fully connected neural network보다 특수한 경우들에 대해 훨씬 잘 동작함을 볼 수 있고, 무엇보다 올바른 classification을 하기위한 policy rule이 (초록색 선으로 표현된 것들) 상당히 human-likely 한 결과를 보인다는 것이 고무적이다. 물론, 이 결과가 GoogleNet이나 AlexNet에 비해 엄청 우수한 결과를 보이느냐하면 그것은 아니지만, 새로운 형태의 접근을 할 수 있다는 가능성을 제시하는 것 만으로도 의미가 있다고 본다. 보다 자세한 실험에 대한 설명은 논문을 참고하면 좋을 것 같다.</p>




<h5>Summary of Visual Attention</h5>


<ul>
<li>기존 CNN 기반 접근 방식의 문제점들 &ndash; 이미지 사이즈에 linear한 computation cost, human-like 하지 않은 처리 방법 등 &ndash; 을 처리하기 위한 목적으로 디자인되었음</li>
<li>사람이 정보를 한 번에 처리하는 것이 아니라 배경을 무시하고 이미지의 일부만 인식하듯, &lsquo;attention&#8217;을 모델에 대입하는 아이디어를 제안함</li>
<li>Attention을 neural network에 도입하기 위하여 RNN과 Reinforcement Learning을 결합한 형태의 모델을 사용함</li>
<li>RNN의 input으로는 이미지 정보, 위치 정보가 있으며, 그것들을 조금 더 retina-like하게 처리하기 위한 glimpse network라는 것을 추가로 붙여서 input으로 사용함</li>
<li>output으로는 action network, location network가 있는데, action network는 classification을 위한 linear classifier이고, location network는 다음 state에 영향을 미치는 recurrent하게 다음 input과 함께 glimpse network의 input으로 쓰이는 값임</li>
<li>reward는 time t에 올바른 classification을 하였는지 아닌지를 판단하여 0-1 으로 reward를 return함</li>
<li>train을 하기 위하여 reward maximization을 하는데, 직접 gradient를 구하는 것이 non-trivial하여 estimation값을 사용함. 이때 unbaised estimator는 variance가 높아서 low variance estimator를 사용하여 update를 함</li>
<li>MNIST에 대해 실험을 하였으며, centered digit은 기존 state-of-art에 비해 턱없이 모자라지만, 사람은 구분할 수 있지만 머신은 제대로 판단하지 못하는 cluttered non-centered digit을 기존 fully connected network보다 훨씬 잘 판별하는 것을 알 수 있었음</li>
</ul>


<h5>Reference</h5>


<ul><li><p><a href="http://arxiv.org/abs/1406.6247">Mnih, Volodymyr, Nicolas Heess, and Alex Graves. &#8220;Recurrent models of visual attention.&#8221; Advances in Neural Information Processing Systems. 2014.</a></p></li></ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Playing Atari with Deep Reinforcement Learning (NIPS 2013)]]></title>
    <link href="http://SanghyukChun.github.io/90/"/>
    <updated>2015-09-15T19:56:00+09:00</updated>
    <id>http://SanghyukChun.github.io/90</id>
		<content type="html"><![CDATA[<p>이번에 리뷰할 논문은 Google DeepMind가 NIPS 2013에 발표한 Deep Learning과 Reinforcement Learning을 결합한 <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> 이라는 논문이다. 그 전에도 DNN과 RL을 결합하려는 시도는 있었지만, 거의 사람이 플레이하는 수준으로 의미있는 수준까지 도달한 work은 이 work이 처음인듯 하다. 글을 시작하기 전에 먼저 이 논문에서 한 결과부터 살펴보자. Atari라는 게임 콘솔을 가지고 이 논문의 method를 적용한 결과이다. 2분 5초부터 시작되는 터널링 전략이 진짜 걸작이다.</p>




<div style="text-align: center;"><iframe width="420" height="315" align="middle" src="https://www.youtube.com/embed/iqXKQf2BOSE" frameborder="0" allowfullscreen></iframe></div>




<p>충격적인 사실은 다른 hand-coding feature나 parameter 튜닝 없이 오직! vision data만 사용해서 이런 결과를 냈다는 사실이다. 빨간 공이 object고  빨간 판이 내가 움직이는거라는 기본적인 hand-craft feature 조차 없이 이런 결과를 냈다는 것이다. Deep leanring이 RL에서 뛰어난 성과를 보이지 못한 이유가 주로 데이터에 관련된 것이었음을 생각해보면 대단한 결과라고 할 수 있다. 최근에는 이 work을 기반으로 <a href="http://arxiv.org/abs/1406.6247">Image Attention (NIPS 2015)</a> 이라고 부르는 work도 나온 것 같다. Image Attention 논문은 다음에 정리해보도록하겠다.</p>




<h5>Background</h5>


<p>이 논문은 RL 중에서도 MDP에 초점을 맞추고 있으며 (사실 MDP가 아닌 RL은 거의 없다고 봐도 무방하지만) 그 중에서도 model-free technique 중 하나인 <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning algorithm</a>을 neural network를 통해 해결하고 있다. 이 글은 RL에 대해 어느 정도 기초적인 지식이 있다고 가정하고 쓸 것이기 때문에 조금 더 자세한 내용은 <s>추후 작성할 RL 관련 포스트나</s> <a href="http://SanghyukChun.github.io/76">이 포스트</a>나 다른 reference들을 참고하면서 읽으면 좋을 것 같다.</p>


<p>Reinforcement Learning을 위해서는 먼저 환경 \(\mathcal E\)을 정의해야한다. 이 논문에서는 Atari 에뮬레이터가 환경이 될 것이다. Atari 에뮬레이터 환경을 구성하는 요소는 action의 sequence들, observe하는 화면과 최종 reward (점수)가 될 것이다. 그림으로 표현하면 다음과 같은 식이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/90-6.png" width="600"></p>

<p>매 시간마다 agent는 legal game action \(\mathcal A = \{1, \ldots, K\}\) 중에서 action \(a_t\)를 하나 선택한다. 예를 들어 아래 그림과 같은 컨트롤러의 버튼 중 어떤 버튼을 누를 거인지를 결정하는 것이다. 옆으로 움직이는 버튼, 기타 다른 버튼들 하나하나가 \(\mathcal A\)의 element이며, 그 중 하나가 action \(a_t\)가 되는 것이다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/90-1.png" width="200"></p>

<p>게임을 하는 동안 컨트롤러의 버튼을 누르는 action들이 모이게 되면 현재 점수에 어떻게든 영향을 주게 되고, 그 결과로 최종 점수가 결정된다. 즉, Atari 게임 환경에서 reward \(r_t\)는 게임 score이며, 현재 내가 선택한 action은 바로 reward에 반영되는 것이 아니라 엄청나게 나중에 반영될 수도 있는 것이다.</p>


<p>또한 게임 조작을 통해 변화하는 것 중 우리가 관측할 수 있는 것은 실제 게임 화면의 pixel 값들 뿐이다 (이 논문에서는 time stamp \(t\)에서의 픽셀 값을 \(x_t\)라고 정의하였다). 때문에 이 논문에서는 vision 데이터를 사용해서 state를 정의하는데, state를 정의하는 방식이 상당히 재미있다. 간단하게 \(x_t\)를 state로 삼으면 될 것 같지만, 실제로는 화면 하나만 보고 알 수 있는 정보가 제한적이고 현재 상태를 정확하게 판단하기 위해서는 vision정보와 내가 행한 action을 포함한 과거 history들까지 모두 있지 않으면 안되기 때문에 이 논문은 state \(s_t\)를 action과 image의 sequence로 정의한다. 예를 들어 아래 스크린샷에서 공은 왼쪽으로 움직일까 오른쪽으로 움직일까? 만약 관성을 implement한 게임이라면 (움직이는 버튼에서 손을 떼도 조금 움직이는 게임이라면) 과연 play agent는 위로 올라가고 있을까 아니면 아래로 내려가고 있을까 그것도 아니면 멈춰있을까? 이렇듯 한 time stamp에 대한 vision 데이터로는 파악할 수 있는 데이터가 너무 제한적이기 때문에 모든 history가 반드시 필요하다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/90-2.png" width="300"></p>

<p>다시 말해서, \(s_t  = x_1, a_1, \ldots, a_{t-1}, x_t\) 가 된다. 이런 방식으로 modeling을 하게 되면 policy 혹은 strategy를 learning할 때 모든 과거 sequence를 고려해서 strategy가 결정되게 된다. 게임은 언젠가 끝나게 되어있기 때문에 모든 \(s_t\)는 finite한 길이를 가지고 있으며 (비록 어마어마하게 크지만) \(s_t\)의 domain 역시 유한하다. 따라서 이 모델은 엄청나게 large하지만 어쨌거나 finite한 Markov dicision process(MDP)가 된다.</p>


<p>우리의 목표는 에뮬레이터에 agent가 어떤 strategy를 통해 게임을 조작하여, 최종적으로 게임이 끝났을 때 게임에서 가장 높은 점수를 획득하는 것이다. 즉, reward에 대한 수학적 정의만 있다면 이 문제는 간단한 optimization 문제가 된다. 게임은 보통 시간 마다 reward를 받는다. 하지만 일반적으로 시간이 오래 지날수록 해당 reward의 가치는 점점 내려가는데 이를 고려하기 위하여 discount factor \(\gamma\)가 정의된다. 시간 \(t\)의 reward를 t라고 한다면 time \(T\)에서의 discount factor를 고려한 future reward는 다음과 같이 정의한다.</p>


<p>\[R_t = \sum_{t^\prime=t}^T \gamma^{t^\prime-t} r_{t^\prime}.\]</p>


<p>Reward function을 정의했으니 Q-function (action-value function) 역시 정의할 수 있다. \(\pi\)를 \(s_t\)에서 \(a_t\)를 mapping하는 policy function이라하면, optimal Q-function \(Q^*\)는 다음과 같이 정의할 수 있다.</p>


<p>\[Q^*(s,a) = \max_\pi \mathbb E \big[ R_t \big| s_t = s, a_t = a, \pi \big]. \]</p>


<p>MDP에서는 이 optimal action value function 혹은 optimal Q-function 하나만 제대로 알고 있다면 언제나 주어진 state에 대해 가장 \(Q^*\)의 값을 크게 만드는 action을 고르는 간단한 policy만으로도 반드시 항상 optimal한 action을 고를 수 있다는 이론적 결과가 있기 때문에 Q-function은 매우매우 중요하다. 이 논문에서도 \(Q^*\)를 찾는 neural network를 만들어서 문제를 해결하고 있으니, Q-function에 대한 이해가 필수적이다.</p>


<p>다시 본문으로 돌아와서, Optimal Q-function은 <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>이라는 중요한 특성을 따르게 된다. 이 equation을 intuition은 대충 이런식이다: seqeunce \(s^\prime\)의 다음 time stamp에서의 optimal Q-function \(Q^* (s^\prime, a^\prime)\) 값이 모든 action \(a^\prime\)에 대해서 알려져 있다면, optimal strategy는 \(r + \gamma Q^* (s^\prime, a^\prime)\)의 expected value를 maximize하는 것이라는 것이다. 이를 수식으로 표현하면 다음과 같다.</p>


<p>\[Q^*(s,a) = \mathbb E_{s^\prime \sim \mathcal E} \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].\]</p>


<p>많은 RL algorithm 들에서는 Q-function을 estimate하기 위하여 이 Bellman equation을 value iteration algorithm이라는 것을 통해 iterative update하여 구하게 된다. Value iteration algorithm에서는 매 \(i\) 번쨰 iteration마다 다음과 같은 procedure를 수행하게 된다.</p>


<p>\[Q_{i+1} (s,a) = \mathbb E \bigg[ r + \gamma Q^* (s^\prime, a^\prime) ~\big|~ s,a \bigg].\]</p>


<p>이런 value iteration algorithm은 MDP에서 \(Q_i \to Q^* \mbox{ as } i\to\infty\)라는 것이 알려져 있다. 그러나 이런 방식은 이론적으로는 의미가 있을지 몰라도 실제로는 완전히 impractical한데, 이 과정을 모든 seqeunce에 대해 독립적으로 시행해야하기 때문이다. 따라서 보통은 action-value function을 다음과 같은 식으로 적절한 approximator를 사용하여 approximation한다.</p>


<p>\[Q(s,a;\theta) \simeq Q^* (s,a).\]</p>


<p>보통 linear function으로 approximation하지만, 간혹 non-linear function으로 모델링하는 경우도 있다. 예를 들어 deep network를 쓰는 방법이 있는데, 이 논문은 기존 방법들에 비해 RL에 적합한 Deep Network 모델을 제안하여 뛰어난 non-linear function approximator를 제안한다.</p>


<p>일반적으로 RL에 deep learning approach를 바로 적용했을 때 몇 가지 이슈가 생기게 되는데, 먼저 deep learning을 하기 위해서는 엄청나게 많은 hand labelled training data가 필요하지만, RL에서는 모든 state와 action에 대한 labelled data가 없기 때문에 이를 어떻게 handle해야할지를 모델에서 고려해야만한다. 또한 현재까지 연구된 많은 deep learning structure들은 data가 i.i.d.하다고 가정하지만, 실제 RL 환경에서는 state들이 엄청나게 correlated되어있기 때문에 제대로 된 learning이 어렵다. 예를 들어 위 핑퐁 게임 화면에서 한 프레임 더 지난 화면과 지금 화면은 정말 엄청나게 높은 correlation을 가지지만, standard feed-forward deep learning은 그것을 처리할 수 있는 모델이 아니기 때문에 문제가 발생한다. 이 논문은 그 문제를 experience replay라는 것으로 해결하는데, 자세한 내용은 다음 section에서 다루도록 하겠다.</p>




<h5>Deep Q-Learning</h5>


<p>앞에서 정의한 Q-function을 modeling한 network를 (\(Q(s,a;\theta) \simeq Q^* (s,a)\), 이때 \(\theta\)는 weight나 bias 등 neural network의 model parameter들) 이 논문에서는 Q-network라고 부르고 있다. 만약 parameter가 \(\theta\)가 정해진다면, 우리는 state \(s\)와 action \(a\)를 Q-network에 넣어서 forward pass를 돌리게 되면 해당하는 Q-value를 얻을 수 있을 것이다. 따라서 parameter가 정해진다면 주어진 \(s\)에 대해 모든 \(a\)에서의 Q-value를 얻을 수 있고, 이를 통해서 \(Q^*\)의 값과 그것을 achieve하는 action \(a^*\)를 구하는 것도 가능해진다. 다시 말해서 이 Q-network를 올바른 방향으로 update하는 알고리즘을 design하기만 한다면 주어진 문제를 해결할 수 있는 것이다. 이 논문에서는 Q-network가 우리가 원하는 목적대로 train되도록 하기 위하여 i 번째 iteration에서 아래와 같은 loss function을 가지도록 design한다. 이 논문은 현재 Q-network가 항상 target Q-value에 가까워지도록 loss를 설정함으로써 마치 value iteration이 converge하듯 Q-network의 update가 converge할 때 까지 iterative algorithm을 돌리도록 하는 것이다. 이때 \(y_i\)는 iteration i의 target value이고, \(\rho(s,a)\)는 sequence \(s\)와 action \(a\)의 probability distribution이며, 이를 이 논문에서는 behaviour distribution이라고 정의한다.</p>


<p>\[L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot)} \bigg[ \big(y_i - Q(s,a;w_i) \big)^2 \bigg], \]</p>


<p>\[\mbox{where, }y_i = \mathbb E_{s^\prime \sim \mathcal E} \bigg[r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime;w_{i-1}) ~\big|~ s, a \bigg]. \]</p>


<p>주의할 점은, optimization 과정에서 parameter \(\theta\)가 update되는 동안 loss function \(L_i(\theta_i)\) 의 이전 iteration paramter \(\theta_{i-1}\)은 고정된다는 것이다. 이를 &#8216;freeze target Q-network&#8217; 라고 부르는데, 이렇게하는 이유는 supervised learning과는 다르게, target의 값이 \(\theta\)의 값에 (민감하게) 영향을 받기 때문에 stable한 learning을 위하여 \(\theta\)값을 고정하는 것이다. 이건 아래에서 조금 더 자세하게 설명하도록 하겠다. 이제 loss function을 정의되었으므로 gradient값만 있다면 그 값을 사용해서 backpropagation을 돌리면 쉽게 update할 수 있다. 이 network의 loss의 gradient는 다음과 같이 구할 수 있다.</p>


<p>\[\nabla_{\theta_i} L_i (\theta_i) = \mathbb E_{s,a\sim \rho(\cdot); s^\prime \sim \mathcal E} \bigg[ \big( r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime; \theta_{i-1}) - Q(s,a;\theta_i) \big) \nabla_{\theta_i} Q(s,a;\theta_i)\bigg]\]</p>


<p>하지만 이렇게 build한 Deep RL을 바로 사용할 수는 없고, 아래와 같은 몇 가지 이슈들을 처리해야한다.</p>


<ul>
<li>Deep Learning은 데이터가 i.i.d.하다고 가정하지만 실제 RL input의 데이터는 sequential하고 highly correlated 되어있다.</li>
<li>Policy 변화에 따른 (이 경우는 w의 변화에 따른) Q-value의 변화량이 너무 크기 때문에 policy가 oscillate하기 쉽다.</li>
<li>위와 같은 세팅에서는 reward와 Q-value의 값이 엄청나게 커질 수 있기 때문에 stable한 SGD 업데이트가 어려워진다.</li>
</ul>


<p>첫 번째 이슈는 이미 전 문단에서 간단하게 언급했었으니 생략한다. 두 번째 문제도 크게 어렵지 않게 생각할 수 있는데, 게임을 하는 방식을 아주 조금만 바꾸더라도 게임의 결과가 완전히 크게 바뀌기 때문에 이런 현상이 발생한다. 핑퐁 게임에서 움직이는 속도를 조금 늦춘다거나 했다가는 바로 한 점을 잃게 될 것이다. 또한 앞에서 설명한 것 처럼 supervised learning과는 다르게 target의 값이 parameter에 영향을 아주 민감하게 받기 때문에, 이 값을 고정해주는 과정이 필요하다. 마지막 조건은 좀 practical한 이슈인데, Q-value의 값이 얼마나 커질지 모르기 때문에 stable update가 힘들 수도 있다. 이 논문에서는 다음과 같은 세 가지 방법으로 각각의 issue를 handling한다</p>


<ul>
<li>Experience replay</li>
<li>Freeze target Q-network</li>
<li>Clip reward or normalize network adaptively to sensible range</li>
</ul>


<p>이 중에서 두 번째 idea는 이미 설명했고 (update하는 동안 target을 계산하기 위해 사용하는 paramter를 고정), 세 번째 idea는 reward의 값을 [-1,0,1] 중에서 하나만 선택하도록 강제하는 아이디어이다. 즉, 내가 100점을 얻거나 10000점을 얻거나 항상 reward는 +1 이다. &#8216;highest&#8217; score를 얻는 것은 불가능하지만, 이렇게 설정함으로써 조금 더 stable한 update가 가능해진다. 그리고 그와는 별개로 모든 게임에 적용가능한 DQL을 learning할 수 있다는 장점도 있다. 실제로 실험에서는 모든 게임을 단 하나의 네트워크로만 learning해서 기존의 모든 방법을 beating한다.</p>


<p>그럼 이제 마지막으로 이 논문의 핵심 아이디어라고 할 수 있는 experience replay에 대해 살펴보자. Experience replay는 agent의 experine를 각 time stamp마다 다음과 같은 튜플 형태로 메모리 \(\mathcal D = e_1, \ldots, e_N\) 에 저장한 후 이를 다시 사용하는 것이다.</p>


<p>\[e_t = (s_t, a_t, r_t, s_{t+1}).\]</p>


<p>Experience replay는 이렇게 experience \(e_t\)를 메모리 \(\mathcal D\)에 저장해두었다가, 일부를 uniformly random하게 sample하여 mini-batch를 구성한 다음 parameter \(\theta\)를 mini-batch에 대해 backpropagation으로 update하는 과정을 의미한다.</p>


<p>Experience replay를 사용함으로써 data의 correlation을 깰 수 있고, 조금 더 i.i.d.한 세팅으로 network를 train할 수 있게 된다. 또한 방대한 과거 데이터가 한 번만 update되고 버려지는 비효율적 접근이 대신에, 지속적으로 추후 update에도 영향을 줄 수 있도록 접근하기 때문에 데이터 사용도 훨씬 효율적이라는 장점이 있다. 실제로 실험에서는 메모리 용량의 한계 때문에 bucket을 \(N\)으로 고정하고, FIFO 형태로 저장을 한 모양이다.</p>


<p>Experience replay가 끝난 후 agent는 action을 \(\epsilon\)-greedy policy라는 것을 사용해 선택하고 실행한다. 이 방법은 action을 \(\epsilon\)의 확률로 random하게 고르거나 \(1-\epsilon\)의 확률로 MDP의 optimal action selection criteria인 \(a_t = \arg\max_a Q^*(s_t, a;\theta) \)로 고르는 policy를 의미한다.</p>


<p>참고로, arbitrary length의 input을 다루는 것이 general feed-forward network에서는 어렵기 때문에, 이 논문에서는 function \(\phi\)라는 것을 정의해서 모든 \(s_t\)의 length를 fix한다. 이 알고리즘을 수식적으로 기술하면 다음과 같이 기술할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/90-3.png" width="600"></p>

<p>Equation 3은 위에서 증명한 gradient 값이고, 함수 \(\phi\)는 다음과 같다: 주어진 history 중에서 가장 마지막 4개의 frame을 stack으로 쌓는 것. 이때 각각의 frame은 원래 128색 210 by 160 픽셀으로 구성되어있지만, gray-scale로 만들고 110 by 84로 down sampling한 후 84 by 84로 크롭한다. 이때 크롭을 하는 이유는 주어진 툴에서 정사각형 사진만 GPU 연산이 되기 때문이라고..</p>


<p>이렇게 experience replay라는 아이디어를 사용한 DQL은 몇 가지 이점이 있다.</p>


<ul>
<li>각각의 experience가 potentially 많은 weight update에 reuse되기 때문에 experience를 weight update 한 번에만 사용하는 기존 방법보다 훨씬 data efficiency하다.</li>
<li>두 번째로, mini-batch를 만드는 sampling 과정을 통해 데이터들 간의 high correlation을 효율적으로 관리하고, 이를 통해 보다 효율적인 update를 할 수 있다. 이 방법은 random하게 sample을 뽑아서 mini-batch로 구성하기 때문에 이런 high correlation을 break해서 update의 효율성을 높이기 때문이다.</li>
<li>마지막으로 이 방법을 통해 parameter를 update하게 되면 다음 training을 위한 data sample을 어느 정도 determine할 수 있다. 예를 들어서 내가 지금 오른쪽으로 움직이는 쪽으로 action을 고른다면 다음 sample들은 내가 오른쪽에 있는 상태의 sample들이 dominate하게 나올 것이라고 예측할 수 있다. 따라서 이 방법을 통해 training을 위한 다음 데이터를 무작정 뽑는 것이 아니라 현재 action을 고려하여 효율적으로 뽑을 수 있다.</li>
</ul>


<p>몇 가지 주의점이라면 freeze target Q-network, 혹은 off-policy가 반드시 필요하다는 점 정도가 있겠다. 한계점으로는 메모리의 한계 때문에 앞에서 말한 것처럼 모든 history를 저장하지 못한다는 점과, uniform sampling을 사용하기 때문에 모든 과거 experience가 동일한 weight를 가진다는 점이다. 이 논문에서는 조금 더 wise한 sampling을 하게 되면 성능 향상이 있을 수도 있다고 언급하고 있다.</p>




<h5>Model Architecture of DQL and Experiment</h5>


<p>이제 구체적으로 어떤 neural network 모델을 사용해 learning을 하게 될지 알아보자. 항상 관심있게 살펴봐야 할 내용은 (1) input data는 무엇인가 (2) output data는 무엇인가 (3) 구체적인 network 구조는 어떻게 되는가 정도가 있겠다. 이 논문에서는 input으로 \(\phi(s_t)\)를 받고, output으로 가능한 모든 action에 대한 Q-value를 출력한다. 즉, 버튼이 4개 있다면 output은 4개이고, 12개 있다면 output은 12개이다. 실제 논문에서는 게임 종류에 따라 action을 4개에서 18개 사이에서 고른 것 같다. 이렇게 모델을 고르게 되면 \(Q^*\)를 단 한번의 forward pass 만으로 구할 수 있다는 장점이 있기 때문에 이 논문에서는 이러한 방법을 선택하였다. 구체적인 네트워크는 CNN을 사용한다. 인풋 데이터는 \(\phi(s_t)\)를 넣는다고 했으므로 84 by 84로 크롭한 후에 4개의 history를 stack으로 쌓은 데이터가 들어오게 된다. 그러니까 대충 다음과 같은 식이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/90-5.png" width="600"></p>

<p>이 네트워크를 사용한 자세한 실험 결과는 다음 표에 나와있다. 각각의 숫자는 모두 reward를 의미하기 때문에 값이 클 수록 좋은 결과이다.</p>


<p>
<img src="http://SanghyukChun.github.io/images/post/90-4.png" width="600"></p>

<p>Breakout, Enduro, Pong은 심지어 사람보다도 좋은 것을 알 수 있고, Space Invaders를 제외하면 기존 모델들보다 best 뿐 아니라 avg 까지 뛰어난 것을 볼 수 있다. 논문에서는 Q*bert, Seaquest, Spae Invaders 등의 게임에서 사람의 performance에 한참 미치지 못하는 이유로 이 게임들은 전략이 엄청나게 긴 time scale로 필요하기 때문에 조금 더 challenge한 문제라고 주장하고 있다. 아마 하드웨어의 발달과 모델의 발달로 언젠가는 극복할 수 있을 것으로 보인다.</p>




<h5>Summary of DQL</h5>


<ul>
<li>문제 정의 자체가 흥미롭다. 특히 state space를 sequence of iamges and action으로 구성했다는 점이 흥미롭다.</li>
<li>State에 대한 hand-craft feature가 전혀 없다. 오직 이미지 sequence만을 사용해서 CNN으로 feature를 자동으로 만들어내는 방법으로 이를 해결하고 있다는 점이 흥미롭다.</li>
<li>Q-function을 learning하는 neural network를 구성하였는데 몇 가지 stable update를 위하여 &lsquo;off-policy&#8217;를 사용하고, &#8216;experience replay&rsquo; 기법을 사용한다.</li>
<li>Experience replay는 매 시간마다 experience tuple (e_t)를 메모리에 저장하고, 메모리에서 (e_t)를 uniformly sample하여 뽑아 mini-batch를 구성하고 이를 (off-policy를 적용한 채로 혹은 target network를 freeze하고) 사용해 parameter를 update하는 아이디어이다.</li>
<li>서로 다른 Atari 게임 7개에 대한 policy를 learning하기 위해 단 하나의 neural network만을 사용했고, 그 결과가 기존 결과를 outperform한다.</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1312.5602">Mnih, Volodymyr, et al. &ldquo;Playing atari with deep reinforcement learning.&rdquo; NIPS (2013).</a></li>
<li><a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf">Deep Reinforcement Learning &ndash; ICLR 2015 tutorial</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Neural Network Regularization]]></title>
    <link href="http://SanghyukChun.github.io/89/"/>
    <updated>2015-09-14T19:26:00+09:00</updated>
    <id>http://SanghyukChun.github.io/89</id>
		<content type="html"><![CDATA[<p>이번에 review할 논문은 <a href="http://arxiv.org/abs/1409.2329">Recurrent neural network regularization</a>이라는 논문이다. 아직 학회나 저널에 publish된 논문은 아니지만 ICLR 2015 review를 기다리는 모양. 이 논문은 최근 머신러닝 분야에서 가장 주목받고 있는 분야는 Deep learning, 그 중에서도 요즘 가장 활발하게 연구 중인 Recurrent Neural Network, 혹은 RNN에 관한 논문이다. RNN은 sequencial data를 처리하는데에 적합한 형태로 디자인 되어있으며, 현재 language model, speech recognition, machine translation 등에서 우수한 결과를 성취하고 있는 neural network model 중 하나이다. 이 논문은 popular한 RNN 중 하나인 LSTM 모델을 regularization 시켜서 보다 기존 결과들보다 더 잘 동작하는 결과를 제안한다.</p>




<h5>Motivation: Regularization of Recurrent Neural Network</h5>


<p>RNN이 sequencial data에 대해 꽤 좋은 성능을 보이고 있는 것은 사실이지만, RNN을 regularization하는 방법은 많이 제안되어있지 않은 상황이다. 기존 MLP (Multi-Layer Perceptron, 혹은 feed-forward network) 쪽에서는 Dropout, ReLU 등의 컨셉들이 연구되면서 상당한 발전이 있었지만, 아직 RNN에는 Dropout조차 제대로 적용되지 않고 있다고 한다. Dropout을 붙이면 오히려 성능이 떨어지기 때문에 아직은 LSTM (Long-Short-Term-Memory) 모델에 dropout없이 연구가 진행되고 있는 모양이다. 이 논문에서는 LSTM에 dropout을 RNN의 특성을 잘 살린 형태로 적용하여 dropout이 잘 동작하도록 하는 방법을 제안한다.</p>




<h5 id="RNN-intro">RNN Introduction</h5>


<p>본 논문을 소개하기 전에 먼저 RNN에 대해 간단하게 설명을 하고 넘어가도록 하겠다. 이름에서도 알 수 있듯 일반적인 Feed-forward network와 RNN의 차이는 recurrent한 loop의 존재 유무이다. 이는 자기 자신을 향한 self-loop일 수도 있고, 아니면 cycle 형태이거나 아니면 undirected edge의 형태일 수도 있다. 보통 일반적으로 RNN이라 하면 아래 그림과 같이 hidden unit에 self-loop이 있는 형태를 일컫는 듯하다. (출처: Bengio Deep Learning book)</p>


<p><img src="http://SanghyukChun.github.io/images/post/89-1.PNG" width="600"></p>

<p>이 그림에서도 알 수 있듯 self-loop의 존재는 RNN으로 하여금 자연스럽게 historical data를 현재 decision에 반영하도록 만들어준다. 즉, RNN 모델은 마치 HMM 등의 sequencial data를 처리하는 모델들처럼 동작하는 것이다. 실제 learning을 할 때는 시간에 대해 self-loop를 &#8216;unfold&#8217; 하여 마치 weight를 공유하는 deep layer를 연산하듯 update한다. RNN에 대한 더 자세한 설명은 추후 다른 포스팅을 통해 다룰 수 있도록 하겠다.</p>




<h5>Long-Short-Term-Memory (LSTM) Architecture</h5>


<p>기존 vanilla RNN은 long-term dependency를 가지도록 learnig을 하게되면 gradient vanishing이나 exploding 문제에 직면하기 쉬워진다. 이유는 dependency를 더 long-term으로 가져갈수록 gradient 값이 시간에 따른 곱하기 형태가 되어 gradient growth가 exponential해지기 때문이다 (역시 위와 마찬가지로 나중에 더 자세하게 다루도록 하겠다). 때문에 이를 해결하기 위한 아이디어 중 하나로 LSTM이라는 것이 존재한다.</p>


<p>LSTM은 historical information을 저장하기 위한 다소 복잡한 dynamics를 가지고 있다. &#8220;long term&#8221; memory라는 것은 memory cell (\(c_t\))에 저장되며, 시간에 따라, 그리고 주어진 input data에 따라 저장해둔 information을 얼마나 간직하고 있을지 forget gate (\(f_t\)) 라는 것을 통해 결정하게 된다. LSTM을 그림으로 표현하면 아래 그림과 같다. (출처: 논문)</p>


<p><img src="http://SanghyukChun.github.io/images/post/89-2.PNG" width="600"></p>

<p>이를 수식으로 한 번 나타내어보자. 먼저 몇 가지 notation을 정의해보자. 먼저 모든 state는 n-dimension이라고 가정하자. \(h_t^l \in \mathbb R^n \)은 layer \(l\)의 timestamp \(t\)일 때의 hidden state라고 하자. 그리고 \(T_{n,m}: \mathbb R^n \to \mathbb R^m\) 을 n차원에서 m차원으로 가는 affine transform이라고 해보자. 예를 들어 parameter \(W\)와 \(b\)로 나타내어지는 \(Wx + b\)도 \(T_{n,m}\)에 포함된다 (즉, 이 논문에서는 복잡한 weight와 bias에 대한 식을 T라는 notation으로 간단하게 치환했다고 생각하면 된다). 마지막으로 \(\odot\)을 두 벡터의 element-wise multiplication이라고 정의해보자. 이렇게 notation을 정의하고 나면 일반적인 vanilla RNN을 다음과 같이 과거의 hidden state와 현재 hidden state의 이전 layer의 state로부터 현재 hidden state를 표현하는 function으로 표현할 수 있다.</p>


<p>\[\mbox{RNN: } h_t^{l-1}, h_{t-1}^l \to h_t^l, \mbox{ where } h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l). \]</p>


<p>이때, function \(f\)는 RNN의 경우 sigmoid나 tanh 함수 중 하나로 선택하는 것이 일반적이다. 그럼 이번에는 LSTM을 수식으로 표현해보자.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/89-3.PNG" width="250"></p>

<p>앞에서 언급했던 LSTM의 graphical representation을 살펴보면서 수식을 읽어보면 어렵지 않게 이해할 수 있을 것이다.</p>




<h5>Dropout Regularization for LSTM</h5>


<p>저자들은 RNN에 Dropout을 붙였을 때 잘 동작하지 않는 이유가 dropout이 지워버리면 안되는 과거 information까지 전부 지워버리기 때문이라고 주장한다. 때문에 RNN에 Dropout을 적용하기 위해서는 recurrent connection이 아닌 connection 들에 대해서만 Dropout을 적용해야한다고 논문에서는 주장하고 있다. 아래 식에 조금 더 자세하게 적혀있다. 이때 \(\mathbf D\) 는 dropout operator라는 것으로, 주어진 argument의 random subset을 0으로 만들어버리는 operator이다. 즉, \(\mathbf D (h)\) 라고 한다면 vector \(h\) 중 random하게 고른 일부를 (보통 50%) 0으로 설정하라는 뜻이다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/89-4.PNG" width="250"></p>

<p>이를 그림으로 표현하면 아래와 같다. 이때 실선은 일반적인 connection이고, 점선이 dropout으로 연결된 connection을 의미한다. (출처: 논문)</p>


<p><img src="http://SanghyukChun.github.io/images/post/89-5.PNG" width="600"></p>

<p>위 그림에서도 알 수 있듯, 과거에서부터 propagation되는 information은 언제나 100% 보존되지만, 아래 layer에서 위 layer로 전달되는 information은 특정 확률로 dropout에 의해 corruption되어 진행된다. 이때, 맨 아래 data layer로부터 맨 위 \(L\)번째 layer까지 information이 전달되는 동안 점선으로 그려진 connection은 정확하게 \(L+1\) 번 만큼만 지나게 된다. 만약 recurrent connection까지 dropout을 적용했다면, 이 횟수는 \(L+1\) 보다 항상 같거나 클 것이며, 더 long-term dependency를 가질수록 그 효과가 더 강해져서 우리가 원하는 과거 정보는 거의 다 희석되고, 결과적으로 안좋은 결과를 얻게 될 확률이 높아질 것이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/89-6.png" width="600"></p>

<p>위 그림은 어떻게 information이 time t-2로부터 t+2까지 전달되는지 그 flow를 표현한 것이다. 굵은 선이 information path를 나타내는데, 앞서 설명한 것 처럼 이런 information flow는 data layer로부터 decision layer까지 정확하게 \(L+1\) 번만 dropout의 영향을 받게 된다. 반면 standard dropout을 적용했더라면 information이 더 많은 dropout들에 의해 영향을 받아서 LSTM이 정보를 더 긴 시간 동안 저장할 수 없도록 만들게 되는 것이다. 때문에 recurrent connection에 dropout을 적용하지 않는 것 만으로도 LSTM에서 좋은 regularization 효과를 얻을 수 있는 것이다.</p>




<h5>Experiments</h5>


<p>논문에서는 총 4개의 실험을 진행한다. Language Modeling (Penn Tree Bank - PTB dataset), Speech Recognition, Machine Translation 그리고 마지막으로 Image Caption Generation이 그것이다. 결과는 순서대로 아래 Table 1,2,3,4에 나열되어있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/89-7.png" width="600"></p>

<h5>Summary of Recurrent Neural Network Regularization</h5>


<ul>
<li>Motivation: Dropout을 RNN에 그냥 적용하면 성능이 좋지 않다</li>
<li>Idea: Dropout을 모든 connection에 적용하는 대신 non recurrent connection에만 dropout을 적용하자</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1409.2329">Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. &ldquo;Recurrent neural network regularization.&rdquo; arXiv preprint arXiv:1409.2329 (2014).</a></li>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (18) Neural Network Introduction]]></title>
    <link href="http://SanghyukChun.github.io/74/"/>
    <updated>2015-09-13T23:13:00+09:00</updated>
    <id>http://SanghyukChun.github.io/74</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p>최근 Machine Learning 분야에서 가장 뜨거운 분야는 누가 뭐래도 Deep Learning이다. 엄청나게 많은 사람들이 관심을 가지고 있고, 공부하고 응용하고 있지만, 체계적으로 공부할 수 있는 자료가 많이 없다는 것이 개인적으로 조금 안타깝다. 이제 막 각광받기 시작한지 10년 정도 지났고, 매년 새로운 자료들이 쏟아져나오기 때문에 책이나 정리된 글을 찾기가 쉽지가 않다. 그러나 Deep Learning은 결국 artificial neural network를 조금 더 복잡하게 만들어놓은 모델이고, 기본적인 neural network에 대한 이해만 뒷받침된다면 자세한 내용들은 천천히 탑을 쌓는 것이 가능하다고 생각한다. 이 글에서는 neural network의 가장 기본적인 model에 대해 다루고, model paramter를 update하는 algorithm인 backpropagation에 대해서 다룰 것이다. 조금 더 advanced한 topic들은 이 다음 글에서 다룰 예정이다. 이 글의 일부 문단은 <a href="http://SanghyukChun.github.io/blog/categories/neural-network/">이전 글들</a>을 참고하였다.</p>


<h5>Motivation of Neural Network</h5>


<p>이름에서부터 알 수 있듯 neural network는 사람의 뇌를 본 따서 만든 머신러닝 모델이다 (참고: 원래 neural network의 full name은 artificial neural network이지만, 일반적으로 neural network라고 줄여서 부른다). 본격적으로 neural network에 대해 설명을 시작하기 전에 먼저 인간보다 컴퓨터가 훨씬 잘 할 수 있는 일들이 무엇이 있을지 생각해보자.</p>


<ul>
<li>1부터 10000000까지 숫자 더하기</li>
<li>19312812931이 소수인지 아닌지 판별하기</li>
<li>주어진 10000 by 10000 matrix 의 determinant값 계산하기</li>
<li>800 페이지 짜리 책에서 &lsquo;컴퓨터&rsquo; 라는 단어가 몇 번 나오는지 세기</li>
</ul>


<p>반면 인간이 컴퓨터보다 훨씬 잘 할 수 있는 일들에 대해 생각해보자</p>


<ul>
<li>다른 사람과 상대방이 말하고자하는 바를 완벽하게 이해하면서 내가 하고 싶은 말을 상대도 이해할 수 있도록 전달하기</li>
<li>주어진 사진이 고양이 사진인지 강아지 사진인지 판별하기</li>
<li>사진으로 찍어보낸 문서 읽고 이해하기</li>
<li>주어진 사진에서 얼마나 많은 물체가 있는지 세고, 사진에 직접 표시하기</li>
</ul>


<p>컴퓨터가 잘 할 수 있는 0과 1로 이루어진 사칙연산이다. 기술의 발달로 인해 지금은 컴퓨터가 예전보다도 더 빠른 시간에, 그리고 더 적은 전력으로 훨씬 더 많은 사칙연산을 처리할 수 있다. 반면 사람은 사칙연산을 컴퓨터만큼 빠르게 할 수 없다. 인간의 뇌는 오직 빠른 사칙연산만을 처리하기 위해 만들어진 것이 아니기 때문이다. 그러나 인지, 자연어처리 등의 그 이상의 무언가를 처리하기 위해서는 사칙연산 그 너머의 것들을 할 수 있어야하지만 현재 컴퓨터로는 인간의 뇌가 할 수 있는 수준으로 그런 것들을 처리할 수 없다.</p>


<p>예를 들어 아래와 같이 주어진 사진에서 각각의 물체를 찾아내는 문제를 생각해보자 (출처: <a href="http://www.engadget.com/2014/09/08/google-details-object-recognition-tech/">링크</a>). 사람에게는 너무나 간단한 일이지만, 컴퓨터가 처리하기에는 너무나 어려운 일이다. 어떻게 어디부터 어디까지가 &#8216;tv or monitor&#8217;라고 판단할 수 있을까? 컴퓨터에게 사진은 단순한 0과 1로 이루어진 픽셀 데이터에 지나지 않기 때문에 이는 아주 어려운 일이다.</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/74-2.jpg" width="400"></p>

<p>그렇기 때문에 자연언어처리, 컴퓨터 비전 등의 영역에서는 인간과 비슷한 성능을 내는 시스템을 만들 수만 있다면 엄청난 기술적 진보가 일어날 수 있을 것이다. 그렇기 때문에 인간의 능력을 쫓아가는 것 이전에, 먼저 인간의 뇌를 모방해보자라는 아이디어를 낼 수 있을 것이다. Neural Network는 이런 모티베이션으로 만들어진 간단한 수학적 모델이다. 우리는 이미 인간의 뇌가 엄청나게 많은 뉴런들과 그것들을 연결하는 시냅스로 구성되어있다는 사실을 알고 있다. 또한 각각의 뉴런들이 activate되는 방식에 따라서 다른 뉴런들도 activate 되거나 activate되지 않거나 하는 등의 action을 취하게 될 것이다. 그렇다면 이 사실들을 기반으로 다음과 같은 간단한 수학적 모델을 정의하는 것이 가능하다.</p>


<h5>Model of Neural Network: neuron, synapse, activation function</h5>


<p>먼저 뉴런들이 node이고, 그 뉴런들을 연결하는 시냅스가 edge인 네트워크를 만드는 것이 가능하다. 각각의 시냅스의 중요도가 다를 수 있으므로 edge마다 weight를 따로 정의하게 되면 아래 그림과 같은 형태로 네트워크를 만들 수 있다. (출처: <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-1.png" width="600"></p>

<p>보통 neural network는 directed graph이다. 즉, information propagation이 한 방향으로 고정된다는 뜻이다. 만약 undirected edge를 가지게 되면, 혹은 동일한 directed edge가 양방향으로 주어질 경우, information propagation이 recursive하게 일어나서 결과가 조금 복잡해진다. 이런 경우를 recurrent neural network (RNN)이라고 하는데, 과거 데이터를 저장하는 효과가 있기 때문에 최근 음성인식 등의 sequencial data를 처리할 때 많이 사용되고 있다. 이번 ICML 2015에서도 RNN 논문이 많이 발표되고 있고, 최근들어 연구가 활발한 분야이다. 이 글에서는 일단 가장 간단한 &#8216;multi layer perceptron (MLP)&#8217;라는 구조만 다룰 것인데, 이 구조는 directed simple graph이고, 같은 layer들 안에서는 서로 connection이 없다. 즉, self-loop와 parallel edge가 없고, layer와 layer 사이에만 edge가 존재하며, 서로 인접한 layer끼리만 edge를 가진다. 즉, 첫번째 layer와 네번째 layer를 직접 연결하는 edge가 없는 것이다. 앞으로 layer에 대한 특별한 언급이 없다면 이런 MLP라고 생각하면 된다. 참고로 이 경우 information progation이 &#8216;forward&#8217;로만 일어나기 때문에 이런 네트워크를 feed-forward network라고 부르기도 한다.</p>


<p>다시 일반적인 neural network에 대해 생각해보자. 실제 뇌에서는 각기 다른 뉴런들이 activate되고, 그 결과가 다음 뉴런으로 전달되고 또 그 결과가 전달되면서 최종 결정을 내리는 뉴런이 activate되는 방식에 따라 정보를 처리하게 된다. 이 방식을 수학적 모델로 바꿔서 생각해보면, input 데이터들에 대한 activation 조건을 function으로 표현하는 것이 가능할 것이다. 이것을 activate function이라고 정의한다. 가장 간단한 activation function의 예시는 들어오는 모든 input 값을 더한 다음, threshold를 설정하여 이 값이 특정 값을 넘으면 activate, 그 값을 넘지 못하면 deactivate되도록 하는 함수일 것이다. 일반적으로 많이 사용되는 여러 종류의 activate function이 존재하는데, 몇 가지를 소개해보도록 하겠다. 편의상 \(t = \sum_i w_i * x_i\) 라고 정의하겠다. (참고로, 일반적으로는 weight 뿐 아니라 bais도 고려해야한다. 이 경우 \(t = \sum_i (w_i * x_i + b_i) \)로 표현이 되지만, 이 글에서는 bais는 weight와 거의 동일하기 때문에 무시하고 진행하도록 하겠다. - 예를 들어 항상 값이 1인 \(x_0\)를 추가한다면 \(w_0\)가 bais가 되므로, 가상의 input을 가정하고 weight와 bais를 동일하게 취급하여도 무방하다.)</p>




<ul>
    <li><p>sigmoid function: \(f(t) = \frac{1}{1+ e^{-t}}\)</p></li>
    <li><p>tanh function: \(f(t) = \frac{e^t - e^{-t}}{e^t + e^{-t} }\)</p></li>
    <li><p>absolute function: \(f(t) = \|t\|\)</p></li>
    <li><p>ReLU function: \(f(t) = max(0, t)\)</p></li>
</ul>




<p>보통 가장 많이 예시로 드는 activation function으로 sigmoid function이 있다. (출처는 위의 <a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions">위키</a>와 같음)</p>


<p><img class="center" src="http://SanghyukChun.github.io/images/post/74-4.png" width="300"></p>

<p>이 함수는 미분이 간단하다거나, 실제 뉴런들이 동작하는 것과 비슷하게 생겼다는 등의 이유로 과거에는 많이 사용되었지만, 별로 practical한 activation function은 아니고, 실제로는 ReLU를 가장 많이 사용한다 (2012년 ImageNet competition에서 우승했던 <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> publication을 보면, ReLU와 dropout을 쓰는 것이 그렇지 않은 것보다 훨씬 더 우수한 결과를 얻는다고 주장하고 있다. 이에 대한 자세한 내용은 다른 포스트를 통해 보충하도록 하겠다). 참고로 neuron을 non-linearity라고 부르기도 하는데, 그 이유는 activation function으로 linear function을 사용하게 되면 아무리 여러 neuron layer를 쌓는다고 하더라도 그것이 결국 하나의 layer로 표현이 되기 때문에 non-linear한 activation function을 사용하기 때문이다.</p>


<p>따라서 이 모델은 처음에 node와 edge로 이루어진 네트워크의 모양을 정의하고, 각 node 별 activation function을 정의한다. 이렇게 정해진 모델을 조절하는 parameter의 역할은 edge의 weight가 맡게되며, 가장 적절한 weight를 찾는 것이 이 수학적 모델을 train할 때의 목표가 될 것이다.</p>




<h5>Inference via Neural Network</h5>


<p>먼저 모든 paramter가 결정되었다고 가정하고 neural network가 어떻게 결과를 inference하는지 살펴보도록하자. Neural network는 먼저 주어진 input에 대해 다음 layer의 activation을 결정하고, 그것을 사용해 그 다음 layer의 activation을 결정한다. 이런 식으로 맨 마지막까지 결정을 하고 나서, 맨 마지막 decision layer의 결과를 보고 inference를 결정하는 것이다 (아래 그림 참고, 빨간 색이 activate된 뉴런이다).</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-3.png" width="600"></p>

<p> 이때, classification이라고 한다면 마지막 layer에 내가 classification하고 싶은 class 개수만큼 decision node를 만든 다음 그 중 하나 activate되는 값을 선택하는 것이다. 예를 들어 0부터 9까지 손글씨 데이터를 (MNIST라는 유명한 dataset이 있다) classification해야한다고 생각해보자. 그 경우는 0부터 9까지 decision이 총 10개이므로 마지막 decision layer에는 10개의 neuron이 존재하게 되고 주어진 데이터에 대해 가장 activation된 크기가 큰 decision을 선택하는 것이다.</p>




<h5 id="backprop">Backpropagation Algorithm</h5>


<p>마지막으로 이제 weight를 어떻게 찾을 수 있는지 weight paramter를 찾는 알고리즘에 대해 알아보자. 먼저 한 가지 알아두어야 할 점은 activation function들이 non-linear하고, 이것들이 서로 layer를 이루면서 복잡하게 얽혀있기 때문에 neural network의 weight optimization이 non-convex optimization이라는 것이다. 따라서 일반적인 경우에 neural network의 paramter들의 global optimum을 찾는 것은 불가능하다. 그렇기 때문에 보통 gradient descent 방법을 사용하여 적당한 값까지 수렴시키는 방법을 사용하게 된다.</p>


<p>Neural network (이 글에서는 multi-layer feed-forward network)의 parameter를 update하기 위해서는 backpropagation algorithm이라는 것을 주로 사용하는데, 이는 단순히 neural network에서 gradient descent를 chain rule을 사용하여 단순화시킨 것에 지나지 않는다 (Gradient descent에 대해서는 이전에 쓴 <a href="http://SanghyukChun.github.io/63">Convex Optimization글</a>에서 자세히 다루고 있으니 참고하면 좋을 것 같다). 모든 optimization 문제는 target function이 정의되어야 풀 수 있다. Neural network에서는 마지막 decision layer에서 우리가 실제로 원하는 target output과 현재 network가 produce한 estimated output끼리의 loss function을 계산하여 그 값을 minimize하는 방식을 취한다. 일반적으로 많이 선택하는 loss에는 다음과 같은 함수들이 있다. 이때 우리가 원하는 <a class="red tip" title="만약 MNIST라면 d=10">d-dimensional</a> target output을 \(t=[t_1, \ldots, t_d]\)로, estimated output을 \(x=[x_1, \ldots, x_d]\) 로 정의해보자.</p>




<ul>
    <li><p>sum of squares (Euclidean) loss: \(\sum_{i=1}^d (x_i - t_i)^2 \)</p></li>
    <li><p>softmax loss: \(-\sum_{i=1}^d \bigg[ t_i \log \big(\frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) + (1 - t_i) \log \big(1 - \frac{e^{x_i} }{\sum_{j=1}^d e^{x_j} }\big) \bigg] \)</p></li>
    <li><p>cross entropy loss: \(\sum_{i=1}^d [ -t_i \log x_i - (1-t_i) \log (1-x_i) ]\)</p></li>
    <li><p>hinge loss: \(\max(0,1-t \cdot x)\), 이때 \(\cdot\)은 내적을 의미한다.</p></li>
</ul>




<p>상황에 따라 조금씩 다른 loss function을 사용하지만, classification에 대해서는 보통 softmax loss가 gradient의 값이 numerically stable하기 때문에 softmax loss를 많이 사용한다. 이렇게 loss function이 주어진다면, 이 값을 주어진 paramter들에 대해 gradient를 구한 다음 그 값들을 사용해 parameter를 update하기만 하면 된다. 문제는, 일반적인 경우에 대해 이 paramter 계산이 엄청 쉬운 것만은 아니라는 것이다.</p>


<p>Backpropagtaion algorithm은 chain rule을 사용해 gradient 계산을 엄청 간단하게 만들어주는 알고리즘으로, 각각의 paramter의 grdient를 계산할 때 parallelization도 용이하고, 알고리즘 디자인만 조금 잘하면 memory도 많이 아낄 수 있기 때문에 실제 neural network update는 이 backpropagtaion 알고리즘을 사용하게 된다.</p>


<p>Gradient descent method를 사용하기 위해서는 현재 parameter에 대한 gradient를 계산해야하지만, 네트워크가 복잡해지면 그 값을 바로 계산하는 것이 엄청나게 어려워진다. 그 대신 backpropataion algorithm에서는 먼저 현재 paramter를 사용하여 loss를 계산하고, 각각의 parameter들이 해당 loss에 대해 얼마만큼의 영향을 미쳤는지 chain rule을 사용하여 계산하고, 그 값으로 update를 하는 방법이다. 따라서 backpropagation algorithm은 크게 두 가지 phase로 나눌 수가 있는데, 하나는 propagation phase이며, 하나는 weight update phase이다. propagation phase에서는 training input pattern에서부터 에러, 혹은 각 뉴런들의 변화량을 계산하며, weight update phase에서는 앞에서 계산한 값을 사용해 weight를 update시킨다.</p>


<h6>Phase 1: Propagation</h6>


<ol>
    <li>Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 &#8216;forward&#8217; propagation이라 한다.)</li>
    <li>Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 <a class="red tip" title="이 경우는 hidden layer가 하나이므로 hidden layer를 지칭한다.">이전 layer</a>의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 &#8216;back&#8217; propagation이라 한다.)</li>
</ol>


<h6>Phase 2: Weight update</h6>


<ol>
    <li>Chain rule을 사용해 paramter들의 gradient를 계산한다.</li>
</ol>


<p>이때, chain rule을 사용한다는 의미는 아래 그림에서 나타내는 것처럼, 앞에서 계산된 gradient를 사용해 지금 gradient 값을 update한다는 의미이다. (그림은 bengio의 <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">deep learning book</a> <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/mlp.html">Ch6</a> 에서 가져왔다.)</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-6.png" width="400"></p>

<p>두 그림 모두 \(\frac{\partial z}{\partial x}\)를 구하는 것이 목적인데, 직접 그 값을 계산하는 대신, \(y\) layer에서 이미 계산한 derivative인 \(\frac{\partial z}{\partial y}\)와 \(y\) layer와 \(x\)에만 관계있는 \(\frac{\partial y}{\partial x}\)를 사용하여 원하는 값을 계산하고 있다. 만약 \(x\) 아래에 \(x^\prime\)이라는 parameter가 또 있다면, \(\frac{\partial z}{\partial x}\)와 \(\frac{\partial x}{\partial x^\prime}\)을 사용하여 \(\frac{\partial z}{\partial x^\prime}\)을 계산할 수 있는 것이다. 때문에 우리가 backpropagation algorithm에서 필요한 것은 내가 지금 update하려는 paramter의 바로 전 variable의 derivative와, 지금 paramter로 바로 전 variable을 미분한 값 두 개 뿐이다. 이 과정을 output layer에서부터 하나하나 내려오면서 반복된다. 즉, output -> hidden k, hidden k -> hidden k-1, &#8230; hidden 2 -> hidden 1, hidden 1 -> input의 과정을 거치면서 계속 weight가 update되는 것이다. 예를 들어서 decision layer와 가장 가까운 weight는 직접 derivative를 계산하여 구할 수 있고, 그보다 더 아래에 있는 layer의 weight는 그 바로 전 layer의 weight와 해당 layer의 activation function의 미분 값을 곱하여 계산할 수 있다. 이해가 조금 어렵다면 아래의 <a href="74#example">예제</a>를 천천히 읽어보기를 권한다.</p>


<p>이 과정을 맨 위에서 아래까지 반복하면 전체 gradient를 구할 수 있고, 이 gradient를 사용해 parameter들을 update할 수 있다. 이렇게 한 번의 iteration이 진행되고, 충분히 converge했다고 판단할 때 까지 이런 iteration을 계속 반복하는 것이 feed-forward network의 parameter를 update하는 방법이다.</p>


<p>이를 그림으로 표현하면 아래와 같다. (출처: <a href="http://tex.stackexchange.com/questions/162326/drawing-back-propagation-neural-network">링크</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/42-1.png" width="600"></p>

<p>이렇듯 backpropagation은 직접 weight를 바로 변화시키는 것이 아니라 오직 error만을 보고 gradient descent method based approach를 사용해 error를 minimize하는 방향으로 계속 weight를 update시키는 것이다. 또한 한 번 error가 연산된 이후에는 output layer에서부터 그 이전 layer로 &#8216;역으로&#8217; 정보가 update되기 때문에 이를 backpropagation, 한국어로는 역전사라고 하는 것이다.</p>




<h5 id="sgd">Stochastic Gradient Descent</h5>


<p>Gradient를 계산했으니 이제 직접 Gradient Descent를 써서 parameter만 update하면 된다. 그러나 문제가 하나 있는데, 일반적으로 neural network의 input data의 개수가 엄청나게 많다는 것이다. 때문에 정확한 gradient를 계산하기 위해서는 모든 training data에 대해 gradient를 전부 계산하고, 그 값을 평균 내어 정확한 gradient를 구한 다음 &#8216;한 번&#8217; update해야한다. 그러나 이런 방법은 너무나도 비효율적이기 때문에 Stochastic Gradient Descent (SGD) 라는 방법을 사용해야한다.</p>


<p>SGD는 모든 데이터의 gradient를 평균내어 gradient update를 하는 대신 (이를 &#8216;full batch&#8217;라고 한다), 일부의 데이터로 &#8216;mini batch&#8217;를 형성하여 한 batch에 대한 gradient만을 계산하여 전체 parameter를 update한다. Convex optimization의 경우, 특정 조건이 충족되면 SGD와 GD가 같은 global optimum으로 수렴하는 것이 증명되어있지만, neural network는 convex가 아니기 때문에 batch를 설정하는 방법에 따라 수렴하는 조건이 바뀌게 된다. Batch size는 일반적으로 메모리가 감당할 수 있을 정도까지 최대한 크게 잡는 것 같다.</p>




<h5 id="example">Backpropagation Algorithm: example</h5>


<p>이전에 chain rule로 gradient를 계산한다고 언급했었는데, 실제 이 chain rule이 어떻게 적용되는지 아래의 간단한 예를 통해 살펴보도록하자. 이때 계산의 편의를 위해 각각의 neuron은 sigmoid loss를 가지고 있다고 가정하도록 하겠다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-5.png" width="600"></p>

<p>이때 각각의 neuron의 input으로 들어가는 값을 \(in_{o_5}\), output으로 나가는 값을 \(out_{h_3}\)와 같은 식으로 정의해보자 (이렇게 된다면 in과 out은 \(out_{h_3} = \sigma(in_{h_3})\) 으로 표현 가능하다. - 이때 \(\sigma\)는 sigmoid function). 먼저 error를 정의하자. error는 가장 간단한 sum of square loss를 취하도록 하겠다. 우리가 원하는 target을 \(t\)라고 정의하면 loss는 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)가 될 것이다 (1/2는 미분한 값을 깔끔하게 쓰기 위해 붙인 상관없는 값이므로 무시해도 좋다). 그리고 우리가 원하는 값들은 \(\frac{\partial E}{\partial w_{13}}, \frac{\partial E}{\partial w_{14}}, \ldots, \frac{\partial E}{\partial w_{46}}\)이 될 것이다. 이제 가장 먼저 \(\frac{\partial E}{\partial w_{35}}\) 부터 계산해보자.</p>


<p>\[\frac{\partial E}{\partial w_{35}} = \frac{\partial E}{\partial out_{o_5}} * \frac{\partial out_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial w_{35}}. \]</p>


<p>즉, 우리가 원하는 derivative를 계산하기 위해서는 세 개의 다른 derivative (\(\frac{\partial E}{\partial out_{o_5}}, \frac{\partial out_{o_5}}{\partial in_{o_5}}, \frac{\partial in_{o_5}}{\partial w_{35}}\))를 계산해야한다. 각각을 구하는 방법은 다음과 같다.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{o_5}}\): error를 \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)라고 정의했으므로, \(\frac{\partial E}{\partial out_{o_5}} = out_{o_5} - t_5\)이다. - 이때 \(out_{o_5}\)와 \(t_5\)는 weight update이전 propagation step에서 계산된 값이다.</p></li>
    <li><p>\(\frac{\partial out_{o_5}}{\partial in_{o_5}}\): \(o_5\)는 sigmoid activation function을 사용하므로 \(out_{o_5} = \sigma(in_{o_5})\)이다. 또한 sigmoid function의 미분 값은 \(\frac{\partial \sigma(x)}{\partial x} = \sigma(x) (1 - \sigma(x))\)으로 주어지므로, 이 값을 대입하면 \(\frac{\partial out_{o_5}}{\partial in_{o_5}} = out_{o_5} (1 - out_{o_5})\)가 된다. - 역시 여기에서도 미리 계산한 \(out_{o_5}\)를 사용한다.</p></li>
    <li>\(\frac{\partial in_{o_5}}{\partial w_{35}}\): \(o_5\)로 들어온 값의 총 합은 앞선 layer의 output과 \(o_5\)로 들어오는 weight를 곱하면 되므로 \(in_{o_5} = w_{35} out_{h_3} + w_{45} out_{h_4}\)이고, 이것을 통해 \(\frac{\partial in_{o_5}}{\partial w_{35}} = out_{h_3}\)가 됨을 알 수 있다. - \(out_{h_3}\) 역시 이전 propagation에서 계산된 값이다.</li>
</ul>


<p>따라서 \(\frac{\partial E}{\partial w_{35}}\)의 derivative 값은 위의 세 값을 모두 곱한 것으로 계산 할 수 있다. 그림으로 표현하면 아래와 같은 그림이 될 것이다. 즉, &#8216;backward&#8217; 방향으로 derivative에 대한 정보를 &#8216;propagation&#8217;하면서 parameter의 derivative를 계산하는 것이다. 마찬가지 방법으로 \(w_{36}, w_{45}, w_{46}\)에 대한 derivative도 계산할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-7.png" width="300"></p>

<p>그럼 이번에는 그 전 layer의 paramter들 중 하나인 \(w_{13}\)의 derivative를 계산해보자. 이번에 계산할 과정도 위와 비슷한 그림으로 표현해보면 아래와 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/74-8.png" width="300"></p>

<p>그러면 이제 \(\frac{\partial E}{\partial w_{13}}\)을 구해보자.</p>


<p>\[\frac{\partial E}{\partial w_{13}} = \frac{\partial E}{\partial out_{h_3}} * \frac{\partial out_{h_3}}{\partial in_{h_3}} * \frac{\partial in_{h_3}}{\partial w_{13}}.\]</p>


<p>마찬가지로 각각을 구하는 방법에 대해 적어보자.</p>


<ul>
    <li><p>\(\frac{\partial E}{\partial out_{h_3}}\): \(E = \frac{1}{2}(t_5-out_{o_5})^2 + \frac{1}{2}(t_6-out_{o_6})^2\)를 \(E = E_{o_5} + E_{o_6}\)로 decompose 하면 이 미분 식은 \(\frac{\partial E_{o_5}}{\partial out_{h_3}} + \frac{\partial E_{o_6}}{\partial out_{h_3}}\)로 쓸 수 있다. 각각의 계산은 다음과 같다.</p></li>
    <ul>
        <li><p>\(\frac{\partial E_{o_5}}{\partial out_{h_3}} = \frac{\partial E_{o_5}}{\partial in_{o_5}} * \frac{\partial in_{o_5}}{\partial out_{h_3}}\)으로 쓸 수 있다. 이 중 앞의 값인 \(\frac{\partial E_{o_5}}{\partial in_{o_5}}\)은 이미 전 과정에서 계산했던 \(\frac{\partial E}{\partial out_{o_5}}\)과 \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)의 곱으로 계산가능하다. 뒤의 값은 \(\frac{\partial in_{o_5}}{\partial out_{h_3}} = w_{35}\)이므로 간단하게 계산할 수 있다.</p></li>
        <li><p>\(\frac{\partial E_{o_6}}{\partial out_{h_3}}\)도 위와 같은 방법으로 연산이 가능하다.</p></li>
    </ul>
    <li><p>\(\frac{\partial out_{h_3}}{\partial in_{h_3}}\): \(\frac{\partial out_{o_5}}{\partial in_{o_5}}\)와 같다. 따라서 \(out_{h_3} (1-out_{h_3})\)이다.</p></li>
    <li><p>\(\frac{\partial in_{h_3}}{\partial w_{13}}\): \(\frac{\partial in_{o_5}}{\partial w_{35}}\)와 같다. 따라서 \(out_{i_1}\)이다.</p></li>
</ul>


<p>이렇게 \(\frac{\partial E}{\partial out_{h_3}}\)에서는 앞에서 계산했던 값들을 재활용하고, 아래의 값들은 activation function과 network의 topological property에 맞는 derivative를 곱하는 방식으로 \(\frac{\partial E}{\partial w_{13}}\)을 구할 수 있다.</p>


<p>이렇듯 backpropagation algorithm은 forward propagation을 통해 필요한 값들을 미리 저장해두고, backward propagation이 진행되면서 위에서부터 loss에 대한 derivative를 하나하나 계산해나가면서 다음 layer에서 바로 전 layer에서 계산한 값들과 각 neuron 별로 추가적으로 필요한 derivative들을 곱해나가면서 weight의 derivative를 계산하는 알고리즘이다.</p>


<p>이렇게 한 번 전체 gradient를 계산한 다음에는 learning rate를 곱하여 전체 parameter의 값을 update한 다음, 다시 처음부터 이 과정을 반복한다. 보통 에러가 감소하는 속도를 관측하면서 &#8216;이 정도면 converge한 것 같다&#8217; 하는 수준까지 돌린다.</p>


<p>익숙해지려면 다소 시간이 걸리지만, 개념적으로 먼저 &#8216;error를 먼저 계산하고, 그 값을 아래로 전달해나가면서 바로 전 layer에서 계산한 미분값들을 사용해 현재 layer의 미분값을 계산한 다음, 그 값을 사용해 다음 layer의 미분값을 계산한다.&#8217; 라고 개념만 이해해두고 다시 차근차근 chain rule을 계산해나가면서 계산하면 조금 편하게 익숙해 질 수 있을 것이다.</p>




<h5>Backpropagation Algorithm: In Practice</h5>


<p>실제 backpropagtion을 계산해야한다고 가정해보자. 편의상 \(l\)번째 hidden layer를 \(y_l\) 이라고 해보자. 이 경우 각 layer에 대해 backpropagation algorithm을 위해 계산해야할 것은 총 두 가지 이다. Loss를 \(E\)라고 적었을 때 먼저 layer \(l\)의 parameter \(\theta_l\)의 gradient인 \(\frac{\partial E}{\partial w_l }\)을 구해야한다. 이 값은 \(\frac{\partial E}{\partial w_l } = \frac{\partial E}{\partial y_{l} } \frac{\partial y_{l} }{\partial w_{l} } \)을 통해 계산한다. 이때, \(\frac{\partial E}{\partial y_{l} } = \frac{\partial E}{\partial y_{l+1} } \frac{\partial y_{l+1} }{\partial y_{l} } \)이므로 \(\frac{\partial E}{\partial y_{l} }\)은 바로 전 layer에서 넘겨준 \(\frac{\partial E}{\partial y_{l+1} }\)의 값을 사용하여 계산하게 된다. 정리하면 실제 계산해야하는 값은 \(\frac{\partial y_{l+1} }{\partial y_{l}},\frac{\partial y_{l} }{\partial w_{l} } \) 두 가지이고, 이 값들을 사용해 \(\frac{E}{\partial y_{l}}, \frac{E}{\partial w_{l}}\)을 return하게 된다. 앞의 값은 다음 layer에 넘겨줘서 다음 input으로 사용하고, 두 번째 값은 저장해두었다가 gradient descent update할 때 사용한다.</p>


<p>두 가지 예를 들어보자. 먼저 Inner Product layer 혹은 fully connected layer이다. 이 layer가 inner product layer라고 불리는 이유는 input \(y_l\)에 대해 output \(y_{l+1}\)이 간단한 inner product 들이 모여있는 형태로 표현되기 때문이다. 예를 들어 \(y_{l+1, i}\)를 l+1 번째 layer의 i 번째 node라고 한다면, \(y_{l+1, i} = \sum_{j} w_{ij} y_{l, j}\)으로 표현할 수 있음을 알 수 있다. 그런데 이 값은 사실 vector \(w\)와 \(y_{l}\)의 inner product로 표현됨을 알 수 있다. 그렇기 때문에 fully connected layer를 inner product라고 부른다. 다시 본론으로 돌아와서 inner product의 output은 input과 weight의 matrix-vector multiplication인 \(y_{l+1} = W_l * y_l\)으로 표현할 수 있다.</p>


<p>따라서 \(\frac{\partial y_{l+1} }{\partial y_{l}} = W_l^\top\)이고, \(\frac{\partial y_{l} }{\partial W_{l} } = y_l\)이다. 이 값을 통해 실제 return하는 값은 \(\frac{\partial E }{\partial y_{l} } = \frac{\partial E }{\partial y_{l+1} } * W_l^\top \)와 \(\frac{\partial E }{\partial w_{l} } = \frac{\partial E }{\partial y_{l+1} } * y_{l} \)이 된다.</p>


<p>두 번째로 많이 사용하는 ReLU non-linearity의 gradient를 계산해보자. 이때 activation function은 마치 하나의 layer가 더 있는 것처럼 생각할 수 있다. 즉 \(y_{l+1} = max(0, y_{l})\)로 표현할 수 있을 것이다. Parameter는 없으니까 생략하면 만약 \(y_{l} \geq 0\)라면 \(\frac{\partial y_{l+1} }{\partial y_{l} = 1} \)이고, 아니라면 0이 될 것이다. 따라서 \(y_{l} \geq 0\)라면 \(\frac{\partial E }{\partial y_{l} } = \frac{\partial E }{\partial y_{l+1} } \)이 되고, 0보다 작다면 0이 될 것이다.</p>




<h5>정리</h5>


<p>Deep learning을 다루기 위해서는 가장 먼저 aritifitial neural network의 model에 대한 이해와 gradient descent라는 update rule에 대한 이해가 필수적이다. 이 글에서는 가장 기초적이라고 생각하는 feed-forward network의 model을 먼저 설명하고, paramter를 update하는 gradient descent algorithm의 일종인 backpropagation에 대한 개념적인 설명을 다루었다. 조금 어려울 수 있는 내용이니 다른 글들을 계속 참고하면서 보면 좋을 것 같다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 9월 13일: 글 등록</li>
<li>2015년 9월 14일: 오타수정, SGD 내용 추가 등</li>
<li>2015년 9월 20일: BP in practice 추가</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning, Yoshua Bengio and Ian J. Goodfellow and Aaron Courville, Book in preparation for MIT Press, 2015</a></li>
<li>wiki (<a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks">링크</a>)</li>
<li><a href="http://courses.cs.tau.ac.il/Caffe_workshop/Bootcamp/pdf_lectures/Lecture%203%20CNN%20-%20backpropagation.pdf">Caffe workshop &ndash; CNN backpropagation</a></li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning 1 &ndash; RBM, DNN, CNN</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Batch Normalization (ICML 2015)]]></title>
    <link href="http://SanghyukChun.github.io/88/"/>
    <updated>2015-08-25T21:25:00+09:00</updated>
    <id>http://SanghyukChun.github.io/88</id>
		<content type="html"><![CDATA[<p>Batch Normalization은 현재 <a href="http://image-net.org/challenges/LSVRC/2015/">ImageNet competition</a>에서 state-of-art (Top-5 error: 4.9%)를 기록하고 있는 Neural Network model의 기본 아이디어이다. 이 글에서는 arXiv에 제출된 (그리고 ICML 2015에 publish된) <a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 논문을 리뷰하고, batch normalization이 어떤 기술이고, 어떤 원리에 의해 작동하는지 등에 대해 다룰 것이다.</p>


<h5>Motivation: Deep learning의 속도를 어떻게 더 빠르게 만들 수 있을까?</h5>


<p>Deep learning이 잘 동작하고, 뛰어난 성능을 보인다는 것은 이제 누구나 알고 있다. 그러나 여전히 deep learning은 굉장히 시간이 오래 걸리는 작업이고, 그만큼 computation power도 많이 필요로 한다. 그 동안의 연구 결과를 보면, converge한 것 처럼 보이더라도 더 많이 돌리게 된다면 더 좋은 결과로 수렴한다는 것을 알 수 있는 만큼, deep neural network의 train 속도를 높이는 것은 전체적인 성능 향상에 도움이 될 것이다.</p>


<p>보통 Deep learning을 train할 때에는 stochastic gradient descent (SGD) method를 사용한다. SGD의 속도를 높이는 가장 naive한 방법은 learning rate를 높이는 것이지만, 높은 learning rate는 보통 gradient vanishing 혹은 gradient exploding problem을 야기한다는 문제가 있다.</p>


<p>Gradient vanishing은 backpropagation algorithm에서 아래 layer로 내려갈수록, 현재 parameter의 gradient를 계산했을 때 앞에서 받은 미분 값들이 곱해지면서 그 값이 거의 없어지는 (vanish하는) 현상을 의미한다. Gradient exploding은 learning rate가 너무 높아 diverge하는 현상을 말한다. Learning rate의 값이 크면 이 두 가지 현상이 발생할 확률이 높기 때문에 우리는 보통 작은 learning rate를 고르게 된다. 그러나 우리는 이미 일반적으로 learning rate의 값이 diverge하지 않을 정도로 크면 gradient method의 converge 속도가 향상된다는 것을 알고 있다. 따라서 이 논문이 던지는 질문은 다음과 같다. 자연스럽게 나오는 궁금증은 Gradient vanishing/exploding problem이 발생하지 않도록 하면서 learning rate 값을 크게 설정할 수 있는 neural network model을 design할 수 있는가?</p>


<h5>Internal Covariate Shift: learning rate의 값이 작아지는 이유</h5>


<p>Gradient vanishing problem이 발생하는 이유에 대해서는 여러가지 설명이 가능하지만 (exploding은 그냥 우리가 값을 작게 설정하여 해결할 수 있다) 이 논문에서는 internal covariate shift라는 개념을 제안한다. Covariate shift는 machine learning problem에서 아래 그림과 같이 train data와 test data의 data distribution이 다른 현상을 의미한다. 아래 그림 참고 (<a href="http://blog.bigml.com/2014/01/03/simple-machine-learning-to-detect-covariate-shift/">출처</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-1.jpg" width="500"></p>

<p>이 논문에서는 단순히 train/test input data의 distribution이 변하는 것 뿐 아니라, 각각의 layer들의 input distribution이 training 과정에서 일정하지 않기 때문에 문제가 발생한다고 주장하며, 이렇게 각각의 layer들의 input distribution이 consistent하지 않은 현상을 internal convariate shift라고 정의한다. 이 논문에서 이것이 문제가 된다고 주장하는 이유는, 각각의 layer parameter들은 현재 layer에 들어오는 input data 뿐만 아니라 다른 model parameter들에도 영향을 받기 때문이라고한다. 즉, gradient vanishing problem이 발생하는 이유를 backpropagation 과정에서 아래로 내려갈수록 이전 gradient들의 영향이 더 커져서 지금 parameter가 거의 update되지 않는다고 설명하는 것과 같은 맥락이다.</p>


<p>기존에는 이런 현상을 방지하기 위하여 ReLU neuron을 사용하거나 (Nair & Hinton, 2010), cafeful initialization을 사용하거나 (Bengio & Glorot, 2010; Saxe et al., 2013), leanring rate를 작게 취하는 등의 전략을 사용했지만, 그런 방법이 아닌 다른 방법을 통해 internal covariate shift 문제가 해결이 된다면 더 높은 learning rate를 선택하여 learning 속도를 빠르게하는 것이 가능할 것이다.</p>


<p></p>

<h5>Navie approach: Whitening</h5>


<p>따라서 이 논문의 목표는 internal covariate shift를 줄이는 것이다. 그렇다면 internal covariate shift는 어떻게 줄일 수 있을까? 이 논문에서는 엄청 간단하게 input distribution을 zero mean, unit variance를 가지는 normal distribution으로 normalize 시키는 것으로 문제를 해결하며, 이를 whitening이라한다 (LeCun 1998, Wiesler & Ney 2011). 주어진 column data \(X\in R^{d\times n}\)에 대해 whitening transform은 다음과 같다.</p>


<p>\[\hat{X} = Cov(X)^{-1/2} X, Cov(X) = E[( X - E[X] ) ( X - E[X] )^\top ].\]</p>


<p>그러나 이런 naive한 approach에서는 크게 두 가지 문제점들이 발생하게 된다.</p>


<ol>
<li>multi variate normal distribution으로 normalize를 하려면 inverse의 square root를 계산해야 하기 때문에 필요한 계산량이 많다.</li>
<li>mean과 variance 세팅은 어떻게 할 것인가? 전체 데이터를 기준으로 mean/variance를 training마다 계산하면 계산량이 많이 필요하다.</li>
</ol>


<p>따라서 이 논문에서는 이런 문제점들을 해결할 수 있으면서, 동시에 everywhere differentiable하여 backpropagation algorithm을 적용하는 데에 큰 문제가 없는 간단한 simplification을 제안한다.</p>


<h5>Batch Noramlization Transform</h5>


<p>앞서 제시된 문제점들을 해결하기 위하여 이 논문에서는 두 가지 approach를 제안한다.</p>


<ol>
<li>각 차원들이 서로 independent하다고 가정하고 각 차원 별로 따로 estimate를 하고 그 대신 표현형을 더 풍성하게 해 줄 linear transform도 함께 learning한다</li>
<li>전체 데이터에 대해 mean/variance를 계산하는 대신 지금 계산하고 있는 batch에 대해서만 mean/variance를 구한 다음 inference를 할 때에만 real mean/variance를 계산한다</li>
</ol>


<p>먼저 naive approach에서 covariance matrix의 inverse square root를 계산해야했던 이유는 모든 feature들이 서로 correlated되었다고 가정했기 때문이지만, 각각이 independent하다고 가정함으로써, 단순 scalar 계산만으로 normalization이 가능해진다. 이를 수식으로 표현하면 다음과 같다.</p>


<p></p>

<p>d dimensional data \(x = (x^{(1)}, x^{(2)}, \ldots, x^{(d)})\)에 대해 각각의 차원 \(k\) 마다 다음과 같은 식을 계산하여 \(\hat x\)를 계산한다</p>


<p>\[\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}.\]</p>


<p>그러나 이렇게 correlation을 무시하고 learning하는 경우 각각의 관계가 중요한 경우 제대로 되지 못한 training을 하게 될 수도 있으므로 이를 방지하기 위한 linear transform을 각각의 dimension \(k\)마다 learning해준다. 이 transform은 scaling과 shifting을 포함한다.</p>


<p>\[y^{(k)} = \gamma \hat{x}^{(k)} + \beta.\]</p>


<p>이때 parameter \(\gamma, \beta\)는 neural network를 train하면서 마치 weight를 update하듯 같이 update하는 model parameter이다.</p>


<p>두 번째로, 전체 데이터의 expectation을 계산하는 대신 주어진 mini-batch의 sample mean/variance를 계산하여 대입한다.</p>


<p></p>

<p>이제 앞서 설명한 두 가지 simplification을 적용하여 다음과 같은 batch normalization transform이라는 것을 정의할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-2.png" width="500"></p>

<p>이때, backpropagation에 사용되는 \(\gamma, \beta\) 그리고 layer를 위한 chain rule은 다음과 같이 계산된다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-4.png" width="500"></p>

<h5>Train/Inference with BN network</h5>


<p>앞에서 batch normalization transform을 각각의 layer input을 normalization하는데에 사용할 것이라는 설명을 했었다. 다시말해서 BN network는 기존 network에서 각각의 layer input 앞에 batch normalization layer라는 layer를 추가한 것과 구조가 동일하다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-5.png" width="500"></p>

<p>이때 자세한 알고리즘은 다음과 같다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-3.png" width="500"></p>

<p>주의해야할 점 하나는 train 과정에서는 mini-batch의 sample mean/variance를 사용하여 BN transform을 계산하였지만, inference를 할 때에도 같은 규칙을 적용하게 되면 mini-batch 세팅에 따라 inference가 변할 수도 있기 때문에 각각의 test example마다 deterministic한 결과를 얻기 위하여 sample mean/variance 대신 그 동안 저장해둔 sample mean/variance들을 사용하여 unbiased mean/variance estimator를 계산하여 이를 BN transform에 이용한다.</p>


<h5>BN network의 장점</h5>


<p>저자들이 주장하는 BN network의 장점은 크게 두 가지이다.</p>


<ol>
<li>더 큰 learning rate를 쓸 수 있다. internal covariate shift를 감소시키고, parameter scaling에도 영향을 받지 않고, 더 큰 weight가 더 작은 gradient를 유도하기 때문에 parameter growth가 안정화되는 효과가 있다.</li>
<li>Training 과정에서 mini-batch를 어떻게 설정하느냐에 따라 같은 sample에 대해 다른 결과가 나온다. 따라서 더 general한 model을 learning하는 효과가 있고, drop out, l2 regularization 등에 대한 의존도가 떨어진다.</li>
</ol>


<p>논문을 살펴보면 BN transform이 scale invariant하고, 큰 weight에 대해 작은 gradient가 유도되기 때문에 paramter growth를 안정화시키는 효과가 있다는 언급이 있다. 또한 regularization효과를 더 강화하기 위하여 매 mini-batch마다 training data를 shuffling하여 input으로 넣는데, 이때 한 mini-batch 안에서는 같은 데이터가 중복으로 나오지 않도록 shuffling하여 대입한다.</p>


<h5>실험</h5>


<p>먼저 BN network가 주장하는대로 잘 동작하는지 보여주는 실험이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-6.png" width="500"></p>

<p>가장 왼쪽은 MNIST data에 대해 BN을 쓴 것과 쓰지 않은 것의 convergence speed를 비교한 것이며, 다음 그림들은 BN을 사용했을 때와 사용하지 않았을 때, internal covariate shift가 어떻게 변화하는지를 보여주는 것이다. 한 뉴런의 training 동안 activation value의 변화를 plot한 것으로, 가운데 있는 선이 평균 값이고, 위 아래가 variance를 의미한다고 생각하면 된다. BN을 사용하면 처음부터 끝까지 거의 비슷한 distribution을 가진다는 것을 알 수 있다.</p>


<p></p>

<p>다음으로 single network에 대해 inception network와의 성능을 비교한 실험이다</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-7.png" width="500"></p>

<p>세팅은 전부 같고 inception network와 비교하여 BN이 추가되었는지 여부와 learning rate가 몇 배인지 (x5는 5배의 leanring rate를 취한 것이다) 여부만 다르게 설정하였음에도 불구하고 convergence speed와 심지어 최종 max acc까지 차이가 나는 것을 볼 수 있다.</p>


<p>이런 결과들을 기반으로 약간의 paramter tunning을 거쳐 아래와 같은 ImageNet state-of-art를 기록했다고 한다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/88-8.png" width="500"></p>

<h5>BN 네트워크 성능 accelerating하기</h5>


<p>BN을 추가하는 것 만으로 성능 개선이 엄청나게 일어나는 것은 아니며 다음과 같은 parameter tunning이 추가로 필요하다고 한다.
1. learning rate 값을 키운다 (0.0075 &ndash;> 0.045, 5)
2. drop out을 제거한다 (BN이 regularization 효과가 있기 때문이라고 한다)
3. l2 weight regularization을 줄인다 (BN이 regularization 효과가 있기 때문이라고 한다)
4. learning rate decay를 accelerate한다 (6배 더 빠르게 가속한다)
5. local response normalization을 제거한다 (BN에는 적합하지 않다고 한다)
6. training example의 per-batch shuffling을 추가한다 (BN이 regularization 효과를 증폭시키기 위함이다)
7. photometric distortion을 줄인다 (BN이 속도가 더 빠르고 더 적은 train example을 보게 되기 때문에 실제 데이터에 더 집중한다고 한다)</p>

<p>이런 parameter tunning이 추가로 이루어지고 나면, 기존 neural network보다 ImageNet에서 훨씬 좋은 성능을 내는 neural network를 구성할 수 있다고 한다.</p>


<h5>Summary of BN network</h5>


<ul>
<li>아이디어: 각 nonlinearity의 input으로 BN transform을 추가한다</li>
<li>BN transform은 다음과 같은 두 가지 simplification으로 구성된다

<ul>
<li>각 feature들의 correlation을 무시하고 각각 따로 normalize 하고 각각에 대한 linear transform을 같이 learning한다</li>
<li>Mini-batch의 sample mean/variance로 normalize 한다</li>
</ul>
</li>
<li>BN network의 train/inference에서는 다음과 같은 특이점이 있다

<ul>
<li>Train/inference의 forward rule이 다르다 (각각이 사용하는 mean/variance가 다르다)</li>
<li>Train 과정에서 mini-batch를 (중복없이) shuffling하여 train시킨다</li>
</ul>
</li>
<li>BN network를 사용함으로써 다음과 같은 효과를 볼 수 있다

<ul>
<li>Learning rate를 큰 값으로 설정할 수 있어 converge가 빠르다</li>
<li>Generalized model을 learning하는 효과가 있다</li>
</ul>
</li>
</ul>


<h5>Reference</h5>


<ul>
<li><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, Sergey Ioffe, Christian Szegedy, ICML 2015</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (16) Dimensionality Reduction (PCA, LDA)]]></title>
    <link href="http://SanghyukChun.github.io/72/"/>
    <updated>2015-06-17T05:21:00+09:00</updated>
    <id>http://SanghyukChun.github.io/72</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p>Machine Learning problem을 풀다보면, 종종 high dimensional 데이터를 다뤄야할 일이 생긴다. 그런데 dimension 높은 데이터를 다루다보면 여러 문제가 발생하는데, 높은 dimension으로 인해 생기는 대표적인 문제가 <a href="http://SanghyukChun.github.io/59#59-4-cd">예전 글</a>에서 다뤘던 Curse of dimensionality이다. 또한 많은 algorithm들에서 dimension이 complexity에 영향을 주는 경우가 많으므로 높은 dimension은 알고리즘의 성능에 악영향을 미치는 경우가 많다. 그렇기 때문에 많은 경우 데이터의 dimension이 높다면 다양한 방식의 dimenionality reduction 기술을 적용해 데이터의 차원을 낮추는 작업을 한다. 일반적으로 많이 사용하는 Dimensionality Reduction으로는 LDA와 PCA가 있으며, ICA, CCA 등의 방법도 종종 사용되며, 그 이외에도 RBM, Auto-encoder 등의 Neural network와 관련된 모델들도 존재한다. 이 글에서는 가장 많이 사용되는 방법들인 LDA와 PCA에 대해서만 다룰 것이다.</p>


<h5>Recall: Curse of Dimensonality</h5>


<p>Curse of Dimensionality는 데이터의 차원이 높아질수록 발생하는 여러 문제들을 통틀어 일컫는 말이다. 이런 문제가 발생하는 이유는 차원이 높아질수록 우리가 일반적으로 사용하는 Euclidean distance가 예상치 못한 방식으로 동작하기 때문이다. 예를 들어 d-차원 공간에서 임의의 점으로부터 거리가 1인 점들을 모아놓은 공간을 생각해봤을 때, d가 점점 커지면 커질수록 그 구의 대부분의 부피가 거의 surface에 가까운 엄청나게 얇은 shell에 존재한다는 것을 이미 예전 글에서 증명한바 있다. 다시 말해서 아주 높은 차원의 데이터는 우리가 원하지 않는 방향으로 움직일 가능성이 크다.</p>


<h5>Feature Extraction</h5>


<p>많은 상황에서 차원의 크기는 feature의 개수를 의미한다. 예를 들어 키, 몸무게, 나이, 성별이라는 네 가지 정보를 가지고 클러스터링을 한다고 생각해보자. 이 경우 데이터의 차원은 4이다. 그런데 아마도 이 네 가지 정보 이외에도 소득, 학력, 자산크기 등의 정보 등을 추가로 사용해 클러스터링을 한다면 더 좋은 클러스터링이 가능할지도 모른다. 문제는, 모든 feature가 전부 의미있는 feature는 아닐 수 있다는 것이다. 몸무게 정보를 사용하여 클러스터링한 것과 사용하지 않고 클러스터링한 것 중에서 몸무게 정보를 사용하지 않고 클러스터링 한 것이 더 좋을 수도 있다는 것이다. 이렇게 주어진 정보들 중에서 정말 의미 있는 feature를 뽑아내는 과정을 feature extration이라고 한다. 가장 간단한 feature extraction은 모든 feature를 사용해보기도 하고 사용해보지 않기도 하면서 \(2^d\) 개의 조합을 모두 확인해보는 것이다. 그러나 이 방법은 차원의 크기에 exponential할 뿐 아니라, 만약 기존 feature가 highly correlate되어있고, 여러 개의 feature를 묶어서 한 feature로 만들어야 성능이 좋아지는 경우 등에 대해 좋은 성능을 내기 어렵다. 따라서 이런 상황에서도 dimensionality reduction 방법을 사용해 feature를 뽑아낼 수 있다. 만약 우리가 100개의 feature를 가지고 있을 때, &#8216;가장 좋은&#8217; 30개의 feature만 뽑기 위해서 30차원으로 dimensionality reduction을 하는 것이다.</p>


<h5>Dimensionality Reduction</h5>


<p>데이터의 차원을 낮춘다는 것의 의미는, 현재 데이터가 존재하는 차원에서 그보다 낮은 다른 차원으로 데이터들을 mapping시키는 map을 찾는다는 것과 같다고 할 수 있다. 이때 어떤 임의의 차원에서 그보다 낮은 임의의 낮은 차원으로 가는 mapping은 셀 수 없이 많다. 그렇다면 우리는 어떤 mapping을 선택해야할까? 예를 들어 가장 간단한 방법으로, d 차원 데이터를 d&#8217; 차원으로 보내고 싶을 때, 앞에서 설명했던 간단한 방법처럼 임의의 d&#8217; 개의 축을 골라서 그 축만 사용하는 방법도 있을 수 있고, <a class="red tip" title="projection이라고도 한다.">d에서 d&#8217;으로 가는 linear map</a>을 임의로 하나 고르는 것도 가능하다. 물론 non linear map도 가능하지만, 이 글에서 다룰 LDA와 PCA 두 가지 방법은 모두 linear mapping을 찾는 알고리즘이다. LDA는 supervised learning이며, PCA는 unsupervised learning에 해당하게 된다. 모든 문제에서 데이터는 matrix \(X\)로 표기되며, 데이터의 차원은 d이고, 개수는 n개이다. 따라서 \(X\)는 d by n matrix가 된다.</p>


<h5>LDA</h5>


<p><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis (LDA)</a>는 Dimensionality reduction만을 위한 방법은 아니다. LDA로 약어가 표시되는 것들이 꽤 많아서 (예: Latent Dirichelt Allocation) 이 모델을 처음 제안한 사람의 이름을 따서 Fisher&#8217;s LDA 라고 부르기도 한다. LDA는 여러 클래스가 존재할 때 그 클래스들을 최대한 잘 분리시키는 projection을 찾는다. 철학은 굉장히 단순한데, projection 시킨 데이터들에서 같은 클래스에 속하는 데이터들의 variance는 최대한 줄이고 (\(\sigma_{within}\)), 각 데이터들의 평균 값들의 variance는 최대한 키워서 (\(\sigma_{between}\)) 클래스들끼리 최대한 멀리 떨어지게 만드는 것이다. 이를 수식으로 표현하면 다음과 같은 수식을 얻을 수 있다.</p>


<p>\[S = \frac{\sigma_{between}^2}{\sigma_{within}^2}\]</p>


<p>일단 가장 간단한 상황인 클래스가 2개일 때의 상황만 고려해보도록하자. 참고로 LDA의 결과는 항상 클래스 개수 - 1 개까지의 벡터 밖에 찾을 수 없기 때문에, 이 상황에서 우리가 찾게 될 projection은 1차원 projection을 찾는 것이므로 간단하게 vector \(w\)로 기술하도록 하겠다. 클래스가 2개 뿐이라면, 위의 식은 엄청 간단한 수식으로 바뀌게 된다. 먼저 \(\sigma_{between}\)은 데이터가 단 두 개 뿐이기 때문에 간단하게 \( (w \cdot \mu_1 - w \cdot \mu_2 )^2 \)으로 표현이 된다. 이때, \(\mu_1, \mu_2\)는 각각 1번째 클래스와 2번째 클래스에 속한 데이터들의 평균 값이다. 다음으로 \(\sigma_{within}\) 역시 어렵지 않게 계산할 수가 있다. Projection을 하게 되면 데이터의 variance는 \(w^\top \Sigma w\)로 표현이 되기 때문에, 1번 클래스와 2번 클래스에 대해 이 값을 계산하고 더해주기만 하면 된다. 식을 정리해보면</p>


<p>\[w = \arg\min_w \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_w\frac{(w \cdot \mu_1 - w \cdot \mu_2 )^2}{w^\top \Sigma_1 w + w^\top \Sigma_2 w} \]</p>


<p>그리고 위 식을 w에 대해 미분하고 좀 정리해보면 \(w \propto    (\Sigma_1 + \Sigma_2)^{-1}(\mu_1 - \mu_2) \) 라는 식을 얻을 수 있다.</p>


<h5>Multiclass LDA</h5>


<p></p>

<p>그러면 클래스가 2개보다 많을 때, 벡터 \(w\)가 아닌 subspace \(U\)를 찾는 과정은 어떻게 되는지 살펴보도록하자. 다시 \(\sigma_{within}\)과 \(\sigma_{between}\)을 살펴보자. 먼저 \(\sigma_{between}\)은 위와 비슷한 형태로 표현할 수 없다. 그러나 우리가 각각의 클래스의 평균을 \(\mu_i\)라고 정의한다면, 그냥 이 값들의 variance를 계산하기만 하면 된다. 이 variance는 \(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\)로 표현이 된다. 이때 \(\mu\)는 모든 평균들의 평균이다. 이 variance가 있으므로, projection하여 얻는 variance도 앞 뒤에 proejction matrix를 곱해 쉽게 계산할 수 있다. \(\sigma_{within}\)은 위와 비슷한 방식으로 \(\sigma_{within} = U^\top \left(\sum_i \Sigma_i \right) U \)로 표현할 수 있다. 이때 \(\Sigma_i\)는 i번째 클래스의 variance이다. 이 식을 정리해보면 다음과 같은 식을 얻는다.</p>


<p>\[U = \arg\min_U \frac{\sigma_{between}^2}{\sigma_{within}^2} = \arg\min_U \frac{U^\top \left(\sum_i (\mu_i - \mu)(\mu_i-\mu)^\top\right) U}{U^\top \left(\sum_i \Sigma_i \right) U} = \arg\max_U \frac{U^\top A U}{U^\top B} \]</p>


<p>\(A\)와 \(B\)는 각각 괄호 안에 있는 값을 의미한다. 위 식에서 보게 되면, 분자에 해당하는 부분이 rank가 클래스 개수 - 1 이기 때문에, 우리가 이 식을 풀었을 때도 \(U\)의 rank가 클래스 개수 - 1이 되므로 LDA가 구할 수 있는 subspace의 축 개수가 클래스 개수 - 1로 제한되는 것이다. 이 식을 풀기 위해서는 eigenvalue를 계산하는 것으로 간단하게 식을 풀 수 있다. 지금부터 왜 이 식이 eigenvalue를 푸는 것으로 해결이 가능한지를 살펴보자.</p>


<h5>General Eigenvector problem</h5>


<p>문제를 간단하게 하기 위해 전체 subspace가 아닌 vector \(w\)만 고려해보도록 하자. 따라서 식은</p>


<p>\[\min_w \frac{w^\top A w}{w^\top B w}\]</p>


<p>로 표현 가능하다. 이제 이 식을 w에 대해서 미분해보면 다음과 같은 식을 얻게 된다</p>


<p>\[\left(w^\top B w\right)^2 \big[ 2A w \left( w^\top B w \right) - 2B w \left( w^\top A w \right) \big] = 0\]</p>


<p>이를 잘 정리하면</p>


<p>\[Aw = \frac{w^\top A w} {w^\top B w} B w\]</p>


<p>로 표현이 되고, 우리가 원래 풀려고 했던 \(\frac{w^\top A w} {w^\top B w}\)를 \(\lambda\)라고 정의하면 식이 \(A w = \lambda B w\)라는 엄청 간단한 식이 나오게 된다. 이런 형태를 만족하는 \(\lambda\)를 찾는 문제를 general eigenvalue problem이라 부른다. 만약 \(B\)가 Identity matrix라면 우리가 아는 일반 eigenvalue problem이 된다. 따라서 원래 문제가 \(\lambda\)의 minimum을 찾는 것이었으니 이제 가장 작은 general eigenvalue를 찾기만 하면 우리가 풀고 싶었던 문제를 풀 수 있는 것이다.</p>


<h5>PCA</h5>


<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>는 Dimensionality reduction의 가장 대표적인 방법 중 하나이다. PCA는 projection된 데이터의 variance가 최대화되는 projection matrix를 찾는 문제이다. 그런데 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Low-rank_matrix_approximation">Eckart–Young theorem</a>에 의해서, 이 문제의 답이 데이터 \(X\)에 대한 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition(SVD)</a>으로 계산할 수 있다는 것이 알려져 있다. 그래서 PCA를 variance를 maximize하는 원래 정의대로 설명이 하는 경우도 많고, low rank matrix 문제로 설명하는 경우도 있다. 이 글에서는 low rank matrix estimation 문제로 설명하도록 하겠다. 이 문제의 objective는 아래 식과 같다.</p>


<p>\[\min_{rank(Z)=k} \| X - Z \|_F^2\]</p>


<p>이때 \(\|A\|_F\)는 <a href="http://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a>을 의미한다. 이 norm은 matrix의 모든 element들의 제곱을 더한 값이며, \(\| A \|_F^2 = {\tt tr} (A A^\top) = {\tt tr} (A^\top A) \) 라는 특성이 알려져있다. 이제 위의 식에서 \(Z\)를 UV decomposition한 후 대입해보면 다음과 같은 식을 얻는다.</p>


<p>\[\min_{U\in R^{d \times k}, V\in R^{n \times k}, U^\top U = I} \|X - UV^\top\|_F^2\]</p>


<p>이 식을 \(V\)에 대해 미분하면 optimal한 V는 \(V=X^\top U\)로 표현이 됨을 알 수 있으며, 이를 위의 식에 대입한 후, Frobenius norm의 성질을 잘 활용하면 아래와 같은 식이 나온다.</p>


<p>\[\max_{U\in R^{d \times k}, U^\top U = I} {\tt tr} (U^\top X X^\top U)\]</p>


<p>가 되며, 이 식의 답은 SVD를 통해 계산할 수 있다는 것을 알 수 있다. 그러나 만약 \(X\)의 mean이 0가 아니게 되면 UV decomposition을 하는 과정에서 문제가 생기게 되서 같은 방식으로 깔끔하게 구할 수가 없다. 이 과정은 다소 복잡하므로 생략하고 결과만 얘기하면, 그냥 \(\hat X = X - \frac{1}{n} X \mathbf 1\)을 SVD하면 된다. 뒤에 있는 term은 그냥 X의 평균값이다.</p>


<p>정리하면, 임의의 데이터 X에 대한 PCA는 다음과 같이 구할 수 있다.</p>


<ol>
    <li>데이터 \(X\)의 empirical mean을 계산한 후 모든 데이터에서 평균을 빼준다.</li>
    <li>새로 만들어진 데이터 \(\hat X\)의 가장 큰 singular value부터 k번째 큰 singular value까지에 대응하는 singular vector들을 구한다. (SVD를 통해)</li>
    <li>뽑아낸 k개의 singular vector로 U를 구성하고 return한다.</li>
</ol>


<p>쉽지만 그만큼 강력하다. 하지만 PCA의 경우 Frobenius norm의 제곱값을 사용하므로 각 element들이 norm을 계산할 때 한 번 제곱되고, 다시 전체에 제곱을 취할 때 또 제곱이 취해지므로 조금이라도 원래 데이터와 다른 outlier가 존재하게 된다면 그 효과가 굉장히 극적으로 증폭되기 때문에 noise에 취약하다. 이를 방지하기 위해 objective의 norm을 1-norm으로 바꾸거나 하는 등의 robust PCA 연구도 활발하게 진행되고 있다.</p>


<h5>정리</h5>


<p>Dimensionality Reduction은 매우 유용하고 많이 쓰이는 툴이다. 특히 PCA는 굉장히 빈번하게 사용되고 알고리즘도 매우 간단하기 때문에 알아두면 쓰임새가 많다. 이 글에서는 LDA와 PCA만 다뤘지만, ICA, CCA, RBM 등 굉장히 많은 dimensionality reduction 기술들이 존재한다. 이 중 RBM은 추후에 다시 한 번 자세하게 설명하도록 하겠다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 17일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Nie, Feiping, Jianjun Yuan, and Heng Huang. &ldquo;Optimal mean robust principal component analysis.&rdquo; Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (14) EM algorithm]]></title>
    <link href="http://SanghyukChun.github.io/70/"/>
    <updated>2015-06-14T03:50:00+09:00</updated>
    <id>http://SanghyukChun.github.io/70</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM 알고리즘</a>은 <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 하나이다. 굉장히 많은 probabilistic 모델을 풀기 위해 널리 사용되는 알고리즘 중 하나이며, iterative한 알고리즘 중 하나이다. Clustering에서 다뤘던 GMM은 물론이고, HMM, RBM 등의 문제를 해결하는데 있어서도 사용되는 알고리즘이다. 이 글에서는 EM 알고리즘이 무엇인지, latent variable이 존재하는 probabilistic model은 무엇이며 어떤 장점이 있는지를 다룰 것이며, EM 알고리즘의 의미와 더 나아가 이 알고리즘이 어떻게 MLE나 MAP문제를 해결하는지에 대해 다룰 것이다.</p>


<h5>Probabilistic model having latent variable</h5>


<p>EM 알고리즘에 대해 다루기 전에 먼저 latent variable을 가지고 있는 probabilistic model에 대해 설명하도록 하겠다. Latent variable은 우리가 본래 가지고 있는 random variable이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다. 예를 들어, 아래 그림과 같은 Grapical model을 고려해보자. 이때 우리가 관측할 수 있는 random variable은 paramter \(\theta\)로 parameterized 되어있는 \(\mathbf X\) 하나이고, \(\mathbf Z\)은 우리가 관측할 수 없는 hidden variable이라고 해보자.</p>


<p><img src="http://SanghyukChun.github.io/images/post/70-1.png" width="200"></p>

<p>만약 위의 grapical model에서 \(\mathbf X\)의 maximum likelihood를 계산하고 싶다면 어떻게 해야할까? 먼저 \(\mathbf X\)의 maximum likelihood는 다음과 같이 표현된다.</p>


<p>\[\max_{\theta} p(\mathbf X | \theta) = \sum_{\mathbf Z} p(\mathbf X,Z | \theta). \]</p>


<p>문제를 조금 더 간단하게 하기 위하여 위의 식에서 \(\mathbf Z\)는 discrete variable이라고 정의하였다. 이 문제에서 우리가 가정할 것이 하나있다. 바로 marginal distribution \(p(\mathbf X | \theta)\)를 직접 계산하는 것이 매우 까다롭다는 것이다. 이때, \(\mathbf Z\)는 우리 마음대로 정할 수 있는 latent variable이기 때문에, joint distribution \(p(\mathbf X,Z | \theta)\)가 marginal distribution보다 쉬운 \(\mathbf Z\)를 잡는 것이 가능하다.</p>


<h5>Decomposition of log-likelihood</h5>


<p>만약 우리가 latent variable \(\mathbf Z\)의 marginal distribution을 \(q(\mathbf Z)\)라고 정의한다면, 앞에서 설명한 log-likelihood를 다음과 같이 decompose할 수 있다.</p>


<p>\[\ln p(\mathbf X | \theta) = \mathcal L(q,\theta) + ~\mbox{KL}(q\|p),\]
이때, \(\mathcal L(q,\theta)\)와 \(\mbox{KL}(q\|p)\)는 다음과 같이 정의된다. \[\mathcal L(q,\theta) = \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf X, \mathbf Z | \theta)}{q(\mathbf Z)} ~\mbox{and}~ \mbox{KL}(q\|p) = - \sum_{\mathbf Z} q(\mathbf Z) \ln \frac{p(\mathbf Z | \mathbf X, \theta)}{q(\mathbf Z)}. \]</p>


<p>위의 식에서 \(\mathcal L(q,\theta)\)는 hidden variable \(\mathbf Z\)의 marginal distribution \(q(\mathbf Z)\)의 functional이고, \(\mbox{KL}(q\|p)\)는 \(q,p\)의 KL divergence를 의미한다. 이렇게 log-likelihood를 decompose하게 되면, 한 쪽에는 random variable \(\mathbf X, \mathbf Z\)의 joint distribution, 그리고 또 한 쪽은 conditional distribution으로 표현이 된다는 것을 알 수 있다. 또한 KL divergence의 특성에서부터 재미있는 사실을 하나 더 유추할 수 있는데, 바로 KL divergence가 반드시 0보다 크거나 같기 때문에 \(\mathcal L(q,\theta)\)이 곧 log-likelihood의 lower bound가 된다는 사실이다. 이를 그림으로 나타내면 아래 그림의 오른쪽 그림과 같다. (일단 \(theta^{\mbox{old}}, theta^{\mbox{new}}\)는 무시하자)</p>


<p><img src="http://SanghyukChun.github.io/images/post/70-2.png" width="500"></p>

<h5>EM algorithm</h5>


<p>위와 같은 사실로부터 lower bound가 maximum이 되도록하는 \(\theta\)와 \(q(\mathbf Z)\)의 값을 찾고, 그에 해당하는 log-likelihood의 값을 찾는 알고리즘을 설계하는 것이 가능할 것이다. 만약 \(\theta\)와 \(q(\mathbf Z)\)를 jointly optimize하는 문제가 어려운 문제라면 이 문제를 해결하는 가장 간단한 방법은 둘 중 한 variable을 고정해두고 나머지를 update한 다음, 나머지 variable을 같은 방식으로 update하는 alternating method일 것이다. EM 알고리즘은 이런 아이디어에서부터 시작하게 된다. EM 알고리즘은 E-step과 M-step 두 가지 단계로 구성된다. 각각의 step에서는 앞서 설명한 방법처럼 \(\theta\)와 \(q(\mathbf Z)\)를 번갈아가면서 한 쪽은 고정한채 나머지를 update한다. 이런 alternating update method는 한 번에 수렴하지 않기 때문에, EM 알고리즘은 E-step과 M-step을 알고리즘이 수렴할 때 까지 반복하는 iterative 알고리즘이 된다.</p>


<p>현재 우리가 가지고 있는 parater \(\theta\)의 값을 \(\theta^{\mbox{old}}\)라고 정의해보자. EM 알고리즘의 E-step은 먼저 \(\theta^{\mbox{old}}\) 값을 고정해두고 \(\mathcal L(q,\theta)\)의 값을 최대로 만드는 \(q(\mathbf Z)\)의 값을 찾는 과정이다. 이 과정은 매우 간단하게 계산 수 있는데, 그 이유는 log-likelihood \(\ln p(\mathbf X | \theta^{\mbox{old}})\)의 값이 \(q(\mathbf Z)\) 값과 전혀 관계가 없기 때문에, 항상 \(\mathcal L(q,\theta)\)를 최대로 만드는 조건은 KL divergence가 0이 되는 상황이기 때문이다. KL divergence는 \(q(\mathbf Z) = p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\) 인 상황에서 0이 되기 때문에, \(q(\mathbf Z)\)에 posterior distribution \(p(\mathbf Z | \mathbf X, \theta^{\mbox{old}})\)을 대입하는 것으로 해결할 수 있다. 따라서 E-step은 언제나 KL-divergence를 0으로 만들고, lower bound와 likelihood의 값을 일치시키는 과정이 된다.</p>


<p>E-step에서 \(\theta^{\mbox{old}}\)을 고정하고 \(q(\mathbf Z)\)에 대한 optimization 문제를 풀었으므로 M-step에서는 그 반대로, \(q(\mathbf Z)\)를 고정하고 log-likelihood를 가장 크게 만드는 새 paramter \(\theta^{\mbox{new}}\)을 찾는 optimization 문제를 푸는 단계가 된다. E-step에서는 update하는 variable과 log-likelihood가 서로 무관했기 때문에 log-likelihood가 증가하지 않았지만, M-step에서는 \(\theta\)가 log-likelihood에 직접 영향을 미치기 때문에 log-likelihood 자체가 증가하게 된다. 또한 M-step에서 \(\theta^{\mbox{old}}\)가 \(\theta^{\mbox{new}}\)로 바뀌었기 때문에 E-step에서 구했던 \(p(\mathbf Z)\)로는 더 이상 KL-divergence가 0이 되지 않는다. 따라서 다시 E-step을 진행시켜 KL-divergence를 0으로 만들고, log-likelihood의 값을 M-step을 통해 키우는 과정을 계속 반복해야만한다.</p>


<p>위에 나왔던 그림에서 왼쪽이 E-step을 의미하고, 오른쪽 그림이 M-step을 의미한다. E-step을 의미하는 왼쪽 그림에서 KL divergence는 0이 되고, lower bound인 functional과 log-likelihood의 값이 같아진다. 오른쪽 그림은 M-step을 표현하고 있으며, \(\theta\)가 update되면서 log-likelihood의 값이 증가하게 되지만, 더 이상 KL divergence의 값이 0이 아니게 된다. 이 과정을 더 이상 값이 변화하지 않을 때 까지 충분히 많이 돌리게 되면 이 값은 log-likelihood의 어떤 값으로 수렴하게 될 것이다. 그리고 매 step마다 항상 optimal한 값으로 진행하기 때문에 이 값은 log-likelihood의 local optimum으로 수렴하게 된다는 사실까지 알 수 있다. EM algorithm은 아래와 같은 그림으로 표현할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/70-3.png" width="500"></p>

<p>각 curve는 \(\theta\) 값이 고정이 되어있을 때 \(q(\mathbf Z)\)에 대한 lower bound \(\mathcal L(q,\theta)\)의 값을 의미한다. 매 E-step마다 고정된 \(\theta\)에 대해 \(p(\mathbf Z)\)를 풀게 되는데, 이는 곧 log-likelihood와 curve의 접점을 찾는 과정과 같다. 또한 M-step에서는 \(\theta\) 값 자체를 현재 값보다 더 좋은 지점으로 update시켜서 curve 자체를 이동시키는 것이다. 이런 과정을 계속 반복하면 알고리즘은 언젠가 local optimum으로 수렴하게 될 것이다. Local optimum에 수렴한다는 성질은 얼핏보면 나빠보일 수도 있지만, 이 글의 도입부에서 latent variable이 introduce되는 이유 자체가 원래 log-likelihood를 계산하는 것이 불가능에 가깝기 때문이었다는 사실을 돌이켜본다면, latent variable을 잘 잡기만 한다면 반드시 local optimum으로 수렴하는 EM 알고리즘은 매우 훌륭한 알고리즘이라는 사실을 알 수 있다. 즉, 아예 문제를 풀지 못하는 것 보다는 local optimum으로 수렴하는 것이 훨씬 좋다.</p>


<h5>Pratical issues</h5>


<p>대부분의 probabilistic model의 MLE 혹은 MAP는 EM 알고리즘을 사용하면 구할 수 있다. 그러나 EM 알고리즘이 항상 잘 동작하는 것은 아닌데, E-step 혹은 M-step의 optimization 문제를 푸는 것이 어려운 상황이 그러하다. E-step은 posterior를 계산하는 과정이므로 크게 문제가 되는 경우는 많지 않지만, M-step은 \(\theta\)에 대한 optimization 문제를 풀어야하는 과정인데, 이 과정에서 문제가 발생하는 경우가 많다. 예를 들어 모든 \(\theta\)를 한 번에 jointly optimize하는 것이 어려워 또 다른 alternative method를 사용해야할 수 도 있다. 그렇게 되면 iterative 알고리즘 안에 nested iterative 알고리즘이 발생하게 되어 전체 알고리즘의 수렴 속도가 매우 느려지게 된다. 가장 단순하게 이 문제를 해결하는 방법으로는 nested iterative 알고리즘을 완전히 푸는 것이 아니라, 수렴여부와 관계없이 iteration을 조금만 돌리고 다시 E-step을 구하고, 다시 M-step을 정확히 푸는 대신 iteration을 몇 번만 돌리는 등의 방식이 있을 것이다. 일반적인 경우에는 이런 방식이 수렴하지 않지만, 몇몇 경우에는 이런 방식이 local optimum에 수렴한다는 것이 증명되어있다. 가장 대표적인 예가 RBM을 푸는 Contrastive Divergence이다. 이에 대한 더 자세한 설명은 추후에 Deep learning에 대해 다루는 글에서 더 자세히 다루도록 하겠다.</p>


<h5>정리</h5>


<p>EM 알고리즘은 latent variable이 존재하는 probabilistic model의 maximum likelihood 혹은 maximum a posterior 문제를 풀기 위한 알고리즘 중 가장 대표적인 알고리즘이다. 본문에서는 MLE를 계산하는 과정만 다뤘지만, MAP도 비슷한 방식으로 구할 수 있다. Latent variable을 쉽게 잡기만한다면, 아무리 풀기 어려운 문제일지라도 구하고자 하는 문제의 local optimum에 수렴한다는 좋은 성질을 가지고 있기 때문에 매우 다양한 모델에서 이 알고리즘을 사용해 문제를 해결하고는 한다. 그러나 간혹 특히 M-step의 optimization을 푸는 과정에서 시간이 너무 오래 걸리는 문제가 발생하는 경우가 생길 수 있는데, 이런 경우 몇 가지 휴리스틱을 사용해 문제를 해결하기도 한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 6월 14일: 글 등록</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (13) Clustering (K-means, Gaussian Mixture Model)]]></title>
    <link href="http://SanghyukChun.github.io/69/"/>
    <updated>2015-03-25T03:20:00+09:00</updated>
    <id>http://SanghyukChun.github.io/69</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="57-4-ClassML">첫 번째 글</a>에서 설명했던 것 처럼 Machine Learning은 크게 Supervised Learning, Unsupervised Learning 그리고 Reinforcement Learning으로 구분된다. 앞서 이미 그 중 <a href="http://SanghyukChun.github.io/64">Supervised Learning</a>을 간략하게 다룬 글이 있었고, 이 글에서는 그 중 Unsupervised Learning의 가장 대표적인 예시인 Clustering 대해 다룰 것이며 가장 대표적이고 간단한 두 가지 알고리즘에 대해서 역시 다룰 것이다.</p>


<h5>What is Clustering?</h5>


<p><a href="http://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a>은 <a href="http://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning</a>의 일종으로, label 데이터 없이 주어진 데이터들을 가장 잘 설명하는 cluster를 찾는 문제이다. 왜 클러스터링이 필요할까? Classification을 하기 위해서는 데이터와 각각의 데이터의 label이 필요하지만, 실제로는 데이터는 존재하지만 그 데이터의 label이나 category가 무엇인지 알 수 없는 경우가 많기 때문에 classfication이 아닌 다른 방법을 통해 데이터들을 설명해야하는 경우가 발생한다. 아래 그림은 클러스터링이 어떤 것인지 잘 보여주는 그림이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/69-1.png" width="500"></p>

<p>우리에게 처음 주어진 것은 왼쪽 파란 데이터이다. 각각의 데이터에 대한 정보는 아무 것도 없는 상태에서 주어진 데이터들을 가장 잘 설명하는 클러스터를 찾아내는 것이 클러스터링의 목적이다. 따라서 클러스터링은 대부분 Optimization 문제를 푸는 경우가 많다. 이는 클러스터링 뿐 아니라 다른 많은 unsupervised learning에서도 역시 마찬가지이다.</p>


<p><a href="http://SanghyukChun.github.io/57">첫 번째 글</a>과 <a href="http://SanghyukChun.github.io/64">이전 글</a>에서 머신러닝 문제는 먼저 주어진 데이터에 대한 가정을 하고, 해당 가정을 만족하는 best hypothesis를 찾는 문제라고 언급한 적이 있다. Clustering 문제 역시 Machine Learning 문제이므로 데이터에 대한 가정을 먼저 해야하고, best hypothesis를 찾는 과정을 거친다. 따라서 각각의 서로 다른 clustering algorithm들은 서로 다른 assumption을 가지고 있으며, 해당 assumption을 가장 잘 만족하는 function parameter를 계산하는 과정이다. 앞으로 설명하게 될 알고리즘들에 대한 설명을 읽을 때 데이터에 대해 어떤 가정을 하였는지 꼼꼼히 확인하면서 읽으면 알고리즘을 이해하기 한결 수월할 것이다.</p>


<h5>K-means</h5>


<p>클러스터를 정의하는 방법에는 여러 가지가 있을 수 있지만, 가장 간단한 정의 중 하나는 클러스터 내부에 속한 데이터들이 서로 &#8216;가깝다&#8217;라고 정의하고, &#8216;가장 가까운&#8217; 내부 거리를 가지는 클러스터를 고르는 것이다. <a href="http://en.wikipedia.org/wiki/K-means_clustering">K-means</a>는 같은 클러스터에 속한 데이터는 서로 &#8216;가깝다&#8217; 라고 가정한다. 이때 각각의 클러스터마다 &#8216;중심&#8217;이 하나씩 존재하고, 각각의 데이터가 그 중심과 &#8216;얼마나 가까운가&#8217;를 cost로 정의한다. K-means는 이렇게 정의된 cost를 가장 줄이는 클러스터를 찾는 알고리즘이다. 수식으로 적으면 다음과 같다.</p>


<p>$$ \min_{b,w} \sum_i^n \sum_j^k w_{ij} \| x_i - b_j \|_2^2 \text{ s.t. } \sum_j w_{ij} = 1, \forall j$$</p>


<p>데이터는 \(n\)개 있으며 클러스터는 \(k\)개 있다고 가정했다. 이때, \(b_j\)는 \(j\) 번째 클러스터의 &#8216;중심&#8217;을 의미하며, \(w_{ij}\)는 \(i\) 번째 데이터가 \(j\) 번째 클러스터에 속하면 1, 아니면 0을 가지는 binary variable이다. 또한 뒤에 붙는 조건은 반드시 각 데이터 별로 한 개의 클러스터에 assign이 되어야한다는 constraint이다.</p>


<p>이 문제는 풀기 쉬운 문제가 아니다. Binary variable \(w_{ij}\) 때문에, 모든 cluster 조합을 하나하나 확인해야만 optimal한 값을 구할 수 있다. 즉, jointly optimize하는 것이 매우 어렵다. 그러나 재미있게도 \(b\)와 \(w\) 둘 중 하나를 고정하고 나머지 하나를 update하는 것은 매우 간단하다. 나머지 값이 고정되었을 때 \(b_j\)의 optimal값은 \(j\) 번째 클러스터의 &#8216;mean&#8217;을 계산하는 것이다 (이 때문에 &#8216;\(k\)&#8217; 개의 &#8216;mean&#8217;을 찾는다고 해서 k-means 알고리즘이다). \(w_{ij}\)의 optimal 값은 모든 데이터 i에 대해, 각각 모든 클러스터 중에서 \(x_i - b_j\)가 가장 작은 클러스터에 assign하는 것이 optimal한 solution이다. 이렇듯 만약 다른 변수 하나를 정확하게 알고 있다고 생각하면 아주 간단한 방법으로 alternative optimization이 가능하다. 사실 이 개념은 예전에 적은 <a href="http://SanghyukChun.github.io/63">convex optimization</a>글에서 잠깐 언급했던 coordinate descent 방법과 거의 유사하다. K-means 알고리즘은 이렇게 \(b\)와 \(w\)를 alternative하게 계속 update하면서 \(b\)와 \(w\)가 더 이상 바뀌지 않을 때 까지 계산을 반복하는 알고리즘이다. 안타깝게도 K-means는 global optimum에 수렴하지 않고 local한 optimum에 수렴하므로 initialization에 매우매우 취약하다는 단점이 존재한다.</p>


<p>또한 여담으로 K-means objective function에 사용한 \(\ell_2\) norm의 제곱이 outlier 혹은 noise에 매우 취약하기 때문에 조금 더 outlier에 덜 sensitive한 &#8216;robust한&#8217; norm을 사용하는 방법도 존재한다. 예를 들어 \(\ell_2\) norm의 제곱을 \(\ell_1\) norm으로 바꾸면 &#8216;mean&#8217; 대신에 &#8216;median&#8217;을 찾는 문제로 바뀌게 된다. 이를 <a href="http://en.wikipedia.org/wiki/K-medians_clustering">k-median</a>이라고 부른다. 그 밖에도 k-means의 robustness를 개선하기 위한 다양한 방법들이 개발이 되어있지만 이 글에서는 다루지 않도록 하겠다.</p>


<h5>Gaussian Mixture Model</h5>


<p>K-means algorithm의 key idea는 &#8216;alternative update&#8217;이다. 즉, coordinate wise로 다른 변수들을 고정한 채로 &#8216;alternative&#8217;하게 변수들을 update함으로써 jointly optimization을 할 수 없는 문제를 푸는 것이다. 비록 그 결과가 global하지 않은 local에 converge하더라도, 찾지 못하는 것보다는 훨씬 낫기 때문에 실제로 이런 방법이 많이 쓰인다. 이번 섹션에서는 이런 방법을 사용하는 또 다른 알고리즘을 하나 소개하도록 하겠다.</p>


<p>Gaussian Mixture Model, Mixture of Gaussian, GMM, 혹은 MoG는 데이터가 &#8216;Gaussian&#8217;의 &#8216;Mixture&#8217;로 구성이 되어있다고 가정한다. 보통 GMM이라고 많이 부르며, 이 글에서 다루는 GMM은 가장 optimal한 GMM을 찾는 알고리즘을 의미한다. 즉, 데이터가 \(k\)개의 gaussian으로 구성되어있다고 했을 때, 가장 데이터를 잘 설명하는 \(k\)개의 평균과 covariance를 찾는 알고리즘이다. 아래 그림은 3개의 gaussian으로 구성되어있다고 가정하고 그 gaussian 분포들을 찾은 결과이다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/69-2.png" width="500"></p>

<p>모든 machine learning 문제는 &#8216;performance measure&#8217;를 가진다고 예전에 얘기한 적이 있다. GMM의 performance measure는 log likelihood function이다. 즉, 주어진 paramter에 대해 데이터 X의 확률을 가장 크게 만드는 parameter를 찾는 것이 목표가 된다. log likelihood는 \(\ln p(X|\theta)\)로 정의가 된다. 우리가 찾아야하는 parameter \(\theta\)는 각각의 gaussian의 평균 \(\mu_j\), covariance \(\Sigma_j\), 그리고 마지막으로 각각의 데이터가 각각의 gaussian에 속할 확률 \(\pi_j\)로 구성된다. 따라서 주어진 \(\mu_j, \Sigma_j\)에 대한 \(x_i\)의 multinomial gaussian distribution을 \(N(x_i|\mu_j, \Sigma_j)\)라고 정의한다면 log likelihood function은 다음과 같이 기술할 수 있다.</p>


<p>$$ \ln p(X|\pi, \mu, \Sigma) = \sum_i^n \ln \sum_j^k \pi_j N (x_i | \mu_j, \Sigma_j) $$</p>


<p>그러나 역시 이 문제도 jointly update가 매우매우 어려운 문제이다. 그러나 K-means와 비슷하게도 \(\pi\)를 고정하고 \(\mu, \Sigma\)를 계산하는 것은 쉬우며, 그 반대 역시 쉽다. 따라서 비슷하게 alternative update를 통해 문제를 해결하는 알고리즘을 어렵지 않게 디자인 할 수 있다. 역시 이 알고리즘도 global로 수렴하지 않고 local로만 수렴하게 된다. GMM을 풀기 위해서 사용되는 알고리즘 중 가장 유명한 알고리즘으로는 <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">&#8216;EM&#8217; 알고리즘</a>이 존재한다. 이 알고리즘에 대해서는 <a href="http://SanghyukChun.github.io/70">다음 글</a>에서 더 자세하게 설명하도록 하고, 이 글에서는 EM 알고리즘이라는 것을 알고 있는 상태에서 어떻게 각 step을 해결할 수 있는지에 대해서만 다루도록 하겠다. E-step에서는 현재 주어진 \(\mu, \Sigma, \pi\)들을 사용해 가장 &#8216;expectation&#8217;이 높은 latent variable의 값을 찾아내며, M-step에서는 새로 estimate된 latent variable을 사용해 그 값을 maximize하는 \(\mu, \Sigma, \pi\)를 찾는다. EM 알고리즘은 E-step과 M-step이 계속 번갈아 진행되며, 더 이상 값이 변하지 않을 때 까지 반복된다. K-means를 이런 관점으로 바라보게 된다면, cluster information \(w_{ij}\)를 update하는 과정이 E-step, \(b\)를 update하는 과정을 M-step이라고 할 수 있다. (그러나 K-means을 푸는 알고리즘은 엄밀하게 말하면 EM 알고리즘이 아니다)</p>


<p>사실 엄밀하게 설명하면 GMM을 푸는 EM에서 E-step 때 update하는 것은 정확하게 \(\pi\)와 같은 것은 아니다. EM으로 이 문제를 풀기 위해서 우리는 새로운 &#8216;latent&#8217; variable을 introduce해야한다. latent variable은 쉽게 생각하면 graphical model에서 hidden variable에 해당하는 값이다. 다음에 EM 알고리즘에 대해 자세히 다룰 때 다시 설명하겠지만, 이렇게 latent variable을 설정하는 이유는 어떤 특정 variable의 marginal distribution을 optimize하는 것은 어려울 때, latent variable을 사용해 그 variable과 latent variable의 joint distribution을 다루는 것은 간단할 수 있기 때문이다. GMM에서는 latent variable \(z\)를 introduce하게 된다. \(z\)는 \(k\)-ary variable로, \(z\)의 \(j\) 번째 dimension인 \(z_j\)는 Binary random variable이며, \(p(z_j=1) = pi_j, \) where \(\sum_j z_j = 1\) and \(\sum_j \pi_j = 1\) 이라는 조건을 가지고 있다. \(z\)의 marginal probability는 \(p(z) = \prod_j \pi_j^{z_j}\)로 어렵지 않게 계산할 수 있으며, 비슷하게 주어진 데이터 \(x\)에 대한 conditional distribution 역시 간단하게 다음과 같이 표현된다. \(p(x|z) = \prod_j N(x|\mu_j, \Sigma_j)^{z_j}\).</p>


<p>따라서 앞선 식들로부터 joint distribution을 얻을 수 있고, 그 값을 marginalize해 다음과 같은 결과를 얻을 수 있다.</p>


<p>$$ p(x) = \sum_z p(x,z) = \sum_z p(z)p(x|z) = \sum_j \pi_j N(x|\mu_j, \Sigma_j) $$</p>


<p>정리하자면, 각각의 data point \(x_i\) in GMM의 graphical representation은 다음과 같이 표현할 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/69-3.png" width="300"></p>

<p>위의 결과들을 토대로 이제 간단하게 EM algorithm을 돌릴 수 있다. 먼저 E step에서는 \(p(z_j = 1|x)\)를 계산할 것이다. 각 데이터에 대해 \(j\) 번째 클러스터에 속할 확률, 혹은 posterior를 계산하는 과정이다. 이 값은 Bayes rule을 통해 간단하게 다음과 같이 계산할 수 있다.</p>


<p>$$ p(z_j = 1|x) = \frac{p(z_j=1)p(x|z_j=1)}{\sum_j^k p(z_j=1)p(x|z_j=1)} = \frac{\pi_j N(x|\mu_j, \Sigma_j)}{\sum_j^k \pi_j N(x|\mu_j \Sigma_j)} $$</p>


<p>다음으로 \(z\)를 fix했을 때 다음과 같이 나머지 paramter를 계산할 수 있다.</p>


<p>$$ \mu_j = \frac{1}{\sum_i p(z_j=1|x)} \sum_i p(z_{ij}=1|x) x_i$$</p>


<p>$$ \Sigma_j = \frac{1}{\sum_i p(z_j=1|x)} \sum_i p(z_{ij}=1|x) (x_i-\mu_j) (x_i-\mu_j)^\top$$</p>


<p>$$ \pi_j = \frac{\sum_i p(z_j=1|x)}{N}$$</p>


<p>정리하자면, GMM을 풀기 위한 EM 알고리즘은, 먼저 각각의 데이터가 어느 클러스터에 속할지에 대한 정보를 update해 (\(z\)를 업데이트 하여) expectation을 계산하고, 다음으로 업데이트 된 정보들을 사용해 나머지 값들로 가장 log likelihood를 최대화하는 parameter들을 (\(\mu, \Sigma, \pi\)를) 찾아낸다. 알고리즘을 돌리면 아래처럼 iteration이 지날 때 마다 점점 좋은 값을 찾아준다. (출처: <a href="http://kipl.tistory.com/64">Geometry & Recognition :: Gaussian Mixture Model & K-means</a>)</p>


<p><img src="http://SanghyukChun.github.io/images/post/69-4.gif" width="500"></p>

<p>EM 알고리즘에 대한 심도있는 이해 없이 이 글을 이해하는 것은 조금 어려울 수 있다. 이 글이 잘 이해가 되지 않는다면 먼저 EM 알고리즘에 대해 설명한 <a href="http://SanghyukChun.github.io/70">다음 글</a>을 읽어본 다음 다시 읽어보기를 권한다.</p>


<h5>정리</h5>


<p>Clustering은 unsupervised learning 분야에서 가장 활발히 연구되는 분야 중 하나이다. 이 글에서는 여러 종류의 클러스터링 알고리즘 중에서 optimization function을 (1) 거리 기반으로 세우고 그것을 푸는 알고리즘과 (2) 확률과 확률분포를 기반으로 세우고 그것을 푸는 알고리즘을 소개하였다. 특히 GMM은 다음 글에서 다룰 주제인 EM algorithm과 밀접하게 관련되는 내용이므로 한 번 쯤은 책이나 렉쳐노트를 정독하는 것을 권한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 3월 25일: 글 등록</li>
<li>2015년 6월 14일: EM 알고리즘 링크 추가 및 설명 변경</li>
</ul>


<h5>Reference</h5>


<ul>
<li>Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006. Chapter 9</li>
<li><a href="http://kipl.tistory.com/64">Geometry &amp; Recognition :: Gaussian Mixture Model &amp; K-means</a></li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine learning 스터디 (8) Classification Introduction (Decision Tree, Naïve Bayes, KNN)]]></title>
    <link href="http://SanghyukChun.github.io/64/"/>
    <updated>2015-03-25T02:10:00+09:00</updated>
    <id>http://SanghyukChun.github.io/64</id>
		<content type="html"><![CDATA[<h5>들어가며</h5>


<p><a href="http://SanghyukChun.github.io/57#57-4-ClassML">첫 번째 글</a>에서 설명했던 것 처럼 Machine Learning은 크게 Supervised Learning, Unsupervised Learning 그리고 Reinforcement Learning으로 구분된다. 이 글에서는 그 중 Supervised Learning의 가장 대표적인 예시인 Classification에 대해 다룰 것이며 가장 대표적이고 간단한 세 가지 알고리즘에 대해서 역시 다룰 것이다.</p>


<h5>What is Classification?</h5>


<p><a href="http://en.wikipedia.org/wiki/Statistical_classification">Classification</a>은 <a href="http://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a>의 일종으로, 기존에 존재하는 데이터와 category와의 관계를 learning하여 새로 관측된 데이터의 category를 판별하는 문제이다. 스팸 필터를 예로 들어들어보자. 스팸 필터의 데이터는 이메일이고, category, 혹은 label, class는 spam메일인지 일반 메일인지를 판별하는 것이 될 것이다. 스팸필터는 먼저 스팸 메일, 그리고 일반 메일을 learning을 한 이후, 새로운 데이터 (혹은 메일)이 input으로 들어왔을 때 해당 메일이 스팸인지 일반 메일인지 판별하는 문제를 풀어야하며, 이런 문제를 classification이라고 한다.</p>


<p><a href="http://SanghyukChun.github.io/57">첫 번째 글</a>에서 머신러닝 문제는 먼저 주어진 데이터에 대한 가정을 하고, 해당 가정을 만족하는 best hypothesis를 찾는 문제라고 언급한 적이 있다. Classification 문제 역시 Machine Learning 문제이므로 데이터에 대한 가정을 먼저 해야하고, best hypothesis를 찾는 과정을 거친다. 따라서 각각의 서로 다른 classification algorithm들은 서로 다른 assumption을 가지고 있으며, 해당 assumption을 가장 잘 만족하는 function parameter를 계산하는 과정이다. 앞으로 설명하게 될 알고리즘들에 대한 설명을 읽을 때 데이터에 대해 어떤 가정을 하였는지 꼼꼼히 확인하면서 읽으면 알고리즘을 이해하기 한결 수월할 것이다.</p>


<h5>Decision Tree</h5>


<p><a href="http://en.wikipedia.org/wiki/Decision_tree">Decision Tree</a>는 가장 단순한 classifier 중 하나이다. 이 Decision Tree의 구조는 매우 단순하다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/64-1.png" width="600"></p>

<p>위의 그림은 오늘 외출을 할까 말까를 결정하는 decision tree이다. 이렇듯 decision tree는 tree구조를 가지고 있으며, root에서부터 적절한 node를 선택하면서 진행하다가 최종 결정을 내리게 되는 model이다. Decision tree의 가장 좋은 점은 단순하다는 점이다. 누구나 쉽게 이해할 수 있고, 그렇기 때문에 쉽게 디버깅할 수 있다. 예를 들어 위의 예시에서 습도가 높아도 나갈만하다는 생각이 든다면 맨 왼쪽의 No를 Yes로 바꾸기만 하면 간단하게 로직을 바꿀 수 있다. 그러나 다른 모델들은 그런 점들이 비교적 어렵다. Machine Learning에서 말하는 decision tree는 <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">decision tree learning</a>으로, 일일이 node마다 로직을 사람이 써넣어 만드는 것을 의미하는게 아니라, node 개수, depth, 각각의 node에서 내려야하는 결정 등을 데이터를 통해 learning하는 algorithm들을 사용해 만든 decision tree를 의미한다.</p>


<p>많이 쓰이는 알고리즘들로는 <a href="http://en.wikipedia.org/wiki/ID3_algorithm">ID3</a>, <a href="http://en.wikipedia.org/wiki/C4.5_algorithm">C4.5</a>, <a href="http://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees">CART</a>, <a href="http://en.wikipedia.org/wiki/CHAID">CHAID</a>, <a href="http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines">MARS</a> 등이 있으며, 보통 C4.5를 가장 많이 사용한다.</p>


<p>C4.5는 ID3의 몇 가지 문제점들을 개선한 알고리즘으로, 그 기본 개념은 ID3와 크게 다르지 않다. 추후 이 글 혹은 다른 글에 ID3 알고리즘에 대한 내용을 추가하도록 하겠다.</p>


<h5>Regression Tree and Ensemble method</h5>


<p>Decision tree는 output value가 반드시 binary여야한다는 제약조건이 있기 때문에 스팸 필터 등에서는 사용할 수 있지만, 실제 모든 데이터가 binary만을 output으로 가지지 않으므로 모든 데이터에 사용하려면 변형이 필요하다. <a href="http://en.wikipedia.org/wiki/Decision_tree_learning#Types">Regression tree</a>는 binary가 아니라 real value를 output으로 가지는 모델로, learning하는 방법은 크게 다르지 않다고 한다.</p>


<p>가끔은 하나의 decision tree를 사용하는 것이 아니라 한 번에 여러 개의 decision tree들을 만들어서 각각의 decision tree들이 내리는 결정을 종합적으로 판단하여 (ensemble) 결정을 내리기도 한다. <a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">Bagging decision tree</a>, <a href="http://en.wikipedia.org/wiki/Random_forest">random forest</a>등이 이에 속한다. 이런 식으로 여러 개의 classifier를 사용해 decision을 내리는 방법을 ensemble method라고 하는데, industry에서는 machine learning algorithm의 성능을 높이기 위해서 여러 개의 알고리즘들을 ensemble method를 사용하여 한 번에 같이 사용하기도 한다. 대표적인 예로 <a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a> 등이 있다.</p>


<p>Ensemble method에 대해서는 나중에 따로 다시 설명할 예정이므로 그 글을 참고하면 좋을 것 같다. (링크는 추후 추가 예정)</p>


<h5>Naïve Bayes</h5>


<p><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naïve Bayes Classifier</a>는 <a href="58-1-Bayes">Bayesian rule</a>에 근거한 classifier이다. Naïve Bayes는 일종의 확률 모델로, 약간의 가정을 통해 문제를 간단하게 푸는 방법을 제안한다. 만약 데이터의 feature가 3개 있고, 각각이 binary라고 해보자. 예를 들어 남자인지 여자인지, 성인인지 아닌지, 키가 큰지 작인지 등의 feature를 사용해 사람을 구분해야한다고 생각해보자. 이 경우 적어도 8개의 데이터는 있어야 모든 경우의 수를 설명할 수 있게 된다. 그런데 보통 데이터를 설명하는 feature의 개수는 이보다 훨씬 많은 경우가 많다. 예를 들어 feature가 10개 정도 있고 각각이 binary라면, 제대로 모든 데이터를 설명하기 위해서는 \(2^{10}\), 약 1000개 이상의 데이터가 필요하다. 즉, 필요한 데이터의 개수가 feature 혹은 데이터의 dimension에 exponential하다. 이런 경우 그냥 Bayes rule을 사용해 분류를 하게 되면 overfitting이 되거나 데이터 자체가 부족해 제대로 된 classification을 하기 어려울 수 있다. Naïve bayes는 이런 문제를 해결하기 위해 새로운 가정을 하나 하게 된다. 바로 모든 feature들이 i.i.d.하다는 것이다. i.i.d는 independent and identically distributed의 준말로, 모든 feature들이 서로 independent하며, 같은 분포를 가진다는 의미이다. 당연히 실제로는 feature들이 서로 긴밀하게 관련되어있고 다른 분포를 가질 것이므로 이 가정은 틀린 가정이 될 수 있다. 그러나 만약 모든 feature가 i.i.d.하다고 가정하게 된다면 우리가 필요한 최소한의 데이터 개수는 feature의 개수에 exponential하게 필요한게 아니라 linear하게 필요하게 된다. 간단한 가정으로 모델의 complexity를 크게 줄일 수 있는 것이다. 때문에 Naïve Bayes 뿐 아니라 많은 모델에서 실제 데이터가 그런 분포를 보이지 않더라도 그 데이터의 분포를 특정한 형태로 가정하여 문제를 간단하게 만드는 기술을 사용한다.</p>


<p>조금 더 엄밀하게 수식을 사용해 설명을 해보자. 우리가 가지고 있는 input data 를 \(x = (x_1, \ldots, x_n)\)이라고 가정해보자. 즉 우리는 총 \(n\)개의 feature를 가지고 있다고 가정해보자 (보통 \(n\)은 데이터의 개수를 의미하지만, wikipedia의 notation을 따라가기 위하여 이 글에서도 dimension을 나타내기 위해 \(n\)을 사용하였다). 그리고 Class의 개수는 \(k\)라고 해보자. 우리의 목표는 \(p(C|x_1, \ldots, x_n) = p(C|x)\)를 구하는 것이다. 즉, 1부터 \(k\)까지의 class 중에서 가장 확률이 높은 class를 찾아내어 이를 사용해 classification을 하겠다는 것이다. Bayes rule을 알고 있으므로 이 식을 bayes rule을 사용해 전개하는 것은 간단하다.</p>


<p>$$ p(C|x) = \frac{p(C) p(x|C)}{p(x)} $$</p>


<p>이 때 분모에 있는 데이터의 확률은 normalize term이기 때문에 모든 값을 계산하고 나서 한 번에 계산하면 되므로 우리는 \(p(x,C) = p(C) p(x|C)\), 다시 말해 prior와 likelihood를 계산해야만한다. 그러나 이 값은 joint probability이므로 데이터에서부터 이 값을 알아내기 위해서는 &#8216;엄청나게 많은&#8217; 데이터가 필요하다. 구체적으로는 앞서 말한 것 처럼 dimension에 exponential하게 많은 데이터 개수를 필요로 한다. 그러나 만약 우리가 x가 모두 indepent하다고 가정한다면 간단하게 다음과 같은 식으로 나타낼 수 있다.</p>


<p>$$ p(C) p(x_1, \ldots, x_n | C) = p(C) p(x_1|C) p(x_2|C) \ldots = p(C) \prod p(x_i|C)$$</p>


<p>따라서 normalize term을 \(Z\)로 표현한다면, 우리가 구하고자 하는 최종 posterior는 \(p(C|x) = \frac{1}{Z} p(C)\prod p(x_i|C)\)로 나타낼 수 있게 된다.</p>


<h5>KNN</h5>


<p><a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K-Nearest Neighbors algorithm (KNN)</a>은 구현하기 어렵지 않으면서도 비교적 나쁘지 않은 성능을 내는 Classification Algorithm 중 하나이다. KNN은 가까운 데이터는 같은 label일 가능성이 크다고 가정하고 새로운 데이터가 들어오면, 그 데이터와 가장 가까운 k개의 데이터를 training set에서 뽑는다. 뽑은 k개의 데이터들의 label을 관측하고 그 중 가장 많은 label을 새로운 데이터의 label로 assign하는 알고리즘이다 (이런 방식을 majority voting이라고 한다). 이때 &#8216;가까움&#8217;은 Euclidean distance로 측정해도 되고, 다른 metric이나 measure를 사용해도 된다. 이때 distance 혹은 similarity를 측정하기 위해서 반드시 metric을 사용해야하는 것은 아니다. 즉, metric의 세 가지 성질을 만족하지 않는 measure일지라도 두 데이터가 얼마나 &#8216;비슷하냐&#8217;를 measure할 수 있는 measure라면 KNN에 적용할 수 있다. 아래 그림은 KNN이 어떻게 동작하는지 알 수 있는 간단한 예시이다. 아래 그림을 보면 k를 3으로 골랐을 때 초록색 데이터의 label은 빨강이 되고, k를 5로 골랐을 때는 파란색이 됨을 알 수 있다.</p>


<p><img src="http://SanghyukChun.github.io/images/post/64-2.png" width="500"></p>

<p>KNN은 구현하기에도 매우 간단하고 (새로 들어온 점과 나머지 점들간의 distance를 측정한 후 sorting하기만 하면 된다) 성능도 보통 크게 나쁘지 않은 값을 보이기 때문에 간단하게 개발할 필요가 있는 경우에 많이 사용하게 된다. 사실 대부분의 머신러닝 툴박스들은 KNN의 다양한 variation까지 built-in function으로 지원한다. Matlab의 knnclassify가 대표적인 예. 또한 KNN은 <a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Properties">유용한 성질들</a>이 많이 있다. 예를 들어 만약 데이터의 개수가 거의 무한하게 있다면, KNN classifier의 error는 bayes error rate의 두 배로 bound가 된다는 특성이 있다. 즉, 데이터가 엄청나게 많다면, KNN은 상당히 좋은 error bound를 가지게 된다는 것이다. 즉, 단순한 휴리스틱 알고리즘이 아니라 엄밀하게 수학적으로 우수한 알고리즘임을 증명할 수 있는 알고리즘이라는 뜻이다. 또한 distance를 마음대로 바꿀 수 있기 때문에 KNN은 변형하기에도 간단한 편이므로 데이터에 대한 가정을 모델에 반영하여 변형하기에 간편하다는 장점이 있다.</p>


<h5>정리</h5>


<p>가장 간단하게 적용할 수 있는 세 가지 classification algorithm에 대해 훑어보았다. 개인적으로 KNN은 정말 직관적일뿐 아니라 잘 동작하는 알고리즘이기 때문에, 개인적으로 어떤 문제를 해결해야할 때 가장 먼저 이 데이터가 어느 정도 잘 분류되는지 테스트하는 용도로 애용한다. 중요한 점은, 각각의 classification algorithm이 풀려고 하는 &#8216;문제&#8217; 혹은 model은 서로 다른 가정을 가지고 있으며, 그 가정에 따라 문제를 푸는 방법이 아주 많이 바뀐다는 것이다. 즉, 어떤 새로운 classification algorithm을 만들어야 할 때는 (이는 classification 뿐 아니라 모든 알고리즘을 만들어야 할 때도 마찬가지인데) 먼저 어떤 문제를 풀어야하는지 문제를 정의해야하며, 문제를 정의하기 위해 어떤 모델을 가정해야한다는 것이다. 예를 들어 Naïve Bayes는 데이터의 각각의 feature들이 서로 i.i.d하다는 가정을 하고 있고, KNN은 &#8216;가까운&#8217; 데이터와 내가 같은 label을 가지고 있을 확률이 높다는 가정을 하고 있다. 이렇듯 Machine learning algorithm을 개발하는 일에서 가장 중요한 것은 좋은 문제를 먼저 정의하는 것에서부터 시작하는 것이 아닐까 생각한다.</p>




<h5>변경 이력</h5>


<ul>
<li>2015년 3월 25일: 글 등록</li>
</ul>


<hr>


<p><a href="http://SanghyukChun.github.io/blog/categories/machine-learning-study/">Machine Learning 스터디</a>의 다른 글들</p>


<ul>
<li><a href="http://SanghyukChun.github.io/57">Machine Learning이란?</a></li>
<li><a href="http://SanghyukChun.github.io/58">Probability Theory</a></li>
<li><a href="http://SanghyukChun.github.io/59">Overfitting</a></li>
<li><a href="http://SanghyukChun.github.io/60">Algorithm</a></li>
<li><a href="http://SanghyukChun.github.io/61">Decision Theory</a></li>
<li><a href="http://SanghyukChun.github.io/62">Information Theory</a></li>
<li><a href="http://SanghyukChun.github.io/63">Convex Optimzation</a></li>
<li><a href="http://SanghyukChun.github.io/64">Classification Introduction (Decision Tree, Naïve Bayes, KNN)</a></li>
<li>Regression and Logistic Regression</li>
<li>PAC Learning &amp; Statistical Learning Theory</li>
<li>Support Vector Machine</li>
<li>Ensemble Learning (Random Forest, Ada Boost)</li>
<li>Graphical Model</li>
<li><a href="http://SanghyukChun.github.io/69">Clustering (K-means, Gaussian Mixture Model)</a></li>
<li><a href="http://SanghyukChun.github.io/70">EM algorithm</a></li>
<li>Hidden Markov Model</li>
<li><a href="http://SanghyukChun.github.io/72">Dimensionality Reduction (LDA, PCA)</a></li>
<li>Recommendation System (Matrix Completion, Collaborative Filtering)</li>
<li><a href="http://SanghyukChun.github.io/74">Neural Network Introduction</a></li>
<li><a href="http://SanghyukChun.github.io/75">Deep Learning</a></li>
<li><a href="http://SanghyukChun.github.io/76">Reinforcement Learning</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[블룸버그 폰 인터뷰 후기]]></title>
    <link href="http://SanghyukChun.github.io/86/"/>
    <updated>2015-03-14T03:57:00+09:00</updated>
    <id>http://SanghyukChun.github.io/86</id>
		<content type="html"><![CDATA[<p>운이 좋게도, 교수님의 추천을 받아 블룸버그 software internship을 지원하게 되었다. 지원은 작년 12월에 하였고, 인터뷰는 1월 말부터 시간을 계속 조율하다가 오늘에서야 phone interview를 보게 되었다. 블룸버그가 뉴욕에 있다보니 현지 시간으로 2시에 면접인데 나는 새벽 3시.. 그나마 섬머타임이 실행되서 3시였지 안그랬으면 4시였다. 늦은 시간이지만 잠깐 시간을 내서 이 여운이 사라지기 전에 기록을 남겨볼까 한다.</p>


<p>먼저 phone interview는 생각보다도 더 어렵다. 일단 전화라는게 상대방의 말을 명료하게 전달해주지 못하는게 문제가 된다. 안그래도 긴장해서 영어가 잘 안들리는 상황에서 전화 연결 상황까지 안좋으니 더더더 긴장할 수 밖에 없었다. 하지만 이런 상황에도 못알아들었으면 당당하게 다시 말해달라고 하면 인터뷰어가 친절하게 대답해주니까 너무 겁먹을 필요는 없더라. 그리고 내가 phone interview에서 코딩을 위해 사용한 <a href="https://www.hackerrank.com/">HackerRank</a>라는 녀석이 구글 독스처럼 서로 글씨를 적으면 바로바로 상대방이 볼 수 있는 방식이라 말을 하면서 동시에 타이핑을 하는 것으로 충분히 내 부족한 영어를 메꿀 수도 있었다.</p>


<p>맨 처음 전화를 받고나서는 내 resume를 기반으로 몇 가지 질문들이 들어왔다. 너 이런 연구했는데 이거에 대해서 간단하게 설명할 수 있니? 여기에서 한 work은 어떤 work이니? 이 회사에서 일할 때 어떤 일들을 했니? 등의 질문들이었는데 아무래도 내 영어가 길지 못해 충분히 설명하지 못한게 아쉬운 요소였다. 시간을 재본건 아닌데 대충 10분 정도 레쥬메 스캔을 한 것 같다.</p>


<p>레쥬메 스캔이 끝나자마자 바로 코딩 문제로 넘어갔다. 여러 언어를 사용할 수 있는 것 같던데, 나는 Python을 사용해서 코딩을 했다. 내가 가장 멋진 코드를 쓸 수 있는건 (당연히 MATLAB을 제외하면) Ruby이지만, 내가 Ruby를 손에 안잡은지 너무 오래되었기 때문에 그나마 최근에도 가끔 사용하는 Python을 쓰기로 했다. C나 JAVA보다는 Python이 내 장점을 어필하기에 그나마 조금 나을 것 같아서. 그리고 사실 C랑 자바도 까먹었다. 질문은 두 가지 였는데, 하나는 여러 개의 list를 intersection하는 function을 짜라는 문제였고, 또 하나는 캐시를 구현하는거였다. 첫 번째 문제를 듣자마자 그간 코딩을 게을리한 것을 후회하게 되더라. 분명 어렵지 않은 문제인데 갑자기 코딩을 하려니 좋은 방법이 잘 떠오르지 않더라. 구글링을 하면 간단하게 해결할 수 있는 문제지만 그럴 수 있는 상황은 아니니까. 두 번째 문제는 듣자마자 OS에서 배웠던 내용이라 기뻐했는데 막상 어떤 데이터스트럭쳐를 사용해서 어떻게 빠르게 할 수 있는지 설명하려니까 그냥 딕셔너리를 써서 sorting한다는 대답 밖에 할 수 없었다.</p>


<p>첫 번째 문제를 정확하게 기술해보면 Input list들은 [ [1,2,3,&#8230;], [2,3,4,&#8230;], [5,6,7,&#8230;], &#8230; ] 처럼 생긴 list of list로 들어오고, output은 그 list들의 intersection을 구하는 문제였다. 내가 제안한 방법은 functional language 처럼 문제를 푸는 방법이었다. 먼저 두 개의 list를 비교하는 function을 만들고, 그것을 reduce했다. 대략 아래와 같은 느낌</p>


<figure class='code'><div class="highlight"><table><tr> <td class='code'><pre><code class=''><span class='line'>f = lambda x,y : [a for a in x if a in y]
</span><span class='line'>print reduce(f,L)</span></code></pre></td></tr></table></div></figure>


<p>코드 퀄리티는 아주 만족한다. 처음에 문제를 듣자마자 그냥 for loop으로 일단 돌아가게만 다 풀어버릴까 고민도 했었는데 그래도 그것보다는 이게 훨씬 아름답고 functional한 철학에도 맞고 여러모로 내가 추구하는 이상적인 코드에 가깝다. 문제는 여기까지 가는데에 시간이 너무 오래걸렸다는 것. 처음에 열심히 헤메느라 점수 다 까먹었을 것 같다. List comprehension을 사용하면 Lambda를 금방 정의할 수 있는데 그 생각을 못해서 for loop으로 naive한 것을 먼저 만들고 그걸 lambda로 넣으려고 하고 막 그랬는데.. 암튼 좀 헤메다가 위 처럼 문제를 해결했다. 그런데 저 List comprehension도 내가 naive way라면서 일단 element wise로 다 비교해보자고 하면서 넣은거라 interviewer가 내가 코딩을 끝내자마자 바로 이 방법을 더 좋게 만들 수 없는지 물어보더라. Sorting이 되어있냐고 물어보고, 되어있지 않다고 하길래 일단 각각을 sorting했다고 가정하고 filter를 사용해서 개선할 수 있다고 하고 내가 코드를 적으려고 했는데 시간이 부족하다면서 (이미 여기에서부터 15분 남음) 스킵하고 넘어갔다. 내가 적고 싶었던 솔루션은 아래와 같았다. 진짜 잘 동작하는지모르고, complexity가 진짜 더 낮은지 생각해봐야함. 근데 아마 O(n)이 O(log n)이 되어서 더 빠를거임.</p>


<figure class='code'><div class="highlight"><table><tr> <td class='code'><pre><code class=''><span class='line'>for l in L:
</span><span class='line'>  l = sort(l)
</span><span class='line'>f = lambda x,y : [a for a in x if a in filter(lambda b : b &gt;= a, y)]
</span><span class='line'>print reduce(f,L)</span></code></pre></td></tr></table></div></figure>


<p>사실 for loop쓴 sorting도 한 줄에 쓸 수 있을 것 같지만 귀찮으니까 넘어가야지. 으 딱 15초만 더 줬으면 바로 타이핑해서 보여줬을텐데. 시간이 많이 부족하다그래서 그냥 넘어갔다.</p>


<p>두 번째 문제는 input이 어떻게 생겼는지 물어보다가 대답이 내가 원하는 대답이 아니라 그냥 내가 stream을 새로 정의했다. S = [1,2,3,4,1,2,1,4,2,1,&#8230;] 같은 list로 들어온다고 가정하고, 내가 할 일은 이 리스트를 앞에서부터 읽으면서 캐쉬에 넣다가, 캐쉬 메모리가 모자라면 가장 오래 전에 마지막으로 사용된 element를 drop하는 algorithm을 짜는거였다. 이런 초 쉬운걸 기억이 안나서&#8230; 일단 내 대답은 dictionary를 만드는거였다. 참고로 이 문제는 시간이 없어서 말로 때움. 스트림별로 key를 가지는 dictionary를 만들고 value는 키에 해당하는 스트림이 들어왔을 때의 시간 값을 넣는다. 그리고 메모리가 모자라면 dictionary를 보고 가장 오래된 시간을 쓰는 녀석을 지워낸다.. 가 내 솔루션이었는데, 가장 오래된 녀석을 찾는게 log라서 그걸 더 빠르게 할 수 없냐는 질문이 들어왔음. (구체적으로 log라고 한건 아니고, 그때그때 찾는게 비싸니까 더 빠르게 할 수 없냐는 질문) 내가 기억하는 O(1)짜리 data structure가 큐랑 스택 밖에 없어서 그걸 사용해서 막 삽질을 하다가 결국 시간도 모자라고해서 거기에서 멈췄다. 솔직히 이건 시간 더 줬어도 내가 명석하게 해결하지 못했을 듯 ㅠㅠ 이런거 안한지 너무 오래됐다..</p>


<p>마지막에 다 끝나고 질문있냐 물어봐서 블룸버그에서 어떤 언어를 쓰냐 물어봤더니 팀마다 다르단다. 본인은 팀에서 C++랑 Javascript를 사용한다고. 파이썬이나 루비를 사용하는 팀도 있다고 한다. 사실 내가 지금 software internship으로 들어가게 되면 도대체 어떤 position으로 들어가게 되는건지 알 수가 없어서 (pure developer인지 researcher인지 어느 정도 level로 코딩하는지..) 내가 어떤 언어를 쓰게 될지는 모르겠지만, 최소한 내가 가서 고를 수 있는 일말의 여지가 있다면 내가 최대한 잘 할 수 있는 팀으로 배정되면 된다는 결론을 내릴 수 있었다. 뭐 될지는 모르겠지만. 결과는 2~3주 뒤에 나온다고 했으니 또 막 메일 왔다갔다하다보면 한 달 예상해본다. Optimal case라면 2주 뒤인 3월 말에 ICML 리뷰를 보고 결과도 대충 알 수 있을거고 블룸버그 결과도 같이 나오는 셈이다. 이번에는 좀 좋은 결과가 있었으면 좋겠는데..</p>


<p>사실 크게 별 생각안하고 코딩하면서 떨지만 않게 적당히 인터넷에서 <a href="https://sites.google.com/site/steveyegge2/five-essential-phone-screen-questions">코딩 인터뷰 관련 글</a>이나 <a href="http://www.bogotobogo.com/python/python_interview_questions.php">문제들</a> 푸는 정도로 몸풀고 인터뷰를 봤는데 굉장히 좋은 경험이 되었다. 영어로 official software engineer job recuiting phone interview라니, 정말 중요한 경험인데 굉장히 어린 나이에 운좋게 경험할 수 있었다. 무엇보다 학위를 마치고 장래에 미국에서 job을 구할 생각을 하고 있는 상황에서 이런 좋은 회사와 phone interview 과정을 겪어보는 것만으로도 진짜 좋은 경험이 되었다. 가서 일을 해보면 훨씬 더 좋은 경험이 될 것 같지만, 그건 내가 결정하는게 아니니 일단은 두고 봐야겠지.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2015년 02월 27일 새벽 5시 반]]></title>
    <link href="http://SanghyukChun.github.io/85/"/>
    <updated>2015-02-27T05:34:00+09:00</updated>
    <id>http://SanghyukChun.github.io/85</id>
		<content type="html"><![CDATA[<p>2월 6일. 드디어 짧게 보면 반 년 넘게, 길게 보면 1년이 넘게 진행해온 연구로 논문을 제출했다. 제출한 학회는 ICML. 처음으로 쓰는 논문이고, 처음으로 내가 1저자가 되어 제출하는 논문인데 처음부터 너무 좋은 학회에 제출하게 되었다. 여러모로 운이 좋았다. Author feedback period는 3월 말이나 되어야하고, 최종 decision notification은 4월 25일이다. 만약 accept이 된다면 7월에 프랑스 Lille에서 내 연구를 수 많은 사람들 앞에서 발표하게 된다. 아직 결과가 나오려면 2달 가까이 남았기 때문에 연구에 대한 압박감은 많이 사라졌다. 다만, 추가적으로 더 해보고 싶은 것들이 있어 추가 실험이나 앞으로 시도해볼 분야의 review paper를 읽어야 하는데, 쉽사리 의욕이 나지 않는다.</p>


<p>Paper를 제출하고, 약 2주 반 정도의 휴가를 다녀왔다. 원래는 블로그에 ML study 포스트도 하루에 한 개씩 올리고, 책도 많이 읽고 Machine Learning 공부를 처음부터 천천히 할 수 있는 가장 좋은 시기라고 생각했었는데, 중간에 설 연휴도 있었고 잠깐 외국도 다녀오고 하니까 그럴 수 있는 시간이 하나도 없었다. 아마 머신러닝 공부, 그리고 관련 포스팅은 전부 학기 중에 수업도 들으면서 동시에 진행하게 될 것 같다.</p>


<p>쉬는 것에도 관성이 있는 것 같다. 어제 다시 학교로 돌아왔는데 아직까지 하루 종일 일이 손에 잡히지 않는다.</p>


<p>1월부터 2월초까지는 논문을 쓰는 것에 내 모든 역량을 쏟아부었다. 그래서 그런지 나에게는 이번 겨울이 너무나 짧게 느껴진다. 논문 쓰느라 한참 바쁘게 지내고 잠깐 외국다녀오니 겨울이 끝나있었다. 마찬가지로 2015년도 너무 급작스럽게 찾아왔다. 벌써 3월을 눈 앞에 두고있다니. 남들은 연말에 하거나 연초에 하는 2015년 계획을 오늘에서야 세우게 되었다. 사실 올해 목표 예년과 크게 다르지 않다. 올해 목표는 딱 두 가지인데, 하나는 체중 감량이고 또 하나는 제2외국어 공부이다. 체중감량은 워낙 오래전부터 내세우던 목표이고 제대로 성공한 적이 한 번도 없기 때문에 이번에는 정말 제대로 해봐야겠다. 혼자 계속 운동하다가 정체기가 오는 것 같으면 바로 PT를 받아볼 생각이다. 바로 PT를 받기에는 학교 헬스장이 워낙 시설이 좋아서 돈이 조금 아깝다. 그리고 내가 운동이나 식이요법을 &#8216;몰라서&#8217; 못하는게 아니라 귀찮거나 의지가 부족해서 안하는 것이기 때문에 의지가 부족해졌다고 느낄 때 쯤에 PT를 받아야겠다. 제2외국어공부는 여러가지를 생각해보았으나 역시 내가 빠른 시간에 금방 배울 수 있는 언어는 일본어이다. 오늘 저녁 먹기 전에 학원에 가서 상담받고 등록해야겠다.</p>


<p>한 번 연구를 해보니까 자신감이 생기기도 하지만 반대로 내가 이런 작업을 또 해낼 수 있을까 두렵기도 하다. 다행히 지금 나는 해야 할 일이 명확하니 내가 해야 할 일에 최선을 다해야겠다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[블로그에 Disqus 설치]]></title>
    <link href="http://SanghyukChun.github.io/82/"/>
    <updated>2014-12-13T04:58:00+09:00</updated>
    <id>http://SanghyukChun.github.io/82</id>
		<content type="html"><![CDATA[<p>예전부터 블로그 댓글 기능을 제대로 만들어야겠다는 생각을 늘 하고 있었는데, 갑자기 삘이 꽂혀서 disqus를 설치했다. 아예 생각난 김에 트윗이나 라이크 버튼도 기능 테스트를 더 해볼까 싶었는데 그건 그냥 귀찮아서 스킵하기로 했다.</p>


<p>사용 방법은 심플하다. <a href="http://www.disqus.com">http://www.disqus.com</a> 에 접속해서 아이디 만들고, 새로 disqus 하나 만든 다음에, 옥토프레스 _config.yml에 disqus short name만 넣어주면 된다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[맥 요세미티 업데이트 이후 Homebrew 문제점 troubleshooting]]></title>
    <link href="http://SanghyukChun.github.io/81/"/>
    <updated>2014-11-30T03:24:00+09:00</updated>
    <id>http://SanghyukChun.github.io/81</id>
		<content type="html"><![CDATA[<p>나는 맥 OS 장비가 두 개 있다. 하나는 연구실에서 사용하는 아이맥이고 또 하나는 연구실 밖에서 사용하는 맥북프로이다. 이번 요세미티 업데이트는 프리뷰 때 부터 기대를 많이 했기에 나오자마자 바로 두 머신 모두 요세미티 설치를 했다. 그리고 그게 내가 이 글을 쓰게 된 시발점이 되었다.</p>


<p>Homebrew는 맥에서 가장 많이 사용하는 패키지 관리 프로그램 중 하나로, 간단히 생각하면 ubuntu의 apt-get과 같은 역할을 (더 fancy하게!) 해준다. 그런데 문제는 이 녀석이 ruby base로 돌아가고, ruby path가 하드코딩되어있는데, 요세미티는 system ruby의 버전을 강제로 업그레이드시켜버리기 때문에 brew의 모든 명령이 깨진다는 것. 참고로 이건 예전 버전의 homebrew에서나 그렇고, 새로 나온 버전은 아무 문제가 없다.</p>


<p>가장 간단한 해결책은 임시로 brew.rb 파일에 있는 path를 current ruby로 바꿔준 다음 brew를 최신 버전으로 다시 내려받는 것이다. 하지만 늘 인생은 쉽지 않지. 쉬운 길을 눈 앞에 두고 돌아가기 마련인데, 두 머신 모두 (약간의 시간 차이를 두고 한 일이지만) brew를 삭제하고, 다시 설치한다음 rvm으로 ruby 버전을 다시 다운로드 받는&#8230; 삽질을 했다.</p>


<p>그런데 이번에 rvm으로 루비 버전을 먼저 내려받다가, brew가 깨져있는 상황에서 실수로 지금 이미 있는 루비조차 날려버리는 최악의 실수를 해버렸다. 다시 말해 ruby 를 입력해도 아무 반응이 없고, rvm list는 비어있는 최악의 상황. 먼저 rvm install 2.0.0 을 실행했는데, 컴파일이 거의 한 시간 정도의 긴 시간이 지나도 끝나지를 않아 찾아보니 xcode 버전이 낮으면 그럴 수 있다더라. 다시 내 xcode를 보니 5버전.. 최신은 6버전이다. 바로 xcode부터 재설치를 했다. 근데 xcode가 보통 무거워야지.. 설치하는데 시간이 꽤 걸렸다.</p>


<p>xcode 재설치를 끝내고 homebrew를 설치하려고 보니 ruby 명령어를 실행해야하는터라, 강제로 system ruby path에 들어가 system ruby로 실행을 시켰다. brew를 제대로 지우지 않은 상황이라면 시키는대로 하면 된다. 설치 후에는 brew doctor 한 번 돌려줘야한다. 꼬여있는 dependency를 정리해야하기 때문.</p>


<p>이제 rvm install 2.0.0, rvm install 1.9.3, rvm install 2.1.5 모두 잘 실행된다. (라고 적었지만 사실 1.9.3은 gcc48설치하는게 너무 오래 걸려서 아직 끝은 안난게 함정&#8230;)</p>


<p>Gitlab도 그렇고, 지금 쓰고 있는 Octopress도 그렇고, 많은 웹에서 쓰는 프레임워크 혹은 어플리케이션들이 rails 기반, 루비 기반인 경우가 많아 이런 문제가 왕왕 생기고는 하는데, 당황하지 않고 천천히 찾아보면 해결하는게 크게 어렵지는 않다. 루비는 sudo user로 설치하는게 아니다보니까 더 쉬운 듯. 가장 중요한건 인내심을 가지는 것. 너무 이상할 정도로 오래걸리는건 생각해볼 필요가 있지만, 대부분의 경우 생각보다 시간이 훨씬 오래 걸린다.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gitlab을 깔자]]></title>
    <link href="http://SanghyukChun.github.io/80/"/>
    <updated>2014-11-20T03:05:00+09:00</updated>
    <id>http://SanghyukChun.github.io/80</id>
		<content type="html"><![CDATA[<p>버전관리의 중요성은 몇 번을 강조해도 부족하지 않다. 버전관리를 하기 위한 여러 도구들이 존재하지만, 난 분산 버전 관리 (DVCS) 를 좋아한다. Mercurial 로 처음 버전관리를 공부했고, 지금은 주로 Github에 소스를 올리기 때문에 git 을 많이 쓰고 있다. 콘솔에서는 큰 차이가 있지만, 정작 둘 다 <a href="http://www.sourcetreeapp.com/">SourceTree</a>로만 관리하기 때문에.. 예전에는 맥버전만 있었지만, 이제는 윈도우 버전도 생겨서 다른 사람들도 용이하게 쓸 수 있다. 이런 이유로 최근에는 거의 모든 레포지토리를 git 레포지토리로 만들게 되는데, 연구실에서 내가 쓰고 있는 논문이나 프로젝트용 코드 같은 경우는 공개된 레포지토리로 올려도 곤란하고, 연구실 단위로 관리를 해야할 필요를 느껴서 <a href="https://about.gitlab.com/">GitLab</a>이라는 놈을 깔아보기로 헀다.</p>


<p>Gitlab을 한 마디로 요약하자면 &#8216;개인용 github&#8217; 라고 할 수 있다. 코드를 훑어보니 rails 기반에 nginx를 서버로 사용하고 있어서 내가 친숙한 환경이기도 해서 좋더라. 늘 깔아야지 깔아야지했는데 이게 생각보다 설치가 머리가 아파서.. 나중에 시간이 넉넉해지면 설치하려 했으나 어쩌다보니 갑작스럽게 설치를 하게 되었다.</p>


<p>내 설치환경은 Ubuntu 14.04, <a href="https://about.gitlab.com/downloads/">다운로드 페이지</a>에서 ubuntu 14.04를 선택하는 아래와 같은 명령어를 실행하면 알아서 옴니버스 버전을 깔아준다고 한다.</p>


<figure class='code'><div class="highlight"><table><tr> <td class='code'><pre><code class=''><span class='line'>wget https://downloads-packages.s3.amazonaws.com/ubuntu-14.04/gitlab_7.4.3-omnibus.5.1.0.ci-1_amd64.deb
</span><span class='line'>sudo apt-get install openssh-server
</span><span class='line'>sudo apt-get install postfix # Select 'Internet Site', using sendmail instead also works, exim has problems
</span><span class='line'>sudo dpkg -i gitlab_7.4.3-omnibus.5.1.0.ci-1_amd64.deb</span></code></pre></td></tr></table></div></figure>


<p>난 openssh-server, postfix는 깔려있으니 생략했다.</p>


<p>다음으로는 <code>sudo vim /etc/gitlab/gitlab.rb</code> 를 실행해 <code>gitlab.rb</code> 를 수정해야한다. 다시 한 번 말하지만, gitlab은 rails로 돌아가기 떄문에 rails setting 을 해줘야한다. 원래 레일즈 프로젝트의 설정을 바꾸기 위해서는 yml 파일이나 다른 rb 파일들을 직접 수정해주어야하는데, gitlab 옴니버스 버전에서는 친절하게 이 루비 파일 하나만 바꾸면, 알아서 yml 등을 generate해준다. 아 편하고 좋다! 라고 생각했지만 이것이 그 모든 재앙의 시작이었다..</p>


<p>편하게 해주려고 만든 ruby setting 파일이 왜 문제가 되었느냐, 사실 이 대부분은 문제가 생기지 않는다. 예를 들어서 <code>www.example.com</code> 이라는 도메인을 가지고 있고, gitlab 의 접속 경로를 <code>gitlab.example.com</code> 으로 사용한다면 옴니버스 버전을 바로 사용하면 된다. 하지만 내가 사용하는 서버는 학교 도메인에서 서브도메인을 받아서 사용하기 때문에 <code>sanghyuk.kaist.ac.kr</code> 이런 식의 도메인을 가지고 있다. 따라서 위와 같은 경로를 취하게 되면 <code>gitlab.sanghyuk.kaist.ac.kr</code> 이라는 기괴한 경로가 생기게 되고, 당연하지만 이런 경로는 허용되지 않는다. 따라서 <code>sanghyuk.kaist.ac.kr/gitlab</code> 같은 relative domain을 사용해야한다. nginx에서 이런 서브 도메인을 루트로 삼는 것은 규칙에 위반되지만 사용하는 것이 가능하긴하다. rails에서도 root url을 relative url로 바꾸고 하면 돌릴 수 있지만.. 이건 내가 rails파일을 직접 바꿀 때 얘기였다.</p>


<p>옴니버스 버전에서 이런저런 삽질을 하다가 찾아낸 이슈.. <a href="https://gitlab.com/gitlab-org/omnibus-gitlab/issues/238">옴니버전에서는 Relative URL root를 지원할 계획이 없다</a> 아&#8230; <code>gitlab.rb</code> 에서 삽질을 하고 있었는데 다 쓸데 없는 짓에 불과했던 것이다.</p>


<p>그래서 relative url은 포기하고 다른 쪽으로 알아보니 포트를 바꿔서 접속을 하는 방법이 있더라. 이건 그나마 훨씬 할 만 헀다. <code>/etc/gitlab/gitlab.rb</code> 에서 다음과 같이 설정해준다.</p>


<figure class='code'><div class="highlight"><table><tr> <td class='code'><pre><code class=''><span class='line'>external_url = 'http://sanghyuk.kaist.ac.kr:1234'</span></code></pre></td></tr></table></div></figure>


<p>아 참고로, 내가 깐 버전은 = 을 안쓰면 에러가 나서 내가 = 을 따로 넣어줬다. 옴니버스 버전별로 다른 모양</p>


<p>이렇게 하고 <code>sudo gitlab-ctl reconfigure</code> 을 실행시켜서 yml 등을 자동으로 generate시키기만 하면 문제 해결! &#8230; 이 아니었다. 아예 접속 자체가 되지를 않아서 이제 여기에서 다시 삽질을 시작했는데, 먼저 netstat으로 포트는 열려있나 봤다. 안타깝게도 아예 포트가 열려있지도 않았다. 돌아버리겠는건 <code>gitlab-ctl status</code> 에서는 잘 실행되는 것으로 나오는 것.</p>


<p>이제 또 한참 삽질을 하다가, 아예 nginx 조차 돌아가는 것 같지 않아서 nginx 세팅을 수동으로 뜯어고치기로 결정했다. 이게 상당히 위험한 짓인데, 나중에 내가 생각없이 또 reconfigure를 때리면 내가 고친 파일들이 새로운 파일들로 덮어씌일 것이기 때문이다. 때문에 이렇게 설정을 시작한다면 reconfigure 대신 <code>gitlab-ctl restart</code> 로 configure 파일을 다시 생성시키지 말고 레일즈랑 nginx 등만 내렸다가 올려야한다. 이제 세팅을 바꿔보자.</p>


<p>가장 먼저 nginx 설정을 찾아봤다. <code>/var/opt/gitlab/nginx/conf/gitlab-http.conf</code> 에 설정 파일이 있는데, listen과 server name이 엉망으로 되어있더라. 당연히 nginx가 설정이 잘못되었으니 서버에 접속도 못하고 포트도 안열려 있던 것. 이 둘을 제대로 바꿔주고 restart를 했다.</p>


<p>드디어 &#8216;페이지를 찾을 수 없습니다&#8217; 창말고 다른 창을 볼 수 있었다. 그러나 여기에서 또 502 에러가 발생했는데, 아마도 내부 rails가 제대로 올라오지 않은 모양인가보다. 그래서 혹시 레일즈 세팅도 이상한가 싶어서 yml 파일을 찾아봤다.</p>


<p><code>/var/opt/gitlab/gitlab-rails/etc/gitlab.yml</code> 을 열어봤더니 여기도 host랑 port가 엉망이었다. 이 부분을 바꿔주고 나서 다시 <code>sudo gitlab-ctl restart</code></p>


<p>아 드디어 잘 실행된다. 기존에 쓰던 다른 레포지토리를 추가해주기 위해서 <a href="http://git-scm.com/book/ko/v1/Git%EC%9D%98-%EA%B8%B0%EC%B4%88-%EB%A6%AC%EB%AA%A8%ED%8A%B8-%EC%A0%80%EC%9E%A5%EC%86%8C">remote</a>를 해주려고 보니 ssh key를 생성해서 넣어줘야 하더라. 이건 <a href="http://git-scm.com/book/ko/v1/Git-%EC%84%9C%EB%B2%84-SSH-%EA%B3%B5%EA%B0%9C%ED%82%A4-%EB%A7%8C%EB%93%A4%EA%B8%B0">이 글</a>을 보면 된다. 이건 예전에 다 등록해뒀던거라서 금방금방했다. 조금 가지고 놀아보니 아직까지는 매우 만족스럽다.</p>


<p>이렇게 일단 주먹구구식으로 깃랩을 돌리는 것에는 성공했지만, 아직 reconfigure를 함부로 하면 안된다. 내가 바꾼 <code>/var/opt/gitlab/gitlab-rails/etc/gitlab.yml</code>, <code>/var/opt/gitlab/nginx/conf/gitlab-http.conf</code> 는 <code>gitlab-ctl reconfigure</code> 를 할 때 자동으로 generate 되는 파일이다. 따라서 내가 바꾼 설정이 저장이 되지 않기 때문에 함부로 reconfigure를 했다가는&#8230; 나중에 시간이 나면 이 부분을 gitlab.rb 만 바꿔서 수정할 수 있는지 알아보자.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[새로운 front framework Webplate에 대한 소견]]></title>
    <link href="http://SanghyukChun.github.io/79/"/>
    <updated>2014-10-10T02:30:00+09:00</updated>
    <id>http://SanghyukChun.github.io/79</id>
		<content type="html"><![CDATA[<p>얼마전 <a href="http://getwebplate.com/">Webplate</a>라는 front-end framework를 접하게 되었는데 상당히 좋은 인상을 받아서 블로그에 소개 겸 사용 소감을 남겨보려 한다.</p>


<p>그 동안 내가 웹을 개발하면서 느꼈던 가장 불편한 점은, 디자이너가 없이는 내가 할 수 있는 것들이 상당히 제한된다는 것이었다. 그러던 와중에 <a href="http://getbootstrap.com/">twitter bootstrap</a>을 알게 되었고, 덕분에 상당히 많은 웹 페이지들을 만들 수 있었다. 이 블로그도 그렇고, 내 aboutMe 페이지도 그렇고 홈페이지도 그렇고 요즘 만들고 있는 연구실 홈페이지도 그렇고.</p>


<p>하지만 bootstrap을 사용하면서 불편한 점이 없는건 아니었다. Bootstrap은 매우 훌륭한 디자인 가이드를 제공하고 있기는 하지만, 결국 기본적인 웹 frame조차 내가 처음부터 html 코딩을 해야하는 점은 매우 귀찮은 일이 아닐 수 없었다. 지금은 이것에 어느 정도 익숙해지고 내가 내 나름의 template을 가지게 되었지만 그 이전에는 html부터 코딩을 하는 것이 여간 귀찮은 일이 아니었다. 특히 aboutMe만들 때 생각하면&#8230; 정말 끝없이 고치고 또 고쳤던 기억이 난다.</p>


<p>그러던 와중 webplate를 보게 되었는데, 처음 본 순간 정말 센세이션이었다. 마침 <a href="https://github.com/chrishumboldt/webplate">github</a>도 있길래 fork해서 한 10분 정도 둘러봤는데,</p>


<ol>
<li>Twitter Bootstrap과 비교를 하지 않을 수 없는데, 사실 Webplate에서 가능한건 Bootstrap에서도 다 가능할 뿐 아니라 자유도 역시 Bootstrap이 더 좋다. 하지만 Bootstrap에서는 내가 다 구현을 했어야했던 것들이 Webplate에서는 구현이 되어있다는 것이 좋은 듯. 
한 마디로 Bootstrap은 style에 대한 기본 base느낌이라면 Webplate는 진짜 front framework다</li>
<li>비록 내가 따로 설정을 해줘야하지만, example project에서 기본 글씨체를 Lato로 강제해서 참 좋다. Open Sans 애리얼 헬베티카 꺼졍&#8230;</li>
<li>다시 framework에 대한 얘기인데, pre태그 에서 코드 이쁘게 보여주는 기능을 class로 부르기만 하면 되게 구현이 되어있어서 짱짱 편하다. 내가 라이브러리 찾아서 부를 수도 있지만 귀찮으니까&#8230; </li>
<li>기본적으로 필요한 대부분의 라이브러리가 포함되어있다. response.js를 포함해서! 때문에 반응형으로 만들기도 편하고 호환에 대한 두려움 없이 작업을 하는게 가능하지만 ie에서 완전 호환이 되는지 잘 모르겠다. (아마 되겠지)</li>
<li>이 framework 자체가 vertical UI를 강제하고 있다. 최근 핫한 UI이기는 하지만 (stripe UI같은거) 다른 모양을 원하는 경우에는, 특히 상단 바가 아니라 좌측에 메뉴를 넣고 싶은 경우라면 이 framework을 쓰는게 아니라 따로 만들어야할 것 같다. (하지만 요즘 모든 웹은 다 기본이 vertical UI..)</li>
<li>Service, Product 등의 introduction page를 간단하게 만들 생각이라면 이보다 더 Cool한 web framework는 없어보인다. (전시회 소개나 festival page로도 좋을 것 같네) 그 이외에 어떤 곳에서 이 framework를 사용할 수 있을지 아직은 잘 모르겠다. Vertical UI를 쓸 수 있는 곳이라면 전부 쓸 수 있을 것 같기는 하다.</li>
</ol>


<p>한 마디로 간단하게 만들 수 있는 웹은 이 녀석으로 대략 커버가 가능하다는 점, 그리고 반응형에 대한 이슈를 내가 따로 고민할 필요가 없다는 점이 너무 마음에 들었다. <a href="http://getwebplate.com/documentation">Document</a>링크를 첨부할테니 관심있는 사람들은 한 번 읽어봤으면 좋겠다. 최근 이걸 사용해 따로 개발할 웹이 없어서 정작 내가 사용해볼 수가 없다는게 아쉽기는하다.</p>

]]></content>
  </entry>
  
</feed>
